---
title: "Práctica - Unidad 2"
bibliography: ../references.bib
nocite: |
    @Martin2021, @Downey2021, @Johnson2021, @Reich2020
pdf_file: https://github.com/estadisticaunr/estadistica-bayesiana/raw/pdf/practica/practica_02.pdf
format:
    pdf: # https://quarto.org/docs/reference/formats/pdf.html
        template: ../templates/template.tex
        template-partials:
            - ../templates/title.tex
        # https://pandoc.org/MANUAL.html#placement-of-the-bibliography
        suppress-bibliography: true
    html: default
year: "2023"
course: "Estadística Bayesiana"
practica: "Práctica 2"
logo: ../templates/logo.png
---

```{r}
#| echo: false
#| include: false
is_html <- knitr::is_html_output()
options("knitr.graphics.error" = FALSE)
source(here::here("scripts", "utils.R"))
caption_data <- list(
    "tomates" = list(author = "Markus Spiske", handle = "markusspiske", code = "vrbZVyX2k4I")
)
captions <- lapply(caption_data, function(x) do.call(make_unsplash_caption, x))
captions[["monty_hall"]] <- "Las 3 puertas del problema de Monty Hall"
```

::: {.content-visible when-format="html"}
[Descargar PDF](%7B%7B%3C%20meta%20pdf_file%20%3E%7D%7D)
:::

## Modelos de Distribuciones Conjugadas

Esta sección contiene ejercicios para trabajar con modelos basados en distribuciones
conjugadas. En general, los ejercicios requieren cálculos o derivaciones que se pueden
realizar a mano. Sin embargo, se promueve fuertemente el uso de la computadora y el 
lenguaje `R` para verificar los resultados, mostrar soluciones alternativas y ejercitar
el uso de una herramienta que será de suma utilidad a lo largo de todo el curso y de la 
vida profesional.

1.  **¿Quién domina el _posterior_?**

    Para cada una de las situaciones siguientes, se da una distribución _a priori_ Beta
    para el parámetro $\pi$ de un ensayo binomial. Para cada escenario, identificar cuál de
    estos se cumple: el _prior_ tiene mayor influencia en el _posterior_, los datos tienen
    más influencia en el _posteriori_, o la creencia _a priori_ y los datos influyen de manera
    similar en la creencia _a posteriori_

    i. _Prior_: $\pi \sim \text{Beta}(1,4)$, observaciones: $y=8$ éxitos en $n=10$ ensayos
    i. _Prior_: $\pi \sim \text{Beta}(20,3)$, observaciones: $y=0$ éxitos en $n=1$ ensayos
    i. _Prior_: $\pi \sim \text{Beta}(4,2)$, observaciones: $y=1$ éxitos en $n=3$ ensayos
    i. _Prior_: $\pi \sim \text{Beta}(3,10)$, observaciones: $y=10$ éxitos en $n=13$ ensayos
    i. _Prior_: $\pi \sim \text{Beta}(20,2)$, observaciones: $y=10$ éxitos en $n=200$ ensayos


1.  **Más o menos certeza**

    Sea $\theta$ la proporción de personas que prefieren los perros a los gatos. Suponga que
    se elige una distribución $\text{Beta}(7,2)$ para representar la creencia _a priori_

    i. De acuerdo al _prior_ ¿cuáles son valores razonables para $\theta$?
    i. Se observa en una encuesta que $y=19$ de $n=20$ personas prefieren perros, ¿cómo cambia
    eso el conocimiento acerca de $\theta$? Comenta en términos de la evolución de la credibilidad
    media y del grado de certidumbre acerca de $\theta$.
    i. Si, en lugar de eso, se determina que $y=1$ de $n=20$ personas prefieren perros, ¿cómo cambia
    ahora el grado de credibilidad de los diferentes valores de $\theta$?
    i. Si, en lugar de eso, se determina que $y=10$ de $n=20$ personas prefieren perros, ¿cómo cambia
    ahora el grado de credibilidad de los diferentes valores de $\theta$?
    
1.  **Pasito a pasito**

    Sea $\theta$ la probabilidad de éxito de un evento de interés. Sea $\text{Beta}(2,3)$ la distribución
    _a priori_ para $\theta$. Actualiza la distribución _a posteriori_ para $\theta$ secuencialmente:

    i. Primera observación: éxito
    i. Segunda observación: éxito
    i. Tercera observación: fracaso
    i. Cuarta observación: éxito
    
1.  **Pasitos tras pasitos**

    Sea $\theta$ la probabilidad de éxito de un evento de interés. Sea $\text{Beta}(2,3)$ la distribución
    _a priori_ para $\theta$. Actualiza la distribución _a posteriori_ para $\theta$ secuencialmente dados conjuntos
    de cinco observaciones:

    i. Primeras observaciones: tres éxitos
    i. Segundas observaciones: un éxito
    i. Terceras observaciones: un éxito
    i. Cuartas observaciónes: dos éxitos
    
1.  **Diferentes observaciones, diferentes _posteriors_**

    Una empresa que fabrica zapatillas está diseñando una publicidad para Instagram. 
    Tres empleados comparten que la creencia _a priori_ para $\pi$, 
    la probabilidad de que un cliente haga clic en el anuncio cuando lo ve, 
    puede expresarse con una distribución $\text{Beta}(4, 3)$.
    No obstante, los tres empleados realizan tres experimentos distintos y
    por ende tienen acceso a datos diferentes. El primer empleado prueba el anuncio en una persona, que
    no cliquea el anuncio. El segundo lo prueba en 10 personas, de las cuales 3 cliquean el anuncio. El último
    lo prueba en 100 personas, 20 de las cuales cliquean el anuncio.

    i. Describa el entendimiento _a priori_ que los empleados tienen sobre $\pi$
    i. Especifique la distribución _a posteriori_ de cada uno de los empleados
    i. Compare las distribuciones _a posteriori_ de cada empleado
    
1. **¿Galletitas o masitas?**

    La UNR reúne cada año a estudiantes provenientes de diferentes localidades. Cuántas cuadras constituyen
    una distancia "caminable" suele ser motivo de discusión, entre otros. Pero la verdadera grieta está entre
    la denominación _galletitas_ versus _masitas_. Un rosarino pone un _prior_ $\text{Beta}(20,2)$ a la proporción
    de personas que dicen _galletitas_, mientras que un oriundo de una localidad del interior dirá que la credibilidad
    _a priori_ es $\text{Beta}(2,8)$.
    
    i. Resuma ambas distribuciones _a priori_ y explique con sus palabras lo que implican
    i. Con la información de sus compañerxs de curso, actualice ambas distribuciones _a priori_. 
    ¿Es suficiente esa información para acercar ambas posturas?

1.  **Mi primera huerta**

    En un campamento de verano para infantes se realizaron actividades
    que promueven el contacto con la naturaleza. Una de las tareas
    consistió en germinar semillas de tomate. Josefina plantó 18
    semillas en su almaciguera. Al cabo de 5 días, 8 de ellas germinaron.
    Sea $\theta$ la probabilidad de que una semilla de tomate germine 
    y sea $\text{Beta}(1, 1)$ su distribución _a priori_.

    i. ¿Qué información implica el _prior_ sobre la probabilidad de germinación?
    i. Calcule la media y el desvío estándar _a posteriori_ de $\theta$ a mano.
    i. Verifique el cálculo utilizando `R`.
    i. Obtenga un intervalo de credibilidad del 95% para $\theta$.
         a. A mano
         a. Usando `R`

    ```{r}
    #| echo: false
    #| out-width: 70%
    #| fig-align: center
    #| fig-cap: !expr captions[["tomates"]]
    if (is_html) knitr::include_graphics(file.path("imgs", "tomates.jpg"))
    ```

1.  **¿Quién dijo que el fútbol siempre da revancha?**

    En la final del 2018 de la Copa del Mundo de la FIFA, Francia le ganó a Croacia por
    4 a 2. En función de este resultado,

    i. ¿Qué probabilidad hay de que Francia fuera un mejor equipo que Croacia?
    i. Si el mismo partido se jugara de nuevo (cosa que los franceses en aquella 
    oportunidad no pidieron), ¿cuál es la probabilidad de que Francia ganara de nuevo?
    <!-- http://allendowney.github.io/ThinkBayes2/chap08.html -->

1.  **Mirá si me va a pasar a mi...**

    Durante el desarrollo de las vacunas contra el COVID-19, un medio anunció para una 
    determinada vacuna una eficacia del 100%.

    > En la fase 3 de un ensayo en adolescentes de entre 12 y 15 años, la
    > vacuna BNT162b2 de Pfizer-BioNTech para el COVID-19 demostró una
    > eficacia del 100% y una respuesta robusta de anticuerpos. El ensayo
    > clínico involucró 2260 jóvenes estadounidenses. En el ensayo, 18 casos
    > de COVID-19 fueron observados en el grupo placebo ($n=1129$) y ninguno
    > en el grupo vacunado ($n=1131$)

    Es de esperar que, en un ensayo más grande, aparezca algún caso de COVID-19 en el 
    grupo que recibió el tratamiento. 
    ¿Cómo se estima la probabilidad de algo que aún no ocurrió?
    <!-- http://allendowney.github.io/ThinkBayes2/vaccine2.html -->

1.  **La regla del tres**

    Una estudiante de Licenciatura en Estadística está releyendo su
    tesina antes de entregarla. Si en 20 páginas encontrara 5 _typos_,
    sería _razonable_ estimar $\frac{5}{20} = \frac{1}{4}$ _typos_ por página.
    ¿Pero qué ocurre si en 20 páginas no encuentra ningún error?
    
    Verifcar que, partiendo de un _prior_ uniforme, $\frac{3}{N}$ es una estimación 
    razonable para $\tau$ (la tasa de _typos_ por página), siendo $N$ el número de páginas.
    Para ello, hallar la probabilidad de que $\tau > \frac{3}{N}$ para diferentes valores
    de $N$. 
    <!-- https://www.johndcook.com/blog/2010/03/30/statistical-rule-of-three/ -->

1.  **¿Tenés alguien para recomendar?**

    Una colega quiere comprar un producto por Internet. Tres vendedores ofrecen el mismo
    producto al mismo precio. Un vendedor tiene 100% de evaluaciones positivas, con 10 
    _reviews_. Otro tiene 96% de evaluaciones positivas, con 50 _reviews_. El último tiene
    90% de comentarios positivos, con 200 evaluaciones. 
    ¿Cuál de los tres vendedores le recomendarías?
    <!-- http://allendowney.github.io/ThinkBayes2/chap18html -->

1. **Bichos**

    Un biólogo quiere determinar la densidad de un insecto en su región. Su conocimiento _a priori_ del número
    promedio de insectos por unidad de área ($\text{m}^2$) se puede representar con una distribución Gamma de 
    media 0.50 y desvío estándar 0.25. En una investigación en 20 $\text{m}^2$ de área, se hallan 3, 2, 5, 1 y 2
    insectos en los primeros 5 $\text{m}^2$ y ninguno en la fracción de tierra restante.
    
    i. Halle la distribución _a posteriori_ del número medio de insectos por unidad de área
    i. Halle la distribución predictiva _a posteriori_ del número de insectos que se espera encontrar en una
    exploración de un área de 10 $\text{m}^2$
    
1.  **Alter-ego**

    El profesor Caprista y el profesor Evangetto están dando sus primeros cursos de Estadística Bayesiana. 
    Sus colegas les dijeron que el puntaje promedio en un examen final, $\mu$, varía normalmente año a año
    con media 8 y desvío estándar 0.4. Y además, que los puntajes individuales de lxs estudiantes $Y$ varían
    normalmente alrededor de $\mu$ con una desviación estándar de 0.4
    
    i. ¿Cuál es la probabilidad _a priori_ de que un estudiante se saque más de 9 en un examen final?
    i. El profesor Caprista toma el examen final y observa que sus 20 estudiantes obtuvieron una nota
    media de 8.6. Halle la distribución _a posteriori_ de $\mu$.
    i. El profesor Evangetto toma el examen final y observa que sus 20 estudiantes obtuvieron una nota
    media de 8.2. Halle la distribución _a posteriori_ de $\mu$.
    i. Combine las notas de ambos exámenes para obtener la distribución _a posteriori_ de $\mu$
    i. ¿Cuál es la probabilidad _a posteriori_ de que un estudiante se saque más de 9 en un examen final?


1.  **Inferencia sobre una distribución de Poisson**

    La distribución de masa de probabilidad Poisson se define como

    $$
    \begin{array}{lcr}
    \displaystyle p(x | \lambda) = \frac{e^{-\lambda}\lambda^x}{x!} & 
    \text{con} &
    x \in \{0, 1, 2, \cdots \}
    \end{array}
    $$

    donde $\lambda > 0$ es la cantidad promedio de veces que ocurre el evento de interés 
    en un periodo o espacio determinado.

    i. Derive el estimador de máxima verosimilitud del parámetro $\lambda$.
    i. Derive el _posterior_ $p(\lambda|D)$ suponiendo que el _prior_ sobre $\lambda$ es 
    $p(\lambda) = \text{Gamma}(\lambda | a, b) \propto \lambda^{\alpha-1}e^{-\lambda b}$.  
    Ayuda: El _posterior_ también es una distribución Gamma.
    i. ¿A qué valor tiende la media a _posteriori_ cuando $a \to 0$ y $b \to 0$?  
    Recuerde que la media de una distribución $\text{Gamma}(a, b)$ es $a/b$.
    <!-- @Murphy2012 -->
    
    
1.  **El modelo Gamma-Poisson**

    Sea $\lambda$ la tasa de mensajes de WhatsApp que una persona recibe en una hora. 
    Suponga inicialmente que se cree que la tasa de mensajes por hora tiene media 5
    con desvío estándar de 0.25 mensajes.
    
    i. Elija una distribución Gamma que represente adecuadamente lo que se cree acerca de $\lambda$
    i. ¿Cuál es la probabilidad _a priori_ de que la tasa de mensajes sea mayor a 10?
    i. ¿Cuántos mensajes se espera que reciba una persona en promedio en una hora?
    
    Se sondea a un grupo de seis personas que recibieron 7, 3, 8, 9, 10 y 12 mensajes en la última hora.
    
    i. Graficar la verosimilitud de $\lambda$
    i. Determinar la distribución _a posteriori_ de $\lambda$
    i. ¿Cuál es la probabilidad _a posteriori_ de que la tasa de mensajes sea mayor a 10? 
    i. ¿Cuántos mensajes se espera ahora que reciba una persona en promedio en una hora?

1.  **Inferencia sobre una distribución Uniforme**

    Considere una distribución uniforme crentrada en $0$ y rango $2a$. 
    La función de densidad de probabilidad es

    $$
    p(x) = \frac{1}{2a}I(x \in [-a, a])
    $$

    Sea $\mathbf{X} = (X_1,..., X_n)$ un vector de $n$ variables aleatorias independientes
    e idénticamente distribuidas según $p(x)$

    **Inferencia máximo-verosímil**

    i. ¿Cuál es el estimador máximo verosímil de $a$ (llámelo $\hat{a}$)?
    i. ¿Qué probabilidad le asigna el modelo a una nueva observación $x_{n + 1}$
    usando $\hat{a}$?
    i. ¿Observa algún problema con el resultado anterior? Si es así, sugiera una 
    alternativa mejor.

    **Inferencia Bayesiana**

    El _prior_ conjugado de la distribución uniforme es la distribución de Pareto.

    Si $X \sim \text{Pareto}(\alpha, m)$, luego

    $$
    f(x| \alpha, m) = \frac{\alpha m^\alpha}{x^{\alpha+1}} \mathbb{I}(x \ge m)
    $$

    Si el _prior_ es una distribución de Pareto, la distribución conjunta de $\theta$ y 
    $\mathbf{X} = (X_1,..., X_n)$ es 

    $$
    f(\theta, \mathbf{X}) 
        = \frac{\alpha m^\alpha}{\theta^{n + \alpha + 1}} 
        \mathbb{I}(\theta \ge \text{max}(\mathbf{X}))
    $$

    Llamando $M_x = \text{max}(\mathbf{X})$. 
    La evidencia es

    $$
    \begin{aligned}
    p(\mathbf{X}) &= \int_{M_x}^\infty
                     \frac{\alpha m^\alpha}{\theta^{n + \alpha + 1}}
                     d\theta \\
    &=  \begin{cases}
        \frac{\alpha}{(n+\alpha)m^n} & \text{Si } M_x \le m \\
        \frac{\alpha m^\alpha}{(n+\alpha)m^{n+\alpha}} & \text{Si } M_x > m \\
        \end{cases}
    \end{aligned}
    $$

    Derive el _posterior_ y muestre que puede ser expresado como una distribución de 
    Pareto.
    <!-- @Murphy2012 -->
    <!-- Hay que mejorar la redaccion y ver que la notacion es consistente -->

1.  **Inferencia sobre una distribución Exponencial**

    El tiempo de vida de una máquina en años $X$ es modelado con una distribución 
    exponencial con parámetro $\theta$ desconocido. La función de densidad es 

    $$
    \begin{array}{lcrr}
    f(x | \theta) = \theta e^{-\theta x} & \text{con} & x \ge 0, & \theta \ge 0
    \end{array}
    $$

    i. Muestre que el estimador máximo verosímil (MV) es $\hat{\theta} = 1/\bar{x}$  
    i. Suponga que se observan los siguientes tiempos de vida de tres máquinas 
    independientes $x_1 = 5$, $x_2 = 6$, $x_3 = 4$.
    ¿Cuál es el valor del estimador MV?
    i. Una experta del área sugiere que $\theta$ debe tener una distribución _a priori_ 
    que también sea exponencial.
    $$
    \begin{aligned}
        \theta &\sim \text{Exp}(\lambda) \\
        f(\theta | \lambda) &= \lambda e^{-\lambda \theta}
    \end{aligned}
    $$
    Elija un valor para el hiperparámetro $\lambda$ de la distribución _a priori_ tal que
    $\mathbb{E}(\theta) = 1/3$. Utilice $\hat{\lambda}$ para representar al valor.

    i. ¿Cuál es el _posterior_ $f(\theta | \mathbf{X}, \hat{\lambda})$?
    i. ¿Es la distribución exponencial conjugada con un _likelihood_ exponencial?
    i. Encuentre la media del _posterior_, $\mathbb{E}(\theta | \mathbf{X}, \hat{\lambda})$
    i. Explique por que difieren el estimador MV de la media _a posteriori_. 
    ¿Cuál es más razonable en este ejemplo?
    <!-- @Murphy2012 -->
       
1.  **Otras distribuciones conjugadas (I)**
    
    Considere el siguiente modelo:

    $$
    \begin{array}{l}
    Y\mid\theta \sim \text{Geom}(\theta) \\
    \theta \sim \text{Beta}(\alpha,\beta)
    \end{array}
    $$
    donde la función de densidad de la distribución geométrica es 
    $p(y\mid\theta) = \theta(1-\theta)^{y-1}$ para $y \in {1,2,\dots}$
    
    i. ¿Qué debería ocurrir con la distribución _a posteriori_ de $\theta$ para poder 
    afirmar que la distribución geométrica es conjugada de la beta?
    i. Derive la distribución _a posteriori_ de $\theta$ y concluya.

1.  **Otras distribuciones conjugadas (II)**

    Considere el siguiente modelo:
    
    $$
    \begin{array}{l}
    Y\mid\theta \sim \text{BinomialNeg}(\theta,m) \\
    \theta \sim \text{Beta}(\alpha,\beta)
    \end{array}
    $$
    
    donde la función de densidad de la distribución binomial negativa es 
    
    $$p(y\mid \theta, m) = {y+m-1 \choose y} \theta^{m} (1-\theta)^y$$
    
    Obtenga la distribución _a posteriori_ de $\theta$.
    
1.  **Otras distribuciones conjugadas (III)**

    Considere el siguiente modelo:
    
    $$
    \begin{array}{l}
    Y\mid\theta \sim \text{Exp}(\theta) = \text{Gamma}(1,\theta)
    \end{array}
    $$

    donde la función de densidad exponencial es $f(y\mid\theta) = \theta e^{-\theta y}$.
    
    Elija una distribución _a priori_ conjugada de la verosimilitud propuesta y obtenga 
    la expresión para la distribución de probabilidad _a posteriori_
    <!-- Nota: Es la parametrizacion de la gamma basada en forma y escala (versus forma y _rate_) -->

## Simulaciones

A diferencia de la sección anterior, que requiere resolver los ejercicios a mano y 
promueve el uso de la computadora y `R` de manera complementaria, esta sección 
contiene ejercicios que deben ser resueltos mediante técnicas de simulación implementadas
en `R`. Es posible que en algunos casos también se pueda obtener una solución analítica. 
En estos casos, puede resultar de utilidad obtener también una solución a mano para 
comparar resultados, niveles de dificultad y ver que tan intuitivo resultan ambos enfoques.

1.  **Entrada en calor**

    Para cada una de las siguientes situaciones, hallar los intervalos centrales de credibilidad
    
    i. Intervalo del 95\% para $\pi$ siendo $\pi\mid \mathbf{y} \sim \text{Beta}(4,5)$
    i. Intervalo del 60\% para $\pi$ siendo $\pi\mid \mathbf{y} \sim \text{Beta}(4,5)$
    i. Intervalo del 89\% para $\lambda$ siendo $\lambda\mid \mathbf{y} \sim \text{Gamma}(1,8)$
    i. Intervalo del 95\% para $\lambda$ siendo $\lambda\mid \mathbf{y} \sim \text{Gamma}(2,5)$
    i. Intervalo del 81\% para $\mu$ siendo $\mu\mid \mathbf{y} \sim \mathcal{N}(10,2^2)$
    i. Intervalo del 99\% para $\pi$ siendo $\mu\mid \mathbf{y} \sim \mathcal{N}(-3,1^2)$

1.  **Propiedades frecuentistas de inferencias bayesianas (!!)**
    
    Sea una variable $Y$ tal que $Y | \theta \sim \text{Binomial}(n, \theta)$ y 
    $\theta \sim \text{Beta}(1/2, 1/2)$. Mediante un estudio de simulación calcule la 
    cobertura empírica del intervalo de credibilidad del 95% para $n \in \{1, 5, 10, 25\}$ 
    y $\theta \in \{0.05, 0.10, \dots, 0.50 \}$. Describa las propiedades frecuentistas
    del intervalo de credibilidad bayesiano.
    <!-- @Reich2020 -->

1.  **¿Te preguntaste alguna vez cuál es la distribución de un _p-value_?**

    Considere un problema conocido. Se desean comparar dos muestras independientes de 
    tamaño 5 utilizando un test t y utilizando el test de Mann-Whitney.

    i. Considere el caso en que las dos muestras provienen de poblaciones con igual media
    y desvío estándar (supongamos normal de media nula y varianza unitaria). 
    Si se repitiera muchas veces el proceso de tomar las muestras y realizar los tests, 
    ¿qué distribución tendrán los _p-values_ obtenidos para cada test?
    i. Considere ahora el caso en que las dos muestras provienen de poblaciones con 
    diferente media e igual desvío estándar ($\mathcal{N}(0,1)$ y $\mathcal{N}(1,1)$). 
    Si se repitiera muchas veces el proceso de tomar las muestras y realizar los tests, 
    ¿qué distribución tendrán los _p-values_ obtenidos para cada test?

## Elección de distribuciones _a priori_

Esta útima sección de la práctica tiene como propósito ejercitar el uso de distribuciones
de probabilidad como herramienta para reflejar información de un problema determinado.


1.  **Esbozar la distribución de las siguientes variables**

    i. El número de personas que compran café en el bar de la facultad asumiendo 
    distribución de Poisson.
    i. El peso de perros adultos en kilogramos asumiendo una distribución Uniforme.
    i. El peso de elefantes adultos en kilogramos asumiendo una distribución Normal.
    i. El peso de humanos adultos en libras asumiendo una distribución asimétrica hacia 
    la derecha.

1.  **Verificar los resultados de manera computacional**  

    Para cada uno cada uno de los ejemplos del ejercicio anterior,
    graficar la distribución usando `R`. Seleccionar los parámetros que
    creas razonable, tomar una muestra aleatoria de tamaño 1000 y
    graficar la distribución en base a las muestras. ¿Se refleja tu
    conocimiento del problema en la distribución graficada? Si no,
    ajustar los parámetros y repetir el proceso hasta que el resultado
    tenga concuerde con el conocimiento del problema.

1.  **Hay que amigarse con de la distribución Beta**

    Comparar las siguientes distribuciones _a priori_.

    - $\text{Beta}(0.5, 0.5)$
    - $\text{Beta}(1, 1)$
    - $\text{Beta}(1, 1)$
    - $\text{Beta}(1, 4)$
    - $\text{Beta}(5, 1.5)$

    i. ¿En qué se diferencian?
    i. ¿Cuál de ellas es más informativa?
    i. ¿Cómo lo determinaste?

1.  **Elicitación de _priors_**

    En cada una de la situaciones que se describen debajo, ajustar manualmente los 
    parámetros de una distribución $\text{Beta}$ para que reflejen la información brindada.
    No siempre existe una única respuesta correcta.  

    i. Un amigo se postuló para un empleo en LinkedIn y te dijo: "Diría que tengo una 
    chance del 40% de que me den el trabajo, pero no estoy seguro". 
    Cuando le preguntamos un poco mas, dijo que estima sus chances entre un 20% y un 60%.
    i. Un grupo de investigación del CONICET desarrolló una nueva prueba para una 
    enfermedad bastante rara. El grupo espera que esta prueba arroje resultados correctos
    el 80% de las veces, con una varianza de 0.05.
    i. El primo de un amigo es un apasionado de la pesca, lo practica muy seguido, y se 
    dice ser muy bueno. Según comenta tu amigo, en el asado de los Jueves el pescador dijo
    lo siguiente: 
    
    > Si tengo que hacer un promedio, 9 de cada 10 veces que salgo, vuelvo con algo. 
    > Pero últimamente te diría que siempre es 10 de 10. Estoy infalible.
    > La verdad es que soy un crack de la pesca.
    
    Ante el descreimiento de algunos de los comensales supo reconocer que no siempre le 
    fue tan bien:
    
    > Tuve mis malas rachas, pero nunca menos de 8 pescas de cada 10 salidas.
    
1.  **Efecto de la parametrización**
    
    Sea $\theta$ la probabilidad de éxito en un experimento binomial y sea 
    $\gamma = \frac{\theta}{1-\theta}$ la chance de éxito. Utilizar simulaciones para 
    explorar los efectos de las siguientes elecciones de distribuciones _a priori_
    
    i. Si $\theta \sim \text{Uniforme}(0,1)$, ¿cuál es el _prior_ inducido para $\gamma$?
    i. Si $\theta \sim \text{Beta}(0,1)$, ¿cuál es el _prior_ inducido para $\gamma$?
    i. Si $\gamma \sim \text{Uniforme}(0,100)$, ¿cuál es el _prior_ inducido para $\theta$?
    i. Si $\gamma \sim \text{Gamma}(1,1)$, ¿cuál es el _prior_ inducido para $\theta$? 
    
    
## Teoría de la Decisión

1.  Dada la distribución _a posteriori_ $p(\theta \mid y)$, probar que el estimador de Bayes
    que minimiza la función de pérdida $L_1$ es la mediana de $p(\theta \mid y)$.
    <!-- https://stats.stackexchange.com/questions/376867/proof-that-posterior-median-is-the-bayes-estimate-of-absolute-loss -->
    
1.  Suponga que la distribución _a posteriori_ de $\pi$, $p(\pi \mid y)$, es $\text{Beta}(12,4)$.
    Determine mediante simulación el estimador que minimiza la pérdida de Huber:
    $$
    \mathcal{L}(\delta,\pi) =
    \begin{cases}
    \frac{1}{2} (\pi - \delta)^2 \text{ si } |\pi - \delta| \leq \alpha \\
    \alpha \cdot (|\pi - \delta|-\frac{1}{2}\alpha) \text{ en cualquier otro caso}
    \end{cases}
    $$
1.  **To Do** Problema de rankear cinco tratamientos. Se tienen los puntajes de diez tratamientos, ${\theta_1,\theta_2,\dots,\theta_10}$.
    Se dan muestras de las distribuciones _a posteriori_, $p(\theta_i\mid y)$. ¿Cuál de los tratamientos es mejor?

<!-- 
## Notas

En algún lado podríamos tener un glosario, o algo del estilo. El
objetivo es despejar dudas, por ejemplo, sobre las parametrizaciones de
las distribuciones que utilizamos por defecto. Acá podriamos mostrar que
usamos $\text{Beta}(a, b)$ con
$\text{pdf}(x) = \frac{\Gamma(a + b)}{\Gamma(a) + \Gamma(b)} x^{a-1}(1-x)^{b-1}$ 


Let $\pi$  be a random variable which can take any value between 0 and 1, i.e.,  
$\pi \in [0,1]$. Then the variability in $\pi$ might be well modeled by a Beta model 
with **shape hyperparameters** $\alpha > 0$ and $\beta > 0$:

$\pi \sim \text{Beta}(\alpha, \beta).$


$$
\begin{equation}
f(\pi) = \frac{\Gamma(\alpha + \beta)}{\Gamma(\alpha)\Gamma(\beta)} \pi^{\alpha-1} (1-\pi)^{\beta-1} \;\; \text{ for } \pi \in [0,1] \tag{3.1}
\end{equation}
$$

$\Gamma(z) = \int_0^\infty x^{z-1}e^{-y}dx$ $\Gamma(z + 1) = z \Gamma(z)$

Fun fact: 
when  $z$ is a positive integer, then  $\Gamma(z)$ simplifies to  $\Gamma(z) = (z-1)!$

**Hyperparameter**

A hyperparameter is a parameter used in a prior model.
-->
