# Modelos Jerárquicos

```{r setup}
#| echo: false
#| include: false

library(dplyr)
library(tidyr)
library(ggplot2)
library(patchwork)

theme_set(theme_bw() +
          theme(legend.position = "none",
                panel.grid = element_blank(),
                #strip.background = element_rect(fill="#ade658"),
                text = element_text(size=26)))
```

## Introducción

Consideremos el siguiente modelo:
$$
\begin{align*}
    y\mid\pi & \sim  \mathrm{Bi}(N_i,\pi)\\
    \pi & \sim  Beta(a,b)
\end{align*}
$$

## {.smaller}

Sabemos (gracias al TP2) que la función de densidad de la distribución beta se puede expresar en términos de su moda $\omega$ y su concentración $\kappa$

$$
\begin{align*}
    y\mid\pi & \sim  \mathrm{Bi}(N,\pi)\\
    \pi & \sim  Beta(\omega(\kappa-2)+1,\ (1-\omega)(\kappa-2)+1)
\end{align*}
$$

El valor de $\pi$ depende del valor de $\omega$. Lo sabíamos, después de todo, $\omega$ y $\kappa$ son las _constantes de ajuste del prior_ o _hiperparámetros_. $\kappa$ refleja el grado de credibilidad _a priori_ sobre los valores de $\pi$ (alrededor de $\omega$).

## {.smaller}

¿Qué pasa si $\omega$ no es fijo sino otro parámetro a estimar?

. . .

En el contexto de una moneda: $\pi$ es la probabilidad de cara de la moneda y $\omega$ es el valor de probabilidad de cara al que el fabricante de monedas le apunta en la construcción. $\kappa$ (fijo y conocido) es una medida de la dispersión que tiene el proceso de fabricación (de lo consistente que es este proceso) o, en otros términos, mide el grado de asociación entre $\pi$ y $\omega$.

. . .

¿Qué necesitamos para $\omega$? Digamos que, _a priori_, $\omega \sim \mathrm{Beta}(A_\omega,B_\omega)$

. . .

$$
\begin{align*}
    y\mid\pi & \sim  \mathrm{Bi}(N,\pi)\\
    \pi \mid \omega & \sim  \mathrm{Beta}(\omega(\kappa-2)+1,\ (1-\omega)(\kappa-2)+1) \\
    \omega & \sim \mathrm{Beta}(A_\omega,B_\omega)
\end{align*}
$$

¿Cuántos parámetros tiene este modelo?

## {.smaller}

Es un modelo de dos parámetros (hay una distribución conjunta _a priori_ y una distribución conjunta _a posteriori_) pero no como el $\mu$ y el $\sigma$ de una distribución normal o el $\beta_0$ y $\beta_1$ de un modelo de regresión lineal...

¿Cómo funciona el modelo hacia adelante? $\omega \rightarrow \pi \rightarrow y$ ($\omega$ influye en el valor de $y$ solo a través de $\pi$)

¿Y el razonamiento inverso? De $N$ tiradas podemos hacer una inferencia sobre $\pi$, lo que nos permitirá hacer una inferencia sobre $\omega$

## {.smaller}

¿Qué distribución _a posteriori_ buscamos? $p(\pi,\omega\mid y)$

¿Y la Regla de Bayes? ¿Vale? ¿Cómo la escribimos?

$$p(\pi,\omega\mid y) = \frac{p(y\mid\pi,\omega)p(\pi,\omega)}{p(y)} = \frac{p(y\mid\pi)p(\pi\mid\omega)p(\omega)}{p(y)}$$

¿Tenemos forma de expresar $p(y\mid\pi)$, $p(\pi\mid\omega)$, y $p(\omega)$?

##

Estamos haciendo inferencia bayesiana sobre una distribución conjunta (de $\pi$ y $\omega$). Pero la relación entre los parámetros (y la función de verosimilitud) es jerárquica. La jerarquía tiene una interpretación para el modelo.

##

```{r}
#| echo: true
#| eval: true

pi <- seq(0,1,length.out=101)
omega <- seq(0,1,length.out=101)
A_omega <- 2
B_omega <- 2
kappa <- 5
N <- 9
y <- 3
```

## {.smaller}

```{r}
#| echo: true
#| eval: true

prior <- expand.grid(pi = pi, omega = omega) |>
  mutate(p_omega = dbeta(omega, A_omega, B_omega),
         p_pi_given_omega = dbeta(pi, omega*(kappa-2)+1, (1-omega)*(kappa-2)+1),
         prior = p_pi_given_omega * p_omega)

plot_prior <- ggplot(prior) +
  geom_raster(aes(x=pi, y=omega, fill=prior)) +
  geom_hline(yintercept = 0.6) +
  geom_hline(yintercept = 0.9) +
  scale_x_continuous(expression(pi), expand = c(0,0)) +
  scale_y_continuous(expression(omega), expand = c(0,0)) +
  viridis::scale_fill_viridis()

plot_prior_omega <- ggplot(prior) +
  geom_line(aes(x=omega, y=p_omega))

plot_prior_pi_omega1 <- ggplot(prior |> filter(omega == 0.6)) +
  geom_line(aes(x=pi, y=prior)) +
  xlab(expression(pi))

plot_prior_pi_omega2 <- ggplot(prior |> filter(omega == 0.9)) +
  geom_line(aes(x=pi, y=prior)) +
  xlab(expression(pi))
```

##

```{r}
  #| warning: false
  #| echo: false
  #| fig-width: 8
  #| fig-height: 8
  #| fig-align: center

plot_prior + (plot_prior_pi_omega1/plot_prior_pi_omega2)
```

## {.smaller}

```{r}
#| echo: true
#| eval: true

likelihood <- expand.grid(pi = pi, omega = omega) |>
  mutate(likelihood = dbinom(y, size = N, prob = pi))

plot_likelihood <- ggplot(likelihood) +
  geom_raster(aes(x=pi, y=omega, fill=likelihood)) +
  scale_x_continuous(expression(pi), expand = c(0,0)) +
  scale_y_continuous(expression(omega), expand = c(0,0)) +
  viridis::scale_fill_viridis()

posterior <- inner_join(prior,likelihood) |>
  mutate(posterior = prior * likelihood)

plot_posterior <- ggplot(posterior) +
  geom_raster(aes(x=pi, y=omega, fill=posterior)) +
  scale_x_continuous(expression(pi), expand = c(0,0)) +
  scale_y_continuous(expression(omega), expand = c(0,0)) +
  viridis::scale_fill_viridis()
```

## 

```{r}
  #| warning: false
  #| echo: false
  #| fig-width: 8
  #| fig-height: 8
  #| fig-align: center

plot_prior + plot_likelihood + plot_posterior
```

##

```{r}
#| echo: true
#| eval: true

pi <- seq(0,1,length.out=101)
omega <- seq(0,1,length.out=101)
A_omega <- 20
B_omega <- 20
kappa <- 10
N <- 9
y <- 3
```



```{r}
#| echo: false
#| eval: true

prior <- expand.grid(pi = pi, omega = omega) |>
  mutate(p_omega = dbeta(omega, A_omega, B_omega),
         p_pi_given_omega = dbeta(pi, omega*(kappa-2)+1, (1-omega)*(kappa-2)+1),
         prior = p_pi_given_omega * p_omega)

plot_prior <- ggplot(prior) +
  geom_raster(aes(x=pi, y=omega, fill=prior)) +
  geom_hline(yintercept = 0.6) +
  geom_hline(yintercept = 0.9) +
  scale_x_continuous(expression(pi), expand = c(0,0)) +
  scale_y_continuous(expression(omega), expand = c(0,0)) +
  viridis::scale_fill_viridis()

prior_omega <- ggplot(prior) +
  geom_line(aes(x=omega, y=p_omega))

prior_pi_1 <- ggplot(prior |> filter(omega == 0.6)) +
  geom_line(aes(x=pi, y=prior)) +
  xlab(expression(pi))

prior_pi_2 <- ggplot(prior |> filter(omega == 0.9)) +
  geom_line(aes(x=pi, y=prior)) +
  xlab(expression(pi))
```

##

```{r}
  #| warning: false
  #| echo: false
  #| fig-width: 8
  #| fig-height: 8
  #| fig-align: center

plot_prior + (plot_prior_pi_omega1/plot_prior_pi_omega2)
```

##

```{r}
  #| warning: false
  #| echo: false
  #| fig-width: 8
  #| fig-height: 8
  #| fig-align: center

likelihood <- expand.grid(pi = pi, omega = omega) |>
  mutate(likelihood = dbinom(y, size = N, prob = pi))

plot_likelihood <- ggplot(likelihood) +
  geom_raster(aes(x=pi, y=omega, fill=likelihood)) +
  scale_x_continuous(expression(pi), expand = c(0,0)) +
  scale_y_continuous(expression(omega), expand = c(0,0)) +
  viridis::scale_fill_viridis()

posterior <- inner_join(prior,likelihood) |>
  mutate(posterior = prior * likelihood)

plot_posterior <- ggplot(posterior) +
  geom_raster(aes(x=pi, y=omega, fill=posterior)) +
  scale_x_continuous(expression(pi), expand = c(0,0)) +
  scale_y_continuous(expression(omega), expand = c(0,0)) +
  viridis::scale_fill_viridis()

plot_prior + plot_likelihood + plot_posterior
```


##

La inferencia bayesiana en un modelo jerárquico es inferencia en el espacio de la distribución conjunta de los parámetros pero reformulando la distribución conjunta en términos jerárquicos: se refactoriza $p(\pi,\omega)$ como $p(\pi\mid\omega)p(\omega)$

## Extensión {.smaller}

¿Qué pasaría si contamos con más de una moneda creada por la misma fábrica? Cada moneda tiene un valor de $\pi_s$ que es propio y que a su vez tienen algo en común: provienen de la fábrica que tiene parámetro $\omega$.

Con $y_1$ caras en $N_1$ tiradas de la moneda 1 estimamos $\pi_1$, con $y_2$ caras en $N_2$ tiradas de la moneda 1 estimamos $\pi_2$... y luego, con todas las tiradas, podemos estimar $\omega$.

## {.smaller}

Consideremos un caso real. $S$ personas reciben una droga y son sometidos a un test de memoria. La probabilidad de que el sujeto $s$ recuerde un ítem que se le muestra es $\pi_s$. El sujeto $s$ recuerda $y_s$ ítems de $N_s$ que se le presentan. Asumimos que la droga induce un efecto en los sujetos alrededor de una tendencia central $\omega$.

Consideremos por simplicidad que se tienen dos sujetos...

¿Cuántos parámetros tiene el modelo? ¿Cómo podemos representar la relación entre los $\theta_s$?

## {.smaller}

$$
\begin{align*}
    y_s\mid\pi_s & \sim  \mathrm{Bi}(N_s,\pi_s)\\
    \pi_s \mid \omega & \sim  \mathrm{Beta}(\omega(\kappa-2)+1,\ (1-\omega)(\kappa-2)+1) \\
    \omega & \sim \mathrm{Beta}(A_\omega,B_\omega)
\end{align*}
$$

Si fijo $\omega$, los valores de los $\pi_s$ son independientes: $\pi_1$ y $\pi_2$ son independientes dado $\omega$. 

La probabilidad _a priori_ que es $p(\pi_1,\pi_2,\omega)$ ahora puede factorizarse como $p(\pi_1,\pi_2\mid \omega) p(\omega) = p(\pi_1\mid \omega) p(\pi_2\mid \omega) p(\omega)$

##

```{r}
#| echo: true
#| eval: true

pi_1 <- seq(0,1,length.out=101)
pi_2 <- seq(0,1,length.out=101)
omega <- seq(0,1,length.out=101)
A_omega <- 2
B_omega <- 2
kappa <- 5
N_1 <- 20
y_1 <- 5
N_2 <- 8
y_2 <- 4
```

##

```{r}
#| echo: true
#| eval: true

prior <- expand.grid(pi_1 = pi_1, pi_2 = pi_2, omega = omega) |>
  mutate(
    p_omega = dbeta(omega, A_omega, B_omega),
    p_pi1_given_omega = dbeta(pi_1, 
                              omega*(kappa-2)+1, 
                              (1-omega)*(kappa-2)+1),
    p_pi2_given_omega = dbeta(pi_2, 
                              omega*(kappa-2)+1, 
                              (1-omega)*(kappa-2)+1),
    prior = p_pi1_given_omega *  p_pi2_given_omega * p_omega)

prior_pi1 <- ggplot(prior |> 
                      group_by(pi_1,omega) |> 
                      summarise(prior = sum(prior))) +
  geom_raster(aes(x=pi_1, y=omega, fill=prior)) +
  scale_x_continuous(expression(pi[1]), expand = c(0,0)) +
  scale_y_continuous(expression(omega), expand = c(0,0)) +
  viridis::scale_fill_viridis()

prior_pi2 <- ggplot(prior |> 
                      group_by(pi_2,omega) |> 
                      summarise(prior = sum(prior))) +
  geom_raster(aes(x=pi_2, y=omega, fill=prior)) +
  scale_x_continuous(expression(pi[2]), expand = c(0,0)) +
  scale_y_continuous(expression(omega), expand = c(0,0)) +
  viridis::scale_fill_viridis()
```

##

```{r}
  #| warning: false
  #| echo: false
  #| fig-width: 8
  #| fig-height: 8
  #| fig-align: center

prior_pi1 + prior_pi2
```
##

```{r}
#| echo: true
#| eval: true

likelihood <- expand.grid(pi_1 = pi_1, pi_2 = pi_2, omega = omega) |>
  mutate(likelihood_pi1 = dbinom(y_1, size = N_1, prob = pi_1),
         likelihood_pi2 = dbinom(y_2, size = N_2, prob = pi_2),
         likelihood = likelihood_pi1 * likelihood_pi2)

likelihood1 <- ggplot(likelihood |> 
                        group_by(pi_1,omega) |> 
                        summarise(likelihood = sum(likelihood))) +
  geom_raster(aes(x=pi_1, y=omega, fill=likelihood)) +
  scale_x_continuous(expression(pi[1]), expand = c(0,0)) +
  scale_y_continuous(expression(omega), expand = c(0,0)) +
  viridis::scale_fill_viridis()

likelihood2 <- ggplot(likelihood |> 
                        group_by(pi_2,omega) |> 
                        summarise(likelihood = sum(likelihood))) +
  geom_raster(aes(x=pi_2, y=omega, fill=likelihood)) +
  scale_x_continuous(expression(pi[2]), expand = c(0,0)) +
  scale_y_continuous(expression(omega), expand = c(0,0)) +
  viridis::scale_fill_viridis()
```
##

```{r}
  #| warning: false
  #| echo: false
  #| fig-width: 8
  #| fig-height: 8
  #| fig-align: center

likelihood1 + likelihood2
```

##

```{r}
#| echo: true
#| eval: true

posterior <- inner_join(prior, likelihood) |>
  mutate(posterior = prior * likelihood)

posterior_pi1 <- ggplot(posterior |> 
                          group_by(pi_1,omega) |> 
                          summarise(posterior = sum(posterior))) +
  geom_raster(aes(x=pi_1, y=omega, fill=posterior)) +
  scale_x_continuous(expression(pi[1]), expand = c(0,0)) +
  scale_y_continuous(expression(omega), expand = c(0,0)) +
  viridis::scale_fill_viridis()

posterior_pi2 <- ggplot(posterior |> 
                          group_by(pi_2,omega) |> 
                          summarise(posterior = sum(posterior))) +
  geom_raster(aes(x=pi_2, y=omega, fill=posterior)) +
  scale_x_continuous(expression(pi[2]), expand = c(0,0)) +
  scale_y_continuous(expression(omega), expand = c(0,0)) +
  viridis::scale_fill_viridis()

posterior_omega <- ggplot(posterior |> 
                            group_by(omega) |> 
                            summarise(posterior = sum(posterior))) +
  geom_line(aes(x=omega, y=posterior)) +
  xlab(expression(omega))

posterior_pi1_marg <- ggplot(posterior |> 
                               group_by(pi_1) |> 
                               summarise(posterior = sum(posterior))) +
  geom_line(aes(x=pi_1, y=posterior)) +
  xlab(expression(pi[1]))

posterior_pi2_marg <- ggplot(posterior |> 
                               group_by(pi_2) |> 
                               summarise(posterior = sum(posterior))) +
  geom_line(aes(x=pi_2, y=posterior)) +
  xlab(expression(pi[2]))
```
##

```{r}
  #| warning: false
  #| echo: false
  #| fig-width: 8
  #| fig-height: 8
  #| fig-align: center
posterior_pi1 + posterior_pi2
```

##

```{r}
  #| warning: false
  #| echo: false
  #| fig-width: 8
  #| fig-height: 8
  #| fig-align: center
posterior_omega / posterior_pi1_marg / posterior_pi2_marg
```

##

- La función de verosimilitud no depende de $\omega$ 
- La función de verosimilitud es más estrecha para el sujeto 1 que para el sujeto 2
- El _posterior_ marginal de $\pi_1$ está cerca de la proporción muestral
- El _posterior_ marginal de $\pi_2$ está cerca de la proporción muestral
- El _posterior_ marginal de $\pi_1$ tiene menos incertidumbre que el de $\pi_2$

. . .

¿Qué ocurre si se cambia el valor de $\kappa$?

##

```{r}
#| echo: true
#| eval: true

pi_1 <- seq(0,1,length.out=101)
pi_2 <- seq(0,1,length.out=101)
omega <- seq(0,1,length.out=101)
A_omega <- 2
B_omega <- 2
kappa <- 100
N_1 <- 20
y_1 <- 5
N_2 <- 8
y_2 <- 4
```

##

```{r}
#| echo: false
#| eval: true

prior <- expand.grid(pi_1 = pi_1, pi_2 = pi_2, omega = omega) |>
  mutate(p_omega = dbeta(omega, A_omega, B_omega),
         p_pi1_given_omega = dbeta(pi_1, omega*(kappa-2)+1, (1-omega)*(kappa-2)+1),
         p_pi2_given_omega = dbeta(pi_2, omega*(kappa-2)+1, (1-omega)*(kappa-2)+1),
         prior = p_pi1_given_omega *  p_pi2_given_omega * p_omega)

prior_pi1 <- ggplot(prior |> group_by(pi_1,omega) |> summarise(prior = sum(prior))) +
  geom_raster(aes(x=pi_1, y=omega, fill=prior)) +
  scale_x_continuous(expression(pi[1]), expand = c(0,0)) +
  scale_y_continuous(expression(omega), expand = c(0,0)) +
  viridis::scale_fill_viridis()

prior_pi2 <- ggplot(prior |> group_by(pi_2,omega) |> summarise(prior = sum(prior))) +
  geom_raster(aes(x=pi_2, y=omega, fill=prior)) +
  scale_x_continuous(expression(pi), expand = c(0,0)) +
  scale_y_continuous(expression(omega), expand = c(0,0)) +
  viridis::scale_fill_viridis()

likelihood <- expand.grid(pi_1 = pi_1, pi_2 = pi_2, omega = omega) |>
  mutate(likelihood_pi1 = dbinom(y_1, size = N_1, prob = pi_1),
         likelihood_pi2 = dbinom(y_2, size = N_2, prob = pi_2),
         likelihood = likelihood_pi1 * likelihood_pi2)

likelihood1 <- ggplot(likelihood |> group_by(pi_1,omega) |> summarise(likelihood = sum(likelihood))) +
  geom_raster(aes(x=pi_1, y=omega, fill=likelihood)) +
  scale_x_continuous(expression(pi[1]), expand = c(0,0)) +
  scale_y_continuous(expression(omega), expand = c(0,0)) +
  viridis::scale_fill_viridis()

likelihood2 <- ggplot(likelihood |> group_by(pi_2,omega) |> summarise(likelihood = sum(likelihood))) +
  geom_raster(aes(x=pi_2, y=omega, fill=likelihood)) +
  scale_x_continuous(expression(pi[2]), expand = c(0,0)) +
  scale_y_continuous(expression(omega), expand = c(0,0)) +
  viridis::scale_fill_viridis()

posterior <- inner_join(prior, likelihood) |>
  mutate(posterior = prior * likelihood)

posterior_pi1 <- ggplot(posterior |> group_by(pi_1,omega) |> summarise(posterior = sum(posterior))) +
  geom_raster(aes(x=pi_1, y=omega, fill=posterior)) +
  scale_x_continuous(expression(pi[1]), expand = c(0,0)) +
  scale_y_continuous(expression(omega), expand = c(0,0)) +
  viridis::scale_fill_viridis()

posterior_pi2 <- ggplot(posterior |> group_by(pi_2,omega) |> summarise(posterior = sum(posterior))) +
  geom_raster(aes(x=pi_2, y=omega, fill=posterior)) +
  scale_x_continuous(expression(pi), expand = c(0,0)) +
  scale_y_continuous(expression(omega), expand = c(0,0)) +
  viridis::scale_fill_viridis()

posterior_omega <- ggplot(posterior |> group_by(omega) |> summarise(posterior = sum(posterior))) +
  geom_line(aes(x=omega, y=posterior))

posterior_pi1_marg <- ggplot(posterior |> group_by(pi_1) |> summarise(posterior = sum(posterior))) +
  geom_line(aes(x=pi_1, y=posterior))

posterior_pi2_marg <- ggplot(posterior |> group_by(pi_2) |> summarise(posterior = sum(posterior))) +
  geom_line(aes(x=pi_2, y=posterior))
```

```{r}
  #| warning: false
  #| echo: false
  #| fig-width: 8
  #| fig-height: 8
  #| fig-align: center

prior_pi1 + prior_pi2
```

##

```{r}
  #| warning: false
  #| echo: false
  #| fig-width: 8
  #| fig-height: 8
  #| fig-align: center

likelihood1 + likelihood2
```

##

```{r}
  #| warning: false
  #| echo: false
  #| fig-width: 8
  #| fig-height: 8
  #| fig-align: center

posterior_pi1 + posterior_pi2
```

##

```{r}
  #| warning: false
  #| echo: false
  #| fig-width: 8
  #| fig-height: 8
  #| fig-align: center

posterior_omega / posterior_pi1_marg / posterior_pi2_marg
```

##

El _posterior_ marginal de $\pi_2$ se alejó de la proporción muestral. El sujeto 1 tenía un tamaño de muestra mayor (más evidencia) y por lo tanto influyó más en la estimación de $\omega$, lo que a la vez influye en la estimación de $\pi_2$.

## Shrinkage

La estructura jerárquica de los modelos hace que las estimaciones de los parámetros de los niveles más bajos se acerquen más de lo que lo harían si no hubiera una distribución en un nivel superior. Esto es lo que se conoce como _shrinkage_ de las estimaciones.

Las estimaciones de los parámetros de los niveles más bajos son tiradas (_pulled_) o se estrechan o tienden a concentrarse hacia la moda de la distribución superior.

## {.smaller}

El _shrinkage_ ocurre porque los parámetros de los niveles bajos (los $\pi_s$) son influenciados por:

. . .

1.    El conjunto de datos que dependen directamente de ese parámetro 
2.    Los parámetros de niveles más altos de los cuales dependen los parámetros de niveles más bajos (¡y que son afectados por todos los datos!)

. . .

Por ejemplo, sobre $\pi_1$ influyen $y_1$ y $N_1$ pero también $\omega$ (cuya estimación depende de $\pi_2$ y $N_2$).

. . .

Nota: el _shrinkage_ es consecuencia exclusivamente de la estructura jerárquica (y no de la inferencia bayesiana). Existe en la teoría clásica de estimación (ver estimador de James-Stein)

##

![](imgs/shrinkage.png)

## Otro ejemplo

El radón es un gas radioactivo y cancerígeno. Los productos de la desintegración del radón son también radioactivos y en altas concentraciones se sabe que producen cáncer de pulmón. Trabajaremos con datos de mediciones de radón en el estado de Minnesota. Se cuenta con mediciones en hogares de diferentes condados dentro del estado.

##

1.    $i$ es el índice de los hogares
1.    $Y_i$ es el nivel de radón (log radón) del hogar $i$
1.    $j$ (entre $1$ y $J$) es el índice de los condados
1.    $j[i] = \mathrm{county}[i]$ es el condado al que pertenece el hogar $i$

##

```{r}
  #| warning: false
  #| echo: false
  #| fig-width: 8
  #| fig-height: 8
  #| fig-align: center

radon <- read.csv("https://raw.githubusercontent.com/estadisticaunr/estadistica-bayesiana/main/datos/radon_data.csv")

ggplot() +
  geom_jitter(aes(x=log_radon, y=county, fill=county), 
              data=radon %>% 
                filter(county %in% c("LAC QUI PARLE","AITKIN","KOOCHICHING","DOUGLAS","CLAY","STEARNS","RAMSEY","ST LOUIS")), 
              width = 0,
              height = 0.05,
              alpha = 0.5)
```

##

### Complete pooling {.smaller}

$$
\begin{align*}
    Y_i\mid\mu_i,\sigma & \sim  \mathcal{N}(\mu_i,\sigma^2) \\
    \mu_i & = \alpha  \\
    \alpha & \sim  P(\alpha) \\
    \sigma & \sim  P(\sigma)
\end{align*}
$$

Hay una única media $\alpha$ común para todos los $i$, independientemente del grupo $j$ al que pertenezcan

##

### No pooling  {.smaller}

$$
\begin{align*}
    Y_i\mid\mu_i,\sigma & \sim  \mathcal{N}(\mu_i,\sigma^2)\\
    \mu_i & = \alpha_{j[i]}  \\
    \alpha_j & \sim  P(\alpha_j) \\
    \sigma & \sim  P(\sigma)
\end{align*}
$$

Decimos que los $Y_i$ tienen distribución de media $\alpha_{j[i]}$, sin imponer ninguna restricción sobre los $\alpha_j$. $P(\alpha_j)$ es una distribución no informativa (muy ancha y chata). Todos los $\alpha_j$ son independientes. Coincide con la estimación clásica que incluye una variable _dummy_ para cada grupo.

## {.smaller}

Podemos mejorar el modelo anterior incorporando un prior que regularice los $\alpha_j$

\pause
\begin{align*}
    Y_i\mid\mu_i,\sigma & \sim  \mathcal{N}(\mu_i,\sigma^2) \\
    \mu_i & = \alpha_{j[i]}  \\
    \alpha_j & \sim  \mathcal{N}(0,10) \\
    \sigma & \sim  P(\sigma)
\end{align*}

0 y 10 son valores arbitrarios para la media y la varianza de la distribución _a priori_ de los $\alpha_j$. Los $\alpha_j$ dejan de poder ser estimados libremente. Hay _regularización_ y tendemos a evitar el _overfitting_. Hay un _partial pooling_. Si en lugar de 10 se elige un valor más grande, tendemos a _no pooling_; si se elige un valor más chico, tendemos a _pooling completo_

##

Mejor aún, podemos estimar el grado de regularización partir de los datos. ¿Cuánto _pooling_ es necesario? Se estima a partir de los datos...

\begin{align*}
    Y_i\mid\mu_i,\sigma & \sim  \mathcal{N}(\mu_i,\sigma^2) \\
    \mu_i & = \alpha_{j[i]}  \\
    \alpha_j & \sim  \mathcal{N}(\mu_\alpha,\sigma_\alpha^2) \\
    \sigma & \sim  P(\sigma) \\
    \mu_\alpha & \sim  P(\mu_\alpha) \\
    \sigma_\alpha & \sim  P(\sigma_\alpha) \\
\end{align*}

##

$\mu_\alpha$ y $\sigma_\alpha$ son hiperparámetros (parámetros de la distribución de _a priori_ de los parámetros) y por lo tanto tienen _hiperpriors_

El chiste es que _todos_ los datos se usan para estimar $\mu_\alpha$ y $\sigma_\alpha$ y por lo tanto en la estimación de cada $\alpha_j$ hay información de todos los datos. La regularización es adaptativa (se aprende de los datos).

## Shrinkage

1. Siempre que hay regularización, hay _shrinkage_ de parámetros.
1. Los datos de un grupo ayudan en la estimación de los parámetros de los otros grupos (_partial pooling_: préstamo de información).
1. Así, los grupos que tienen menor tamaño de muestra toman más información del resto de los grupos y el _shrinkage_ es más intenso.

## {.smaller}

1. _Pooling completo_: hay una única media para todos los individuos, independientemente del grupo. La variación entre los grupos es cero. _Underfitting_.
1. _No pooling_: cada grupo tiene una media independiente de la de los demás. La variación entre los grupos es infinita. No se comparte información entre los grupos, lo que se sabe de un grupo no ayuda a inferir sobre los demás. _Overfitting_.
1. _Partial pooling_: cada grupo tiene una media pero todas las medias están conectadas. Es una solución de compromiso, un punto medio entre _pooling completo_ y _no pooling_.

## {.smaller}

Para algunos condados: a la izquierda está la estimación de la media _no pooling_ de la media, a la derecha la estimación del modelo multinivel (_pooling parcial_). En línea de trazos el _pooling completo_.

![](imgs/pooling_intercept_algunos.png)

## Otro ejemplo

Corredores que han participado varias veces de una famosa maratón en Washington. Se registraron los tiempos de los participantes.

```{r}
  #| warning: false
  #| echo: false
  #| fig-width: 8
  #| fig-height: 8
  #| fig-align: center

cherry <- read.csv("https://raw.githubusercontent.com/estadisticaunr/estadistica-bayesiana/main/datos/cherry_blossom.csv")
```

```{r}
  #| warning: false
  #| echo: false
  #| fig-width: 8
  #| fig-height: 8
  #| fig-align: center

cherry %>%
  ggplot(aes(x=runner, y=net)) +
  geom_boxplot(aes(group=runner), fill="gray", alpha=0.2) +
  geom_jitter(width = 0.05, size=1.5)
```

##

```{r}
# dv <- cherry %>%
#   group_by(runner) %>%
#   nest() %>%
#   mutate(coefs = purrr::map_df(data, ~lm(net ~ age, data = .x)$coefficients)) %>%
#   select(-data) %>%
#   ungroup() %>%
#   unnest_wider(coefs) %>%
#   setNames(c("runner","intercept","slope"))

ggplot() +
  geom_jitter(data = cherry, aes(x=age, y=net),width=0.1) +
  geom_smooth(data = cherry, 
              aes(x=age, y=net, group=runner), 
              method = "lm",
              size = 0.6,
              col = "gray50",
              se=FALSE) +
  geom_smooth(data = cherry, 
              aes(x=age, y=net), 
              method = "lm",
              size = 1.2,
              se=FALSE) +
  xlim(c(50,61))
  
```
##

![](imgs/no_pooling_slope.png){fig-align="center"}

##

![](imgs/partial_pooling_slope.png){fig-align="center"}

##

- La información de la estimación de la pendiente de un grupo es útil para estimar las otras pendientes
- La información de la estimación de las ordenadas al origen de un grupo es útil para estimar las otras ordenadas al origen
- Las pendientes y las ordenadas al origen trabajan de forma conjunta para describir a un corredor, covarían

##

\begin{align*}
    Y_i  \mid \mu_i,\sigma & \sim  \mathcal{N}(\mu_i,\sigma^2) \\
    \mu_i & = \alpha_{j[i]} + \beta_{j[i]} x_i \\
    \left[\begin{array}{c}\alpha_j\\\beta_j\end{array}\right] & \sim \mathcal{N}\left(\left[\begin{array}{c}\mu_\alpha\\\mu_\beta\end{array}\right],\Sigma\right)
\end{align*}

## {.smaller}

$\left[\begin{array}{c}\alpha_j\\\beta_j\end{array}\right]$ tienen una distribución \textit{a priori} conjunta, normal multivariada de hiperparámetros $\left[\begin{array}{c}\mu_\alpha\\\mu_\beta\end{array}\right]$ y $\Sigma$. ¡Necesitan \textit{hiperpriors}!

. . .

$\Sigma$ puede factorizarse según: \pause

$$
\Sigma = \left( \begin{array}{cc} \sigma_\alpha^2 & \sigma_\alpha\sigma_\beta\rho \\ \sigma_\alpha\sigma_\beta\rho & \sigma_\beta^2 \end{array}\right) = 
\left( \begin{array}{cc} \sigma_\alpha & 0 \\ 0 & \sigma_\beta \end{array}\right) \left( \begin{array}{cc} 1 & \rho \\ \rho & 1 \end{array}\right)
\left( \begin{array}{cc} \sigma_\alpha & 0 \\ 0 & \sigma_\beta \end{array}\right)
$$

Llamando
$$R = \left( \begin{array}{cc} 1 & \rho \\ \rho & 1 \end{array}\right)$$
solo habría que definir una distribución \textit{a priori} para $\sigma_\alpha$, $\sigma_\beta$ y $R$ (o $\rho$)


## {.smaller}

No es solo para modelos lineales... Se tienen 60 tanques con ranitas de la especie _Hyperolius viridiflavus_. Cada tanque $i$ de ellos contiene una cantidad inicial de renacuajos $N_i$. Al cabo de unas semanas se observa el número $S_i$ de renacuajos que sobrevivieron en el tanque $i$.

<center>
![](imgs/frog.jpg){width=300}
</center>

##

Se modeliza la probabilidad de supervivencia de cada tanque con una regresión logística:

\begin{align*}
    S_i  \mid p_i & \sim  Binomial(N_i,p_i) \\
   \log\left( \frac{p_i}{1-p_i} \right) & = \alpha_{i} \\
   \alpha_i & \sim \mathcal{N}(\mu_\alpha,\sigma_\alpha^2)
\end{align*}

Observar que no hay grupos (no hay índice $j$), simplemente hacemos \textit{pooling} de los individuos. 

## {.smaller}

Comparamos la estimación de $p_i = \frac{e^{\alpha_i}}{1+e^{\alpha_i}}$ con la obtenida por máxima verosimilitud en cada tanque: $p_{i,ML} = \frac{S_i}{N_i}$

![](imgs/pooling_ranas.png){fig-align="center"}

## Intercambiabilidad

Si no existe otra información más que los datos observados $y_i$ para distinguir a los individuos $i$ y estos no pueden ordenarse ninguna manera entonces se puede asumir una simetría de los parámetros. Se dice que los parámetros $\theta_i$ son **intercambiables** (_exchangeable_). 

Si las observaciones pueden agruparse y los grupos son _indistinguibles_ (con características propias desconocidas) con propiedades/particularidades ignoradas entonces los grupos son **intercambiables** y los individuos, **parcialmente** o **condicionalmente intercambiables**.

## Distribuciones predictivas {.smaller}

En los modelos jerárquicos hay _dos tipos_ de distribuciones predictivas _a posteriori_:

-   Predicciones para individuos que pertenecen a grupos ya existentes (tiradas de la moneda con la que se realizaron las inferencias, otra tarea de memoria para un individuo que ya participó del experimento)
-   Predicciones para individuos pertenecientes a grupos nuevos (tiradas de una nueva moneda de la fábrica, cómo afectaría la droga a un individuo nuevo)

## Resumen {.smaller}

* Los modelos jerárquicos resultan atractivos para problemas en los cuales los parámetros se pueden considerar vinculados de cierta forma, por ejemplo en grupos.
* Los modelos jerárquicos o multinivel son extensiones de los modelos lineales (y de los modelos lineales generalizados) para datos que tienen algún grado de agrupamiento y en los cuales se permite que los parámetros varíen por grupo 
* Los modelos multinivel permiten mejorar las inferencias en contextos donde la muestra es pequeña. Si un individuo que tiene pocas observaciones pertenece a un determinado grupo, se supone que compartirá características con otros individuos de ese grupo y por lo tanto la estimación de sus parámetros podrá ser informada por la de sus pares.



