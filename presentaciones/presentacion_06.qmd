---
format:
  revealjs:
    incremental: true
    logo: /utils/imgs/logo.png
    footer: "Estadística Bayesiana -- 2023"
    background-transition: fade
    transition: fade
    slide-number: true
    fig-cap-location: top
    theme: [default, style_presentaciones.scss]
---

# Modelos Lineales

```{r setup}
#| echo: false
#| include: false

library(dplyr)
library(tidyr)
library(ggplot2)
library(patchwork)

theme_set(theme_bw() +
          theme(legend.position = "none",
                panel.grid = element_blank(),
                #strip.background = element_rect(fill="#ade658"),
                text = element_text(size=26)))

calc_likelihood <- function(data, w0, w1, sd){
  lik_i <- dnorm(data$y, w0+w1*data$x, sd)
  return(prod(lik_i))
}

calc_squared_error <- function(data, w0, w1){
  e_i <- (data$y - (w0+w1*data$x))^2
  return(sum(e_i))
}

set.seed(610)

w0 <- -1
w1 <- 2
sd <- 0.8

data <- tibble(x = runif(50, -3, 3),
       y = rnorm(50, mean = w0 + w1*x, sd = 0.8))
```

## Optimización

Para modelizar la relación entre una variable dependiente $y$ y ciertos predictores $x_l$ asumimos un modelo de la forma 

$$y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \dots + \beta_p x_p + \eta$$

En realidad tenemos $N$ observaciones y por lo tanto para cada observación $(y_i,\mathbf{x}_i)$ tenemos

$$y_i = \beta_0 + \beta_1 x_{1_i} + \beta_2 x_{2_i} + \dots + \beta_p x_{p_i} + \eta$$

O bien, matricialmente

$$y_i = \boldsymbol{\beta}^T \mathbf{x}_i + \eta$$

El error $\eta$ es desconocido y, en principio, no es necesario asumir nada sobre este.

##

Para predecir valores de $y_i$ es necesario estimar $\boldsymbol{\beta}$ por $\hat{\boldsymbol{\beta}}$ dando lugar al siguiente modelo predictivo:

$$\hat{y}_i = \hat\beta_0 + \hat\beta_1 x_{1_i} + \hat\beta_2 x_{2_i} + \dots + \hat\beta_p x_{p_i} = \hat{\boldsymbol{\beta}}^T \mathbf{x}_i$$

Una forma de estimar $\boldsymbol{\beta}$ es minimizar alguna función del error de aproximar $y$ por $\hat{y}_i$:

$$\hat{\boldsymbol{\beta}} = \underset{\boldsymbol\beta}{\mathrm{arg\,min}}\left[ J(\boldsymbol\beta) \right] = \underset{\boldsymbol\beta}{\mathrm{arg\,min}}\left[ \sum_{i=1}^N \left(y_i - \boldsymbol\beta^T \mathbf{x}_i\right)^2 \right]$$

El $\boldsymbol{\beta}$ que minimiza el error cuadrático se conoce como **estimador de mínimos cuadrados**. Esto es lo que se conoce como enfoque de optimización.

## 

```{r minimos_cuadrados}
#| warning: false
#| echo: false
#| fig-width: 20
#| fig-height: 10
#| fig-align: center

datos <-
ggplot(data) +
  geom_point(aes(x=x, y=y)) +
  geom_abline(slope = 2, intercept = 1) + # slope es w1, intercept es w0
  geom_abline(slope = -0.5, intercept = -2) +
  geom_abline(slope = 2, intercept = -1)

parameters <-
  expand.grid(w0 = seq(w0-5,w0+6,length.out = 100),
              w1 = seq(w1-8,w1+3,length.out = 100))

dv <- 
data %>% 
  tidyr::crossing(parameters) %>% 
  nest(data = c(x,y)) %>%
  mutate(error = purrr::pmap_dbl(list(data = data, w0 = w0, w1 = w1),
                                      calc_squared_error)) %>%
  select(-data)

lsq <-
ggplot(dv) +
  geom_raster(aes(x=w0, y=w1, fill=error)) +
  scale_x_continuous(expand = c(0,0)) +
  scale_y_continuous(expand = c(0,0)) +
  viridis::scale_fill_viridis() +
  geom_point(x = 1, y = 2, shape = 4, size = 1.5, col = "white") + # x es w0, y es w1
  geom_point(x = -2, y = -0.5, shape = 4, size = 1.5, col = "white") +
  geom_point(x = -1, y = 2, shape = 4, size = 1.5, col = "white") +
  xlab(expression(beta[0])) +
  ylab(expression(beta[1]))

datos + lsq + plot_layout(design = "AAB")
```

## Estadística clásica {.smaller}

Asumiendo un modelo probabilístico para el error, $\eta \sim \mathcal{N}(0,\sigma^2)$ se puede obtener el estimador de máxima verosimilitud de $\boldsymbol\beta$.

La función de verosimilitud viene dada por el producto de las funciones de densidad normales:

$$\ell(\boldsymbol\beta,\sigma|\mathbf{y}) = \prod_{i=1}^N p(y_i|\mathbf{x_i},\boldsymbol\beta^T,\sigma^2) = \prod_{i=1}^N \frac{1}{\sqrt{2\pi\sigma}} e^{-\frac{(y_i - \boldsymbol\beta^T\mathbf{x}_i)^2}{2\sigma^2}}$$

Maximizar la verosimilitud equivale a minimizar el opuesto de la log-verosimilitud $\mathcal{L}(\boldsymbol\beta,\sigma|\mathbf{y}) = \log(\ell(\boldsymbol\beta,\sigma|\mathbf{y}))$

$$\hat{\boldsymbol{\beta}}_{ML} = \underset{\boldsymbol\beta}{\mathrm{arg\,min}}\left[ - \sum_{i=1}^N \left( \frac{1}{2\pi\sigma^2} \right)^{1/2} e^{-\frac{(y_i - \boldsymbol\beta^T\mathbf{x}_i)^2}{2\sigma^2}} \right]$$

La expresión anterior puede minimizarse primero respecto de $\boldsymbol\beta$ y luego respecto de $\sigma$. Resulta que maximizar la verosimilitud respecto de $\boldsymbol\beta$ equivale a minimizar el error cuadrático.

## Estadística bayesiana {.smaller}

En estadística bayesiana, consideramos a los parámetros como variables aleatorias y les asignamos una distribución _a priori_.

Además, contamos con un modelo generativo (probabilístico) para las observaciones: ¿cómo obtendríamos observaciones si conociéramos los datos? Es una decisión de la modelización.

Aquí asumimos:

$$Y_i \mid \boldsymbol\beta,\sigma \sim \mathcal{N}(\boldsymbol\beta^T \mathbf{x}_i, \sigma^2)$$

o bien decimos

$$
\begin{align*}
    Y_i & \sim  \mathcal{N}(\mu_i, \sigma^2) \\
    \mu_i & = \beta_0 + \beta_1 x_{1_i} + \beta_2 x_{2_i} + \dots + \beta_p x_{p_i}
\end{align*}
$$

Y completamos el modelo especificando una distribución _a priori_ $p(\boldsymbol\beta,\sigma)$

##

La estimación se hace siempre de la misma manera

$p(\boldsymbol\beta,\sigma\mid\mathbf{y}) \propto p(\mathbf{y}|\boldsymbol\beta,\sigma)p(\boldsymbol\beta,\sigma)$

## 

Observando un dato...

```{r bayesian1}
#| warning: false
#| echo: false
#| fig-width: 22
#| fig-height: 10
#| fig-align: center

parameters <-
  expand.grid(w0 = seq(w0-3,w0+4,length.out = 100),
              w1 = seq(w1-5,w1+3,length.out = 100)) |>
  mutate(prior = purrr::map2_dbl(w0, w1, ~mvtnorm::dmvnorm(x = c(.x,.y), 
                                                           mean = c(0,0), 
                                                           sigma = diag(1,2)*1.5)))

dv1 <-
data %>% 
  slice(1:1) %>% 
  tidyr::crossing(parameters) %>% 
  nest(data = c(x,y)) %>%
  mutate(likelihood = purrr::pmap_dbl(list(data = data, w0 = w0, w1 = w1, sd = 0.8),
                         calc_likelihood)) %>%
  mutate(posterior = likelihood*prior) %>%
  select(-data)

prior <- 
  ggplot(data = dv1,
       aes(x=w0, y=w1, fill=prior)) +
  geom_raster() +
  scale_x_continuous(expand = c(0,0)) +
  scale_y_continuous(expand = c(0,0)) + 
  stat_contour(aes(x = w0, y = w1, z = prior), col = "white") +
  viridis::scale_fill_viridis() +
  xlab(expression(beta[0])) +
  ylab(expression(beta[1]))

likelihood <- 
  ggplot(data = dv1,
         aes(x=w0, y=w1, fill=likelihood)) +
  geom_raster() +
  scale_x_continuous(expand = c(0,0)) +
  scale_y_continuous(expand = c(0,0)) +
  stat_contour(aes(x = w0, y = w1, z = likelihood), col = "white") +
  viridis::scale_fill_viridis() +
  xlab(expression(beta[0])) +
  ylab(expression(beta[1]))

posterior <- 
  ggplot(data = dv1,
         aes(x=w0, y=w1, fill=posterior)) +
  geom_raster() +
  scale_x_continuous(expand = c(0,0)) +
  scale_y_continuous(expand = c(0,0)) +
  stat_contour(aes(x = w0, y = w1, z = posterior), col = "white") +
  viridis::scale_fill_viridis() +
  xlab(expression(beta[0])) +
  ylab(expression(beta[1]))

prior + likelihood + posterior 
```


##

Observando el dato que sigue...

```{r bayesian2}
#| warning: false
#| echo: false
#| fig-width: 22
#| fig-height: 10
#| fig-align: center

dv1 <-
  data %>% 
  slice(4:4) %>% 
  tidyr::crossing(parameters) %>% 
  nest(data = c(x,y)) %>%
  mutate(likelihood = purrr::pmap_dbl(list(data = data, w0 = w0, w1 = w1, sd = 0.8),
                                      calc_likelihood)) %>%
  mutate(posterior = likelihood*prior) %>%
  select(-data)

likelihood2 <- 
  ggplot(data = dv1,
         aes(x=w0, y=w1, fill=likelihood)) +
  geom_raster() +
  scale_x_continuous(expand = c(0,0)) +
  scale_y_continuous(expand = c(0,0)) +
  stat_contour(aes(x = w0, y = w1, z = likelihood), col = "white") +
  viridis::scale_fill_viridis() +
  xlab(expression(beta[0])) +
  ylab(expression(beta[1]))

posterior2 <- 
  ggplot(data = dv1,
         aes(x=w0, y=w1, fill=posterior)) +
  geom_raster() +
  scale_x_continuous(expand = c(0,0)) +
  scale_y_continuous(expand = c(0,0)) +
  stat_contour(aes(x = w0, y = w1, z = posterior), col = "white") +
  viridis::scale_fill_viridis() +
  xlab(expression(beta[0])) +
  ylab(expression(beta[1]))

posterior + likelihood2 + posterior2
```

##

Observando dos puntos juntos...

```{r bayesian3}
#| warning: false
#| echo: false
#| fig-width: 22
#| fig-height: 10
#| fig-align: center

dv1 <-
  data %>% 
  slice(c(1,4)) %>% 
  tidyr::crossing(parameters) %>% 
  nest(data = c(x,y)) %>%
  mutate(likelihood = purrr::pmap_dbl(list(data = data, w0 = w0, w1 = w1, sd = 0.8),
                                      calc_likelihood)) %>%
  mutate(posterior = likelihood*prior) %>%
  select(-data)

prior <- 
  ggplot(data = dv1,
         aes(x=w0, y=w1, fill=prior)) +
  geom_raster() +
  scale_x_continuous(expand = c(0,0)) +
  scale_y_continuous(expand = c(0,0)) + 
  stat_contour(aes(x = w0, y = w1, z = prior), col = "white") +
  viridis::scale_fill_viridis() +
  xlab(expression(beta[0])) +
  ylab(expression(beta[1]))

likelihood <- 
  ggplot(data = dv1,
         aes(x=w0, y=w1, fill=likelihood)) +
  geom_raster() +
  scale_x_continuous(expand = c(0,0)) +
  scale_y_continuous(expand = c(0,0)) +
  stat_contour(aes(x = w0, y = w1, z = likelihood), col = "white") +
  viridis::scale_fill_viridis() +
  xlab(expression(beta[0])) +
  ylab(expression(beta[1]))

posterior <- 
  ggplot(data = dv1,
         aes(x=w0, y=w1, fill=posterior)) +
  geom_raster() +
  scale_x_continuous(expand = c(0,0)) +
  scale_y_continuous(expand = c(0,0)) +
  stat_contour(aes(x = w0, y = w1, z = posterior), col = "white") +
  viridis::scale_fill_viridis()  +
  xlab(expression(beta[0])) +
  ylab(expression(beta[1]))

prior + likelihood + posterior 
```


## 

Comparemos con un _prior_ más fuerte...

```{r bayesian4}
#| warning: false
#| echo: false
#| fig-width: 22
#| fig-height: 10
#| fig-align: center

parameters <-
  expand.grid(w0 = seq(w0-3,w0+4,length.out = 100),
              w1 = seq(w1-5,w1+3,length.out = 100)) |>
  mutate(prior = purrr::map2_dbl(w0, w1, ~mvtnorm::dmvnorm(x = c(.x,.y), 
                                                           mean = c(0,0), 
                                                           sigma = diag(1,2)*0.5)))

dv1 <-
  data %>% 
  slice(c(1,4)) %>% 
  tidyr::crossing(parameters) %>% 
  nest(data = c(x,y)) %>%
  mutate(likelihood = purrr::pmap_dbl(list(data = data, w0 = w0, w1 = w1, sd = 0.8),
                                      calc_likelihood)) %>%
  mutate(posterior = likelihood*prior) %>%
  select(-data)

prior <- 
  ggplot(data = dv1,
         aes(x=w0, y=w1, fill=prior)) +
  geom_raster() +
  scale_x_continuous(expand = c(0,0)) +
  scale_y_continuous(expand = c(0,0)) + 
  stat_contour(aes(x = w0, y = w1, z = prior), col = "white") +
  viridis::scale_fill_viridis() +
  xlab(expression(beta[0])) +
  ylab(expression(beta[1]))

likelihood <- 
  ggplot(data = dv1,
         aes(x=w0, y=w1, fill=likelihood)) +
  geom_raster() +
  scale_x_continuous(expand = c(0,0)) +
  scale_y_continuous(expand = c(0,0)) +
  stat_contour(aes(x = w0, y = w1, z = likelihood), col = "white") +
  viridis::scale_fill_viridis() +
  xlab(expression(beta[0])) +
  ylab(expression(beta[1]))

posterior <- 
  ggplot(data = dv1,
         aes(x=w0, y=w1, fill=posterior)) +
  geom_raster() +
  scale_x_continuous(expand = c(0,0)) +
  scale_y_continuous(expand = c(0,0)) +
  stat_contour(aes(x = w0, y = w1, z = posterior), col = "white") +
  viridis::scale_fill_viridis()  +
  xlab(expression(beta[0])) +
  ylab(expression(beta[1]))

prior + likelihood + posterior 
```


##

```{r}
#| warning: false
#| echo: false
#| fig-width: 20
#| fig-height: 10
#| fig-align: center

dv1 <-
  data %>% 
  slice(1:10) %>% 
  tidyr::crossing(parameters) %>% 
  nest(data = c(x,y)) %>%
  mutate(likelihood = purrr::pmap_dbl(list(data = data, w0 = w0, w1 = w1, sd = 0.8),
                                      calc_likelihood)) %>%
  mutate(posterior = likelihood*prior) %>%
  select(-data)

rectas_prior <- 
  ggplot(data) +
  geom_point(aes(x=x, y=y)) +
  geom_abline(data = parameters %>%
  slice_sample(n = 50, weight_by = prior), mapping = aes(slope=w1, intercept=w0), alpha = 0.3, size = 1) +
  geom_vline(xintercept = c(-1,2), linetype = "dashed", col = "maroon1")

rectas_posterior <-
  ggplot(data) +
  geom_point(aes(x=x, y=y)) +
  geom_abline(data = dv1 %>%
  slice_sample(n = 50, weight_by = posterior), mapping = aes(slope=w1, intercept=w0), alpha = 0.3, size = 1) +
  geom_vline(xintercept = c(-1,2), linetype = "dashed", col = "maroon1")

rectas_prior + rectas_posterior
```

##

$\mu$ depende de los parámetros (y por supuesto del valor de $x$), por lo que tiene una distribución de probabilidad asociada

```{r}
#| warning: false
#| echo: false
#| fig-width: 20
#| fig-height: 10
#| fig-align: center

x1 <- -1
x2 <- 2

p1 <-
parameters %>%
  slice_sample(n = 5000, weight_by = prior, replace = T) %>%
  mutate(mu_i = w0 + w1*x1) %>%
  ggplot(aes(x=mu_i)) +
  geom_histogram(fill="#FFC099",col="#FFA770") +
  scale_y_continuous(expand = c(0,0,0.1,0)) +
  theme(axis.title.y = element_blank()) +
  xlab(expression(mu*"|x=-1")) +
  xlim(c(-7,5))

p2 <-
parameters %>%
  slice_sample(n = 5000, weight_by = prior, replace = T) %>%
  mutate(mu_i = w0 + w1*x2) %>%
  ggplot(aes(x=mu_i)) +
  geom_histogram(fill="#FFC099",col="#FFA770") +
  scale_y_continuous(expand = c(0,0,0.1,0)) +
  theme(axis.title.y = element_blank()) +
  xlab(expression(mu*"|x=2")) +
  xlim(c(-7,5))

p3 <-
dv1 %>%
  slice_sample(n = 5000, weight_by = posterior, replace = T) %>%
  mutate(mu_i = w0 + w1*x1) %>%
  ggplot(aes(x=mu_i)) +
  geom_histogram(fill="#FFC099",col="#FFA770") +
  scale_y_continuous(expand = c(0,0,0.1,0)) +
  theme(axis.title.y = element_blank()) +
  xlab(expression(mu*"|x=-1")) +
  xlim(c(-7,5))

p4 <-
dv1 %>%
  slice_sample(n = 5000, weight_by = posterior, replace = T) %>%
  mutate(mu_i = w0 + w1*x2) %>%
  ggplot(aes(x=mu_i)) +
  geom_histogram(fill="#FFC099",col="#FFA770") +
  scale_y_continuous(expand = c(0,0,0.1,0)) +
  theme(axis.title.y = element_blank()) +
  xlab(expression(mu*"|x=2")) +
  xlim(c(-7,5))

(p1 + p3) /
  (p2 + p4)

```

##

Por supuesto, las predicciones para $y$ también son probabilísticas...

```{r}
#| warning: false
#| echo: false
#| fig-width: 20
#| fig-height: 10
#| fig-align: center

x1 <- -1
x2 <- 2

p3 <-
dv1 %>%
  slice_sample(n = 5000, weight_by = posterior, replace = T) %>%
  mutate(y_i = rnorm(5000, w0 + w1*x1, sd)) %>%
  ggplot(aes(x=y_i)) +
  geom_histogram(fill="#FFC099",col="#FFA770") +
  scale_y_continuous(expand = c(0,0,0.1,0)) +
  theme(axis.title.y = element_blank()) +
  xlab(expression(hat(y)*"|x=-1")) +
  xlim(c(-7,5))

p4 <-
dv1 %>%
  slice_sample(n = 5000, weight_by = posterior, replace = T) %>%
  mutate(y_i = rnorm(5000, w0 + w1*x2, sd)) %>%
  ggplot(aes(x=y_i)) +
  geom_histogram(fill="#FFC099",col="#FFA770") +
  scale_y_continuous(expand = c(0,0,0.1,0)) +
  theme(axis.title.y = element_blank()) +
  xlab(expression(hat(y)*"|x=2")) +
  xlim(c(-7,5))

(p3 + p4)

```

