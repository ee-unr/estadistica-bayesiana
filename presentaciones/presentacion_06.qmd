# Modelos Lineales

```{r setup}
#| echo: false
#| include: false

library(dplyr)
library(tidyr)
library(ggplot2)
library(patchwork)

theme_set(theme_bw() +
          theme(legend.position = "none",
                panel.grid = element_blank(),
                #strip.background = element_rect(fill="#ade658"),
                text = element_text(size=32)))

calc_likelihood <- function(data, w0, w1, sd){
  lik_i <- dnorm(data$y, w0+w1*data$x, sd)
  return(prod(lik_i))
}

calc_squared_error <- function(data, w0, w1){
  e_i <- (data$y - (w0+w1*data$x))^2
  return(sum(e_i))
}

set.seed(610)

w0 <- -1
w1 <- 2
sd <- 0.8

data <- tibble(x = runif(50, -3, 3),
       y = rnorm(50, mean = w0 + w1*x, sd = 0.8))
```

## Optimización {.smaller}

Para modelizar la relación entre una variable dependiente $Y$ y ciertos predictores $X_l$ asumimos un modelo de la forma 

$$Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \dots + \beta_p X_p + \eta$$

En realidad tenemos $N$ observaciones y por lo tanto para cada observación $(y_i,\mathbf{x}_i)$ tenemos

$$y_i = \beta_0 + \beta_1 x_{1_i} + \beta_2 x_{2_i} + \dots + \beta_p x_{p_i} + \eta$$

O bien, matricialmente

$$y_i = \boldsymbol{\beta}^T \mathbf{x}_i + \eta$$

El error $\eta$ es desconocido y, en principio, no es necesario asumir nada sobre este.

## {.smaller}

Para predecir valores de $y_i$ es necesario estimar $\boldsymbol{\beta}$ por $\hat{\boldsymbol{\beta}}$ dando lugar al siguiente modelo predictivo:

$$\hat{y}_i = \hat\beta_0 + \hat\beta_1 x_{1_i} + \hat\beta_2 x_{2_i} + \dots + \hat\beta_p x_{p_i} = \hat{\boldsymbol{\beta}}^T \mathbf{x}_i$$

Una forma de estimar $\boldsymbol{\beta}$ es minimizar alguna función del error de aproximar $y$ por $\hat{y}_i$:

$$\hat{\boldsymbol{\beta}} = \underset{\boldsymbol\beta}{\mathrm{arg\,min}}\left[ J(\boldsymbol\beta) \right] = \underset{\boldsymbol\beta}{\mathrm{arg\,min}}\left[ \sum_{i=1}^N \left(y_i - \boldsymbol\beta^T \mathbf{x}_i\right)^2 \right]$$

El $\boldsymbol{\beta}$ que minimiza el error cuadrático se conoce como **estimador de mínimos cuadrados**. Esto es lo que se conoce como enfoque de optimización.

## 

```{r minimos_cuadrados}
#| warning: false
#| echo: false
#| fig-width: 20
#| fig-height: 10
#| fig-align: center

datos <-
ggplot(data) +
  geom_point(aes(x=x, y=y), size = 3) +
  geom_abline(slope = 2, intercept = 1, size = 1.5) + # slope es w1, intercept es w0
  geom_abline(slope = -0.5, intercept = -2, size = 1.5) +
  geom_abline(slope = 2, intercept = -1, size = 1.5)

parameters <-
  expand.grid(w0 = seq(w0-5,w0+6,length.out = 100),
              w1 = seq(w1-8,w1+3,length.out = 100))

dv <- 
data %>% 
  tidyr::crossing(parameters) %>% 
  nest(data = c(x,y)) %>%
  mutate(error = purrr::pmap_dbl(list(data = data, w0 = w0, w1 = w1),
                                      calc_squared_error)) %>%
  select(-data)

lsq <-
ggplot(dv) +
  geom_raster(aes(x=w0, y=w1, fill=error)) +
  scale_x_continuous(expand = c(0,0)) +
  scale_y_continuous(expand = c(0,0)) +
  viridis::scale_fill_viridis() +
  geom_point(x = 1, y = 2, shape = 4, stroke = 2.5, col = "white") + # x es w0, y es w1
  geom_point(x = -2, y = -0.5, shape = 4, stroke = 2.5, col = "white") +
  geom_point(x = -1, y = 2, shape = 4, stroke = 2.5, col = "white") +
  xlab(expression(beta[0])) +
  ylab(expression(beta[1]))

datos + lsq + plot_layout(design = "AAB")
```

## Estadística clásica {.smaller}

Asumiendo un modelo probabilístico para el error, $\eta \sim \mathcal{N}(0,\sigma^2)$ se puede obtener el estimador de máxima verosimilitud de $\boldsymbol\beta$.

La función de verosimilitud viene dada por el producto de las funciones de densidad normales:

$$\ell(\boldsymbol\beta,\sigma|\mathbf{y}) = \prod_{i=1}^N p(y_i|\mathbf{x_i},\boldsymbol\beta^T,\sigma^2) = \prod_{i=1}^N \frac{1}{\sqrt{2\pi\sigma}} e^{-\frac{(y_i - \boldsymbol\beta^T\mathbf{x}_i)^2}{2\sigma^2}}$$

##

Maximizar la verosimilitud equivale a minimizar el opuesto de la log-verosimilitud $$\mathcal{L}(\boldsymbol\beta,\sigma|\mathbf{y}) = \log(\ell(\boldsymbol\beta,\sigma|\mathbf{y}))$$

$$\hat{\boldsymbol{\beta}}_{ML} = \underset{\boldsymbol\beta}{\mathrm{arg\,min}}\left[ - \sum_{i=1}^N \log \left( \left( \frac{1}{2\pi\sigma^2} \right)^{1/2} e^{-\frac{(y_i - \boldsymbol\beta^T\mathbf{x}_i)^2}{2\sigma^2}} \right) \right]$$

La expresión anterior puede minimizarse primero respecto de $\boldsymbol\beta$ y luego respecto de $\sigma$. Resulta que maximizar la verosimilitud respecto de $\boldsymbol\beta$ equivale a minimizar el error cuadrático.

## Estadística bayesiana {.smaller}

En estadística bayesiana, consideramos a los parámetros como variables aleatorias y les asignamos una distribución _a priori_.

Además, contamos con un modelo generativo (probabilístico) para las observaciones: ¿cómo obtendríamos observaciones si conociéramos los parámetros? Es una decisión de la modelización.

## {.smaller}

Aquí asumimos:

$$Y_i \mid \boldsymbol\beta,\sigma \sim \mathcal{N}(\boldsymbol\beta^T \mathbf{x}_i, \sigma^2)$$

o bien decimos

$$
\begin{aligned}
    Y_i & \sim  \mathcal{N}(\mu_i, \sigma^2) \\
    \mu_i & = \beta_0 + \beta_1 x_{1_i} + \beta_2 x_{2_i} + \dots + \beta_p x_{p_i}
\end{aligned}
$$

Y completamos el modelo especificando una distribución _a priori_ $p(\boldsymbol\beta,\sigma)$

##

La estimación se hace siempre de la misma manera

$p(\boldsymbol\beta,\sigma\mid\mathbf{y}) \propto p(\mathbf{y}|\boldsymbol\beta,\sigma)p(\boldsymbol\beta,\sigma)$

## 

Observando un dato (¿se puede hacer inferencia con un solo punto?)...

```{r bayesian1}
#| warning: false
#| echo: false
#| fig-width: 22
#| fig-height: 10
#| fig-align: center

parameters <-
  expand.grid(w0 = seq(w0-3,w0+4,length.out = 100),
              w1 = seq(w1-5,w1+3,length.out = 100)) |>
  mutate(prior = purrr::map2_dbl(w0, w1, ~mvtnorm::dmvnorm(x = c(.x,.y), 
                                                           mean = c(0,0), 
                                                           sigma = diag(1,2)*1.5)))

dv1 <-
data %>% 
  slice(1:1) %>% 
  tidyr::crossing(parameters) %>% 
  nest(data = c(x,y)) %>%
  mutate(likelihood = purrr::pmap_dbl(list(data = data, w0 = w0, w1 = w1, sd = 0.8),
                         calc_likelihood)) %>%
  mutate(posterior = likelihood*prior) %>%
  select(-data)

post1 <- dv1$posterior

prior <- 
  ggplot(data = dv1,
       aes(x=w0, y=w1, fill=prior)) +
  geom_raster() +
  scale_x_continuous(expand = c(0,0)) +
  scale_y_continuous(expand = c(0,0)) + 
  stat_contour(aes(x = w0, y = w1, z = prior), col = "white", size = 1) +
  viridis::scale_fill_viridis() +
  xlab(expression(beta[0])) +
  ylab(expression(beta[1]))

likelihood <- 
  ggplot(data = dv1,
         aes(x=w0, y=w1, fill=likelihood)) +
  geom_raster() +
  scale_x_continuous(expand = c(0,0)) +
  scale_y_continuous(expand = c(0,0)) +
  stat_contour(aes(x = w0, y = w1, z = likelihood), col = "white", size = 1) +
  viridis::scale_fill_viridis() +
  xlab(expression(beta[0])) +
  ylab(expression(beta[1]))

posterior <- 
  ggplot(data = dv1,
         aes(x=w0, y=w1, fill=posterior)) +
  geom_raster() +
  scale_x_continuous(expand = c(0,0)) +
  scale_y_continuous(expand = c(0,0)) +
  stat_contour(aes(x = w0, y = w1, z = posterior), col = "white", size = 1) +
  viridis::scale_fill_viridis() +
  xlab(expression(beta[0])) +
  ylab(expression(beta[1]))

prior + likelihood + posterior 
```


##

Observando el dato que sigue...

```{r bayesian2}
#| warning: false
#| echo: false
#| fig-width: 22
#| fig-height: 10
#| fig-align: center

dv1 <-
  data %>% 
  slice(4:4) %>% 
  tidyr::crossing(parameters) %>% 
  nest(data = c(x,y)) %>%
  mutate(likelihood = purrr::pmap_dbl(list(data = data, w0 = w0, w1 = w1, sd = 0.8),
                                      calc_likelihood)) %>%
  mutate(posterior = likelihood*post1) %>%
  select(-data)

likelihood2 <- 
  ggplot(data = dv1,
         aes(x=w0, y=w1, fill=likelihood)) +
  geom_raster() +
  scale_x_continuous(expand = c(0,0)) +
  scale_y_continuous(expand = c(0,0)) +
  stat_contour(aes(x = w0, y = w1, z = likelihood), col = "white", size = 1) +
  viridis::scale_fill_viridis() +
  xlab(expression(beta[0])) +
  ylab(expression(beta[1]))

posterior2 <- 
  ggplot(data = dv1,
         aes(x=w0, y=w1, fill=posterior)) +
  geom_raster() +
  scale_x_continuous(expand = c(0,0)) +
  scale_y_continuous(expand = c(0,0)) +
  stat_contour(aes(x = w0, y = w1, z = posterior), col = "white", size = 1) +
  viridis::scale_fill_viridis() +
  xlab(expression(beta[0])) +
  ylab(expression(beta[1]))

posterior + likelihood2 + posterior2
```

##

Observando dos puntos juntos...

```{r bayesian3}
#| warning: false
#| echo: false
#| fig-width: 22
#| fig-height: 10
#| fig-align: center

dv1 <-
  data %>% 
  slice(c(1,4)) %>% 
  tidyr::crossing(parameters) %>% 
  nest(data = c(x,y)) %>%
  mutate(likelihood = purrr::pmap_dbl(list(data = data, w0 = w0, w1 = w1, sd = 0.8),
                                      calc_likelihood)) %>%
  mutate(posterior = likelihood*prior) %>%
  select(-data)

prior1 <- 
  ggplot(data = dv1,
         aes(x=w0, y=w1, fill=prior)) +
  geom_raster() +
  scale_x_continuous(expand = c(0,0)) +
  scale_y_continuous(expand = c(0,0)) + 
  stat_contour(aes(x = w0, y = w1, z = prior), col = "white", size = 1) +
  viridis::scale_fill_viridis() +
  xlab(expression(beta[0])) +
  ylab(expression(beta[1]))

likelihood <- 
  ggplot(data = dv1,
         aes(x=w0, y=w1, fill=likelihood)) +
  geom_raster() +
  scale_x_continuous(expand = c(0,0)) +
  scale_y_continuous(expand = c(0,0)) +
  stat_contour(aes(x = w0, y = w1, z = likelihood), col = "white", size = 1) +
  viridis::scale_fill_viridis() +
  xlab(expression(beta[0])) +
  ylab(expression(beta[1]))

posterior3 <- 
  ggplot(data = dv1,
         aes(x=w0, y=w1, fill=posterior)) +
  geom_raster() +
  scale_x_continuous(expand = c(0,0)) +
  scale_y_continuous(expand = c(0,0)) +
  stat_contour(aes(x = w0, y = w1, z = posterior), col = "white", size = 1) +
  viridis::scale_fill_viridis()  +
  xlab(expression(beta[0])) +
  ylab(expression(beta[1]))

prior1 + likelihood + posterior3
```


## 

Comparemos con un _prior_ más fuerte...

```{r bayesian4}
#| warning: false
#| echo: false
#| fig-width: 22
#| fig-height: 10
#| fig-align: center

parameters <-
  expand.grid(w0 = seq(w0-3,w0+4,length.out = 100),
              w1 = seq(w1-5,w1+3,length.out = 100)) |>
  mutate(prior = purrr::map2_dbl(w0, w1, ~mvtnorm::dmvnorm(x = c(.x,.y), 
                                                           mean = c(0,0), 
                                                           sigma = diag(1,2)*0.5)))

dv1 <-
  data %>% 
  slice(c(1,4)) %>% 
  tidyr::crossing(parameters) %>% 
  nest(data = c(x,y)) %>%
  mutate(likelihood = purrr::pmap_dbl(list(data = data, w0 = w0, w1 = w1, sd = 0.8),
                                      calc_likelihood)) %>%
  mutate(posterior = likelihood*prior) %>%
  select(-data)

prior <- 
  ggplot(data = dv1,
         aes(x=w0, y=w1, fill=prior)) +
  geom_raster() +
  scale_x_continuous(expand = c(0,0)) +
  scale_y_continuous(expand = c(0,0)) + 
  stat_contour(aes(x = w0, y = w1, z = prior), col = "white", size = 1) +
  viridis::scale_fill_viridis() +
  xlab(expression(beta[0])) +
  ylab(expression(beta[1]))

likelihood <- 
  ggplot(data = dv1,
         aes(x=w0, y=w1, fill=likelihood)) +
  geom_raster() +
  scale_x_continuous(expand = c(0,0)) +
  scale_y_continuous(expand = c(0,0)) +
  stat_contour(aes(x = w0, y = w1, z = likelihood), col = "white", size = 1) +
  viridis::scale_fill_viridis() +
  xlab(expression(beta[0])) +
  ylab(expression(beta[1]))

posterior4 <- 
  ggplot(data = dv1,
         aes(x=w0, y=w1, fill=posterior)) +
  geom_raster() +
  scale_x_continuous(expand = c(0,0)) +
  scale_y_continuous(expand = c(0,0)) +
  stat_contour(aes(x = w0, y = w1, z = posterior), col = "white", size = 1) +
  viridis::scale_fill_viridis()  +
  xlab(expression(beta[0])) +
  ylab(expression(beta[1]))

prior + likelihood + posterior4 
```


##

Observando **un punto** (¿se puede hacer inferencia con un solo punto?)

```{r}
#| warning: false
#| echo: false
#| fig-width: 20
#| fig-height: 10
#| fig-align: center

parameters <-
  expand.grid(w0 = seq(w0-3,w0+4,length.out = 100),
              w1 = seq(w1-5,w1+3,length.out = 100)) |>
  mutate(prior = purrr::map2_dbl(w0, w1, ~mvtnorm::dmvnorm(x = c(.x,.y), 
                                                           mean = c(0,0), 
                                                           sigma = diag(1,2)*1.5)))

dv1 <-
  data %>% 
  slice(1) %>% 
  tidyr::crossing(parameters) %>% 
  nest(data = c(x,y)) %>%
  mutate(likelihood = purrr::pmap_dbl(list(data = data, w0 = w0, w1 = w1, sd = 0.8),
                                      calc_likelihood)) %>%
  mutate(posterior = likelihood*prior) %>%
  select(-data)

rectas_prior <- 
  ggplot(data) +
  geom_point(aes(x=x, y=y), size = 3) +
  geom_point(data = data %>% slice(1), aes(x=x, y=y), size=6) +
  geom_abline(data = parameters %>%
  slice_sample(n = 50, weight_by = prior), mapping = aes(slope=w1, intercept=w0), alpha = 0.3, size = 1) +
  geom_vline(xintercept = c(-1,2), linetype = "dashed", col = "maroon1", size = 1)

rectas_posterior <-
  ggplot(data) +
  geom_point(aes(x=x, y=y), size = 3) +
  geom_point(data = data %>% slice(1), aes(x=x, y=y), size=6) +
  geom_abline(data = dv1 %>%
  slice_sample(n = 50, weight_by = posterior), mapping = aes(slope=w1, intercept=w0), alpha = 0.3, size = 1) +
  geom_vline(xintercept = c(-1,2), linetype = "dashed", col = "maroon1", size = 1)

rectas_prior + rectas_posterior
```
##

Observando **diez puntos**

```{r}
#| warning: false
#| echo: false
#| fig-width: 20
#| fig-height: 10
#| fig-align: center

parameters <-
  expand.grid(w0 = seq(w0-3,w0+4,length.out = 100),
              w1 = seq(w1-5,w1+3,length.out = 100)) |>
  mutate(prior = purrr::map2_dbl(w0, w1, ~mvtnorm::dmvnorm(x = c(.x,.y), 
                                                           mean = c(0,0), 
                                                           sigma = diag(1,2)*1.5)))

dv1 <-
  data %>% 
  slice(1:10) %>% 
  tidyr::crossing(parameters) %>% 
  nest(data = c(x,y)) %>%
  mutate(likelihood = purrr::pmap_dbl(list(data = data, w0 = w0, w1 = w1, sd = 0.8),
                                      calc_likelihood)) %>%
  mutate(posterior = likelihood*prior) %>%
  select(-data)

rectas_prior <- 
  ggplot(data) +
  geom_point(aes(x=x, y=y), size = 3) +
  geom_point(data = data %>% slice(1:10), aes(x=x, y=y), size=6) +
  geom_abline(data = parameters %>%
  slice_sample(n = 50, weight_by = prior), mapping = aes(slope=w1, intercept=w0), alpha = 0.3, size = 1) +
  geom_vline(xintercept = c(-1,2), linetype = "dashed", col = "maroon1", size = 1)

rectas_posterior <-
  ggplot(data) +
  geom_point(aes(x=x, y=y), size = 3) +
  geom_point(data = data %>% slice(1:10), aes(x=x, y=y), size=6) +
  geom_abline(data = dv1 %>%
  slice_sample(n = 50, weight_by = posterior), mapping = aes(slope=w1, intercept=w0), alpha = 0.3, size = 1) +
  geom_vline(xintercept = c(-1,2), linetype = "dashed", col = "maroon1", size = 1)

rectas_prior + rectas_posterior
```


##

$\mu$ depende de los parámetros (y por supuesto del valor de $x$), por lo que tiene una <span style="color:#FF729F;">distribución de probabilidad asociada</span>

```{r}
#| warning: false
#| echo: false
#| fig-width: 20
#| fig-height: 10
#| fig-align: center

x1 <- -1
x2 <- 2

p1 <-
parameters %>%
  slice_sample(n = 5000, weight_by = prior, replace = T) %>%
  mutate(mu_i = w0 + w1*x1) %>%
  ggplot(aes(x=mu_i)) +
  geom_histogram(fill="#FF729F",col="#FF729F", alpha = 0.7, size = 1) +
  scale_y_continuous(expand = c(0,0,0.1,0)) +
  theme(axis.title.y = element_blank()) +
  xlab(expression(mu*" | x = –1")) +
  xlim(c(-7,7)) +
  coord_cartesian(xlim = c(-7.5,5), ylim = c(0,1400))

p2 <-
parameters %>%
  slice_sample(n = 5000, weight_by = prior, replace = T) %>%
  mutate(mu_i = w0 + w1*x2) %>%
  ggplot(aes(x=mu_i)) +
  geom_histogram(fill="#FF729F",col="#FF729F", alpha = 0.7, size = 1) +
  scale_y_continuous(expand = c(0,0,0.1,0)) +
  theme(axis.title.y = element_blank()) +
  xlab(expression(mu*" | x = 2")) +
  xlim(c(-7,7)) +
  coord_cartesian(xlim = c(-7.5,5), ylim = c(0,1400))

p3 <-
dv1 %>%
  slice_sample(n = 5000, weight_by = posterior, replace = T) %>%
  mutate(mu_i = w0 + w1*x1) %>%
  ggplot(aes(x=mu_i)) +
  geom_histogram(fill="#FF729F",col="#FF729F", alpha = 0.7, size = 1) +
  scale_y_continuous(expand = c(0,0,0.1,0)) +
  theme(axis.title.y = element_blank()) +
  xlab(expression(mu*" | x = –1")) +
  xlim(c(-7,7)) +
  coord_cartesian(xlim = c(-7.5,5), ylim = c(0,1400))

p4 <-
dv1 %>%
  slice_sample(n = 5000, weight_by = posterior, replace = T) %>%
  mutate(mu_i = w0 + w1*x2) %>%
  ggplot(aes(x=mu_i)) +
  geom_histogram(fill="#FF729F",col="#FF729F", alpha = 0.7, size = 1) +
  scale_y_continuous(expand = c(0,0,0.1,0)) +
  theme(axis.title.y = element_blank()) +
  xlab(expression(mu*" | x = 2")) +
  xlim(c(-7,7)) +
  coord_cartesian(xlim = c(-7.5,5), ylim = c(0,1400))

(p1 + p3) /
  (p2 + p4)

```

##

Por supuesto, las predicciones para $y$ ($\tilde{y}$) también son probabilísticas: <span style="color:#FFC099;">distribución predictiva _a posteriori_</span>

```{r}
#| warning: false
#| echo: false
#| fig-width: 20
#| fig-height: 10
#| fig-align: center

x1 <- -1
x2 <- 2

p3 <-
dv1 %>%
  slice_sample(n = 5000, weight_by = posterior, replace = T) %>%
  mutate(y_i = rnorm(5000, w0 + w1*x1, sd)) %>%
  ggplot(aes(x=y_i)) +
  geom_histogram(fill="#FFC099", col="#FFA770", size = 1) +
  scale_y_continuous(expand = c(0,0,0.1,0)) +
  theme(axis.title.y = element_blank()) +
  xlab(expression(tilde(y)*" | x = –1")) +
  xlim(c(-7,7)) +
  coord_cartesian(xlim = c(-7.5,5), ylim = c(0,1000))

p4 <-
dv1 %>%
  slice_sample(n = 5000, weight_by = posterior, replace = T) %>%
  mutate(y_i = rnorm(5000, w0 + w1*x2, sd)) %>%
  ggplot(aes(x=y_i)) +
  geom_histogram(fill="#FFC099", col="#FFA770", size = 1) +
  scale_y_continuous(expand = c(0,0,0.1,0)) +
  theme(axis.title.y = element_blank()) +
  xlab(expression(tilde(y)*" | x = 2")) +
  xlim(c(-7,7)) +
  coord_cartesian(xlim = c(-7.5,5)) +
  coord_cartesian(xlim = c(-7.5,5), ylim = c(0,1000))

(p3 + p4)

```

## Resumen {.smaller}

-   Tenemos una distribución de probabilidad para los parámetros. Es decir, tenemos incertidumbre en los valores de los parámetros
-   Tenemos que trabajar con todo el _posterior_ (a través de muestras) y no con estimaciones puntuales
-   No confundir <span style="color:#FF729F;">predicción de la media</span> (también llamado predictor lineal) con <span style="color:#FFC099;">distribución predictiva</span> (para las observaciones)
-   A medida que aumenta el tamaño de muestra, los coeficientes de la regresión se estiman cada vez con mayor precisión y la incertidumbre
    del <span style="color:#FF729F;">predictor lineal</span> desaparece (incertidumbre epistémica). No obstante, la incertidumbre en la <span style="color:#FFC099;">distribución predictiva</span> no desaparece (siempre quedará $\sigma$: incertidumbre aleatoria).
    
## Validación interna {.smaller}

-   El modo fundamental de validar el ajuste de un modelo bayesiano es generar réplicas del conjunto de datos (utilizando el modelo ajustado) y compararlas con los datos reales. Esto es lo que se conoce como **validación interna**.
-   Para cada muestra de parámetros del _posterior_ podemos generar un dataset
    $$
    \left[
    \begin{array}{l|l|l|l} 
    Y_1^{(1)} & Y_2^{(1)} & \cdots & Y_{N}^{(1)} \\
    Y_1^{(2)} & Y_2^{(2)} & \cdots & Y_{N}^{(2)} \\
    \vdots & \vdots &  & \vdots \\
    Y_1^{(S)} & Y_2^{(S)} & \cdots & Y_{N}^{(S)} \\
    \end{array}
    \right]
    $$
-   Esta práctica da lugar a los _posterior predictive checks_ (PPC)

##

::: {layout-nrow=2}
![](imgs/ppc_wrong.png){.fragment}

![](imgs/ppc_good.png){.fragment}
:::

##

![](imgs/ppc_predictions.png)

## Validación externa

-   Idealmente quisiéramos ver si nuestro modelo tiene capacidad predictiva para datos nuevos (no usados para ajustarlo)
-   Antes de preocuparnos por los datos nuevos, pensemos en las predicciones... Las predicciones son probabilísticas
-   No podemos simplemente comparar $y_i$ con $\hat{y}_i$
-   Debemos utilizar toda la distribución _a posteriori_ para evaluar el ajuste (y la capacidad predictiva) del modelo

##

Un posible _score_ predictivo para un determinado valor $y_i$ es la probabilidad que el modelo le asocia (también llamada densidad predictiva), 
$$\int p\left(y_i\mid\theta\right) p(\theta\mid y) d\theta \approx \frac{1}{S}\sum_{s=1}^S p(y_i\mid \theta^{(s)})$$ 

## 

El _score_ predictivo total (para todas las observaciones) es la _<span style="color:#EB8A90;">log</span>-[posterior]{.underline} <span style="color:#1f7a8c;">pointwise</span> <span style="color:#683257;">predictive density</span>_. A mayor $\mathrm{lppd}$, mejor es el ajuste del modelo.

::: {.content-visible when-format="html"}

$$
\mathrm{lppd} = \color{#1f7a8c}{\sum_{i=1}^{N}} \color{#EB8A90}{\log} \left( \color{#683257}{\int p\left(y_i\mid\theta\right) \underline{p(\theta\mid y)}d\theta} \right)
$$

:::

::: {.content-visible when-format="pdf"}

$$
\mathrm{lppd} = \textcolor{bteal}{\sum_{i=1}^{N}} \textcolor{bpink}{\log} \left( \textcolor{bpurple}{\int p\left(y_i\mid\theta\right) \underline{p(\theta\mid y)}d\theta} \right)
$$

:::


. . .

Que podemos estimar a través de muestras del _posterior_

::: {.content-visible when-format="html"}

$$
\mathrm{lppd} = \color{#1f7a8c}{\sum_{i=1}^{N}} \color{#EB8A90}{\log} \left( \color{#683257}{\frac{1}{S} \sum_{s=1}^{S} p\left(y_i\mid\theta^{(s)}\right)} \right)
$$

:::

::: {.content-visible when-format="pdf"}

$$
\mathrm{lppd} = \textcolor{bteal}{\sum_{i=1}^{N}} \textcolor{bpink}{\log} \left( \textcolor{bpurple}{\frac{1}{S} \sum_{s=1}^{S} p\left(y_i\mid\theta^{(s)}\right)} \right)
$$

:::


## 

La **deviance** de un modelo es

$$D = -2\ \mathrm{lppd}$$

-   La deviance (o la $\mathrm{lppd}$) evalúa las predicciones de un modelo (el ajuste), no nos dice qué tan correcto es...
-   Son medidas que siempre mejoran con más parámetros
-   En realidad nos importa cómo se desempeña el modelo con datos nuevos. 

## {.smaller}

La $\mathrm{lppd}$ predice el $i$-ésimo valor con un _posterior_ que usa todos los datos, incluido el $i$. Más que la $\mathrm{lppd}$ nos interesa su valor esperado en datos nuevos ($\mathrm{elppd}$). Por supuesto, no conocemos datos nuevos. 

. . .

Podemos aproximar o estimar $\mathrm{elppd}$ haciendo _cross-validation_ (CV) o, en particular, _leave-one-out cross-validation_ (LOO-CV).

$$\mathrm{elppd} \approx \mathrm{lppd}_{LOO} = \sum_{i=1}^{N} \log \left( \frac{1}{S} \sum_{s=1}^{S} p\left(y_i\mid\theta_{-i}^{(s)}\right) \right)$$

donde los $\theta_{-i}^{(s)}$ son muestras del _posterior_ de $\theta$ obtenido sin considerar la $i$-ésima observación.

El problema de hacer LOO-CV es que, si tenemos 1000 observaciones, hay que calcular 1000 distribuciones _a posteriori_.

## 

No contamos con muestras de $p(\theta\mid \mathbf{y}_{-i})$ sino simplemente de $p(\theta\mid \mathbf{y})$. No sabemos la distribución _a posteriori_ de $\theta$ sin considerar la observación $i$. Hay formas de aproximar el desempeño en LOO-CV sin necesidad de reajustar el modelo. Una forma de hacerlo es usar la "importancia" de cada observación en el _posterior_. Esto da lugar a una técnica que se conoce como _Pareto-smoothed importance sampling cross-validation_ (PSIS)

## 

Históricamente se han desarrollado los llamados **criterios de información** que penalizan la verosimilitud con un término adicional para compensar la capacidad de sobreajuste de un modelo que tiene más parámetros.

## 

El **AIC** (_Akaike information criterion_) es 
$$AIC = D + 2p = -2\ lppd + 2p$$
donde $p$ es el número de parámetros del modelo y $D=-2\ \mathrm{lppd}$ se conoce como **deviance**. Penalizamos el $\mathrm{lppd}$ con la tendencia (o capacidad) del modelo de sobreajustar. 

## {.smaller}

El **WAIC** (_widely applicable information criterion_) es un criterio más general que el AIC (y un poquitito más difícil de calcular): 

$$WAIC = - 2\left(\mathrm{lppd} - \sum_{i=1}^N \mathbb{V}_\theta\left[\log\left(p(y_i\mid\theta\right)\right]\right)$$

$\sum_{i=1}^N \mathrm{V}_\theta\left[\log\left(p(y_i\mid\theta\right)\right]$ es un término de penalización que se suele llamar "número efectivo de parámetros". Es la suma de las varianzas en la log-probabilidad de cada observación $i$ (o sea, la varianza total). Si, para un determinado dato $i$, las diferentes muestras del _posterior_ $\theta_{(s)}$ dan como resultado predicciones muy diferentes, es porque el modelo tiene mucha incertidumbre (y es posiblemente muy flexible).

<!-- More generally, the adjustment can be thought of as an approximation
to the number of ‘unconstrained’ parameters in the model, where a parameter
counts as 1 if it is estimated with no constraints or prior information, 0 if it is fully constrained
or if all the information about the parameter comes from the prior distribution, or
an intermediate value if both the data and prior distributions are informative. -->
  
