[
  {
    "objectID": "teoria/u1_teoria_02.html",
    "href": "teoria/u1_teoria_02.html",
    "title": "Unidad 1 - Inferencia Bayesiana",
    "section": "",
    "text": "Descargar presentación en PDF",
    "crumbs": [
      "Teoría",
      "U1 - Inferencia Bayesiana"
    ]
  },
  {
    "objectID": "teoria/u2_teoria_04.html",
    "href": "teoria/u2_teoria_04.html",
    "title": "Unidad 2 - Teoría de la Decisión",
    "section": "",
    "text": "Descargar presentación en PDF",
    "crumbs": [
      "Teoría",
      "U2 - Teoría de la Decisión"
    ]
  },
  {
    "objectID": "teoria/u5_teoria_08.html",
    "href": "teoria/u5_teoria_08.html",
    "title": "Unidad 5 - Modelos Jerárquicos",
    "section": "",
    "text": "Descargar presentación en PDF",
    "crumbs": [
      "Teoría",
      "U5 - Modelos Jerárquicos"
    ]
  },
  {
    "objectID": "teoria/u2_teoria_03.html",
    "href": "teoria/u2_teoria_03.html",
    "title": "Unidad 2 - Distribuciones Conjugadas",
    "section": "",
    "text": "Descargar presentación en PDF",
    "crumbs": [
      "Teoría",
      "U2 - Distribuciones Conjugadas"
    ]
  },
  {
    "objectID": "trabajos_practicos/01_tp1.html",
    "href": "trabajos_practicos/01_tp1.html",
    "title": "¿La gloria es el dinero?",
    "section": "",
    "text": "La idea de convertir unos pocos pesos en una fortuna con un solo clic tiene algo de magia, de esa que seduce con la promesa de éxito instantáneo. Masivo. ¿Qué puede salir mal? Total… “Está todo bajo control, yo decido cuándo frenar”. El problema es que, a veces de manera silenciosa, el juego compulsivo puede desencadenar problemas de largo plazo en la salud mental y el funcionamiento social de los individuos.\nLas apuestas en línea han ganado una creciente popularidad entre los adolescentes, impulsadas por la accesibilidad de plataformas digitales y la constante exposición a la publicidad en redes sociales y eventos deportivos. A esto se suma que se trata de una actividad escasamente regulada por el Estado y a la que cualquier persona puede acceder con relativa facilidad.\nEstas apuestas pueden dividirse en dos grandes categorías: los casinos virtuales, que incluyen juegos como ruleta o blackjack, y las apuestas deportivas, donde los jóvenes apuestan sobre el resultado de eventos deportivos, a menudo influenciados por pronósticos y estrategias compartidas en comunidades en línea. Ambas modalidades presentan riesgos relevantes, ya que la facilidad para depositar dinero y la sensación de control pueden llevar a un espiral de pérdidas y nuevas apuestas en busca de recuperar lo perdido.\n\n\n\n\n\nTanto los casinos virtuales como las apuestas deportivas se han vuelto más populares en los últimos años\n\n\n\n\nLa sensación de ganar y perder plata genera adrenalina. La adolescencia, una etapa crucial de desarrollo de los seres humanos, es especialmente susceptible a la promesa del dinero fácil y a la gratificación instantánea que pueden otorgar los juegos de apuestas, debido a la inmadurez de las áreas cerebrales encargadas del control de impulsos, lo que incrementa la vulnerabilidad a comportamientos adictivos. A esto se suman factores sociales como la presión de grupo y la búsqueda de aceptación, donde ganar una apuesta se convierte en una forma de demostrar conocimiento y obtener reputación entre pares.\nLa ilusión de ganar dinero rápido, junto con la normalización de las apuestas a través de la publicidad, crea un cóctel peligroso que atrapa a los jóvenes en un ciclo de riesgo y compulsión. A esto se suma la aparición de gurúes financieros y esquemas piramidales que prometen riqueza rápida, donde quienes no participan son vistos como ingenuos o fracasados. Esta presión social y la promesa de ganancias fáciles refuerzan la participación en un entorno cada vez más riesgoso y manipulador.\nComo si todo esto fuera poco, indagar en un grupo sobre temas sensibles puede resultar complejo. Muchas personas, por vergüenza, miedo al juicio social o simplemente por desconfianza, optan por no responder con sinceridad a ciertas preguntas, por ejemplo en el marco de una encuesta. En el caso de las apuestas, admitir la frecuencia con la que se participa, las cantidades apostadas o las pérdidas sufridas puede generar resistencia a contestar con la verdad. Algunas personas evitan responder, mientras que otras pueden proporcionar respuestas erróneas de manera deliberada. Este fenómeno, conocido como sesgo de respuesta, representa un desafío para quienes buscan obtener datos fiables en encuestas y estudios sobre este tema.\nPartiendo de la premisa de que una forma de incrementar la cooperación de los encuestados es garantizar la protección de información sensible, una posible forma de mitigar este sesgo es la técnica de respuesta aleatorizada, que permite que los encuestados respondan de manera más sincera sin temor a ser identificados. En la técnica de respuesta aleatorizada, se introduce una cuota de azar con el objetivo de preservar la privacidad de la persona que responde.\n\n\n\n\n\nEn la técnica de respuesta aleatorizada, se introduce una cuota de azar al momento de que el encuestado responda\n\n\n\n\nNos centraremos en el problema de querer realizar inferencias sobre \\(\\pi_A\\), la proporción de estudiantes de una escuela que participan de apuestas deportivas en línea.\nEstudiaremos dos técnicas de respuesta aleatorizada. En lugar de de responder directamente a la pregunta “¿Participaste alguna vez en apuestas deportivas en línea?”, se le pide al encuestado que utilice un mecanismo aleatorio, como arrojar una moneda. Dependiendo del resultado de este mecanismo, el encuestado sigue ciertas instrucciones para responder. En las variantes que estudiaremos, por ejemplo, si sale cara, el estudiante contesta a la pregunta “¿Participaste alguna vez en apuestas deportivas en línea?” mientras que si sale cruz, responde a otra pregunta. Así, como el investigador no conoce el resultado del mecanismo aleatorio no puede saber a qué pregunta está respondiendo el encuestado. No obstante, a partir de los resultados obtenidos para toda la muestra, pueden obtenerse conclusiones válidas.\nEn el caso más general, diremos que \\(p\\) es la probabilidad (conocida), debida al mecanismo aleatorio, de que el encuestado responda a la pregunta “¿Participaste alguna vez en apuestas deportivas en línea?” mientras que \\((1-p)\\) es la probabilidad de que responda a la otra pregunta.",
    "crumbs": [
      "Trabajos Prácticos",
      "TP 1: ¿La gloria es el dinero?"
    ]
  },
  {
    "objectID": "trabajos_practicos/01_tp1.html#introducción",
    "href": "trabajos_practicos/01_tp1.html#introducción",
    "title": "¿La gloria es el dinero?",
    "section": "",
    "text": "La idea de convertir unos pocos pesos en una fortuna con un solo clic tiene algo de magia, de esa que seduce con la promesa de éxito instantáneo. Masivo. ¿Qué puede salir mal? Total… “Está todo bajo control, yo decido cuándo frenar”. El problema es que, a veces de manera silenciosa, el juego compulsivo puede desencadenar problemas de largo plazo en la salud mental y el funcionamiento social de los individuos.\nLas apuestas en línea han ganado una creciente popularidad entre los adolescentes, impulsadas por la accesibilidad de plataformas digitales y la constante exposición a la publicidad en redes sociales y eventos deportivos. A esto se suma que se trata de una actividad escasamente regulada por el Estado y a la que cualquier persona puede acceder con relativa facilidad.\nEstas apuestas pueden dividirse en dos grandes categorías: los casinos virtuales, que incluyen juegos como ruleta o blackjack, y las apuestas deportivas, donde los jóvenes apuestan sobre el resultado de eventos deportivos, a menudo influenciados por pronósticos y estrategias compartidas en comunidades en línea. Ambas modalidades presentan riesgos relevantes, ya que la facilidad para depositar dinero y la sensación de control pueden llevar a un espiral de pérdidas y nuevas apuestas en busca de recuperar lo perdido.\n\n\n\n\n\nTanto los casinos virtuales como las apuestas deportivas se han vuelto más populares en los últimos años\n\n\n\n\nLa sensación de ganar y perder plata genera adrenalina. La adolescencia, una etapa crucial de desarrollo de los seres humanos, es especialmente susceptible a la promesa del dinero fácil y a la gratificación instantánea que pueden otorgar los juegos de apuestas, debido a la inmadurez de las áreas cerebrales encargadas del control de impulsos, lo que incrementa la vulnerabilidad a comportamientos adictivos. A esto se suman factores sociales como la presión de grupo y la búsqueda de aceptación, donde ganar una apuesta se convierte en una forma de demostrar conocimiento y obtener reputación entre pares.\nLa ilusión de ganar dinero rápido, junto con la normalización de las apuestas a través de la publicidad, crea un cóctel peligroso que atrapa a los jóvenes en un ciclo de riesgo y compulsión. A esto se suma la aparición de gurúes financieros y esquemas piramidales que prometen riqueza rápida, donde quienes no participan son vistos como ingenuos o fracasados. Esta presión social y la promesa de ganancias fáciles refuerzan la participación en un entorno cada vez más riesgoso y manipulador.\nComo si todo esto fuera poco, indagar en un grupo sobre temas sensibles puede resultar complejo. Muchas personas, por vergüenza, miedo al juicio social o simplemente por desconfianza, optan por no responder con sinceridad a ciertas preguntas, por ejemplo en el marco de una encuesta. En el caso de las apuestas, admitir la frecuencia con la que se participa, las cantidades apostadas o las pérdidas sufridas puede generar resistencia a contestar con la verdad. Algunas personas evitan responder, mientras que otras pueden proporcionar respuestas erróneas de manera deliberada. Este fenómeno, conocido como sesgo de respuesta, representa un desafío para quienes buscan obtener datos fiables en encuestas y estudios sobre este tema.\nPartiendo de la premisa de que una forma de incrementar la cooperación de los encuestados es garantizar la protección de información sensible, una posible forma de mitigar este sesgo es la técnica de respuesta aleatorizada, que permite que los encuestados respondan de manera más sincera sin temor a ser identificados. En la técnica de respuesta aleatorizada, se introduce una cuota de azar con el objetivo de preservar la privacidad de la persona que responde.\n\n\n\n\n\nEn la técnica de respuesta aleatorizada, se introduce una cuota de azar al momento de que el encuestado responda\n\n\n\n\nNos centraremos en el problema de querer realizar inferencias sobre \\(\\pi_A\\), la proporción de estudiantes de una escuela que participan de apuestas deportivas en línea.\nEstudiaremos dos técnicas de respuesta aleatorizada. En lugar de de responder directamente a la pregunta “¿Participaste alguna vez en apuestas deportivas en línea?”, se le pide al encuestado que utilice un mecanismo aleatorio, como arrojar una moneda. Dependiendo del resultado de este mecanismo, el encuestado sigue ciertas instrucciones para responder. En las variantes que estudiaremos, por ejemplo, si sale cara, el estudiante contesta a la pregunta “¿Participaste alguna vez en apuestas deportivas en línea?” mientras que si sale cruz, responde a otra pregunta. Así, como el investigador no conoce el resultado del mecanismo aleatorio no puede saber a qué pregunta está respondiendo el encuestado. No obstante, a partir de los resultados obtenidos para toda la muestra, pueden obtenerse conclusiones válidas.\nEn el caso más general, diremos que \\(p\\) es la probabilidad (conocida), debida al mecanismo aleatorio, de que el encuestado responda a la pregunta “¿Participaste alguna vez en apuestas deportivas en línea?” mientras que \\((1-p)\\) es la probabilidad de que responda a la otra pregunta.",
    "crumbs": [
      "Trabajos Prácticos",
      "TP 1: ¿La gloria es el dinero?"
    ]
  },
  {
    "objectID": "trabajos_practicos/01_tp1.html#técnicas-de-respuesta-aleatorizada",
    "href": "trabajos_practicos/01_tp1.html#técnicas-de-respuesta-aleatorizada",
    "title": "¿La gloria es el dinero?",
    "section": "Técnicas de respuesta aleatorizada",
    "text": "Técnicas de respuesta aleatorizada\n\nWarner\nUn mecanismo aleatorio selecciona la pregunta que se le presenta a la persona que responde. Con probabilidad \\(p\\) se le pregunta si es cierto que alguna vez participó en apuestas deportivas en línea (\\(Q_1\\)), mientras que con probabilidad \\((1-p)\\) se le pregunta si es cierto que nunca participó en apuestas deportivas en línea (\\(Q_2\\)). Es decir, en general, con probabilidad \\(p\\) se le pregunta si pertenece a una categoría \\(A\\), mientras que con probabilidad \\((1-p)\\) se le pregunta si pertenece a la categoría \\(A^c\\) (el complemento de \\(A\\)).\n\n\nGreenberg y otros\nUn mecanismo aleatorio selecciona la pregunta que se le presenta a la persona que responde. Con probabilidad \\(p\\) se le pregunta si es cierto que alguna vez participó en apuestas deportivas en línea (\\(Q_1\\)), mientras que con probabilidad \\((1-p)\\) se le pregunta si nació en un mes de 31 días (\\(Q_2\\)). Es decir, en general, con probabilidad \\(p\\) se le pregunta si pertenece a una categoría \\(A\\), mientras que con probabilidad \\((1-p)\\) se le pregunta si pertenece a una categoría \\(B\\) (no controversial o no polémica).\nExisten dos versiones de esta modalidad, en relación al conocimiento o desconocimiento de \\(\\pi_B\\), la proporción de elementos en la población que pertenece a la categoría \\(B\\). En el primer caso, se conoce de antemano \\(\\pi_B\\) (como en el caso presentado, en que \\(\\pi_B = \\frac{7}{12}\\)). Mientras que en el segundo caso, no se conoce \\(\\pi_B\\). Nos limitaremos al estudio de la primera situación.",
    "crumbs": [
      "Trabajos Prácticos",
      "TP 1: ¿La gloria es el dinero?"
    ]
  },
  {
    "objectID": "trabajos_practicos/01_tp1.html#actividades",
    "href": "trabajos_practicos/01_tp1.html#actividades",
    "title": "¿La gloria es el dinero?",
    "section": "Actividades",
    "text": "Actividades\nPara todo lo que sigue, tenga en consideración lo siguiente:\n\nSe toma una muestra al azar con reemplazo de tamaño \\(n=100\\) de una población de tamaño \\(N=1000\\).\nPara nuestros estudios comparativos, supondremos que el porcentaje de alumnos que apuesta (lo que se quisiera estimar) es 40%.\nCuando se les pregunta directamente si han hecho alguna vez apuestas, los estudiantes que sí han apostado alguna vez mienten con probabilidad \\(\\mu\\).\nSi se utilizan técnicas de respuesta aleatorizada, los estudiantes no mienten.\n\nComenzaremos realizando un estudio de simulación para estudiar el efecto de la mentira en las estimaciones. En esta primera aproximación, consideraremos que se realiza la pregunta directa.\n\nProponga un modelo bayesiano que, a partir de encuestar a \\(n\\) estudiantes, permita estimar \\(\\pi_a\\). Explique la elección de la función de verosimilitud y el prior e indique cómo se obtiene el posterior.\nUtilizando R, simule la obtención de una muestra para el caso en que los estudiantes no mienten y para el caso en que los estudiantes mienten con tres niveles de mentira \\(\\mu\\) bajo, medio y alto. Compare los resultados de la inferencia.\nRealice ahora 1000 simulaciones y compare los resultados de las inferencias.\n\nConsidere ahora el caso del método propuesto por Warner:\n\nSegún este método, ¿cuál es la probabilidad (llamémosla \\(\\lambda_W\\)) de que un estudiante responda afirmativamente? ¿cuál es la probabilidad de que un estudiante responda por la contraria?\nA partir de lo anterior, proponga un modelo razonable sobre cómo se generan los datos.\nConsidere un prior uniforme y halle el posterior exacto.\n\n\n\n\n\n\n\nAyuda\n\n\n\nEscriba el posterior dejando la constante de normalización expresada como \\(Z\\). Es decir, escriba \\[\np(\\pi_a | y) = \\frac{N(\\pi_a)}{Z}\n\\]\nNaturalmente, \\(Z\\) es una integral. Muestre que el resultado de esa integral es: \\[\nZ = \\frac{B(1 - p; y + 1, n - y + 1) - B(p; y + 1, n - y + 1)}{1 - 2p}\n\\] donde \\(B(x; a, b) = \\int_0^x t^{a - 1} (1 - t)^{b - 1} dt\\) es la función beta incompleta, \\(p\\) es la probabilidad con la que se le pregunta si apuesta e \\(y\\) es la cantidad de personas que responden afirmativamente.\nPara resolver la integral de \\(Z\\), utilice el método de sustitución y hágalo en términos de \\(\\lambda_W\\). No es tan terrible como parece.\n\n\n\nGrafique el posterior para diferentes valores de \\(p\\) y concluya. Si quiere evaluar el comportamiento a través de múltiples muestras, puede hacerlo replicando el proceso generador de los datos que planteó en el ítem 5 (no hace falta obtener la muestra aleatoria simple cada vez).\n¿Qué pasaría si el porcentaje de alumnos que apuesta fuera diferente al \\(40\\%\\)? Analice los resultados en función de diferentes niveles de \\(p\\) y \\(\\pi_a\\).\nEscriba una función de R que le permita realizar la inferencia (en forma aproximada) con un prior beta no necesariamente uniforme. Para eso, utilice una grilla de valores de \\(\\pi_a\\)\n\nPara el método propuesto por Greenberg:\n\n¿Cuál es la probabilidad \\(\\lambda_G\\) de que un estudiante responda que sí? ¿y de que responda que no?\nEscriba una función de R que le permita realizar la inferencia (en forma aproximada) con un prior beta no necesariamente uniforme.\n\nPara terminar, es hora de comparar todos los escenarios.\n\nUtilizando R, simule la obtención de una muestra para el caso en que los estudiantes no mienten, mienten con tres niveles de mentira \\(\\mu\\) (bajo, medio y alto), se utiliza el método de Werner (\\(p = 0.3\\)) y se utiliza el método de Greenberg. Compare los resultados de las inferencias en cada caso.\nRealice ahora 1000 simulaciones y analice los resultados.",
    "crumbs": [
      "Trabajos Prácticos",
      "TP 1: ¿La gloria es el dinero?"
    ]
  },
  {
    "objectID": "presentaciones/presentacion_07.html#introducción",
    "href": "presentaciones/presentacion_07.html#introducción",
    "title": "Estadística Bayesiana",
    "section": "Introducción",
    "text": "Introducción\n\nEn un problema de regresión, no siempre la respuesta (condicionada) es normal\nA veces, la respuesta ni siquiera es cuantitativa"
  },
  {
    "objectID": "presentaciones/presentacion_07.html#section",
    "href": "presentaciones/presentacion_07.html#section",
    "title": "Estadística Bayesiana",
    "section": "",
    "text": "Supongamos que nos interesa modelizar las siguientes variables:\n\nSi una persona vota o no por un determinado candidato\nSi un estudiante aprueba o no un examen\nSi mañana lloverá o no\n\nEs decir, nos interesa modelizar una variable \\(Y\\), una variable respuesta categórica binaria:\n\\[Y = \\begin{cases}\n1 \\text{ si mañana llueve} \\\\\n0 \\text{ en caso contrario}\n\\end{cases}\\]\nen función de ciertas variables explicativas potenciales…\nEn otros contextos se habla de un problema de clasificación"
  },
  {
    "objectID": "presentaciones/presentacion_07.html#section-1",
    "href": "presentaciones/presentacion_07.html#section-1",
    "title": "Estadística Bayesiana",
    "section": "",
    "text": "Según los valores que puede tomar \\(Y\\), ¿qué modelo de probabilidad podemos asumir?\n\\[Y_i \\mid \\pi_i \\sim \\mathrm{Bern}(\\pi_i)\\]\n\\[\\mathbb{E}(Y_i) = \\pi_i\\]"
  },
  {
    "objectID": "presentaciones/presentacion_07.html#section-2",
    "href": "presentaciones/presentacion_07.html#section-2",
    "title": "Estadística Bayesiana",
    "section": "",
    "text": "En la regresión normal que conocíamos, teníamos\n\\[Y_i \\mid \\mu_i \\sim \\mathcal{N}(\\mu_i,\\sigma^2)\\]\n\\[\\mathbb{E}(Y_i) = \\mu_i\\]\nPor analogía, ¿podemos hacer \\(\\pi_i = \\beta_0 + \\beta_1 x_{1_i} + \\beta_2 x_{2_i} + \\dots\\)?\n¿Qué problemas identificamos?"
  },
  {
    "objectID": "presentaciones/presentacion_07.html#section-3",
    "href": "presentaciones/presentacion_07.html#section-3",
    "title": "Estadística Bayesiana",
    "section": "",
    "text": "Tendremos que hacer\n\\[g(\\pi_i) = \\beta_0 + \\beta_1 x_{1_i} + \\beta_2 x_{2_i} + \\dots\\]\n\\(g(\\cdot)\\) se conoce como función de enlace (link function). ¿Cuál es una \\(g\\) apropiada en este caso?"
  },
  {
    "objectID": "presentaciones/presentacion_07.html#section-4",
    "href": "presentaciones/presentacion_07.html#section-4",
    "title": "Estadística Bayesiana",
    "section": "",
    "text": "Si \\(\\pi_i\\) es la probabilidad del evento de interés, \\(\\frac{\\pi_i}{1-\\pi_i}\\) es la chance (odds) del evento de interés.\nMientras que \\(\\pi_i \\in [0,1]\\), \\(\\frac{\\pi_i}{1-\\pi_i} \\in [0,+\\infty)\\)"
  },
  {
    "objectID": "presentaciones/presentacion_07.html#section-5",
    "href": "presentaciones/presentacion_07.html#section-5",
    "title": "Estadística Bayesiana",
    "section": "",
    "text": "Establecemos un modelo lineal para el log-odds del evento de interés\n\\[\\log(\\mathrm{odds}_i)=\\log\\left( \\frac{\\pi_i}{1-\\pi_i} \\right) = \\beta_0 + \\beta_1 x_{1_i} + \\beta_2 x_{2_i} + \\dots\\]\nLa función \\(g(x) = \\log\\left(\\frac{x}{1-x}\\right)\\) se conoce como función logit. Es una función no lineal."
  },
  {
    "objectID": "presentaciones/presentacion_07.html#section-6",
    "href": "presentaciones/presentacion_07.html#section-6",
    "title": "Estadística Bayesiana",
    "section": "",
    "text": "Analicemos lo que vimos hasta ahora:\n\n¿Cuál es el dominio de la función logit?\n¿Para qué necesitamos la función \\(g(\\cdot)\\)?\n¿Cuál es la relación entre el predictor lineal y la variable respuesta?\n¿Cuál es la distribución de la variable respuesta?"
  },
  {
    "objectID": "presentaciones/presentacion_07.html#section-7",
    "href": "presentaciones/presentacion_07.html#section-7",
    "title": "Estadística Bayesiana",
    "section": "",
    "text": "Consideremos el caso con una sola variable explicativa\n\\[\\log\\left(\\frac{\\pi_i}{1-\\pi_i}\\right) = \\beta_0 + \\beta_1 x_{1_i}\\]\nSe cumple:\n\\[\n\\frac{\\pi_i}{1-\\pi_i} = e^{\\beta_0 + \\beta_1 x_{1_i}} \\qquad\n\\pi_i = \\frac{e^{\\beta_0 + \\beta_1 x_{1_i}}}{1 + e^{\\beta_0 + \\beta_1 x_{1_i}}}\n\\]\n¡La esperanza de la variable respuesta se relaciona de manera no lineal con las variables explicativas!"
  },
  {
    "objectID": "presentaciones/presentacion_07.html#ejemplo",
    "href": "presentaciones/presentacion_07.html#ejemplo",
    "title": "Estadística Bayesiana",
    "section": "Ejemplo",
    "text": "Ejemplo\nConsideremos la siguiente relación\n\\[\\log\\left(\\frac{\\pi_i}{1-\\pi_i}\\right) = -4 + 0.1\\ x_{1_i}\\]\ne imaginemos que \\(\\pi_i\\) es la probabilidad de que llueva el día \\(i\\) y \\(x_{1_i}\\) la humedad a las 9 de la mañana del día anterior al día \\(i\\)."
  },
  {
    "objectID": "presentaciones/presentacion_07.html#section-8",
    "href": "presentaciones/presentacion_07.html#section-8",
    "title": "Estadística Bayesiana",
    "section": "",
    "text": "x1 &lt;- seq(0, 100, length.out = 100)\nbeta0 &lt;- -4\nbeta1 &lt;- 0.1\n\ndata &lt;- tibble(x1 = x1,\n               log_odds = beta0 + beta1*x1,\n               odds = exp(log_odds),\n               pi = odds/(1+odds))"
  },
  {
    "objectID": "presentaciones/presentacion_07.html#interpretación-de-los-coeficientes",
    "href": "presentaciones/presentacion_07.html#interpretación-de-los-coeficientes",
    "title": "Estadística Bayesiana",
    "section": "Interpretación de los coeficientes",
    "text": "Interpretación de los coeficientes\n\\(\\beta_0\\)\n\\(\\beta_0\\) es la log-chance (log-odds) del evento de interés cuando todas las variables explicativas valen 0. \\(e^{\\beta_0}\\) es la chance. En términos del problema: \\(e^{\\beta_0}\\) es la chance de que llueva mañana si la humedad de hoy a las 9 de la mañana es 0."
  },
  {
    "objectID": "presentaciones/presentacion_07.html#section-10",
    "href": "presentaciones/presentacion_07.html#section-10",
    "title": "Estadística Bayesiana",
    "section": "",
    "text": "\\(\\beta_1\\)\n\\(\\beta_1\\) no es el incremento en la probabilidad del evento de interés cuando \\(x_1\\) aumenta en una unidad…\n\n\\(\\mathrm{odds}_x\\) es la chance del evento de interés cuando \\(x_1=x\\)\n\\(\\mathrm{odds}_{x+\\Delta x}\\) es la chance del evento de interés cuando \\(x_1 = x + \\Delta x\\)\n\n\\[\\log (\\mathrm{odds}_x) =\\log\\left( \\frac{\\pi_x}{1-\\pi_x} \\right) = \\beta_0 + \\beta_1 x\\]\n\\[\\log (\\mathrm{odds}_{x+\\Delta x}) =\\log\\left( \\frac{\\pi_{x+\\Delta x}}{1-\\pi_{x+\\Delta x}} \\right) = \\beta_0 + \\beta_1 (x+\\Delta x)\\]"
  },
  {
    "objectID": "presentaciones/presentacion_07.html#section-11",
    "href": "presentaciones/presentacion_07.html#section-11",
    "title": "Estadística Bayesiana",
    "section": "",
    "text": "Entonces\n\\[\\log (\\mathrm{odds}_{x+\\Delta x}) - \\log (\\mathrm{odds}_{x}) = \\beta_1 \\Delta x\\]\n\\[ e^{\\beta_1 \\Delta x} = \\frac{\\mathrm{odds}_{x+\\Delta x}}{\\mathrm{odds}_{x}}\\]\nLa chance del evento de interés aumenta \\(e^{\\beta_1 \\Delta x}\\) veces cuando \\(x_1\\) aumenta en \\(\\Delta x\\) (y el resto de las variables se mantienen constantes). En términos del problema: La chance de que llueva mañana aumenta \\(e^{\\beta_1}\\) veces si la humedad aumenta en 1."
  },
  {
    "objectID": "presentaciones/presentacion_07.html#section-12",
    "href": "presentaciones/presentacion_07.html#section-12",
    "title": "Estadística Bayesiana",
    "section": "",
    "text": "Más en términos del problema:\n\\[e^{\\beta_1} = \\frac{\\mathrm{odds}_{x+1}}{\\mathrm{odds}_{x}} \\Rightarrow \\mathrm{odds}_{x+1} = e^{\\beta_1} \\mathrm{odds}_{x}\\]\n\\[e^{\\beta_1} = 1.11\\]\n\nLa chance de que llueva mañana aumenta \\(1.11\\) veces cuando la humedad a las 9 de la mañana de hoy aumenta en una unidad\nLa chance de que llueva mañana aumenta en un \\(11\\%\\) cuando la humedad a las 9 de la mañana de hoy aumenta en una unidad"
  },
  {
    "objectID": "presentaciones/presentacion_07.html#section-13",
    "href": "presentaciones/presentacion_07.html#section-13",
    "title": "Estadística Bayesiana",
    "section": "",
    "text": "Analicemos juntos el siguiente caso:\n\\[\\log\\left(\\frac{\\pi_i}{1-\\pi_i}\\right) = 1.1 - 0.2\\ \\mathrm{despierto}_{i}\\]\ne imaginemos que \\(\\pi_i\\) es la probabilidad de que un estudiante \\(i\\) apruebe el parcial de Análisis de Datos de Duración \\(i\\) y \\(\\mathrm{despierto}_i\\) la cantidad de horas que el estudiante \\(i\\) estuvo despierto la noche anterior al parcial."
  },
  {
    "objectID": "presentaciones/presentacion_07.html#section-14",
    "href": "presentaciones/presentacion_07.html#section-14",
    "title": "Estadística Bayesiana",
    "section": "",
    "text": "¿Y Bayes?\nLa especificación del modelo se completa con la elección de distribuciones a priori para \\(\\beta_0,\\ \\beta_1,\\ \\dots\\)…\nCada cantidad que dependa de los \\(\\beta_0,\\ \\beta_1,\\ \\dots\\) tendrá una distribución de probabilidad.\nLas predicciones también son probabilísticas."
  },
  {
    "objectID": "presentaciones/presentacion_07.html#section-15",
    "href": "presentaciones/presentacion_07.html#section-15",
    "title": "Estadística Bayesiana",
    "section": "",
    "text": "¿Cómo se estiman \\(\\beta_0,\\ \\beta_1,\\ \\dots\\)? Como siempre. Aplicando la Regla de Bayes. Solo que la verosimilitud ahora es Bernoulli.\n\n\n\n\nEstadística Bayesiana – 2025"
  },
  {
    "objectID": "presentaciones/presentacion_02.html#el-problema-de-las-urnas",
    "href": "presentaciones/presentacion_02.html#el-problema-de-las-urnas",
    "title": "Estadística Bayesiana",
    "section": "El problema de las urnas",
    "text": "El problema de las urnas\n\nSe cuenta con 11 urnas etiquetadas según \\(u = 0,1,\\dots,10\\), que contienen diez bolas cada una. La urna \\(u\\) contiene \\(u\\) bolas azules y \\(10-u\\) bolas blancas. Fede elige una urna \\(u\\) al azar y extrae con reposición \\(N\\) bolas, obteniendo \\(n_A\\) azules y \\(N-n_A\\) blancas. Nico, el amigo de Fede, observa atentamente. Si después de \\(N=10\\) extracciones resulta \\(n_A = 3\\), ¿cuál es la probabilidad de que la urna que Fede está usando sea la \\(u\\)?"
  },
  {
    "objectID": "presentaciones/presentacion_02.html#section",
    "href": "presentaciones/presentacion_02.html#section",
    "title": "Estadística Bayesiana",
    "section": "",
    "text": "La teoría de las probabilidades permite predecir una distribución sobre posibles valores de un resultado dado cierto conocimiento (o estado) del universo: probabilidad hacia adelante\n\nPor el contrario, muchas veces estamos interesados en realizar inferencias sobre el estado del universo a partir de observaciones: probabilidad inversa.\n\n\n\\[p(\\mathcal{H}\\mid E) = \\frac{p(E\\mid\\mathcal{H}) p(\\mathcal{H})}{p(E)}\\]\n\\[p(\\mathcal{H}\\mid E) \\propto p(E\\mid\\mathcal{H}) p(\\mathcal{H})\\]"
  },
  {
    "objectID": "presentaciones/presentacion_02.html#section-1",
    "href": "presentaciones/presentacion_02.html#section-1",
    "title": "Estadística Bayesiana",
    "section": "",
    "text": "Conociendo \\(N\\), si conociéramos \\(u\\) podríamos calcular las probabilidades de los diferentes \\(n_A\\): probabilidad hacia adelante.\n\nAquí observamos un \\(n_A\\) y queremos calcular las probabilidades de los posibles valores de \\(u\\): probabilidad inversa.\n\n\n\\[p(u\\mid n_A, N) = \\frac{p(n_A\\mid u, N)p(u)}{p(n_A\\mid N)}\\]\n\n\\(N\\) es una cantidad fija\n\\(n_A\\) es otra cantidad fija: lo que observamos al realizar el experimento\n\\(u\\) es la cantidad desconocida"
  },
  {
    "objectID": "presentaciones/presentacion_02.html#section-2",
    "href": "presentaciones/presentacion_02.html#section-2",
    "title": "Estadística Bayesiana",
    "section": "",
    "text": "Probabilidad conjunta de las cantidades observables (datos) y cantidades no observables (parámetros):\n\n\\[\np(u,n_A\\mid N) = p(n_A\\mid u, N) p(u)\n\\]\n\n\nPodemos escribir la probabilidad de \\(u\\) condicionada a \\(n_A\\):\n\\[\n\\begin{array}{ccl}\np(u\\mid n_A,N) & = & \\frac{p(u,n_A\\mid N)}{p(n_A\\mid N)} \\\\\n& = & \\frac{p(n_A\\mid u, N) p(u)}{p(n_A\\mid N)}\n\\end{array}\n\\]\n\n\nEs la probabilidad de cada valor de \\(u\\) luego de haber observado \\(n_A = 3\\) bolas azules"
  },
  {
    "objectID": "presentaciones/presentacion_02.html#section-3",
    "href": "presentaciones/presentacion_02.html#section-3",
    "title": "Estadística Bayesiana",
    "section": "",
    "text": "La probabilidad marginal de \\(u\\) es\n\n\\[p(u) = \\frac{1}{11}\\]\nEs la probabilidad inicial de haber tomado la urna \\(u\\)"
  },
  {
    "objectID": "presentaciones/presentacion_02.html#section-4",
    "href": "presentaciones/presentacion_02.html#section-4",
    "title": "Estadística Bayesiana",
    "section": "",
    "text": "La probabilidad de \\(n_A\\) dado \\(u\\) (y \\(N\\)) es:\n\n\\[p(n_A\\mid u,N) = {N \\choose n_A} \\left( \\frac{u}{10} \\right)^{n_A} \\left( 1 - \\frac{u}{10} \\right)^{N-n_A}\\]\n\n\nComo \\(n_A=3\\) es fijo (¡son los datos observados!), \\(p(n_A\\mid u,N)\\) es una función de \\(u\\). Indica qué tan compatibles son los datos observados con los distintos valores de \\(u\\)"
  },
  {
    "objectID": "presentaciones/presentacion_02.html#section-5",
    "href": "presentaciones/presentacion_02.html#section-5",
    "title": "Estadística Bayesiana",
    "section": "",
    "text": "El denominador, \\(p(n_A\\mid N) = p(n_A)\\), es\n\n\\[\n\\begin{array}{ccl}\np(n_A\\mid N) & = & \\sum_u p(u,n_A\\mid N) \\\\\n& = & \\sum_u p(n_A\\mid u, N) p(u) \\\\\n& = & \\frac{1}{11} \\sum_u p(n_A\\mid u, N)\n\\end{array}\n\\]"
  },
  {
    "objectID": "presentaciones/presentacion_02.html#section-6",
    "href": "presentaciones/presentacion_02.html#section-6",
    "title": "Estadística Bayesiana",
    "section": "",
    "text": "Finalmente, la probabilidad de interés \\(p(u\\mid n_A,N)\\) es\n\n\\[\np(u\\mid n_A,N) = \\frac{\\color{#00008B}{p(n_A\\mid u,N)p(u)}}{p(n_A\\mid N)}\n\\]\n\n\n\\[\np(u\\mid n_A,N) = \\color{#00008B}{{N \\choose n_A} \\left( \\frac{u}{10} \\right)^{n_A} \\left( 1 - \\frac{u}{10} \\right)^{N-n_A} \\frac{1}{11}} \\frac{1}{p(n_A\\mid N)}\n\\]\n\n\n\n\\(N\\) es una cantidad fija\n\\(n_A\\) es 3, otra cantidad fija: lo que observamos al realizar el experimento\n\\(u\\) es la cantidad desconocida\n\n\n\n\\(p(u\\mid n_A,N)\\) es una función de \\(u\\): es la credibilidad de los valores de \\(u\\) luego de observar los datos (es decir, condicionada a \\(n_A=3\\))."
  },
  {
    "objectID": "presentaciones/presentacion_02.html#section-7",
    "href": "presentaciones/presentacion_02.html#section-7",
    "title": "Estadística Bayesiana",
    "section": "",
    "text": "Gráficamente…"
  },
  {
    "objectID": "presentaciones/presentacion_02.html#section-8",
    "href": "presentaciones/presentacion_02.html#section-8",
    "title": "Estadística Bayesiana",
    "section": "",
    "text": "Gráficamente…"
  },
  {
    "objectID": "presentaciones/presentacion_02.html#section-9",
    "href": "presentaciones/presentacion_02.html#section-9",
    "title": "Estadística Bayesiana",
    "section": "",
    "text": "Pasamos de una credibilidad a priori antes de observar los datos, a una a posteriori luego de observar \\(n_A = 3\\)"
  },
  {
    "objectID": "presentaciones/presentacion_02.html#intolerancia-al-gluten",
    "href": "presentaciones/presentacion_02.html#intolerancia-al-gluten",
    "title": "Estadística Bayesiana",
    "section": "Intolerancia al gluten",
    "text": "Intolerancia al gluten\n\n¿Pueden las personas alérgicas al gluten distinguir harina común de harina sin gluten en un ensayo ciego? En un experimento, de 35 sujetos, 12 identificaron correctamente la harina común y 23 se equivocaron o no supieron decir de qué harina se trataba.\nIncluso si no hubiera alérgicos al gluten en el experimento, esperaríamos encontrar algunas identificaciones correctas… Basándonos en el número de identificaciones correctas, ¿cuántos de los sujetos son alérgicos al gluten y cuántos estaban adivinando?\n\n\nSupongamos que una persona alérgica al gluten tiene una probabilidad de \\(0.90\\) de detectar la harina común mientras que una persona sin alergia detecta harina común con una probabilidad de \\(0.40\\) (y con una probabilidad de \\(0.6\\) se equivoca o no sabe decir)."
  },
  {
    "objectID": "presentaciones/presentacion_02.html#section-10",
    "href": "presentaciones/presentacion_02.html#section-10",
    "title": "Estadística Bayesiana",
    "section": "",
    "text": "Llamemos:\n\n\n\\(N\\) a la cantidad total de personas en el ensayo\n\\(N_a\\) al número de personas alérgicas al gluten\n\\(\\pi_a\\) a la probabilidad de que un alérgico identifique correctamente\n\\(\\pi_f\\) a la probabilidad de que un no alérgico identifique correctamente\n\\(n_i\\) al número de identificaciones correctas\n\n\n\n¿Cuáles son las cantidades conocidas? ¿Cuáles son las cantidades desconocidas? ¿Cómo es el modelo de probabilidad hacia adelante? ¿Cómo es el problema inverso?"
  },
  {
    "objectID": "presentaciones/presentacion_02.html#section-11",
    "href": "presentaciones/presentacion_02.html#section-11",
    "title": "Estadística Bayesiana",
    "section": "",
    "text": "Conociendo \\(N\\), \\(\\pi_a\\) y \\(\\pi_f\\), si conociéramos \\(N_a\\) podríamos calcular las probabilidades de los diferentes \\(n_i\\): probabilidad hacia adelante\n\nAquí observamos \\(n_i\\) y queremos realizar inferencias sobre \\(N_a\\): probabilidad inversa"
  },
  {
    "objectID": "presentaciones/presentacion_02.html#section-12",
    "href": "presentaciones/presentacion_02.html#section-12",
    "title": "Estadística Bayesiana",
    "section": "",
    "text": "Digamos que a priori cualquier número de \\(N_a\\) es igualmente probable o esperable:\n\n\\[p(N_a) = \\frac{1}{36}\\]"
  },
  {
    "objectID": "presentaciones/presentacion_02.html#section-13",
    "href": "presentaciones/presentacion_02.html#section-13",
    "title": "Estadística Bayesiana",
    "section": "",
    "text": "¿Cómo construimos la verosimilitud de los diferentes valores de \\(N_a\\) \\(p(n_i\\mid N_a)\\)?\n\nPensemos de forma generativa (con el modelo de probabilidad hacia adelante). Imaginemos que conocemos \\(N_a\\) (además de \\(N\\), \\(\\pi_a\\) y \\(\\pi_f\\)), ¿podríamos escribir un programa que simule diferentes valores de \\(n_i\\)?"
  },
  {
    "objectID": "presentaciones/presentacion_02.html#section-14",
    "href": "presentaciones/presentacion_02.html#section-14",
    "title": "Estadística Bayesiana",
    "section": "",
    "text": "El número de identificaciones correctas \\(n_i\\) es la suma de las identificaciones correctas entre los \\(N_a\\) alérgicos (\\(n_{ia}\\)) y los \\(N-N_a\\) no alérgicos (\\(n_{if}\\)). ¿Cuántas identificaciones habrá en cada grupo?\n\n\\[n_{ia} \\sim Bi(N_a,\\pi_a)\\] \\[n_{if} \\sim Bi(N-N_a,\\pi_f)\\] \\[n_i = n_{ia} + n_{if}\\]\n\n\n\nN &lt;- 35\npi_a &lt;- 0.9\npi_f &lt;- 0.4\nN_a &lt;- 10 # lo suponemos conocido para simular\n\nn_ia &lt;- rbinom(1, N_a, pi_a)\nn_if &lt;- rbinom(1, N-N_a, pi_f)\n\nn_i &lt;- n_ia + n_if\n\n\n\nSabríamos calcular las probabilidades de los diferentes valores de \\(n_{ia}\\) y \\(n_{if}\\), ¿no?."
  },
  {
    "objectID": "presentaciones/presentacion_02.html#section-15",
    "href": "presentaciones/presentacion_02.html#section-15",
    "title": "Estadística Bayesiana",
    "section": "",
    "text": "Recordemos que no conocemos \\(N_a\\). En nuestro caso, la verosimilitud de cada valor de \\(N_a\\) es la probabilidad de observar \\(n_i=12\\) para ese valor de \\(N_a\\).\n\n\\[\n\\begin{array}{lll}\np(n_i=12\\mid N_a) & = & p(n_{ia}=0\\mid N_a)p(n_{if}=12\\mid N_a) \\\\\n& & \\quad + p(n_{ia}=1\\mid N_a)p(n_{if}=11\\mid N_a) + \\dots\n\\end{array}\n\\]\n\n\nQueda como ejercicio calcular a mano \\(p(n_i\\mid N_a)\\) o, mejor aún, escribir un programita que calcule \\(p(n_i\\mid N_a)\\)"
  },
  {
    "objectID": "presentaciones/presentacion_02.html#section-16",
    "href": "presentaciones/presentacion_02.html#section-16",
    "title": "Estadística Bayesiana",
    "section": "",
    "text": "Finalmente,\n\\[p(N_a\\mid n_i) = \\frac{p(n_i\\mid N_a) p(N_a)}{p(n_i)}\\]"
  },
  {
    "objectID": "presentaciones/presentacion_02.html#vocabulario-limitado",
    "href": "presentaciones/presentacion_02.html#vocabulario-limitado",
    "title": "Estadística Bayesiana",
    "section": "Vocabulario limitado",
    "text": "Vocabulario limitado\n\nSupongamos que existe un idioma con seis palabras: \\[ \\text{\\{perro, parra, farra, carro, corro, tarro\\}} \\]\n\n\nTodas las palabras son igualmente probables, excepto por ‘perro’, que es \\(\\alpha=3\\) veces más probable que las otras.\nCuando se tipean, un caracter se introduce erróneamente con probabilidad \\(\\pi=0.1\\).\nTodas las letras tienen la misma probabilidad de producir un error de tipeo.\nSi una letra se tipeó mal, la probabilidad de cometer un error en otro caracter no cambia.\nLos errores son independientes a lo largo de una palabra."
  },
  {
    "objectID": "presentaciones/presentacion_02.html#section-18",
    "href": "presentaciones/presentacion_02.html#section-18",
    "title": "Estadística Bayesiana",
    "section": "",
    "text": "¿Cuál es la probabilidad de escribir correctamente ‘tarro’?\n¿Cuál es la probabilidad de tipear ‘cerro’ o ‘curro’ al querer escribir ‘carro’?\nDesarrollar un corrector gramatical para esta lengua: para las palabras tipeadas ‘farra’, ‘birra’ y ‘locos’, ¿cuál es la palabra que se quiso escribir?"
  },
  {
    "objectID": "presentaciones/presentacion_02.html#section-19",
    "href": "presentaciones/presentacion_02.html#section-19",
    "title": "Estadística Bayesiana",
    "section": "",
    "text": "La probabilidad de escribir correctamente ‘tarro’ es \\((1-\\pi)^5\\)\nLa probabilidad de escribir correctamente ‘cerro’ o ‘curro’ al querer escribir ‘carro’ es \\(\\pi (1-\\pi)^4\\)\nAllá vamos…"
  },
  {
    "objectID": "presentaciones/presentacion_02.html#section-20",
    "href": "presentaciones/presentacion_02.html#section-20",
    "title": "Estadística Bayesiana",
    "section": "",
    "text": "Estas son las probabilidades a priori de cada una de las palabras del vocabulario"
  },
  {
    "objectID": "presentaciones/presentacion_02.html#section-21",
    "href": "presentaciones/presentacion_02.html#section-21",
    "title": "Estadística Bayesiana",
    "section": "",
    "text": "Alguien escribe ‘farra’, ¿qué quiso escribir?\n\n¿Qué sería en este caso la verosimilitud?\n\n\nLa verosimilitud de ‘perro’ es qué tan probable es escribir ‘farra’ cuando se quería escribir ‘perro’: \\(p(\\mathrm{farra}\\mid\\mathrm{perro})=\\pi^3(1-\\pi)^2\\)"
  },
  {
    "objectID": "presentaciones/presentacion_02.html#section-23",
    "href": "presentaciones/presentacion_02.html#section-23",
    "title": "Estadística Bayesiana",
    "section": "",
    "text": "Para obtener la probabilidad a posteriori de cada palabra, necesitamos combinar la información a priori con los datos (¿cuáles son los datos?). Aplicamos la Regla de Bayes:\n\\[p(\\mathrm{palabra}\\mid \\mathrm{farra}) = \\frac{p(\\mathrm{farra}\\mid \\mathrm{palabra})p(\\mathrm{palabra})}{p(\\mathrm{farra})}\\]"
  },
  {
    "objectID": "presentaciones/presentacion_02.html#section-25",
    "href": "presentaciones/presentacion_02.html#section-25",
    "title": "Estadística Bayesiana",
    "section": "",
    "text": "La inferencia bayesiana es la realocación de la credibilidad del conjunto de cantidades desconocidas (parámetros) de un modelo, una vez observado un conjunto de datos."
  },
  {
    "objectID": "presentaciones/presentacion_02.html#pequeño-mundo",
    "href": "presentaciones/presentacion_02.html#pequeño-mundo",
    "title": "Estadística Bayesiana",
    "section": "Pequeño mundo",
    "text": "Pequeño mundo\n\nSe desea estimar la proporción de agua que cubre el planeta Tierra. Para ello se arroja hacia arriba un “globo terráqueo antiestrés” y se registra la posición del dedo índice al volver a tomarlo.\nSe arroja el globo 11 veces hacia arriba y se obtiene la siguiente secuencia: \\[TAAATTAATAA\\]"
  },
  {
    "objectID": "presentaciones/presentacion_02.html#section-26",
    "href": "presentaciones/presentacion_02.html#section-26",
    "title": "Estadística Bayesiana",
    "section": "",
    "text": "Llamemos:\n\n\n\\(\\pi\\) a la proporción de agua en el planeta Tierra\n\\(N\\) al número de tiradas\n\\(y\\) al número de veces que salió agua\n\n\n\n\\(\\pi\\) es una cantidad continua entre 0 y 1. Esta vez no la discretizaremos."
  },
  {
    "objectID": "presentaciones/presentacion_02.html#prior",
    "href": "presentaciones/presentacion_02.html#prior",
    "title": "Estadística Bayesiana",
    "section": "Prior",
    "text": "Prior\n\n¿Cómo asignamos una credibilidad a priori para los valores de \\(\\pi_a\\)?\n\n\nCon una distribución de probabilidad.\n\n\n\\[\n\\pi \\sim \\mathrm{Beta}(a,b)\n\\]\n\n\n\\[\np(\\pi\\mid a,b) = p(\\pi) = \\frac{\\pi^{a-1} (1-\\pi)^{b-1}}{B(a,b)}\n\\]\n\n\n\\[\nB(a,b) = \\int_0^1 \\pi^{a-1} (1-\\pi)^{b-1} d\\pi = \\frac{\\Gamma(a)\\Gamma(b)}{\\Gamma(a+b)}\n\\]\n\n\n\\[\n\\Gamma(x) = \\int_0^\\infty u^{x-1} e^{-u} du\n\\]"
  },
  {
    "objectID": "presentaciones/presentacion_02.html#section-27",
    "href": "presentaciones/presentacion_02.html#section-27",
    "title": "Estadística Bayesiana",
    "section": "",
    "text": "La distribución beta"
  },
  {
    "objectID": "presentaciones/presentacion_02.html#section-28",
    "href": "presentaciones/presentacion_02.html#section-28",
    "title": "Estadística Bayesiana",
    "section": "",
    "text": "La distribución beta"
  },
  {
    "objectID": "presentaciones/presentacion_02.html#section-29",
    "href": "presentaciones/presentacion_02.html#section-29",
    "title": "Estadística Bayesiana",
    "section": "",
    "text": "Una posible elección de valores para la distribución a priori es \\(\\text{Beta}(2,2)\\)"
  },
  {
    "objectID": "presentaciones/presentacion_02.html#likelihood",
    "href": "presentaciones/presentacion_02.html#likelihood",
    "title": "Estadística Bayesiana",
    "section": "Likelihood",
    "text": "Likelihood\n¿Cuál es la probabilidad de observar los datos que observamos para diferentes valores del parámetro?\n\n\\[\nY \\mid \\pi, N \\sim Bi(N,\\pi)\n\\]\n\\[p(y\\mid \\pi, N) = {N \\choose y }\\pi^y (1-\\pi)^{N-y} = p(y\\mid \\pi)\\]"
  },
  {
    "objectID": "presentaciones/presentacion_02.html#posterior",
    "href": "presentaciones/presentacion_02.html#posterior",
    "title": "Estadística Bayesiana",
    "section": "Posterior",
    "text": "Posterior\n\\[p(\\pi\\mid y) = \\frac{p(y\\mid \\pi)p(\\pi)}{p(y)}\\]\n\n\\[p(\\pi\\mid y) = \\frac{{N \\choose y }\\pi^y (1-\\pi)^{N-y}  \\frac{\\pi^{a-1} (1-\\pi)^{b-1}}{B(a,b)}}{\\int p(y\\mid\\pi) p(\\pi) d \\pi}\\]\n\n\nLa integral en el denominador suele ser un problema. Con dos parámetros es una integral doble, con tres parámetros, una triple, etc. Esta integral puede ser intratable (intractable) (no tener solución exacta, analítica, cerrada). No hay vaca vestida de uniforme que nos salve."
  },
  {
    "objectID": "presentaciones/presentacion_02.html#section-30",
    "href": "presentaciones/presentacion_02.html#section-30",
    "title": "Estadística Bayesiana",
    "section": "",
    "text": "Recordando que: \\[\n\\mathrm{posterior} \\propto \\mathrm{prior}\\times\\mathrm{likelihood}\n\\]\n\nResulta \\[p(\\pi\\mid y) \\propto p(y\\mid\\pi) p(\\pi)\\]\n\n\n\\[p(\\pi\\mid y) \\propto {N \\choose y }\\pi^y (1-\\pi)^{N-y} \\frac{1}{B(a,b)} \\pi^{a-1}(1-\\pi)^{b-1}\\]\n\n\n\\[p(\\pi\\mid y) \\propto {N \\choose y } \\frac{1}{B(a,b)} \\pi^{(y+a)-1} (1-\\pi)^{(N-y+b)-1}\\]\n\n\n\\[p(\\pi\\mid y) = K C  \\pi^{(y+a)-1} (1-\\pi)^{(N-y+b)-1}\\]\n\n\n\\[p(\\pi\\mid y) = K^* \\pi^{(y+a)-1} (1-\\pi)^{(N-y+b)-1}\\]"
  },
  {
    "objectID": "presentaciones/presentacion_02.html#section-31",
    "href": "presentaciones/presentacion_02.html#section-31",
    "title": "Estadística Bayesiana",
    "section": "",
    "text": "Para que \\(\\int_0^1 p(\\pi\\mid y) d \\pi = 1\\), debe ser\n\n\\[K^* = \\frac{1}{B(y+a,N-y+b)} = \\frac{\\Gamma\\left[(y+a)+(N-y+b)\\right]}{\\Gamma(y+a)\\Gamma(N-y+b)}\\]\n\n\nPor lo tanto, resulta que la distribución a posteriori es Beta de parámetros \\(y+a\\) y \\(N-y+b\\)\n\\[\np(\\pi\\mid y) = \\frac{\\pi^{(y+a)-1}(1-\\pi)^{(N-y+b)-1}}{B(y+a,N-y+b)}\n\\]\n\\[\n\\pi\\mid y \\sim  \\text{Beta}(y+a,N-y+b)\n\\]"
  },
  {
    "objectID": "presentaciones/presentacion_02.html#qué-hicimos",
    "href": "presentaciones/presentacion_02.html#qué-hicimos",
    "title": "Estadística Bayesiana",
    "section": "¿Qué hicimos?",
    "text": "¿Qué hicimos?\n\nNos las arreglamos para encontrar la solución exacta al problema de inferir el parámetro de una distribución binomial a partir del número de éxitos observados.\n\n\nEl prior y el posterior tienen la misma forma distribucional. Esto ocurre por la elección del prior y el likelihood."
  },
  {
    "objectID": "presentaciones/presentacion_02.html#section-32",
    "href": "presentaciones/presentacion_02.html#section-32",
    "title": "Estadística Bayesiana",
    "section": "",
    "text": "Una distribución \\(\\mathcal{F}\\) se dice conjugada de una verosimilitud \\(\\mathcal{L}\\) si cuando la distribución a priori es \\(\\mathcal{F}\\), la distribución a posteriori también es \\(\\mathcal{F}\\)"
  },
  {
    "objectID": "presentaciones/presentacion_02.html#pequeño-mundo-1",
    "href": "presentaciones/presentacion_02.html#pequeño-mundo-1",
    "title": "Estadística Bayesiana",
    "section": "Pequeño mundo",
    "text": "Pequeño mundo\n\nSe desea estimar la proporción de agua que cubre el planeta Tierra. Para ello se arroja hacia arriba un “globo terráqueo antiestrés” y se registra la posición del dedo índice al volver a tomarlo.\nSe arroja el globo 11 veces hacia arriba y se obtiene la siguiente secuencia: \\[TAAATTAATAA\\]\n\n\n\\[\n\\begin{aligned}\n  Y \\mid\\pi & \\sim \\text{Binomial}(N,\\pi)\\\\\n  \\pi & \\sim  \\text{Beta}(a,b)\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "presentaciones/presentacion_02.html#section-33",
    "href": "presentaciones/presentacion_02.html#section-33",
    "title": "Estadística Bayesiana",
    "section": "",
    "text": "con \\(N=11\\), \\(a=2\\) y \\(b=2\\).\nAl observar \\(y=7\\) resulta\n\\[\\pi\\mid y \\sim \\text{Beta}(a+y,b+N-y)\\] \\[p(\\pi\\mid y) = \\text{Beta}(2+7,2+4)\\]"
  },
  {
    "objectID": "presentaciones/presentacion_02.html#más-ejemplos",
    "href": "presentaciones/presentacion_02.html#más-ejemplos",
    "title": "Estadística Bayesiana",
    "section": "Más ejemplos",
    "text": "Más ejemplos\nQueremos estimar la probabilidad \\(\\pi\\) de que salga cara al arrojar una moneda.\n\nCredibilidad a priori: \\(\\text{Beta}(2,2)\\)\n\n\n¿Cómo cambia nuestra creencia si…\n\n…realizamos 6 tiradas y observamos 4 caras?\n…realizamos 60 tiradas y observamos 40 caras?\n…realizamos 2 tiradas y observamos 2 caras?\n…realizamos 40 tiradas y observamos 40 caras?\n…realizamos 4 tiradas y obtenemos 3 caras y luego realizamos 2 tiradas más y observamos 1 caras?"
  },
  {
    "objectID": "presentaciones/presentacion_02.html#section-35",
    "href": "presentaciones/presentacion_02.html#section-35",
    "title": "Estadística Bayesiana",
    "section": "",
    "text": "\\(\\pi \\mid y \\sim \\text{Beta}(2+4,2+2)\\)\n\\(\\pi \\mid y \\sim \\text{Beta}(2+40,2+20)\\)\n\\(\\pi \\mid y \\sim \\text{Beta}(2+2,2+0)\\)\n\\(\\pi \\mid y \\sim \\text{Beta}(2+40,2+0)\\)\n\\(\\pi \\mid y \\sim \\text{Beta}((2+3)+1,(2+1)+1)\\)"
  },
  {
    "objectID": "presentaciones/presentacion_02.html#section-36",
    "href": "presentaciones/presentacion_02.html#section-36",
    "title": "Estadística Bayesiana",
    "section": "",
    "text": "4 caras en 6 tiradas"
  },
  {
    "objectID": "presentaciones/presentacion_02.html#section-37",
    "href": "presentaciones/presentacion_02.html#section-37",
    "title": "Estadística Bayesiana",
    "section": "",
    "text": "40 caras en 60 tiradas"
  },
  {
    "objectID": "presentaciones/presentacion_02.html#section-38",
    "href": "presentaciones/presentacion_02.html#section-38",
    "title": "Estadística Bayesiana",
    "section": "",
    "text": "2 caras en 2 tiradas"
  },
  {
    "objectID": "presentaciones/presentacion_02.html#section-39",
    "href": "presentaciones/presentacion_02.html#section-39",
    "title": "Estadística Bayesiana",
    "section": "",
    "text": "40 caras en 40 tiradas"
  },
  {
    "objectID": "presentaciones/presentacion_02.html#section-40",
    "href": "presentaciones/presentacion_02.html#section-40",
    "title": "Estadística Bayesiana",
    "section": "",
    "text": "3 caras en 4 tiradas, luego 1 cara en 2 tiradas"
  },
  {
    "objectID": "presentaciones/presentacion_02.html#características-generales",
    "href": "presentaciones/presentacion_02.html#características-generales",
    "title": "Estadística Bayesiana",
    "section": "Características generales",
    "text": "Características generales\nLa inferencia bayesiana presenta ciertas características que se repiten independientemente de las distribuciones elegidas."
  },
  {
    "objectID": "presentaciones/presentacion_02.html#compromiso",
    "href": "presentaciones/presentacion_02.html#compromiso",
    "title": "Estadística Bayesiana",
    "section": "Compromiso",
    "text": "Compromiso\nVamos a formalizar lo que observamos en el ejemplo para el modelo Beta–Binomial. Para esto será útil el siguiente resultado:\n\\[\\text{Si } X \\sim \\text{Beta}(a,b)\\]\n\\[\\mathbb{E}({X}) = \\frac{a}{a+b}\\]"
  },
  {
    "objectID": "presentaciones/presentacion_02.html#section-41",
    "href": "presentaciones/presentacion_02.html#section-41",
    "title": "Estadística Bayesiana",
    "section": "",
    "text": "La distribución a priori es \\(\\text{Beta}(a,b)\\) y la distribución a posteriori es \\(\\text{Beta}(y+a,N-y+b)\\). La media del es:\n\\[\n\\begin{aligned}\n\\mathbb{E}[{p(\\pi\\mid y)}] & = \\frac{y+a}{a+b+N} \\\\\n& = \\frac{y}{a+b+N} + \\frac{a}{a+b+N} \\\\\n& = \\frac{N}{a+b+N}\\frac{y}{N} + \\frac{a+b}{a+b+N} \\frac{a}{a+b} \\\\\n& = \\frac{N}{a+b+N}\\frac{y}{N} + \\frac{a+b}{a+b+N} \\mathbb{E}{[p(\\pi)]}\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "presentaciones/presentacion_02.html#section-42",
    "href": "presentaciones/presentacion_02.html#section-42",
    "title": "Estadística Bayesiana",
    "section": "",
    "text": "La distribución a posteriori representa un balance (promedio ponderado o combinación convexa) entre la proporción observada y la proporción esperada a priori. Hay un shrinkage hacia la media del prior."
  },
  {
    "objectID": "presentaciones/presentacion_02.html#secuencialidad",
    "href": "presentaciones/presentacion_02.html#secuencialidad",
    "title": "Estadística Bayesiana",
    "section": "Secuencialidad",
    "text": "Secuencialidad\nSi primero observamos \\(y_1\\) en \\(N_1\\) y luego observamos \\(y_2\\) en \\(N_2\\)… Con el primer conjunto de datos pasamos del prior al posterior y luego esa distribución se convierte en el nuevo prior:\n\n\\[\\text{Beta}(a,b) \\rightarrow \\text{Beta}(y_1 + a, N_1 - y_1 + b)\\]\n\n\n\\[\\text{Beta}(y_1 + a, N_1 - y_1 + b) \\rightarrow \\text{Beta}(y_2 + y_1 + a,N_2 - y_2 + N_1 - y_1 + b)\\]\n\n\n\\[\\text{Beta}(a,b) \\rightarrow \\text{Beta}((y_1+y_2) + a, (N_1+N_2) - (y_1+y_2) + b)\\]\n\n\nEs idéntico a observar \\(y_1+y_2\\) en \\(N_1+N_2\\)"
  },
  {
    "objectID": "presentaciones/presentacion_02.html#section-43",
    "href": "presentaciones/presentacion_02.html#section-43",
    "title": "Estadística Bayesiana",
    "section": "",
    "text": "La Regla de Bayes permite combinar dos fuentes de información: la información a priori (lo que sabemos hasta el momento), y la nueva información (representada por la verosimilitud). La distribución a posteriori representa un compromiso entre la verosimilitud de los datos y la credibilidad a priori."
  },
  {
    "objectID": "presentaciones/presentacion_02.html#predicciones",
    "href": "presentaciones/presentacion_02.html#predicciones",
    "title": "Estadística Bayesiana",
    "section": "Predicciones",
    "text": "Predicciones\nDistribución predictiva a posteriori (también distribución posterior predictiva) (en inglés posterior predictive distribution): queremos predecir un valor futuro de la variable de interés, \\(\\tilde{y}\\). Más aún, interesa la distribución de \\(\\tilde{y}\\) a posteriori, es decir, luego de observar los datos \\(y\\): \\(\\tilde{y}\\mid y\\)\n\\[\np(\\tilde{y}\\mid y) = \\int p(\\tilde{y}\\mid\\pi) p(\\pi\\mid y) d\\pi\n\\]"
  },
  {
    "objectID": "presentaciones/presentacion_02.html#section-53",
    "href": "presentaciones/presentacion_02.html#section-53",
    "title": "Estadística Bayesiana",
    "section": "",
    "text": "\\(\\tilde{y}\\) tiene una distribución de probabilidad\nSi \\(\\pi\\) fuera fijo, la distribución de \\(\\tilde{y}\\) viene dada por \\(p(\\tilde{y}\\mid\\pi)\\) (la verosimilitud, aunque ahora es función de \\(\\tilde{y}\\))\nPero ahora hay incertidumbre en \\(\\pi\\) (tiene una distribución a posteriori), por lo tanto se hace una ponderación para los distintos valores de \\(\\pi\\) (\\(\\pi\\) varía en la integral anterior)\nCombinamos lo que no sabemos porque es aleatorio per se, con aquello que desconocemos (aunque podemos reducir nuestra incertidumbre recolectando más información)"
  },
  {
    "objectID": "presentaciones/presentacion_02.html#section-54",
    "href": "presentaciones/presentacion_02.html#section-54",
    "title": "Estadística Bayesiana",
    "section": "",
    "text": "Para el caso binomial que venimos estudiando, consideramos una realización más (tirar el globo terráqueo y agarrarlo). ¿Cuál es la probabilidad de obtener \\(A\\) (agua)?\n\\[\n\\begin{aligned}\np(\\tilde{y} = 1\\mid y) & = \\int_0^1 \\pi \\vphantom{\\tilde{y}}p(\\pi\\mid x) d \\pi = \\mathbb{E}[{p(\\pi\\mid x)}] \\\\\n& = \\frac{y+a}{y+a+N-y+b} = \\frac{y+a}{N+a+b} \\\\\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "presentaciones/presentacion_02.html#section-55",
    "href": "presentaciones/presentacion_02.html#section-55",
    "title": "Estadística Bayesiana",
    "section": "",
    "text": "muestras_pi &lt;- rbeta(2000,a+y,b+N-y) # muestras del posterior\nx_new &lt;- rbinom(2000,1,muestras_pi) # predicciones para cada valor de pi"
  },
  {
    "objectID": "presentaciones/presentacion_02.html#section-56",
    "href": "presentaciones/presentacion_02.html#section-56",
    "title": "Estadística Bayesiana",
    "section": "",
    "text": "Consideremos un caso particular:\n\nEn una bolsa hay bolitas negras y blancas, queremos saber cuál es la probabilidad de sacar una bolita negra. A priori no sabemos nada. Sacamos (con reposición) tres veces una bolita. Las tres veces sale negra. ¿Cuál es la probabilidad de que la próxima bolita sea negra?\n\n\\[p(\\tilde{y}=1\\mid y) = \\frac{y+1}{N+2}\\]"
  },
  {
    "objectID": "presentaciones/presentacion_02.html#section-57",
    "href": "presentaciones/presentacion_02.html#section-57",
    "title": "Estadística Bayesiana",
    "section": "",
    "text": "Los parámetros tienen una distribución de probabilidad. Incorporar la incertidumbre en el valor de \\(\\pi\\) nos permite no entusiasmarnos tanto con los datos, hacer predicciones más conservadoras con a pocos datos, regularizar.\n\n\n\n\nEstadística Bayesiana – 2025"
  },
  {
    "objectID": "presentaciones/presentacion_08.html#introducción",
    "href": "presentaciones/presentacion_08.html#introducción",
    "title": "Estadística Bayesiana",
    "section": "Introducción",
    "text": "Introducción\nConsideremos el siguiente modelo: \\[\n\\begin{aligned}\n  y\\mid\\pi & \\sim  \\text{Binomial}(N_i,\\pi)\\\\\n  \\pi & \\sim  \\text{Beta}(a,b)\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "presentaciones/presentacion_08.html#section",
    "href": "presentaciones/presentacion_08.html#section",
    "title": "Estadística Bayesiana",
    "section": "",
    "text": "Sabemos (gracias al TP2) que la función de densidad de la distribución beta se puede expresar en términos de su moda \\(\\omega\\) y su concentración \\(\\kappa\\)\n\\[\n\\begin{aligned}\n  y\\mid\\pi & \\sim  \\text{Binomial}(N,\\pi)\\\\\n  \\pi & \\sim  \\text{Beta}(\\omega(\\kappa-2)+1,\\ (1-\\omega)(\\kappa-2)+1)\n\\end{aligned}\n\\]\nEl valor de \\(\\pi\\) depende del valor de \\(\\omega\\). Lo sabíamos, después de todo, \\(\\omega\\) y \\(\\kappa\\) son las constantes de ajuste del prior o hiperparámetros. \\(\\kappa\\) refleja el grado de credibilidad a priori sobre los valores de \\(\\pi\\) (alrededor de \\(\\omega\\))."
  },
  {
    "objectID": "presentaciones/presentacion_08.html#section-1",
    "href": "presentaciones/presentacion_08.html#section-1",
    "title": "Estadística Bayesiana",
    "section": "",
    "text": "¿Qué pasa si \\(\\omega\\) no es fijo sino otro parámetro a estimar?\n\nEn el contexto de una moneda: \\(\\pi\\) es la probabilidad de cara de la moneda y \\(\\omega\\) es el valor de probabilidad de cara al que el fabricante de monedas le apunta en la construcción. \\(\\kappa\\) (fijo y conocido) es una medida de la dispersión que tiene el proceso de fabricación (de lo consistente que es este proceso) o, en otros términos, mide el grado de asociación entre \\(\\pi\\) y \\(\\omega\\).\n\n\n¿Qué necesitamos para \\(\\omega\\)? Digamos que, a priori, \\(\\omega \\sim \\mathrm{Beta}(A_\\omega,B_\\omega)\\)\n\n\n\\[\n\\begin{aligned}\n    y\\mid\\pi & \\sim  \\mathrm{Binomial}(N,\\pi)\\\\\n    \\pi \\mid \\omega & \\sim  \\mathrm{Beta}(\\omega(\\kappa-2)+1,\\ (1-\\omega)(\\kappa-2)+1) \\\\\n    \\omega & \\sim \\mathrm{Beta}(A_\\omega,B_\\omega)\n\\end{aligned}\n\\]\n¿Cuántos parámetros tiene este modelo?"
  },
  {
    "objectID": "presentaciones/presentacion_08.html#section-2",
    "href": "presentaciones/presentacion_08.html#section-2",
    "title": "Estadística Bayesiana",
    "section": "",
    "text": "Es un modelo de dos parámetros (hay una distribución conjunta a priori y una distribución conjunta a posteriori) pero no como el \\(\\mu\\) y el \\(\\sigma\\) de una distribución normal o el \\(\\beta_0\\) y \\(\\beta_1\\) de un modelo de regresión lineal…\n¿Cómo funciona el modelo hacia adelante? \\(\\omega \\rightarrow \\pi \\rightarrow y\\) (\\(\\omega\\) influye en el valor de \\(y\\) solo a través de \\(\\pi\\))\n¿Y el razonamiento inverso? De \\(N\\) tiradas podemos hacer una inferencia sobre \\(\\pi\\), lo que nos permitirá hacer una inferencia sobre \\(\\omega\\)"
  },
  {
    "objectID": "presentaciones/presentacion_08.html#section-3",
    "href": "presentaciones/presentacion_08.html#section-3",
    "title": "Estadística Bayesiana",
    "section": "",
    "text": "¿Qué distribución a posteriori buscamos? \\(p(\\pi,\\omega\\mid y)\\)\n¿Y la Regla de Bayes? ¿Vale? ¿Cómo la escribimos?\n\\[p(\\pi,\\omega\\mid y) = \\frac{p(y\\mid\\pi,\\omega)p(\\pi,\\omega)}{p(y)} = \\frac{p(y\\mid\\pi)p(\\pi\\mid\\omega)p(\\omega)}{p(y)}\\]\n¿Tenemos forma de expresar \\(p(y\\mid\\pi)\\), \\(p(\\pi\\mid\\omega)\\), y \\(p(\\omega)\\)?"
  },
  {
    "objectID": "presentaciones/presentacion_08.html#section-4",
    "href": "presentaciones/presentacion_08.html#section-4",
    "title": "Estadística Bayesiana",
    "section": "",
    "text": "Estamos haciendo inferencia bayesiana sobre una distribución conjunta (de \\(\\pi\\) y \\(\\omega\\)). Pero la relación entre los parámetros (y la función de verosimilitud) es jerárquica. La jerarquía tiene una interpretación para el modelo."
  },
  {
    "objectID": "presentaciones/presentacion_08.html#section-5",
    "href": "presentaciones/presentacion_08.html#section-5",
    "title": "Estadística Bayesiana",
    "section": "",
    "text": "pi &lt;- seq(0,1,length.out=101)\nomega &lt;- seq(0,1,length.out=101)\nA_omega &lt;- 2\nB_omega &lt;- 2\nkappa &lt;- 5\nN &lt;- 9\ny &lt;- 3"
  },
  {
    "objectID": "presentaciones/presentacion_08.html#section-6",
    "href": "presentaciones/presentacion_08.html#section-6",
    "title": "Estadística Bayesiana",
    "section": "",
    "text": "prior &lt;- expand.grid(pi = pi, omega = omega) |&gt;\n  mutate(p_omega = dbeta(omega, A_omega, B_omega),\n         p_pi_given_omega = dbeta(pi, omega*(kappa-2)+1, (1-omega)*(kappa-2)+1),\n         prior = p_pi_given_omega * p_omega)\n\nplot_prior &lt;- ggplot(prior) +\n  geom_raster(aes(x=pi, y=omega, fill=prior)) +\n  geom_hline(yintercept = 0.6) +\n  geom_hline(yintercept = 0.9) +\n  scale_x_continuous(expression(pi), expand = c(0,0)) +\n  scale_y_continuous(expression(omega), expand = c(0,0)) +\n  viridis::scale_fill_viridis()\n\nplot_prior_omega &lt;- ggplot(prior) +\n  geom_line(aes(x=omega, y=p_omega))\n\nplot_prior_pi_omega1 &lt;- ggplot(prior |&gt; filter(omega == 0.6)) +\n  geom_line(aes(x=pi, y=prior)) +\n  xlab(expression(pi))\n\nplot_prior_pi_omega2 &lt;- ggplot(prior |&gt; filter(omega == 0.9)) +\n  geom_line(aes(x=pi, y=prior)) +\n  xlab(expression(pi))"
  },
  {
    "objectID": "presentaciones/presentacion_08.html#section-8",
    "href": "presentaciones/presentacion_08.html#section-8",
    "title": "Estadística Bayesiana",
    "section": "",
    "text": "likelihood &lt;- expand.grid(pi = pi, omega = omega) |&gt;\n  mutate(likelihood = dbinom(y, size = N, prob = pi))\n\nplot_likelihood &lt;- ggplot(likelihood) +\n  geom_raster(aes(x=pi, y=omega, fill=likelihood)) +\n  scale_x_continuous(expression(pi), expand = c(0,0)) +\n  scale_y_continuous(expression(omega), expand = c(0,0)) +\n  viridis::scale_fill_viridis()\n\nposterior &lt;- inner_join(prior,likelihood) |&gt;\n  mutate(posterior = prior * likelihood)\n\nplot_posterior &lt;- ggplot(posterior) +\n  geom_raster(aes(x=pi, y=omega, fill=posterior)) +\n  scale_x_continuous(expression(pi), expand = c(0,0)) +\n  scale_y_continuous(expression(omega), expand = c(0,0)) +\n  viridis::scale_fill_viridis()"
  },
  {
    "objectID": "presentaciones/presentacion_08.html#section-10",
    "href": "presentaciones/presentacion_08.html#section-10",
    "title": "Estadística Bayesiana",
    "section": "",
    "text": "pi &lt;- seq(0,1,length.out=101)\nomega &lt;- seq(0,1,length.out=101)\nA_omega &lt;- 20\nB_omega &lt;- 20\nkappa &lt;- 10\nN &lt;- 9\ny &lt;- 3"
  },
  {
    "objectID": "presentaciones/presentacion_08.html#section-13",
    "href": "presentaciones/presentacion_08.html#section-13",
    "title": "Estadística Bayesiana",
    "section": "",
    "text": "La inferencia bayesiana en un modelo jerárquico es inferencia en el espacio de la distribución conjunta de los parámetros pero reformulando la distribución conjunta en términos jerárquicos: se refactoriza \\(p(\\pi,\\omega)\\) como \\(p(\\pi\\mid\\omega)p(\\omega)\\)"
  },
  {
    "objectID": "presentaciones/presentacion_08.html#extensión",
    "href": "presentaciones/presentacion_08.html#extensión",
    "title": "Estadística Bayesiana",
    "section": "Extensión",
    "text": "Extensión\n¿Qué pasaría si contamos con más de una moneda creada por la misma fábrica? Cada moneda tiene un valor de \\(\\pi_s\\) que es propio y que a su vez tienen algo en común: provienen de la fábrica que tiene parámetro \\(\\omega\\).\nCon \\(y_1\\) caras en \\(N_1\\) tiradas de la moneda 1 estimamos \\(\\pi_1\\), con \\(y_2\\) caras en \\(N_2\\) tiradas de la moneda 1 estimamos \\(\\pi_2\\)… y luego, con todas las tiradas, podemos estimar \\(\\omega\\)."
  },
  {
    "objectID": "presentaciones/presentacion_08.html#section-14",
    "href": "presentaciones/presentacion_08.html#section-14",
    "title": "Estadística Bayesiana",
    "section": "",
    "text": "Consideremos un caso real. \\(S\\) personas reciben una droga y son sometidos a un test de memoria. La probabilidad de que el sujeto \\(s\\) recuerde un ítem que se le muestra es \\(\\pi_s\\). El sujeto \\(s\\) recuerda \\(y_s\\) ítems de \\(N_s\\) que se le presentan. Asumimos que la droga induce un efecto en los sujetos alrededor de una tendencia central \\(\\omega\\).\nConsideremos por simplicidad que se tienen dos sujetos…\n¿Cuántos parámetros tiene el modelo? ¿Cómo podemos representar la relación entre los \\(\\theta_s\\)?"
  },
  {
    "objectID": "presentaciones/presentacion_08.html#section-15",
    "href": "presentaciones/presentacion_08.html#section-15",
    "title": "Estadística Bayesiana",
    "section": "",
    "text": "\\[\n\\begin{aligned}\n    y_s\\mid\\pi_s & \\sim  \\mathrm{Binomial}(N_s,\\pi_s)\\\\\n    \\pi_s \\mid \\omega & \\sim  \\mathrm{Beta}(\\omega(\\kappa-2)+1,\\ (1-\\omega)(\\kappa-2)+1) \\\\\n    \\omega & \\sim \\mathrm{Beta}(A_\\omega,B_\\omega)\n\\end{aligned}\n\\]\nSi fijo \\(\\omega\\), los valores de los \\(\\pi_s\\) son independientes: \\(\\pi_1\\) y \\(\\pi_2\\) son independientes dado \\(\\omega\\).\nLa probabilidad a priori que es \\(p(\\pi_1,\\pi_2,\\omega)\\) ahora puede factorizarse como \\(p(\\pi_1,\\pi_2\\mid \\omega) p(\\omega) = p(\\pi_1\\mid \\omega) p(\\pi_2\\mid \\omega) p(\\omega)\\)"
  },
  {
    "objectID": "presentaciones/presentacion_08.html#section-16",
    "href": "presentaciones/presentacion_08.html#section-16",
    "title": "Estadística Bayesiana",
    "section": "",
    "text": "pi_1 &lt;- seq(0,1,length.out=101)\npi_2 &lt;- seq(0,1,length.out=101)\nomega &lt;- seq(0,1,length.out=101)\nA_omega &lt;- 2\nB_omega &lt;- 2\nkappa &lt;- 5\nN_1 &lt;- 20\ny_1 &lt;- 5\nN_2 &lt;- 8\ny_2 &lt;- 4"
  },
  {
    "objectID": "presentaciones/presentacion_08.html#section-17",
    "href": "presentaciones/presentacion_08.html#section-17",
    "title": "Estadística Bayesiana",
    "section": "",
    "text": "prior &lt;- expand.grid(pi_1 = pi_1, pi_2 = pi_2, omega = omega) |&gt;\n  mutate(\n    p_omega = dbeta(omega, A_omega, B_omega),\n    p_pi1_given_omega = dbeta(pi_1, \n                              omega*(kappa-2)+1, \n                              (1-omega)*(kappa-2)+1),\n    p_pi2_given_omega = dbeta(pi_2, \n                              omega*(kappa-2)+1, \n                              (1-omega)*(kappa-2)+1),\n    prior = p_pi1_given_omega *  p_pi2_given_omega * p_omega)\n\nprior_pi1 &lt;- ggplot(prior |&gt; \n                      group_by(pi_1,omega) |&gt; \n                      summarise(prior = sum(prior))) +\n  geom_raster(aes(x=pi_1, y=omega, fill=prior)) +\n  scale_x_continuous(expression(pi[1]), expand = c(0,0)) +\n  scale_y_continuous(expression(omega), expand = c(0,0)) +\n  viridis::scale_fill_viridis()\n\nprior_pi2 &lt;- ggplot(prior |&gt; \n                      group_by(pi_2,omega) |&gt; \n                      summarise(prior = sum(prior))) +\n  geom_raster(aes(x=pi_2, y=omega, fill=prior)) +\n  scale_x_continuous(expression(pi[2]), expand = c(0,0)) +\n  scale_y_continuous(expression(omega), expand = c(0,0)) +\n  viridis::scale_fill_viridis()"
  },
  {
    "objectID": "presentaciones/presentacion_08.html#section-19",
    "href": "presentaciones/presentacion_08.html#section-19",
    "title": "Estadística Bayesiana",
    "section": "",
    "text": "likelihood &lt;- expand.grid(pi_1 = pi_1, pi_2 = pi_2, omega = omega) |&gt;\n  mutate(likelihood_pi1 = dbinom(y_1, size = N_1, prob = pi_1),\n         likelihood_pi2 = dbinom(y_2, size = N_2, prob = pi_2),\n         likelihood = likelihood_pi1 * likelihood_pi2)\n\nlikelihood1 &lt;- ggplot(likelihood |&gt; \n                        group_by(pi_1,omega) |&gt; \n                        summarise(likelihood = sum(likelihood))) +\n  geom_raster(aes(x=pi_1, y=omega, fill=likelihood)) +\n  scale_x_continuous(expression(pi[1]), expand = c(0,0)) +\n  scale_y_continuous(expression(omega), expand = c(0,0)) +\n  viridis::scale_fill_viridis()\n\nlikelihood2 &lt;- ggplot(likelihood |&gt; \n                        group_by(pi_2,omega) |&gt; \n                        summarise(likelihood = sum(likelihood))) +\n  geom_raster(aes(x=pi_2, y=omega, fill=likelihood)) +\n  scale_x_continuous(expression(pi[2]), expand = c(0,0)) +\n  scale_y_continuous(expression(omega), expand = c(0,0)) +\n  viridis::scale_fill_viridis()"
  },
  {
    "objectID": "presentaciones/presentacion_08.html#section-21",
    "href": "presentaciones/presentacion_08.html#section-21",
    "title": "Estadística Bayesiana",
    "section": "",
    "text": "posterior &lt;- inner_join(prior, likelihood) |&gt;\n  mutate(posterior = prior * likelihood)\n\nposterior_pi1 &lt;- ggplot(posterior |&gt; \n                          group_by(pi_1,omega) |&gt; \n                          summarise(posterior = sum(posterior))) +\n  geom_raster(aes(x=pi_1, y=omega, fill=posterior)) +\n  scale_x_continuous(expression(pi[1]), expand = c(0,0)) +\n  scale_y_continuous(expression(omega), expand = c(0,0)) +\n  viridis::scale_fill_viridis()\n\nposterior_pi2 &lt;- ggplot(posterior |&gt; \n                          group_by(pi_2,omega) |&gt; \n                          summarise(posterior = sum(posterior))) +\n  geom_raster(aes(x=pi_2, y=omega, fill=posterior)) +\n  scale_x_continuous(expression(pi[2]), expand = c(0,0)) +\n  scale_y_continuous(expression(omega), expand = c(0,0)) +\n  viridis::scale_fill_viridis()\n\nposterior_omega &lt;- ggplot(posterior |&gt; \n                            group_by(omega) |&gt; \n                            summarise(posterior = sum(posterior))) +\n  geom_line(aes(x=omega, y=posterior)) +\n  xlab(expression(omega))\n\nposterior_pi1_marg &lt;- ggplot(posterior |&gt; \n                               group_by(pi_1) |&gt; \n                               summarise(posterior = sum(posterior))) +\n  geom_line(aes(x=pi_1, y=posterior)) +\n  xlab(expression(pi[1]))\n\nposterior_pi2_marg &lt;- ggplot(posterior |&gt; \n                               group_by(pi_2) |&gt; \n                               summarise(posterior = sum(posterior))) +\n  geom_line(aes(x=pi_2, y=posterior)) +\n  xlab(expression(pi[2]))"
  },
  {
    "objectID": "presentaciones/presentacion_08.html#section-24",
    "href": "presentaciones/presentacion_08.html#section-24",
    "title": "Estadística Bayesiana",
    "section": "",
    "text": "La función de verosimilitud no depende de \\(\\omega\\)\nLa función de verosimilitud es más estrecha para el sujeto 1 que para el sujeto 2\nEl posterior marginal de \\(\\pi_1\\) está cerca de la proporción muestral\nEl posterior marginal de \\(\\pi_2\\) está cerca de la proporción muestral\nEl posterior marginal de \\(\\pi_1\\) tiene menos incertidumbre que el de \\(\\pi_2\\)\n\n\n¿Qué ocurre si se cambia el valor de \\(\\kappa\\)?"
  },
  {
    "objectID": "presentaciones/presentacion_08.html#section-25",
    "href": "presentaciones/presentacion_08.html#section-25",
    "title": "Estadística Bayesiana",
    "section": "",
    "text": "pi_1 &lt;- seq(0,1,length.out=101)\npi_2 &lt;- seq(0,1,length.out=101)\nomega &lt;- seq(0,1,length.out=101)\nA_omega &lt;- 2\nB_omega &lt;- 2\nkappa &lt;- 100\nN_1 &lt;- 20\ny_1 &lt;- 5\nN_2 &lt;- 8\ny_2 &lt;- 4"
  },
  {
    "objectID": "presentaciones/presentacion_08.html#section-30",
    "href": "presentaciones/presentacion_08.html#section-30",
    "title": "Estadística Bayesiana",
    "section": "",
    "text": "El posterior marginal de \\(\\pi_2\\) se alejó de la proporción muestral. El sujeto 1 tenía un tamaño de muestra mayor (más evidencia) y por lo tanto influyó más en la estimación de \\(\\omega\\), lo que a la vez influye en la estimación de \\(\\pi_2\\)."
  },
  {
    "objectID": "presentaciones/presentacion_08.html#shrinkage",
    "href": "presentaciones/presentacion_08.html#shrinkage",
    "title": "Estadística Bayesiana",
    "section": "Shrinkage",
    "text": "Shrinkage\nLa estructura jerárquica de los modelos hace que las estimaciones de los parámetros de los niveles más bajos se acerquen más de lo que lo harían si no hubiera una distribución en un nivel superior. Esto es lo que se conoce como shrinkage de las estimaciones.\nLas estimaciones de los parámetros de los niveles más bajos son tiradas (pulled) o se estrechan o tienden a concentrarse hacia la moda de la distribución superior."
  },
  {
    "objectID": "presentaciones/presentacion_08.html#section-31",
    "href": "presentaciones/presentacion_08.html#section-31",
    "title": "Estadística Bayesiana",
    "section": "",
    "text": "El shrinkage ocurre porque los parámetros de los niveles bajos (los \\(\\pi_s\\)) son influenciados por:\n\n\nEl conjunto de datos que dependen directamente de ese parámetro\nLos parámetros de niveles más altos de los cuales dependen los parámetros de niveles más bajos (¡y que son afectados por todos los datos!)\n\n\n\nPor ejemplo, sobre \\(\\pi_1\\) influyen \\(y_1\\) y \\(N_1\\) pero también \\(\\omega\\) (cuya estimación depende de \\(\\pi_2\\) y \\(N_2\\)).\n\n\nNota: el shrinkage es consecuencia exclusivamente de la estructura jerárquica (y no de la inferencia bayesiana). Existe en la teoría clásica de estimación (ver estimador de James-Stein)"
  },
  {
    "objectID": "presentaciones/presentacion_08.html#otro-ejemplo",
    "href": "presentaciones/presentacion_08.html#otro-ejemplo",
    "title": "Estadística Bayesiana",
    "section": "Otro ejemplo",
    "text": "Otro ejemplo\nEl radón es un gas radioactivo y cancerígeno. Los productos de la desintegración del radón son también radioactivos y en altas concentraciones se sabe que producen cáncer de pulmón. Trabajaremos con datos de mediciones de radón en el estado de Minnesota. Se cuenta con mediciones en hogares de diferentes condados dentro del estado."
  },
  {
    "objectID": "presentaciones/presentacion_08.html#section-33",
    "href": "presentaciones/presentacion_08.html#section-33",
    "title": "Estadística Bayesiana",
    "section": "",
    "text": "\\(i\\) es el índice de los hogares\n\\(Y_i\\) es el nivel de radón (log radón) del hogar \\(i\\)\n\\(j\\) (entre \\(1\\) y \\(J\\)) es el índice de los condados\n\\(j[i] = \\mathrm{county}[i]\\) es el condado al que pertenece el hogar \\(i\\)"
  },
  {
    "objectID": "presentaciones/presentacion_08.html#section-35",
    "href": "presentaciones/presentacion_08.html#section-35",
    "title": "Estadística Bayesiana",
    "section": "",
    "text": "Complete pooling\n\\[\n\\begin{aligned}\n    Y_i\\mid\\mu_i,\\sigma & \\sim  \\mathcal{N}(\\mu_i,\\sigma^2) \\\\\n    \\mu_i & = \\alpha  \\\\\n    \\alpha & \\sim  P(\\alpha) \\\\\n    \\sigma & \\sim  P(\\sigma)\n\\end{aligned}\n\\]\nHay una única media \\(\\alpha\\) común para todos los \\(i\\), independientemente del grupo \\(j\\) al que pertenezcan"
  },
  {
    "objectID": "presentaciones/presentacion_08.html#section-36",
    "href": "presentaciones/presentacion_08.html#section-36",
    "title": "Estadística Bayesiana",
    "section": "",
    "text": "No pooling\n\\[\n\\begin{aligned}\n    Y_i\\mid\\mu_i,\\sigma & \\sim  \\mathcal{N}(\\mu_i,\\sigma^2)\\\\\n    \\mu_i & = \\alpha_{j[i]}  \\\\\n    \\alpha_j & \\sim  P(\\alpha_j) \\\\\n    \\sigma & \\sim  P(\\sigma)\n\\end{aligned}\n\\]\nDecimos que los \\(Y_i\\) tienen distribución de media \\(\\alpha_{j[i]}\\), sin imponer ninguna restricción sobre los \\(\\alpha_j\\). \\(P(\\alpha_j)\\) es una distribución no informativa (muy ancha y chata). Todos los \\(\\alpha_j\\) son independientes. Coincide con la estimación clásica que incluye una variable dummy para cada grupo."
  },
  {
    "objectID": "presentaciones/presentacion_08.html#section-37",
    "href": "presentaciones/presentacion_08.html#section-37",
    "title": "Estadística Bayesiana",
    "section": "",
    "text": "Podemos mejorar el modelo anterior incorporando un prior que regularice los \\(\\alpha_j\\)\n\\[\n\\begin{aligned}\n    Y_i\\mid\\mu_i,\\sigma & \\sim  \\mathcal{N}(\\mu_i,\\sigma^2) \\\\\n    \\mu_i & = \\alpha_{j[i]}  \\\\\n    \\alpha_j & \\sim  \\mathcal{N}(0,10) \\\\\n    \\sigma & \\sim  P(\\sigma)\n\\end{aligned}\n\\]\n0 y 10 son valores arbitrarios para la media y la varianza de la distribución a priori de los \\(\\alpha_j\\). Los \\(\\alpha_j\\) dejan de poder ser estimados libremente. Hay regularización y tendemos a evitar el overfitting. Hay un partial pooling. Si en lugar de 10 se elige un valor más grande, tendemos a no pooling; si se elige un valor más chico, tendemos a pooling completo"
  },
  {
    "objectID": "presentaciones/presentacion_08.html#section-38",
    "href": "presentaciones/presentacion_08.html#section-38",
    "title": "Estadística Bayesiana",
    "section": "",
    "text": "Mejor aún, podemos estimar el grado de regularización partir de los datos. ¿Cuánto pooling es necesario? Se estima a partir de los datos…\n\\[\n\\begin{aligned}\n    Y_i\\mid\\mu_i,\\sigma & \\sim  \\mathcal{N}(\\mu_i,\\sigma^2) \\\\\n    \\mu_i & = \\alpha_{j[i]}  \\\\\n    \\alpha_j & \\sim  \\mathcal{N}(\\mu_\\alpha,\\sigma_\\alpha^2) \\\\\n    \\sigma & \\sim  P(\\sigma) \\\\\n    \\mu_\\alpha & \\sim  P(\\mu_\\alpha) \\\\\n    \\sigma_\\alpha & \\sim  P(\\sigma_\\alpha) \\\\\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "presentaciones/presentacion_08.html#section-39",
    "href": "presentaciones/presentacion_08.html#section-39",
    "title": "Estadística Bayesiana",
    "section": "",
    "text": "\\(\\mu_\\alpha\\) y \\(\\sigma_\\alpha\\) son hiperparámetros (parámetros de la distribución de a priori de los parámetros) y por lo tanto tienen hiperpriors\nEl chiste es que todos los datos se usan para estimar \\(\\mu_\\alpha\\) y \\(\\sigma_\\alpha\\) y por lo tanto en la estimación de cada \\(\\alpha_j\\) hay información de todos los datos. La regularización es adaptativa (se aprende de los datos)."
  },
  {
    "objectID": "presentaciones/presentacion_08.html#shrinkage-1",
    "href": "presentaciones/presentacion_08.html#shrinkage-1",
    "title": "Estadística Bayesiana",
    "section": "Shrinkage",
    "text": "Shrinkage\n\nSiempre que hay regularización, hay shrinkage de parámetros.\nLos datos de un grupo ayudan en la estimación de los parámetros de los otros grupos (partial pooling: préstamo de información).\nAsí, los grupos que tienen menor tamaño de muestra toman más información del resto de los grupos y el shrinkage es más intenso."
  },
  {
    "objectID": "presentaciones/presentacion_08.html#section-40",
    "href": "presentaciones/presentacion_08.html#section-40",
    "title": "Estadística Bayesiana",
    "section": "",
    "text": "Pooling completo: hay una única media para todos los individuos, independientemente del grupo. La variación entre los grupos es cero. Underfitting.\nNo pooling: cada grupo tiene una media independiente de la de los demás. La variación entre los grupos es infinita. No se comparte información entre los grupos, lo que se sabe de un grupo no ayuda a inferir sobre los demás. Overfitting.\nPartial pooling: cada grupo tiene una media pero todas las medias están conectadas. Es una solución de compromiso, un punto medio entre pooling completo y no pooling."
  },
  {
    "objectID": "presentaciones/presentacion_08.html#section-41",
    "href": "presentaciones/presentacion_08.html#section-41",
    "title": "Estadística Bayesiana",
    "section": "",
    "text": "Para algunos condados: a la izquierda está la estimación de la media no pooling de la media, a la derecha la estimación del modelo multinivel (pooling parcial). En línea de trazos el pooling completo."
  },
  {
    "objectID": "presentaciones/presentacion_08.html#otro-ejemplo-1",
    "href": "presentaciones/presentacion_08.html#otro-ejemplo-1",
    "title": "Estadística Bayesiana",
    "section": "Otro ejemplo",
    "text": "Otro ejemplo\nCorredores que han participado varias veces de una famosa maratón en Washington. Se registraron los tiempos de los participantes."
  },
  {
    "objectID": "presentaciones/presentacion_08.html#section-45",
    "href": "presentaciones/presentacion_08.html#section-45",
    "title": "Estadística Bayesiana",
    "section": "",
    "text": "La información de la estimación de la pendiente de un grupo es útil para estimar las otras pendientes\nLa información de la estimación de las ordenadas al origen de un grupo es útil para estimar las otras ordenadas al origen\nLas pendientes y las ordenadas al origen trabajan de forma conjunta para describir a un corredor, covarían"
  },
  {
    "objectID": "presentaciones/presentacion_08.html#section-46",
    "href": "presentaciones/presentacion_08.html#section-46",
    "title": "Estadística Bayesiana",
    "section": "",
    "text": "\\[\n\\begin{aligned}\n    Y_i  \\mid \\mu_i,\\sigma & \\sim  \\mathcal{N}(\\mu_i,\\sigma^2) \\\\\n    \\mu_i & = \\alpha_{j[i]} + \\beta_{j[i]} x_i \\\\\n    \\left[\\begin{array}{c}\\alpha_j\\\\\\beta_j\\end{array}\\right] & \\sim \\mathcal{N}\\left(\\left[\\begin{array}{c}\\mu_\\alpha\\\\\\mu_\\beta\\end{array}\\right],\\Sigma\\right)\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "presentaciones/presentacion_08.html#section-47",
    "href": "presentaciones/presentacion_08.html#section-47",
    "title": "Estadística Bayesiana",
    "section": "",
    "text": "\\(\\left[\\begin{array}{c}\\alpha_j\\\\\\beta_j\\end{array}\\right]\\) tienen una distribución conjunta, normal multivariada de hiperparámetros \\(\\left[\\begin{array}{c}\\mu_\\alpha\\\\\\mu_\\beta\\end{array}\\right]\\) y \\(\\Sigma\\). ¡Necesitan !\n\n\\(\\Sigma\\) puede factorizarse según: \n\\[\n\\Sigma = \\left( \\begin{array}{cc} \\sigma_\\alpha^2 & \\sigma_\\alpha\\sigma_\\beta\\rho \\\\ \\sigma_\\alpha\\sigma_\\beta\\rho & \\sigma_\\beta^2 \\end{array}\\right) =\n\\left( \\begin{array}{cc} \\sigma_\\alpha & 0 \\\\ 0 & \\sigma_\\beta \\end{array}\\right) \\left( \\begin{array}{cc} 1 & \\rho \\\\ \\rho & 1 \\end{array}\\right)\n\\left( \\begin{array}{cc} \\sigma_\\alpha & 0 \\\\ 0 & \\sigma_\\beta \\end{array}\\right)\n\\]\nLlamando \\[R = \\left( \\begin{array}{cc} 1 & \\rho \\\\ \\rho & 1 \\end{array}\\right)\\] solo habría que definir una distribución para \\(\\sigma_\\alpha\\), \\(\\sigma_\\beta\\) y \\(R\\) (o \\(\\rho\\))"
  },
  {
    "objectID": "presentaciones/presentacion_08.html#section-48",
    "href": "presentaciones/presentacion_08.html#section-48",
    "title": "Estadística Bayesiana",
    "section": "",
    "text": "No es solo para modelos lineales… Se tienen 60 tanques con ranitas de la especie Hyperolius viridiflavus. Cada tanque \\(i\\) de ellos contiene una cantidad inicial de renacuajos \\(N_i\\). Al cabo de unas semanas se observa el número \\(S_i\\) de renacuajos que sobrevivieron en el tanque \\(i\\)."
  },
  {
    "objectID": "presentaciones/presentacion_08.html#section-49",
    "href": "presentaciones/presentacion_08.html#section-49",
    "title": "Estadística Bayesiana",
    "section": "",
    "text": "Se modeliza la probabilidad de supervivencia de cada tanque con una regresión logística:\n\\[\n\\begin{aligned}\n    S_i  \\mid p_i & \\sim  \\text{Binomial}(N_i,p_i) \\\\\n   \\log\\left( \\frac{p_i}{1-p_i} \\right) & = \\alpha_{i} \\\\\n   \\alpha_i & \\sim \\mathcal{N}(\\mu_\\alpha,\\sigma_\\alpha^2)\n\\end{aligned}\n\\]\nObservar que no hay grupos (no hay índice \\(j\\)), simplemente hacemos de los individuos."
  },
  {
    "objectID": "presentaciones/presentacion_08.html#section-50",
    "href": "presentaciones/presentacion_08.html#section-50",
    "title": "Estadística Bayesiana",
    "section": "",
    "text": "Comparamos la estimación de \\(p_i = \\frac{e^{\\alpha_i}}{1+e^{\\alpha_i}}\\) con la obtenida por máxima verosimilitud en cada tanque: \\(p_{i,ML} = \\frac{S_i}{N_i}\\)"
  },
  {
    "objectID": "presentaciones/presentacion_08.html#intercambiabilidad",
    "href": "presentaciones/presentacion_08.html#intercambiabilidad",
    "title": "Estadística Bayesiana",
    "section": "Intercambiabilidad",
    "text": "Intercambiabilidad\nSi no existe otra información más que los datos observados \\(y_i\\) para distinguir a los individuos \\(i\\) y estos no pueden ordenarse ninguna manera entonces se puede asumir una simetría de los parámetros. Se dice que los parámetros \\(\\theta_i\\) son intercambiables (exchangeable).\nSi las observaciones pueden agruparse y los grupos son indistinguibles (con características propias desconocidas) con propiedades/particularidades ignoradas entonces los grupos son intercambiables y los individuos, parcialmente o condicionalmente intercambiables."
  },
  {
    "objectID": "presentaciones/presentacion_08.html#distribuciones-predictivas",
    "href": "presentaciones/presentacion_08.html#distribuciones-predictivas",
    "title": "Estadística Bayesiana",
    "section": "Distribuciones predictivas",
    "text": "Distribuciones predictivas\nEn los modelos jerárquicos hay dos tipos de distribuciones predictivas a posteriori:\n\nPredicciones para individuos que pertenecen a grupos ya existentes (tiradas de la moneda con la que se realizaron las inferencias, otra tarea de memoria para un individuo que ya participó del experimento)\nPredicciones para individuos pertenecientes a grupos nuevos (tiradas de una nueva moneda de la fábrica, cómo afectaría la droga a un individuo nuevo)"
  },
  {
    "objectID": "presentaciones/presentacion_08.html#resumen",
    "href": "presentaciones/presentacion_08.html#resumen",
    "title": "Estadística Bayesiana",
    "section": "Resumen",
    "text": "Resumen\n\nLos modelos jerárquicos resultan atractivos para problemas en los cuales los parámetros se pueden considerar vinculados de cierta forma, por ejemplo en grupos.\nLos modelos jerárquicos o multinivel son extensiones de los modelos lineales (y de los modelos lineales generalizados) para datos que tienen algún grado de agrupamiento y en los cuales se permite que los parámetros varíen por grupo\nLos modelos multinivel permiten mejorar las inferencias en contextos donde la muestra es pequeña. Si un individuo que tiene pocas observaciones pertenece a un determinado grupo, se supone que compartirá características con otros individuos de ese grupo y por lo tanto la estimación de sus parámetros podrá ser informada por la de sus pares.\n\n\n\n\n\nEstadística Bayesiana – 2025"
  },
  {
    "objectID": "presentaciones/presentacion_05.html#el-problema",
    "href": "presentaciones/presentacion_05.html#el-problema",
    "title": "Estadística Bayesiana",
    "section": "El Problema",
    "text": "El Problema\nTípicamente interesa resolver los siguientes problemas:\n\n\nCalcular integrales de la forma \\(\\mathbb{E}[\\phi(x)] = \\int \\phi(x) p(x) d x\\) (law of the unconscious statistician)\nGenerar \\(S\\) muestras independientes \\(x^{(s)}\\) de una distribución de probabilidad \\(p(x)\\)\n\n\n\nEn la estadística bayesiana, \\(x\\) es \\(\\theta\\), el parámetro desconocido de alguna distribución de probabilidad y \\(p(x)\\) es el posterior."
  },
  {
    "objectID": "presentaciones/presentacion_05.html#métodos-de-montecarlo",
    "href": "presentaciones/presentacion_05.html#métodos-de-montecarlo",
    "title": "Estadística Bayesiana",
    "section": "Métodos de Montecarlo",
    "text": "Métodos de Montecarlo"
  },
  {
    "objectID": "presentaciones/presentacion_05.html#section",
    "href": "presentaciones/presentacion_05.html#section",
    "title": "Estadística Bayesiana",
    "section": "",
    "text": "Para el primer problema, sabemos que si \\(X_i \\sim p(x)\\), bajo ciertas condiciones podemos aproximar\n\n\\[\\mathbb{E}[X] \\approx \\frac{1}{N} \\sum_{i=1}^N x_i\\]\n\n\nSi \\(X\\) es una variable aleatoria, entonces para funciones continuas \\(\\phi\\) tenemos que \\(\\phi(X)\\) también es una variable aleatoria y por lo tanto\n\n\n\\[\\mathbb{E}[\\phi(X)] = \\int\\phi(x)p(x)dx \\approx \\frac{1}{N} \\sum_{i=1}^N \\phi(x_i)\\]\n\n\nEs decir, si los \\(x_i\\) son muestras de \\(p(x)\\), entonces la integral \\(\\int\\phi(x)p(x)dx\\) puede aproximarse por \\(\\frac{1}{N} \\sum_{i=1}^N \\phi(x_i)\\)."
  },
  {
    "objectID": "presentaciones/presentacion_05.html#section-1",
    "href": "presentaciones/presentacion_05.html#section-1",
    "title": "Estadística Bayesiana",
    "section": "",
    "text": "Esto ya lo hemos hecho\n\n\nLa distribución predictiva a posteriori es \\(\\int p(\\tilde{y} \\mid \\theta) p(\\theta\\mid y) d\\theta\\) y puede aproximarse por \\(\\frac{1}{N} \\sum_{i=1}^N p(\\tilde{y}\\mid \\theta_i)\\)\nEl riesgo bayesiano es \\(\\int L(\\theta,\\hat\\theta) p(\\theta\\mid y) d\\theta\\) y puede aproximarse por \\(\\frac{1}{N} \\sum_{i=1}^N L(\\theta_i,\\hat\\theta)\\)\nSi consideramos la integral \\(\\int \\mathbb{I}_{\\theta \\in A} p(\\theta\\mid y) d\\theta = \\int_A p(\\theta\\mid y)d\\theta\\) es la probabilidad de que \\(\\theta\\) esté en \\(A\\) y puede aproximarse por \\(\\frac{1}{N} \\sum_{i=1}^N \\mathbb{I}_{\\theta_i \\in A}\\)"
  },
  {
    "objectID": "presentaciones/presentacion_05.html#section-2",
    "href": "presentaciones/presentacion_05.html#section-2",
    "title": "Estadística Bayesiana",
    "section": "",
    "text": "Teniendo muestras de \\(p(x)\\) es fácil estimar las integrales \\(\\mathbb{E}[\\phi(x)] = \\int \\phi(x) p(x) d x\\) por lo que nos centraremos en el problema de cómo obtener muestras de \\(p(x)\\).\n\nPara algunas distribuciones de probabilidad es fácil obtener muestras. Pero no siempre existe una función rbinom, rbeta, rnorm, rpois, etc."
  },
  {
    "objectID": "presentaciones/presentacion_05.html#section-3",
    "href": "presentaciones/presentacion_05.html#section-3",
    "title": "Estadística Bayesiana",
    "section": "",
    "text": "Tomar muestras de una distribución de probabilidad \\(p(x)\\) implica obtener valores que provienen, con mayor frecuencia, de regiones donde \\(p(x)\\) es grande.\n¿Por qué es difícil tomar muestras de una distribución de probabilidad?\n\nEn estadística bayesiana tenemos \\(p(\\theta \\mid y ) \\propto p(y\\mid\\theta) p(\\theta)\\) por lo que en general llegamos a \\(p^*(\\theta \\mid y) = Z\\ p(\\theta\\mid y)\\)\n\n\n\nLa determinación de \\(Z\\) implica resolver una integral (potencialmente multivariada) que puede no tener solución analítica (intractability of the integral)\nAún conociendo \\(Z\\), no hay una manera determinada de obtener muestras de \\(p(\\theta\\mid y)\\)\nTomar muestras de distribuciones discretas es más fácil que hacerlo de distribuciones continuas"
  },
  {
    "objectID": "presentaciones/presentacion_05.html#section-4",
    "href": "presentaciones/presentacion_05.html#section-4",
    "title": "Estadística Bayesiana",
    "section": "",
    "text": "¿Cómo tomamos muestras de una distribución discreta?"
  },
  {
    "objectID": "presentaciones/presentacion_05.html#grid-approximation",
    "href": "presentaciones/presentacion_05.html#grid-approximation",
    "title": "Estadística Bayesiana",
    "section": "Grid approximation",
    "text": "Grid approximation\nUna solución puede ser discretizar la variable. Esta solución vale incluso si no conocemos \\(Z\\). Conocemos \\(p^*(x) = Z \\ p(x)\\) (izquierda) y pasamos a una discreta \\(\\tilde{p}^*(x) = \\tilde{Z}\\ \\tilde{p}(x)\\) (centro).\n\nEvaluando \\(\\tilde{p}^*\\) en todos los posibles \\(x_i\\) de la grilla podemos calcular \\(\\tilde{Z}=\\sum_{i} \\tilde{p}^*(x_i)\\). Luego tomamos muestras de \\(\\tilde{p}(x)\\) (derecha)."
  },
  {
    "objectID": "presentaciones/presentacion_05.html#section-5",
    "href": "presentaciones/presentacion_05.html#section-5",
    "title": "Estadística Bayesiana",
    "section": "",
    "text": "En código:\n\nprob &lt;- function(x) return(exp(0.4*(x-0.4)^2 - 0.08*x^4)) # sabemos evaluar p\nx &lt;- seq(-4.5, 4.5, 0.5)\np_ &lt;- prob(x) # ~p*\nZ_rulito &lt;- sum(p_) # ~Z\np_rulito &lt;- p_/Z # ~p\nsample(x, replace = TRUE, prob = p_rulito)"
  },
  {
    "objectID": "presentaciones/presentacion_05.html#section-6",
    "href": "presentaciones/presentacion_05.html#section-6",
    "title": "Estadística Bayesiana",
    "section": "",
    "text": "¿Cómo se aplica esto en estadística bayesiana?\n\nEl posterior es \\(\\frac{1}{Z} p(y\\mid \\theta) p(\\theta)\\). Sabemos calcular el valor del posterior (sin normalizar) para cualquier valor de \\(\\theta\\): haciendo el producto del prior por el likelihood.\n\n\nPodemos considerar una grilla de valores del parámetro (o los parámetros), computar el posterior sin normalizar para cada valor de la grilla, normalizarlo y tomar muestras de él.\n\n\nPero, escala muy mal con el número de parámetros…"
  },
  {
    "objectID": "presentaciones/presentacion_05.html#section-7",
    "href": "presentaciones/presentacion_05.html#section-7",
    "title": "Estadística Bayesiana",
    "section": "",
    "text": "Ejemplo\n\n\n\n\n\n\nQueremos realizar inferencias sobre la media y la varianza de una normal. Para eso proponemos el siguiente modelo: \\[\n\\begin{aligned}\n    y_i\\mid\\mu,\\sigma^2 & \\sim  \\mathcal{N}(\\mu,\\sigma^2) \\\\\n    \\mu,\\sigma^2 & \\sim  \\frac{1}{K} \\frac{e^{-\\sigma^2}}{\\eta} e^{-\\frac{(\\mu - \\xi)^2}{2\\psi^2}}\n\\end{aligned}\n\\] (¿Cuáles son las constantes que ajustan el prior?)"
  },
  {
    "objectID": "presentaciones/presentacion_05.html#section-8",
    "href": "presentaciones/presentacion_05.html#section-8",
    "title": "Estadística Bayesiana",
    "section": "",
    "text": "Deberíamos tomar valores de \\(\\mu\\) en el intervalo \\((-4,4)\\) y valores de \\(\\sigma\\) en el intervalo \\((0,3)\\) y construir una grilla de valores.\nPara cada valor de la grilla podríamos calular el posterior sin normalizar haciendo el producto del prior por el likelihood (necesitamos la muestra)."
  },
  {
    "objectID": "presentaciones/presentacion_05.html#rejection-sampling",
    "href": "presentaciones/presentacion_05.html#rejection-sampling",
    "title": "Estadística Bayesiana",
    "section": "Rejection sampling",
    "text": "Rejection sampling\nSe basa en buscar una distribución de probabilidad candidata \\(q(x)\\) tal que \\(Cq(x)\\geq p^*(x)\\). Se toma una muestra de \\(q(x)\\). Luego se toma una muestra \\(u\\) de \\(\\mathrm{Unif}(0,Cq(x))\\). La muestra de \\(q(x)\\) se retiene si \\(u&lt;p^*(x)\\)."
  },
  {
    "objectID": "presentaciones/presentacion_05.html#section-9",
    "href": "presentaciones/presentacion_05.html#section-9",
    "title": "Estadística Bayesiana",
    "section": "",
    "text": "Necesitamos elegir con cuidado \\(q(x)\\)"
  },
  {
    "objectID": "presentaciones/presentacion_05.html#markov-chain-monte-carlo",
    "href": "presentaciones/presentacion_05.html#markov-chain-monte-carlo",
    "title": "Estadística Bayesiana",
    "section": "Markov chain Monte-Carlo",
    "text": "Markov chain Monte-Carlo\nQueremos obtener muestras de \\(p(x)\\). Vamos a hacer un viaje por los distintos valores de \\(x\\) tratando de pasar más tiempo (más iteraciones) en los puntos donde \\(p(x)\\) es grande.\n\nIdea general:\n\nVisitar los distintos valores posibles de \\(x\\)\nGenerar una secuencia de iteraciones: \\(\\{x^{(1)},x^{(2)},\\dots,x^{(S)}\\}\\)\nEn general, para obtener \\(x^{(i+1)}\\) usamos \\(x^{(i)}\\)\n\n\nEn nuestro caso tenemos \\(p(\\theta\\mid y) \\propto p(y\\mid\\theta)p(\\theta) = p^*(\\theta\\mid y)\\) (unnormalized posterior)\n¿Qué necesitamos? Poder evaluar el prior y poder evaluar el likelihood para cualquier valor de \\(\\theta\\)"
  },
  {
    "objectID": "presentaciones/presentacion_05.html#metropolis-hastings-mh",
    "href": "presentaciones/presentacion_05.html#metropolis-hastings-mh",
    "title": "Estadística Bayesiana",
    "section": "Metropolis-Hastings (MH)",
    "text": "Metropolis-Hastings (MH)\nEl algoritmo de Metropolis–Hastings (1953)\n\nEn la iteración \\(i\\) estamos en el valor del parámetro \\(\\theta^{(i)}\\)\nEn función del valor de parámetro actual \\(\\theta^{(i)}=\\theta\\), proponemos un nuevo valor \\(\\theta'\\) en función de \\(q(\\theta'\\mid\\theta)\\)\nDecidimos si vamos a la nueva ubicación \\(\\theta^{(i+1)} = \\theta'\\) o si nos quedamos \\(\\theta^{(i+1)} = \\theta\\):\n\nCalcular la probabilidad de salto: \\[\\alpha_{\\theta \\rightarrow \\theta'} = \\min\\left\\{ 1,\\frac{f(\\theta')}{f(\\theta)} \\right\\}\\]\nPasar a \\(\\theta'\\) con probabilidad \\(\\alpha_{\\theta \\rightarrow \\theta'}\\): \\[\\theta^{(i+1)} =\n\\begin{cases}\n\\theta' \\text{ con probabilidad } \\alpha_{\\theta \\rightarrow \\theta'} \\\\\n\\theta \\text{ con probabilidad } (1-\\alpha_{\\theta \\rightarrow \\theta'})\n\\end{cases}\\]"
  },
  {
    "objectID": "presentaciones/presentacion_05.html#section-10",
    "href": "presentaciones/presentacion_05.html#section-10",
    "title": "Estadística Bayesiana",
    "section": "",
    "text": "\\(q(\\theta'\\mid\\theta)\\) se llama distribución de proposición o de salto propuesto. Todo lo que necesitamos saber es dónde estamos \\(f(\\theta)\\) y hacia donde queremos ir \\(f(\\theta')\\).\n\nPuede probarse que para cualquier \\(q(\\theta'\\mid\\theta)\\), cuando \\(s\\to \\infty\\) la distribución de probabilidad de la secuencia \\(\\left\\{\\theta^{(s)} \\right\\}_{s=1}^S\\) tiende a \\(f(\\theta)\\). No sabemos nada sobre la rapidez con la que lo hace.\n\n\nEn infinitos pasos, cualquier cadena dará muestras de la distribución \\(f(\\theta)\\), en la práctica hay que tener algunos cuidados."
  },
  {
    "objectID": "presentaciones/presentacion_05.html#section-12",
    "href": "presentaciones/presentacion_05.html#section-12",
    "title": "Estadística Bayesiana",
    "section": "",
    "text": "Necesitamos muestras de \\(p(\\theta)\\)\nTomamos un punto inicial\nElegimos una distribución de saltos posibles \\(q(\\theta'\\mid\\theta)\\)\nProponemos un salto\n¿Saltamos?"
  },
  {
    "objectID": "presentaciones/presentacion_05.html#section-13",
    "href": "presentaciones/presentacion_05.html#section-13",
    "title": "Estadística Bayesiana",
    "section": "",
    "text": "Notar que la probabilidad de transicionar de \\(\\theta\\) a \\(\\theta'\\) es \\(t(\\theta'\\mid\\theta) = q(\\theta'\\mid\\theta) \\alpha_{\\theta \\rightarrow \\theta'}\\) (la probabilidad de proponer el salto a ese \\(\\theta'\\) y de aceptarla)\n\nEn realidad, una de las condiciones necesarias para que la secuencia de muestras tienda a la distribución buscada \\(f(\\theta)\\) es que el salto sea reversible: es decir, que la probabilidad de estar en \\(\\theta\\) y transicionar a \\(\\theta'\\) tiene que ser igual que la de estar en \\(\\theta'\\) y transicionar a \\(\\theta\\): \\(f(\\theta)t(\\theta'\\mid\\theta) = f(\\theta')t(\\theta\\mid\\theta')\\).\n\n\nSi la distribución propuesta \\(q\\) es simétrica, esto está resuelto. ¿Se puede elegir una \\(q\\) que no sea simétrica? ¿Qué es lo que nos define la probabilidad de pasar de \\(\\theta\\) a \\(\\theta'\\) (y viceversa)?\n\n\nTenemos que ajustar la probabilidad de transición:\n\\[\\alpha = \\min\\left\\{ 1,\\frac{f(\\theta')}{f(\\theta)} \\frac{q(\\theta \\mid \\theta')}{q(\\theta' \\mid \\theta)} \\right\\}\\]"
  },
  {
    "objectID": "presentaciones/presentacion_05.html#section-14",
    "href": "presentaciones/presentacion_05.html#section-14",
    "title": "Estadística Bayesiana",
    "section": "",
    "text": "Para el caso de \\(q\\) simétrica:\n\ntheta &lt;- double()\ntheta[1] &lt;- -1    \ni &lt;- 1\n\npropuesta &lt;- rnorm(1, mean = theta[i], sd = 0.8)\n\nf_actual &lt;- fx(theta[i])\nf_propuesta &lt;- fx(propuesta)\n\nalpha &lt;- min(c(1,f_propuesta/f_actual))\n\nquehacemos &lt;- sample(c(\"salto\",\"no salto\"), \n                    size = 1, \n                    prob = c(alpha,1-alpha))\n\nif(quehacemos==\"salto\") {\n  theta[i+1] &lt;- propuesta \n} else {\n  theta[i+1] &lt;- theta[i]\n  }\n\nDebe repetirse el proceso en un for"
  },
  {
    "objectID": "presentaciones/presentacion_05.html#section-15",
    "href": "presentaciones/presentacion_05.html#section-15",
    "title": "Estadística Bayesiana",
    "section": "",
    "text": "\\[\\sigma = 0.8\\]\n\n\n \\[\\sigma = 0.1\\]\n\n\n \\[\\sigma = 0.6\\]\n\n\n \\[\\sigma = 4.8\\]"
  },
  {
    "objectID": "presentaciones/presentacion_05.html#section-16",
    "href": "presentaciones/presentacion_05.html#section-16",
    "title": "Estadística Bayesiana",
    "section": "",
    "text": "¿Qué esperamos de nuestra cadena?\n\nRepresentatividad: haber explorado el rango completo de la distribución a posteriori, independientemente de las condiciones iniciales\nPrecisión y estabilidad: a lo largo de diferentes cadenas (distintas condiciones iniciales)\nEficiencia: esperamos requerir la menor cantidad posible de muestras\n\nNingún objetivo se alcanza absolutamente, existen chequeos gráficos y numéricos para saber si las cadenas de MCMC están sanas."
  },
  {
    "objectID": "presentaciones/presentacion_05.html#trace-plots",
    "href": "presentaciones/presentacion_05.html#trace-plots",
    "title": "Estadística Bayesiana",
    "section": "Trace Plots",
    "text": "Trace Plots\nGraficar los valores que toma el algoritmo como función del tiempo (lo que típicamente llamamos la cadena). Se tiene que ver como un fuzzy caterpillar (buen mixing). Para los impresionables: ruido blanco sin ningún patrón particular."
  },
  {
    "objectID": "presentaciones/presentacion_05.html#autocorrelación",
    "href": "presentaciones/presentacion_05.html#autocorrelación",
    "title": "Estadística Bayesiana",
    "section": "Autocorrelación",
    "text": "Autocorrelación\nLas muestras tienen que ser independientes. La dependencia de valores anteriores tiene que desaparecer rápido . Podemos medirlo con la autocorrelación.\nPara cada valor de lag \\(k\\) se calcula la correlación de la serie consigo misma originando la función de autocorrelación (\\(ACF(k)\\))"
  },
  {
    "objectID": "presentaciones/presentacion_05.html#número-efectivo-de-muestras",
    "href": "presentaciones/presentacion_05.html#número-efectivo-de-muestras",
    "title": "Estadística Bayesiana",
    "section": "Número efectivo de muestras",
    "text": "Número efectivo de muestras\nLas muestras no son independientes. ¿A cuántas muestras independientes equivalen nuestras \\(S\\) muestras? \\(N_{eff}\\) es el número de muestras independientes que tienen el mismo poder de estimación que \\(S\\) muestras correlacionadas (el error de estimación es proporcional a \\(\\frac{1}{\\sqrt{N_{eff}}}\\))\n\\[N_{eff} = \\frac{S}{1 + 2 \\sum_{k=1}^\\infty ACF(k)}\\]"
  },
  {
    "objectID": "presentaciones/presentacion_05.html#hatr",
    "href": "presentaciones/presentacion_05.html#hatr",
    "title": "Estadística Bayesiana",
    "section": "\\(\\hat{R}\\)",
    "text": "\\(\\hat{R}\\)\nEl estadístico de Rubin–Gelman \\(\\hat{R}\\) es un indicador de convergencia. Si múltiples cadenas se establizaron en un muestreo representativo del posterior, la diferencia promedio entre cadenas debe ser similar a la diferencia promedio en la cadena.\n\\[\\hat{R} = \\sqrt{\\frac{\\frac{S-1}{S} W  +  \\frac{1}{S}  B}{W}}\\]\nEl valor 1 indica convergencia. Si una cadena se perdió/divergió, el \\(\\hat{R}\\) será mucho mayor a 1."
  },
  {
    "objectID": "presentaciones/presentacion_05.html#section-18",
    "href": "presentaciones/presentacion_05.html#section-18",
    "title": "Estadística Bayesiana",
    "section": "",
    "text": "Si tenemos \\(M\\) cadenas, \\(\\theta_m\\), cada una de las cuales tiene \\(S\\) muestras \\(\\theta_m^{(s)}\\). La varianza entre cadenas (\\(B\\)) es:\n\\[B = \\frac{S}{M-1} \\sum_{m=1}^M (\\bar{\\theta}^{(\\bullet)}_{m} - \\bar{\\theta}^{(\\bullet)}_{\\bullet})^2\\]\n\\[\\bar{\\theta}_m^{(\\bullet)} = \\frac{1}{S} \\sum_{s = 1}^S \\theta_m^{(s)}\\]\n\\[\\bar{\\theta}^{(\\bullet)}_{\\bullet} = \\frac{1}{M} \\, \\sum_{m=1}^M \\bar{\\theta}_m^{(\\bullet)}\\]\nLa varianza intra cadena (\\(W\\)) es:\n\\[W = \\frac{1}{M} \\, \\sum_{m=1}^M s_m^2\\]\n\\[ s_m^2 = \\frac{1}{S-1} \\, \\sum_{s=1}^S (\\theta^{(s)}_m - \\bar{\\theta}^{(\\bullet)}_m)^2\\]"
  },
  {
    "objectID": "presentaciones/presentacion_05.html#section-19",
    "href": "presentaciones/presentacion_05.html#section-19",
    "title": "Estadística Bayesiana",
    "section": "",
    "text": "El estimador de la varianza total\n\\[\\widehat{\\mbox{var}}^{+}\\!(\\theta|y) = \\frac{N-1}{N}\\, W \\, + \\, \\frac{1}{N} \\, B\\]\n\\[\\hat{R} \\, = \\, \\sqrt{\\frac{\\widehat{\\mbox{var}}^{+}\\!(\\theta|y)}{W}}\\]"
  },
  {
    "objectID": "presentaciones/presentacion_05.html#hamiltonian-montecarlo",
    "href": "presentaciones/presentacion_05.html#hamiltonian-montecarlo",
    "title": "Estadística Bayesiana",
    "section": "Hamiltonian Montecarlo",
    "text": "Hamiltonian Montecarlo\n\nMetropolis-Hastings (MH) es una exploración a ciegas del espacio de parámetros\nLa distribución de propuesta de salto es fija\nEn las colas de la distribución, se proponen tanto saltos que se acercan al grueso (bulk) de la distribución como saltos que se alejan. Se rechazan muchos saltos propuestos.\nHamiltonian-Montecarlo (HMC) es una variante más eficiente de MCMC. Para lograr la eficiencia, los saltos propuestos se adaptan a la forma del posterior.\nLa forma del posterior está en su gradiente"
  },
  {
    "objectID": "presentaciones/presentacion_05.html#section-21",
    "href": "presentaciones/presentacion_05.html#section-21",
    "title": "Estadística Bayesiana",
    "section": "",
    "text": "HMC trata de aprovechar la geometría local del posterior para decidir dónde ir en la próxima iteración.\nSi bien MH no ignora por completo la forma del posterior, HMC utiliza más información (el gradiente)\nPara entender conceptualmente HMC se necesita un poco de imaginación y entender algo de Física"
  },
  {
    "objectID": "presentaciones/presentacion_05.html#section-22",
    "href": "presentaciones/presentacion_05.html#section-22",
    "title": "Estadística Bayesiana",
    "section": "",
    "text": "\\(p^*(\\theta\\mid y)\\) es el posterior sin normalizar. Consideraremos \\(-\\log[p^*(\\theta\\mid y)]\\).\nLos puntos de alta densidad de probabilidad (máximos locales de \\(p^*(\\theta\\mid y)\\)) se convierten en mínimos locales de \\(-\\log[p^*(\\theta\\mid y)]\\)\nLa lógica es la misma que en MH (después de todo, se trata de un algoritmo de MCMC): estamos en algún punto del espacio de parámetros y decidimos movernos a otro… Aquí cambia cómo proponemos un salto.\nPara ello, imaginamos un trineo (o culipatín, o bolita) que puede deslizarse por la superficie determinada por \\(-\\log[p^*(\\theta\\mid y)]\\)"
  },
  {
    "objectID": "presentaciones/presentacion_05.html#section-23",
    "href": "presentaciones/presentacion_05.html#section-23",
    "title": "Estadística Bayesiana",
    "section": "",
    "text": "Si soltamos el trineo en algún punto de la superficie, tenderá a deslizar hacia abajo de \\(-\\log[p^*(\\theta\\mid y)]\\) por efecto de la gravedad. E irá cada vez más rápido.\nEstá bueno que el trineo deslice hacia los mínimos de \\(-\\log[p^*(\\theta\\mid y)]\\) pues son zonas de alta densidad de probabilidad\nQuisiéramos que nuestro trineo explore otras zonas del posterior, para eso en lugar de soltar el trineo le damos un impulso inicial (velocidad inicial o momento).\nEste impulso inicial será aleatorio"
  },
  {
    "objectID": "presentaciones/presentacion_05.html#section-24",
    "href": "presentaciones/presentacion_05.html#section-24",
    "title": "Estadística Bayesiana",
    "section": "",
    "text": "Conociendo la posición inicial del trineo y el impulso que se le da (la velocidad inicial), la Física permite calcular cuál será su trayectoria (y por ende su posición después de un tiempo)\nLa posición final después de un tiempo será el nuevo \\(\\theta\\) propuesto. Es decir: mientras que en MH proponíamos un salto con la distribución \\(q(\\theta'\\mid\\theta)\\), aquí lo hacemos con un momento inicial y estudiando la posición del trineo.\nLuego se acepta o se rechaza el salto propuesto"
  },
  {
    "objectID": "presentaciones/presentacion_05.html#section-27",
    "href": "presentaciones/presentacion_05.html#section-27",
    "title": "Estadística Bayesiana",
    "section": "",
    "text": "HMC propone nuevos saltos de manera más sofisticada que MH\nBusca que los saltos propuestos sean hacia valores del parámetro más prometedores"
  },
  {
    "objectID": "presentaciones/presentacion_05.html#section-28",
    "href": "presentaciones/presentacion_05.html#section-28",
    "title": "Estadística Bayesiana",
    "section": "",
    "text": "Cómo calcular la trayectoria del trineo es una de las cuestiones claves del algoritmo. Planteamos la conservación de la energía:\n\\[\\mathcal{H}(\\theta,v) = U(\\theta) + K(v)\\]\n\\(\\mathcal{H}\\) se conoce como hamiltoniano y representa la energía total del sistema que es la suma de la energía potencial \\(U(\\theta)\\) (función de la posición \\(\\theta\\)) y la energía cinética \\(K(v)\\) (función de la velocidad \\(v\\)).\nSe toma \\(U(\\theta) = -\\log[p^*(\\theta\\mid y)]\\) y \\(K(v) = \\frac{1}{2} m v^2\\)"
  },
  {
    "objectID": "presentaciones/presentacion_05.html#section-29",
    "href": "presentaciones/presentacion_05.html#section-29",
    "title": "Estadística Bayesiana",
    "section": "",
    "text": "Las ecuaciones de Hamilton describen el cambio de \\(\\theta\\) y de \\(v\\) en función del tiempo\n\\[\\frac{d\\theta}{dt} = \\frac{\\partial \\mathcal{H}}{\\partial v}\\] \\[\\frac{dv}{dt} = -\\frac{\\partial \\mathcal{H}}{\\partial \\theta}\\]"
  },
  {
    "objectID": "presentaciones/presentacion_05.html#section-30",
    "href": "presentaciones/presentacion_05.html#section-30",
    "title": "Estadística Bayesiana",
    "section": "",
    "text": "Es necesario resolver estas ecuaciones… Queremos hallar la posición (\\(\\theta\\)) del trineo tras un tiempo. No se pueden resolver analíticamente. Discretizamos el tiempo estudiando \\(L\\) pequeños intervalitos de duración \\(\\varepsilon\\)"
  },
  {
    "objectID": "presentaciones/presentacion_05.html#section-31",
    "href": "presentaciones/presentacion_05.html#section-31",
    "title": "Estadística Bayesiana",
    "section": "",
    "text": "Se tiene: \\[\n\\small{\n  \\begin{aligned}\n  \\frac{dv}{dt} &\\approx \\frac{v_{t+\\varepsilon} - v_{t}}{\\varepsilon} = \\frac{v_{t_2} - v_{t_1}}{\\varepsilon} \\\\\n  \\frac{d\\theta}{dt} &\\approx \\frac{\\theta_{t+\\varepsilon} - \\theta_{t}}{\\varepsilon} = \\frac{\\theta_{t_2} - \\theta_{t_1}}{\\varepsilon}\n  \\end{aligned}\n}\n\\]\ncon lo cual: \\[\n\\small{\n  \\begin{aligned}\n  v_{t_2} &= v_{t_1} + \\varepsilon \\frac{dv}{dt} = v_{t_1}-\\varepsilon \\frac{\\partial \\mathcal{H}}{\\partial \\theta} \\\\\n  \\theta_{t_2} &= \\theta_{t_1} + \\varepsilon \\frac{d\\theta}{dt} = \\theta_{t_1} + \\varepsilon \\frac{\\partial \\mathcal{H}}{\\partial v}\n  \\end{aligned}\n}\n\\]\nEstas aproximaciones no son buenas…"
  },
  {
    "objectID": "presentaciones/presentacion_05.html#leapfrog-integrator",
    "href": "presentaciones/presentacion_05.html#leapfrog-integrator",
    "title": "Estadística Bayesiana",
    "section": "Leapfrog integrator",
    "text": "Leapfrog integrator\nSe parte de \\(t\\) y se busca \\(v\\) en \\(t+\\frac{\\varepsilon}{2}\\). Luego se busca \\(\\theta\\) en \\(t+\\varepsilon\\) usando el resultado anterior \\(v\\) en \\(\\frac{\\varepsilon}{2}\\).\n\\[v(t+\\frac{\\varepsilon}{2}) = v(t) - \\frac{\\varepsilon}{2} \\frac{\\partial \\mathcal{H}}{\\partial \\theta}\\rvert_t\\]\n\\[\\theta(t+\\varepsilon) = \\theta(t) + \\varepsilon \\frac{\\partial \\mathcal{H}}{\\partial v}\\rvert_{t+\\frac{\\varepsilon}{2}}\\]\n\\[v(t+\\varepsilon) = v(t+\\frac{\\varepsilon}{2}) - \\frac{\\varepsilon}{2} \\frac{\\partial \\mathcal{H}}{\\partial \\theta}\\rvert_{t+\\varepsilon}\\]"
  },
  {
    "objectID": "presentaciones/presentacion_05.html#section-33",
    "href": "presentaciones/presentacion_05.html#section-33",
    "title": "Estadística Bayesiana",
    "section": "",
    "text": "La elección de \\(\\varepsilon\\) es clave para el algoritmo. Si \\(L\\cdot\\varepsilon\\) es pequeño, tomará mucho tiempo explorar el posterior. Con un valor muy grande, ocurrirán giros en U."
  },
  {
    "objectID": "presentaciones/presentacion_05.html#section-35",
    "href": "presentaciones/presentacion_05.html#section-35",
    "title": "Estadística Bayesiana",
    "section": "",
    "text": "Propuesta: A partir de \\(\\theta^{(i)}\\) disparar una bolita en alguna dirección aleatoria, con una velocidad (momento lineal) aleatoria\nLeapfrog integration: Calcular una serie de \\(L\\) pasos (leapfrog steps) de duración fija \\(\\varepsilon\\) (step size): instantes dónde vamos a sacar una foto de la posición de la partícula\nAceptación: Obtener la posición final \\(\\theta^{(i+1)}\\) como la posición final luego de \\(L\\) steps siempre y cuando la aproximación haya sido buena (la energía se haya conservado)\n\n\nUn \\(\\varepsilon\\) pequeño da más resolución sobre la trayectoria, permitiendo que la bolita gire ángulos pronunciados (¿pero?).\n\n\nUn \\(\\varepsilon\\) grande hará que los saltos sean largos y podemos saltear el punto donde la partícula iba a girar (divergent transition)."
  },
  {
    "objectID": "presentaciones/presentacion_05.html#section-36",
    "href": "presentaciones/presentacion_05.html#section-36",
    "title": "Estadística Bayesiana",
    "section": "",
    "text": "Estadística Bayesiana – 2025"
  },
  {
    "objectID": "practica/practica_00.html",
    "href": "practica/practica_00.html",
    "title": "Práctica - Unidad 0",
    "section": "",
    "text": "El objetivo de los ejercicios de esta unidad es refrescar y consolidar los conocimientos relacionados con el cálculo de probabilidades y la manipulación de distribuciones de probabilidad.\n\n📌 ¡Argentina campeón!\nDe las siguientes expresiones cual(es) se corresponde(n) con el enunciado “la probabilidad de que Argentina gane la copa del mundo dado que es 18 de Diciembre de 2022”?\n\n\\(P(\\text{18 de Diciembre de 2022} \\mid \\text{Argentina campeón})\\).\n\\(P(\\text{Argentina campeón})\\).\n\\(P(\\text{Argentina campeón}, \\text{18 de Diciembre de 2022}) / P(\\text{18 de Diciembre de 2022})\\).\n\\(P(\\text{Argentina campeón} \\mid \\text{Diciembre})\\).\n\\(P(\\text{Argentina campeón} \\mid \\text{18 de Diciembre de 2022})\\). \n\n\n\n\n\n\nEl festejo de los campeones del mundo\n\n\n\n\n📌 De formulas al español\nEnuncie con palabras cada una de las expresiones del punto anterior. \n📌 Probabilidades condicionales\nSegún la definición de probabilidad condicional\n\n¿Cuál es el valor de \\(P(A \\mid A)\\)?\n¿Cuál es la probabilidad de \\(P(A, B)\\)?\n¿Cuál es la probabilidad de \\(P(A, B)\\) en el caso que \\(A\\) y \\(B\\) sean independientes?\n¿Cuándo se cumple que \\(P(A \\mid B) = P(A)\\)?\n¿Es posible que \\(P(A \\mid B) &gt; P(A)\\)? ¿Cuándo?\n¿Es posible que \\(P(A \\mid B) &lt; P(A)\\)? ¿Cuándo? \n\n📌 Amigarse con la función de densidad (I)\nSuponga \\(X \\sim \\text{Uniforme}(a, b)\\). Su soporte es \\(\\mathcal{S} = [a, b]\\) y su función de densidad de probabilidad es \\(p(x) = 1 / (b - a)\\) para todo \\(x \\in \\mathcal{S}\\).\n\nPruebe que \\(p(x)\\) es una función de densidad de probabilidad válida.\nEncuentre la media y la varianza de \\(X\\). \n\n📌 Amigarse con la función de densidad (II)\nSea \\(X\\) una variable aleatoria con soporte \\(X \\in \\mathcal{S} = [1, \\infty)\\). Encuentre la constante \\(c\\), en función de \\(\\theta\\), que haga que \\(p(x) = c \\exp(-x / \\theta)\\) sea una función de densidad de probabilidad válida. \n📌 En búsqueda de la distribución deseada\nSegún personas expertas en un problema determinado, se indica que el valor de un parámetro debe ser positivo y su distribución a priori debe tener media igual a 5 y varianza igual a 3. Encuentre una distribución que satisfaga estas condiciones. \n📌 Distribución conjunta, marginal y condicional\nSean \\(X_1\\) y \\(X_2\\) dos variables aleatorias con función de probabilidad conjunta dada por la siguiente tabla:\n\n\n\n\n\n\\(X_1\\) / \\(X_2\\)\n\\(X_2=0\\)\n\\(X_2=1\\)\n\n\n\n\n\\(X_1=0\\)\n\\(0.15\\)\n\\(0.15\\)\n\n\n\\(X_1=1\\)\n\\(0.15\\)\n\\(0.20\\)\n\n\n\\(X_1=2\\)\n\\(0.15\\)\n\\(0.20\\)\n\n\n\n\n\ndonde la celda de la primer fila y primer columna se lee \\(P(X_1=0, X_2=0)=0.15\\)\n\nObtenga la distribución marginal de \\(X_1\\).\nObtenga la distribución marginal de \\(X_2\\).\nObtenga la distribución condicional de \\(X_1\\) dado \\(X_2\\).\nObtenga la distribución condicional de \\(X_2\\) dado \\(X_1\\). \n\nDistribuciones marginales y condicionales de una normal\nSean \\(X_1\\) y \\(X_2\\) tales que el vector \\((X_1, X_2)\\) sigue una distribución normal bivariada con \\(\\mathbb{E}(X_1) = \\mathbb{E}(X_2) = 0\\), \\(\\mathbb{V}(X_1) = \\mathbb{V}(X_2) = 1\\) y \\(\\text{cor}(X_1, X_2) = \\rho\\)\n\nEncuentre la distribución marginal de \\(X_1\\).\nEncuentre la distribución condicional de \\(X_1\\) dado \\(X_2\\). \n\n📌 De bolas, pesos y distribuciones de probabilidad\nSuponga una urna \\(S\\) contiene un 30% de bolas verdes y un 70% de bolas rojas, y otra urna \\(E\\) contiene un 45% de bolas verdes y un 55% de bolas rojas. Una persona arroja una moneda de un peso argentino y selecciona una bola de una de las dos urnas dependiendo de si la moneda cae en sol o escudo. Si la moneda cae en sol, saca una bola de la urna \\(S\\) y si la moneda cae en escudo, saca una bola de la urna \\(E\\).\nConsidere las siguientes variables aleatorias:\n\\[\n\\begin{aligned}\nX &=\n    \\begin{cases}\n    1 & \\text{Si la moneda cae en sol} \\\\\n    0 & \\text{Si la moneda cae en escudo}\n    \\end{cases}\n\\\\\n\\\\\nY &=\n    \\begin{cases}\n    1 & \\text{Si la bola es verde} \\\\\n    0 & \\text{Si la bola es roja}\n    \\end{cases}\n\\end{aligned}\n\\]\n\nEncuentre la distribución conjunta de \\(X\\) e \\(Y\\) en una tabla.\nEncuentre \\(\\mathbb{E}(Y)\\). ¿Cuál es la probabilidad de que la bola sea verde?\nEncuentre \\(\\mathbb{V}(Y \\mid X = 0)\\), \\(\\mathbb{V}(Y \\mid X = 1)\\) Y \\(\\mathbb{V}(Y)\\). Considerando a la varianza como una medida de incertidumbre, explique de manera intuitiva por que algunas variancias son mas grandes que otras.\nSuponga que observa que la bola es verde. ¿Cuál es la probabilidad de que la moneda haya caido en escudo? \n\n\n\n\n\n\nMoneda de un peso argentino acuñada en 1995\n\n\n\n\nLuces de giro\nLas luces de giro en los automóviles se utilizan para indicar que se va a realizar alguna acción determinada. La acción depende del escenario donde se conduzca (urbano, ruta, rotonda, etc.) y la luz que se encienda (izquierda o derecha). En el uso urbano, se debe colocar la luz de giro correspondiente para indicar que se va a girar en un sentido determinado. Sin embargo, esto no siempre se realiza. Muchas veces sucede que un vehículo no muestra luz de giro, y sin embargo, gira. Aunque menos frecuente, también se da que el vehículo coloca la luz de giro, pero no gira. Un estudio reveló un vehículo gira en una de cada diez intersecciones que cruza, que la probabilidad de colocar la luz de giro cuando se va a doblar es de 0.87 y que la probabilidad de que se coloque la luz de giro cuando no se va a doblar es de 0.04. Si observa que un vehículo coloca la luz de giro, ¿cuál es la probabilidad de que efectivamente doble?\nProblema del cumpleaños\nHay \\(k\\) personas en una sala. Suponga que el cumpleaños de cada persona tiene la misma probabilidad de ocurrir en cualquiera de los 365 días del año (se excluye el 29 de febrero) y que los cumpleaños de las personas son independientes entre si. ¿Cuál es la probabilidad de que al menos un par de personas en el grupo cumplan los años el mismo día?\n\n\n\n\n\n\nFoto de Adi Goldstein en Unsplash\n\n\n\n\n🧩 Problema de concordancia de de Montmort\nConsidere un mazo de \\(n\\) cartas bien mezcladas, etiquetadas con números de 1 a \\(n\\). Se seleccionan las cartas de a una y se la da vuelta, diciendo en voz alta el número de cartas dadas vueltas desde 1 a \\(n\\). Para ganar el juego tiene que coincidir el número que se dice en voz alta con el número de la carta que se está dando vuelta – por ejemplo, si la séptima carta dada vuelta contiene el número 7. ¿Cuál es la probabilidad de ganar? ¿Depende de \\(n\\)? \n🧩 Problema de los dos sobres\nSupongamos que te presentan dos sobres con dinero. Un sobre contiene el doble de dinero que el otro, pero a simple vista son indistinguibles. Se te pide que escojas uno de los sobres. Antes de abrirlo se te ofrece la posibilidad de cambiarlo por el otro. ¿Cambiarías el sobre? ¿Por qué?",
    "crumbs": [
      "Práctica",
      "Unidad 0"
    ]
  },
  {
    "objectID": "practica/practica_00.html#repaso-de-probabilidad",
    "href": "practica/practica_00.html#repaso-de-probabilidad",
    "title": "Práctica - Unidad 0",
    "section": "",
    "text": "El objetivo de los ejercicios de esta unidad es refrescar y consolidar los conocimientos relacionados con el cálculo de probabilidades y la manipulación de distribuciones de probabilidad.\n\n📌 ¡Argentina campeón!\nDe las siguientes expresiones cual(es) se corresponde(n) con el enunciado “la probabilidad de que Argentina gane la copa del mundo dado que es 18 de Diciembre de 2022”?\n\n\\(P(\\text{18 de Diciembre de 2022} \\mid \\text{Argentina campeón})\\).\n\\(P(\\text{Argentina campeón})\\).\n\\(P(\\text{Argentina campeón}, \\text{18 de Diciembre de 2022}) / P(\\text{18 de Diciembre de 2022})\\).\n\\(P(\\text{Argentina campeón} \\mid \\text{Diciembre})\\).\n\\(P(\\text{Argentina campeón} \\mid \\text{18 de Diciembre de 2022})\\). \n\n\n\n\n\n\nEl festejo de los campeones del mundo\n\n\n\n\n📌 De formulas al español\nEnuncie con palabras cada una de las expresiones del punto anterior. \n📌 Probabilidades condicionales\nSegún la definición de probabilidad condicional\n\n¿Cuál es el valor de \\(P(A \\mid A)\\)?\n¿Cuál es la probabilidad de \\(P(A, B)\\)?\n¿Cuál es la probabilidad de \\(P(A, B)\\) en el caso que \\(A\\) y \\(B\\) sean independientes?\n¿Cuándo se cumple que \\(P(A \\mid B) = P(A)\\)?\n¿Es posible que \\(P(A \\mid B) &gt; P(A)\\)? ¿Cuándo?\n¿Es posible que \\(P(A \\mid B) &lt; P(A)\\)? ¿Cuándo? \n\n📌 Amigarse con la función de densidad (I)\nSuponga \\(X \\sim \\text{Uniforme}(a, b)\\). Su soporte es \\(\\mathcal{S} = [a, b]\\) y su función de densidad de probabilidad es \\(p(x) = 1 / (b - a)\\) para todo \\(x \\in \\mathcal{S}\\).\n\nPruebe que \\(p(x)\\) es una función de densidad de probabilidad válida.\nEncuentre la media y la varianza de \\(X\\). \n\n📌 Amigarse con la función de densidad (II)\nSea \\(X\\) una variable aleatoria con soporte \\(X \\in \\mathcal{S} = [1, \\infty)\\). Encuentre la constante \\(c\\), en función de \\(\\theta\\), que haga que \\(p(x) = c \\exp(-x / \\theta)\\) sea una función de densidad de probabilidad válida. \n📌 En búsqueda de la distribución deseada\nSegún personas expertas en un problema determinado, se indica que el valor de un parámetro debe ser positivo y su distribución a priori debe tener media igual a 5 y varianza igual a 3. Encuentre una distribución que satisfaga estas condiciones. \n📌 Distribución conjunta, marginal y condicional\nSean \\(X_1\\) y \\(X_2\\) dos variables aleatorias con función de probabilidad conjunta dada por la siguiente tabla:\n\n\n\n\n\n\\(X_1\\) / \\(X_2\\)\n\\(X_2=0\\)\n\\(X_2=1\\)\n\n\n\n\n\\(X_1=0\\)\n\\(0.15\\)\n\\(0.15\\)\n\n\n\\(X_1=1\\)\n\\(0.15\\)\n\\(0.20\\)\n\n\n\\(X_1=2\\)\n\\(0.15\\)\n\\(0.20\\)\n\n\n\n\n\ndonde la celda de la primer fila y primer columna se lee \\(P(X_1=0, X_2=0)=0.15\\)\n\nObtenga la distribución marginal de \\(X_1\\).\nObtenga la distribución marginal de \\(X_2\\).\nObtenga la distribución condicional de \\(X_1\\) dado \\(X_2\\).\nObtenga la distribución condicional de \\(X_2\\) dado \\(X_1\\). \n\nDistribuciones marginales y condicionales de una normal\nSean \\(X_1\\) y \\(X_2\\) tales que el vector \\((X_1, X_2)\\) sigue una distribución normal bivariada con \\(\\mathbb{E}(X_1) = \\mathbb{E}(X_2) = 0\\), \\(\\mathbb{V}(X_1) = \\mathbb{V}(X_2) = 1\\) y \\(\\text{cor}(X_1, X_2) = \\rho\\)\n\nEncuentre la distribución marginal de \\(X_1\\).\nEncuentre la distribución condicional de \\(X_1\\) dado \\(X_2\\). \n\n📌 De bolas, pesos y distribuciones de probabilidad\nSuponga una urna \\(S\\) contiene un 30% de bolas verdes y un 70% de bolas rojas, y otra urna \\(E\\) contiene un 45% de bolas verdes y un 55% de bolas rojas. Una persona arroja una moneda de un peso argentino y selecciona una bola de una de las dos urnas dependiendo de si la moneda cae en sol o escudo. Si la moneda cae en sol, saca una bola de la urna \\(S\\) y si la moneda cae en escudo, saca una bola de la urna \\(E\\).\nConsidere las siguientes variables aleatorias:\n\\[\n\\begin{aligned}\nX &=\n    \\begin{cases}\n    1 & \\text{Si la moneda cae en sol} \\\\\n    0 & \\text{Si la moneda cae en escudo}\n    \\end{cases}\n\\\\\n\\\\\nY &=\n    \\begin{cases}\n    1 & \\text{Si la bola es verde} \\\\\n    0 & \\text{Si la bola es roja}\n    \\end{cases}\n\\end{aligned}\n\\]\n\nEncuentre la distribución conjunta de \\(X\\) e \\(Y\\) en una tabla.\nEncuentre \\(\\mathbb{E}(Y)\\). ¿Cuál es la probabilidad de que la bola sea verde?\nEncuentre \\(\\mathbb{V}(Y \\mid X = 0)\\), \\(\\mathbb{V}(Y \\mid X = 1)\\) Y \\(\\mathbb{V}(Y)\\). Considerando a la varianza como una medida de incertidumbre, explique de manera intuitiva por que algunas variancias son mas grandes que otras.\nSuponga que observa que la bola es verde. ¿Cuál es la probabilidad de que la moneda haya caido en escudo? \n\n\n\n\n\n\nMoneda de un peso argentino acuñada en 1995\n\n\n\n\nLuces de giro\nLas luces de giro en los automóviles se utilizan para indicar que se va a realizar alguna acción determinada. La acción depende del escenario donde se conduzca (urbano, ruta, rotonda, etc.) y la luz que se encienda (izquierda o derecha). En el uso urbano, se debe colocar la luz de giro correspondiente para indicar que se va a girar en un sentido determinado. Sin embargo, esto no siempre se realiza. Muchas veces sucede que un vehículo no muestra luz de giro, y sin embargo, gira. Aunque menos frecuente, también se da que el vehículo coloca la luz de giro, pero no gira. Un estudio reveló un vehículo gira en una de cada diez intersecciones que cruza, que la probabilidad de colocar la luz de giro cuando se va a doblar es de 0.87 y que la probabilidad de que se coloque la luz de giro cuando no se va a doblar es de 0.04. Si observa que un vehículo coloca la luz de giro, ¿cuál es la probabilidad de que efectivamente doble?\nProblema del cumpleaños\nHay \\(k\\) personas en una sala. Suponga que el cumpleaños de cada persona tiene la misma probabilidad de ocurrir en cualquiera de los 365 días del año (se excluye el 29 de febrero) y que los cumpleaños de las personas son independientes entre si. ¿Cuál es la probabilidad de que al menos un par de personas en el grupo cumplan los años el mismo día?\n\n\n\n\n\n\nFoto de Adi Goldstein en Unsplash\n\n\n\n\n🧩 Problema de concordancia de de Montmort\nConsidere un mazo de \\(n\\) cartas bien mezcladas, etiquetadas con números de 1 a \\(n\\). Se seleccionan las cartas de a una y se la da vuelta, diciendo en voz alta el número de cartas dadas vueltas desde 1 a \\(n\\). Para ganar el juego tiene que coincidir el número que se dice en voz alta con el número de la carta que se está dando vuelta – por ejemplo, si la séptima carta dada vuelta contiene el número 7. ¿Cuál es la probabilidad de ganar? ¿Depende de \\(n\\)? \n🧩 Problema de los dos sobres\nSupongamos que te presentan dos sobres con dinero. Un sobre contiene el doble de dinero que el otro, pero a simple vista son indistinguibles. Se te pide que escojas uno de los sobres. Antes de abrirlo se te ofrece la posibilidad de cambiarlo por el otro. ¿Cambiarías el sobre? ¿Por qué?",
    "crumbs": [
      "Práctica",
      "Unidad 0"
    ]
  },
  {
    "objectID": "practica/practica_04.html",
    "href": "practica/practica_04.html",
    "title": "Práctica - Unidad 4",
    "section": "",
    "text": "El objetivo principal de esta unidad es la aplicación de modelos de regresión lineales desde una perspectiva bayesiana, considerando a los parámetros del modelo como cantidades aleatorias que se corresponden con una distribución de probabilidad a priori. A diferencia del enfoque frecuentista o máximo verosímil, el resultado de la inferencia bayesiana es una distribución de probabilidad a posteriori, la cual se utiliza como fuente de todas las conclusiones. Además, se emplean técnicas propias de la estadística bayesiana para evaluar la adecuación y comparar los modelos utilizados.\n\n📌 Mi primer regresión bayesiana\nEl conjunto de datos sales contiene los montos semanales de inversión en publicidad y de ingresos de una determinada compañía. Considere el siguiente modelo de regresión lineal simple:\n\\[\n\\begin{aligned}\n\\text{ventas}_i &\\sim \\text{Normal}(\\mu_i, \\sigma^2) \\\\\n\\mu_i &= \\beta_0 + \\beta_1 \\text{publicidad}_i \\\\\n\\end{aligned}\n\\]\n\nAjuste el modelo utilizando {Rstan} y distribuciones uniformes como priors.\nConstruya un gráfico que muestre las ventas en función de la inversión en publicidad y superponga la recta de regresión estimada.\n\n📌 Mejorando mi regresión bayesiana\nConsidere la siguiente versión del modelo del ejercicio anterior que propone distribuciones a priori para los parámetros del modelo: \\[\n\\begin{aligned}\n\\text{ventas}_i &\\sim \\text{Normal}(\\mu_i, \\sigma^2) \\\\\n\\mu_i &= \\beta_0 + \\beta_1 \\text{publicidad}_i \\\\\n\\beta_0  &\\sim \\text{Normal}(\\overline{\\text{ventas}}, 10^2) \\\\\n\\beta_1  &\\sim \\text{Normal}(0, 0.5^2) \\\\\n\\sigma &\\sim \\text{Normal}^+(5)\n\\end{aligned}\n\\]\n\nAjuste el modelo utilizando {RStan} y los priors sugeridos.\nConstruya un gráfico que muestre las ventas en función de la inversión en publicidad, superponga la recta de regresión estimada, y el intervalo de credibilidad del 95% para la recta de regresión.\n\nRegresiones frecuentistas y bayesianas\nUtilice datos simulados para comparar la estimación por mínimos cuadrados con la estimación Bayesiana en modelos de regresión.\n\nSimule 100 observaciones del modelo \\(Y = 2 + 3X + \\varepsilon\\) donde los valores del predictor \\(X\\) se obtienen de una distribución \\(\\text{Uniforme}(0, 20)\\) y los errores son obtenidos de manera independiente de una distribución \\(\\text{Normal}(0, 5^2)\\).\nAjuste el modelo de regresión utilizando lm() y un modelo bayesiano mediante {RStan} utilizando priors uniformes.\nVerifique que ambos métodos arrojan resultados similares.\nRepresente gráficamente los datos y las dos rectas de regresión.\nIntente repetir la simulación, pero esta vez cree las condiciones para que ambos enfoques den resultados diferentes. \n\n📌 La altura… ¿se hereda?\nEl conjunto de datos de las alturas (heights) contiene las alturas (en pulgadas) de 5524 pares de madres e hijas registradas en un estudio realizado por Karl Pearson y Alice Lee en 1903.\n\nElabore un gráfico que permita ver la relación entre las alturas de las madres y las hijas. Aplique las técnicas que crea necesaria para obtener una visualización informativa y fidedigna.\n¿Por qué es adecuado utilizar un modelo de regresión lineal?\nAjuste el siguiente modelo de regresión lineal utilizando {Rstan} y priors que crea convenientes:\n\n\\[\n\\begin{aligned}\n\\text{altura hija}_i &\\sim \\text{Normal}(\\mu_i, \\sigma^2) \\\\\n\\mu_i &= \\beta_0 + \\beta_1 \\text{altura madre}_i\n\\end{aligned}\n\\]\n\nCalcule la media, el desvío estándar y el intervalo de credibilidad del 95% para los parámetros del modelo utilizando el posterior.\nInterprete los coeficientes del modelo.\nSuperponga la recta de regresión en el gráfico donde se visualiza la relación entre las variables.\nObtenga el posterior del peso medio de una hija cuya madre mide 58 pulgadas. \n\n📌 Clima en Australia\nEl conjunto de datos weather_WU datos climáticos correspondientes a 100 días en dos ciudades de Australia: Uluru y Wollongong. Se intentará predecir la temperatura a las 3 de la tarde, utilizando otras variables.\nConsidere los siguientes cuatro modelos:\n\n\\(m_1\\): temp3pm ~ temp9am;\n\\(m_2\\): temp3pm ~ location;\n\\(m_3\\): temp3pm ~ temp9am + location;\n\\(m_4\\): temp3pm ~ ..\n\n\nAjuste cada uno de los modelos y construya gráficas para mostrar los parámetros obtenidos.\nRealice pruebas predictivas a posteriori para comparar los modelos.\nCompare los ELPD de cada modelo utilizando LOO. \n\n\n\n\n\n\nParque Nacional Uluṟu-Kata Tjuṯa en Uluru, Australia. Foto de Snowscat en Unsplash\n\n\n\n\n📌 Pingüinos\nConsidere el dataset de pingüinos de Palmer (penguins) y los siguientes modelos:\n\n\\(m_1\\): body_mass_g ~ flipper_length_mm;\n\\(m_2\\): body_mass_g ~ species;\n\\(m_3\\): body_mass_g ~ flipper_length_mm + species;\n\\(m_4\\): body_mass_g ~ flipper_length_mm + species + flipper_length_mm:species;\n\\(m_5\\): body_mass_g ~ flipper_length_mm + bill_length_mm + bill_depth_mm.\n\n\nAjuste cada uno de los modelos y construya gráficas para mostrar los parámetros obtenidos.\nRealice pruebas predictivas a posteriori para comparar los modelos.\nCompare los ELPD de cada modelo utilizando LOO. \n\n📌 De tal palo…\nEl dataset child_iq contiene información de los resultados de tests de coeficiente intelectual de niños de 3 años, educación de la madre, y edad de la madre cuando dio a luz.\n\nAjuste un modelo de regresión del puntaje del bebé a los 3 años en función de la edad de la madre.\nAjsute ahora un modelo que incluya la educación de la madre.\nConstruya gráficas para mostrar los parámetros obtenidos.\nRealice pruebas predictivas a posteriori para comparar los modelos.\nCompare los ELPD de cada modelo utilizando LOO.\n\n\n📌 Ingresos\nEl dataset earnings contiene los resultados de la encuesta realizada por Ross sobre Trabajo, Familia y Bienestar.\n\nAjuste un modelo que prediga ingreso en función de altura e interprete los parámetros.\n¿Qué transformación sería necesaria para interpretar el intercepto como el ingreso promedio de una persona con altura promedio?\nAjuste un nuevo modelo utilizando la transformación propuesta en el punto anterior y compare los posteriors de los coeficientes.\n\n📌 !Kung\nLos !Kung son un pueblo que habita en el desierto de Kalahari entre Botsuana, Namibia y Angola. Hablan la lengua !Kung, que se destaca por su amplio uso de consonantes clic (chasquido consonántico). El !K del nombre ǃKung es un sonido como cuando sale un corcho de una botella.\nEl archivo Howell1 contiene datos de un censo parcial realizado por Dobe Howell acerca de la población !Kung.\nConsidere un modelo de altura en función del peso.\n\nDetermine e interprete las distribuciones a posteriori de los parámetros.\nConstruya un gráfico de altura en función del peso, incluya las observaciones de los individuos, la recta de regresión MAP, el intervalo del 80% para la media y y el intervalo del 80% para la altura predicha.\nRealice predicciones para individuos cuyos pesos son: 46.95, 43.72, 64.78, 32.59 y 54.63. Calcule la altura esperada y el intervalo del 89%.\n\n📌 Zorros urbanos\nConsidere del conjunto de datos sobre zorros urbanos (foxes). Ajuste tres modelos:\n\n\\(m_1\\): weight ~ area;\n\\(m_2\\): weight ~ groupsize;\n\\(m_3\\): weight ~ area + groupsize.\n\n\nPara los modelos \\(m_1\\) y \\(m_2\\), represente gráficamente los resultados, incluyendo la recta de regresión MAP, su intervalo del 89% y el intervalo de predicción del 89%. ¿Es alguna de las dos variables importantes para predecir la masa de un zorro?\nRepresentar gráficamente las predicciones del modelo para cada predictor, dejando el otro constante en su valor medio. ¿Qué puede decirse sobre la importancia de las variables para predecir la masa de un zorro?\n\n📌 Un prior informativo marca la diferencia\nConsidere el conjunto de datos sobre belleza y proporción de sexos (sexratio) . Estos datos provienen de un estudio de adolescentes estadounidenses cuyo atractivo en una escala de cinco puntos fue evaluado por entrevistadores en una encuesta cara a cara. Años más tarde, muchos de estos encuestados tuvieron hijos y se registraron ciertos atributos entre los cuales se incluyó el sexo. El objetivo del análisis es comparar la proporción de sexos de los hijos según la belleza de los padres. Para ello considere el siguiente modelo de regresión:\n\\[\n\\begin{aligned}\n\\text{pf}_i &\\sim \\text{Normal}(\\mu_i, \\sigma^2) \\\\\n\\mu_i &= \\beta_0 + \\beta_1 \\text{belleza}_i \\\\\n\\end{aligned}\n\\]\nDonde \\(\\text{pf}\\) representa la proporción de bebés de sexo femenino y \\(\\text{belleza}\\) representa el grupo de belleza de los padres.\n\nAjuste el modelo utilizando mínimos cuadrados.\nAjuste el modelo con {RStan} y priors uniformes.\nCompare el ajuste de ambos modelos.\nExplore los priors utilizados por {RStan} y la distribución predictiva a priori. ¿Qué puede concluir?\nProponga distribuciones a priori informativas.\nAjuste el modelo utilizando {RStan} y los priors informativos.\nCompare el resultado con los obtenidos anteriormente y concluya.\n\n📌 ¡A la pesca de priors!\nEl conjunto de datos fish-market contiene mediciones morfológicas realizadas sobre pescados de diferentes especies. El objetivo es construir un modelo de regresión lineal que permita predecir el peso de los pescados en base a sus otros atributos.\nUno de los modelos propuestos es el siguiente:\n\\[\n\\begin{aligned}\n\\log(\\text{Weight}_i) &\\sim \\text{Normal}(\\mu_i, \\sigma) \\\\\n\\mu_i  &= \\beta_{0, j[i]} + \\beta_{1, j[i]} \\log(\\text{Length1}_i)\n\\end{aligned}\n\\]\n\\(\\text{Weight}_i\\) es el peso del i-ésimo pescado en gramos y \\(\\text{Length1}_i\\) es la longitud del i-ésimo pescado en centímetros. La letra \\(j\\) indexa las especies de los pescados. Por lo tanto, en este modelo cada especie tiene su propio intercepto y pendiente.\n\nImplemente el modelo utilizando {RStan} y los siguientes priors no informativos: \\[\n\\begin{aligned}\n\\beta_{0, j[i]} &\\sim \\text{Normal}(0, 10) \\\\\n\\beta_{1, j[i]} &\\sim \\text{Normal}(0, 5)\n\\end{aligned}\n\\]\nObtenga y visualice la distribución predictiva a priori.\nElabore un gráfico y describa la función de densidad a priori de los parámetros \\(\\beta_{0, j[i]}\\) y \\(\\beta_{1, j[i]}\\).\nProponga priors más adecuados en base a la interpretación de los parámetros del modelo y la información que tenga del problema.\nNuevamente, obtenga y visualice la distribución predictiva a priori y compare con el resultado obtenido anteriormente.\nAjuste el modelo, obtenga la distribución predictiva a posteriori y grafíquela.\n\n\n\n\n\n\nEspecies de peces de todas variedades y tamaños.\n\n\n\n\nEn búsqueda del modelo adecuado\nContinuando con los datos del ejercicio anterior, El objetivo es construir un modelo de regresión lineal que permita predecir el peso de los pescados en base a sus otros atributos.\nConsidere los siguientes modelos\n\n\\(m_1\\): log(Weight) ~ 0 + Species;\n\\(m_2\\): log(Weight) ~ 0 + Species + log(Length1);\n\\(m_3\\): log(Weight) ~ 0 + Species + log(Length1):Species;\n\\(m_4\\): log(Weight) ~ 0 + Species + log(Length1):Species + log(Height);\n\\(m_5\\): log(Weight) ~ 0 + Species + log(Length1):Species + log(Height):Species.\n\n\nAjuste cada uno de los modelos.\nEstime el ELPD de cada modelo utilizando LOO y seleccione el modelo más adecuado de acuerdo a este criterio.\nExplique el resultado.\n\nComparación de modelos\nSe recopilaron datos (mesquite) con el fin de desarrollar un método para estimar la producción total (biomasa) de hojas de mesquite utilizando parámetros fácilmente medibles de la planta, antes de que se realice la cosecha real. Se tomaron dos conjuntos separados de mediciones, uno en un grupo de 26 arbustos de mesquite y otro en un grupo diferente de 20 arbustos de mesquite medidos en un momento diferente del año. Todos los datos se obtuvieron en la misma ubicación geográfica, pero ninguno constituyó una muestra estrictamente aleatoria. La variable de resultado es el peso total (en gramos) de material fotosintético obtenido de la cosecha real del arbusto. Las variables de entrada son:\n\n\n\n\n\n\n\nNombre\nDescripción\n\n\n\n\ndiam1\nDiámetro de la copa medido a lo largo del eje más largo del arbusto (metros)\n\n\ndiam2\nDiámetro de la copa medido a lo largo del eje más corto (metros)\n\n\ncanopy_height\nAltura de la copa\n\n\ntotal_height\nAltura total del arbusto\n\n\ndensity\nNúmero de tallos primarios por planta\n\n\ngroup\nGrupo de mediciones (0 para el primer grupo, 1 para el segundo)\n\n\n\n\nRealice un análisis exploratorio de los datos.\nAjuste el modelo weight ~ diam1 + diam2 + canopy_height + total_height + density + group.\nExplore y describa el posterior.\nEstime ELPD mediante PSIS-CV con la función loo(), analice los valores de las estimaciones del parámetro \\(k\\) de la distribución generalizada de Pareto y otros valores de la salida que crea relevant, ¿qué puede concluir?\nEstime ELPD mediante K-fold cross validation con \\(K=10\\). Compare la estimación con el resultado obtenido mediante PSIS-CV y concluya.\nAjuste el modelo transformando todas las variables numéricas con la función logarítmica. ¿Cómo afecta esta transformación la interpretación de los coeficientes?\nEstime ELPD mediante PSIS-CV con la función loo(). Concluya acerca de la estabilidad del cómputo. ¿Es posible comparar la la estimación con la obtenida en el inciso iv? ¿Por qué?\nCon ambos modelos, obtenga y grafique la distribución predictiva a posteriori comparándola con los datos observados ¿Cuál de los modelos representa mejor a los datos?\n\n\n\n\n\n\nUn árbol de mesquite.Foto de Sergei Bogomyakov, Alamy Stock Photo.\n\n\n\n\n\n\n🧩 Secundarios en Portugal\nSe cuenta con un conjunto de datos sobre 343 estudiantes de secundaria de Portugal (portugal) y se desea predecir la calificación final en matemáticas del último año en base a un gran número de predictores potencialmente relevantes.\nEl listado de variables se compone por: escuela del estudiante, sexo del estudiante, edad del estudiante, tipo de domicilio del estudiante, tamaño de la familia, estado de convivencia de los padres, educación de la madre, educación del padre, tiempo de viaje del hogar a la escuela, tiempo de estudio semanal, número de fracasos escolares pasados, apoyo educativo adicional, clases pagadas adicionales dentro de la materia del curso, actividades extracurriculares, si el estudiante asistió a una guardería, si el estudiante desea cursar estudios superiores, acceso a Internet en el hogar, si el estudiante tiene una relación romántica, calidad de las relaciones familiares, tiempo libre después de la escuela, si el estudiante sale con amigos, consumo de alcohol entre semana, consumo de alcohol los fines de semana, estado de salud actual y número de ausencias escolares.\nPriors débilmente informativos\n\nAjuste un modelo de regresión lineal utilizando todos los predictores luego de estandarizarlos y con los siguientes priors: \\[\n\\begin{aligned}\n\\beta_k &\\sim \\text{Normal}(0, 2.5) \\\\\n\\sigma  &\\sim \\text{Exponential}(1 / \\text{std}(y))\n\\end{aligned}\n\\]\nElabore un gráfico para visualizar los posteriors marginales y compárelos. ¿Qué puede concluir acerca de su incertidumbre?\nCalcule y compare la mediana del \\(R^2\\) bayesiano y del \\(R^2\\) calculado mediante LOO ¿Qué conclusión puede extraer de esta comparación?\n¿Cuál es el número efectivo de parámetros según LOO? ¿Qué indica?\nObtenga muestras del prior y del posterior del \\(R^2\\) bayesiano, compárelos utilizando una visualización y concluya considerando la elección de los priors débilmente informativos sobre \\(\\beta_k\\) y \\(\\sigma\\).\n\nPriors alternativos (I)\nSi se asume que muchos predictores pueden tener poca relevancia, se pueden escalar los priors independientes para que la suma de la varianza de los priors se encuentre alrededor de un valor razonable. En este caso, se cuenta con 26 predictores y se podría suponer que la proporción de la varianza explicada por los predictores está alrededor de 0.3. Entonces, un enfoque simple consiste en asignar priors independientes a los coeficientes de regresión con media 0 y desviación estándar \\(\\sqrt{0.3/26}\\text{sd}(y)\\) y un prior exponencial con media \\(\\sqrt{0.7}\\text{sd}(y)\\) para \\(\\sigma\\).\n\nAjuste el modelo nuevamente utilizando los siguientes priors: \\[\n\\begin{aligned}\n\\beta_k &\\sim \\text{Normal}(0, \\sqrt{\\frac{0.3}{26}}\\text{sd}(y)) \\\\\n\\sigma  &\\sim \\text{Exponential}(1 / \\sqrt{0.7}\\text{sd}(y))\n\\end{aligned}\n\\]\nExplore la distribución a priori sobre \\(R^2\\) y compárela con la distribución obtenida con los priors débilmente informativos.\nCalcule ELPD mediante LOO y compare este modelo con el anterior.\nElabore un gráfico para visualizar los posteriors marginales. Compare este resultado con el obtenido con los priors débilmente informativos.\n\nPriors alternativos (II)\nOtra alternativa es asumir que solo algunos de los predictores tienen alta relevancia y que el resto de los predictores tienen una relevancia insignificante. Una posibilidad para modelar bajo este supuesto es el horseshoe prior regularizado1. Este prior utiliza distribuciones normales independientes con media 0 y varianza \\(\\tau^2\\lambda_k^2\\) para los coeficientes de regresión \\(\\beta_k\\) y se describe a continuación:\n\\[\n\\begin{aligned}\n\\beta_k   &\\sim \\text{Normal}(0, \\tau^2\\tilde{\\lambda}_k^2) \\\\\n\\tilde{\\lambda}_k^2 &= \\frac{c^2\\lambda_k^2}{c^2 + \\tau^2\\lambda_k^2} \\\\\nc & = \\sqrt{c'} \\text{SS} \\\\\nc' &\\sim \\text{InvGamma}(0.5 \\cdot SDF, 0.5 \\cdot SDF) \\\\\n\\lambda_k &\\sim \\text{StudentT}^+(\\text{df} = 1, \\mu = 0, \\sigma = 1 ) \\\\\n\\tau      &\\sim \\text{StudentT}^+(\\text{df} = 1, \\mu = 0, \\sigma = \\text{GS})\n\\end{aligned}\n\\]\ncon \\[\n\\begin{aligned}\n\\text{GS} = \\frac{p_0}{p - p_0} \\frac{\\sigma}{\\sqrt{n}} \\\\\n\\text{SS} = \\sqrt{\\frac{0.3}{p_0}} \\text{sd}(y) \\\\\n\\text{SDF} = 4\n\\end{aligned}\n\\]\ndonde \\(\\text{GS}\\), \\(\\text{SS}\\) y \\(\\text{SDF}\\) representan global scale, slab scale y slab degrees of freedom, respectivamente. Además, \\(p\\) representa la cantidad de predictores, 26, y \\(p_0\\) la cantidad de predictores que se espera que sean relevantes.\nIntuitivamente, el parámetro global \\(\\tau\\) empuja todos los \\(\\beta_k\\) hacia el 0, mientras que los parámetros locales \\(\\lambda_k\\) contribuyen a que algunos de los \\(\\beta_k\\) escapen del 0.\n\nUtilice \\(p_0 = 6\\) para ajustar el modelo con todos los predictores y grafique y analice los posteriors marginales.\nCompare este modelo con los ajustados anteriormente en base a sus ELPD estimados con LOO y concluya.\n\nPriors débilmente informativos con menos predictores\nAjuste el modelo de regresión con un subconjunto de predictores que crea conveniente y los priors débilmente informativos que se utilizaron inicialmente.\n\nVisualice los posteriors marginales.\nNuevamente, calcule y compare la mediana del \\(R^2\\) bayesiano y del \\(R^2\\) calculado mediante.\nCompare este modelo con el ajustado anteriormente en base a sus ELPD estimados con LOO y concluya sobre la capacidad predictiva de este modelo.",
    "crumbs": [
      "Práctica",
      "Unidad 4"
    ]
  },
  {
    "objectID": "practica/practica_04.html#regresión-lineal",
    "href": "practica/practica_04.html#regresión-lineal",
    "title": "Práctica - Unidad 4",
    "section": "",
    "text": "El objetivo principal de esta unidad es la aplicación de modelos de regresión lineales desde una perspectiva bayesiana, considerando a los parámetros del modelo como cantidades aleatorias que se corresponden con una distribución de probabilidad a priori. A diferencia del enfoque frecuentista o máximo verosímil, el resultado de la inferencia bayesiana es una distribución de probabilidad a posteriori, la cual se utiliza como fuente de todas las conclusiones. Además, se emplean técnicas propias de la estadística bayesiana para evaluar la adecuación y comparar los modelos utilizados.\n\n📌 Mi primer regresión bayesiana\nEl conjunto de datos sales contiene los montos semanales de inversión en publicidad y de ingresos de una determinada compañía. Considere el siguiente modelo de regresión lineal simple:\n\\[\n\\begin{aligned}\n\\text{ventas}_i &\\sim \\text{Normal}(\\mu_i, \\sigma^2) \\\\\n\\mu_i &= \\beta_0 + \\beta_1 \\text{publicidad}_i \\\\\n\\end{aligned}\n\\]\n\nAjuste el modelo utilizando {Rstan} y distribuciones uniformes como priors.\nConstruya un gráfico que muestre las ventas en función de la inversión en publicidad y superponga la recta de regresión estimada.\n\n📌 Mejorando mi regresión bayesiana\nConsidere la siguiente versión del modelo del ejercicio anterior que propone distribuciones a priori para los parámetros del modelo: \\[\n\\begin{aligned}\n\\text{ventas}_i &\\sim \\text{Normal}(\\mu_i, \\sigma^2) \\\\\n\\mu_i &= \\beta_0 + \\beta_1 \\text{publicidad}_i \\\\\n\\beta_0  &\\sim \\text{Normal}(\\overline{\\text{ventas}}, 10^2) \\\\\n\\beta_1  &\\sim \\text{Normal}(0, 0.5^2) \\\\\n\\sigma &\\sim \\text{Normal}^+(5)\n\\end{aligned}\n\\]\n\nAjuste el modelo utilizando {RStan} y los priors sugeridos.\nConstruya un gráfico que muestre las ventas en función de la inversión en publicidad, superponga la recta de regresión estimada, y el intervalo de credibilidad del 95% para la recta de regresión.\n\nRegresiones frecuentistas y bayesianas\nUtilice datos simulados para comparar la estimación por mínimos cuadrados con la estimación Bayesiana en modelos de regresión.\n\nSimule 100 observaciones del modelo \\(Y = 2 + 3X + \\varepsilon\\) donde los valores del predictor \\(X\\) se obtienen de una distribución \\(\\text{Uniforme}(0, 20)\\) y los errores son obtenidos de manera independiente de una distribución \\(\\text{Normal}(0, 5^2)\\).\nAjuste el modelo de regresión utilizando lm() y un modelo bayesiano mediante {RStan} utilizando priors uniformes.\nVerifique que ambos métodos arrojan resultados similares.\nRepresente gráficamente los datos y las dos rectas de regresión.\nIntente repetir la simulación, pero esta vez cree las condiciones para que ambos enfoques den resultados diferentes. \n\n📌 La altura… ¿se hereda?\nEl conjunto de datos de las alturas (heights) contiene las alturas (en pulgadas) de 5524 pares de madres e hijas registradas en un estudio realizado por Karl Pearson y Alice Lee en 1903.\n\nElabore un gráfico que permita ver la relación entre las alturas de las madres y las hijas. Aplique las técnicas que crea necesaria para obtener una visualización informativa y fidedigna.\n¿Por qué es adecuado utilizar un modelo de regresión lineal?\nAjuste el siguiente modelo de regresión lineal utilizando {Rstan} y priors que crea convenientes:\n\n\\[\n\\begin{aligned}\n\\text{altura hija}_i &\\sim \\text{Normal}(\\mu_i, \\sigma^2) \\\\\n\\mu_i &= \\beta_0 + \\beta_1 \\text{altura madre}_i\n\\end{aligned}\n\\]\n\nCalcule la media, el desvío estándar y el intervalo de credibilidad del 95% para los parámetros del modelo utilizando el posterior.\nInterprete los coeficientes del modelo.\nSuperponga la recta de regresión en el gráfico donde se visualiza la relación entre las variables.\nObtenga el posterior del peso medio de una hija cuya madre mide 58 pulgadas. \n\n📌 Clima en Australia\nEl conjunto de datos weather_WU datos climáticos correspondientes a 100 días en dos ciudades de Australia: Uluru y Wollongong. Se intentará predecir la temperatura a las 3 de la tarde, utilizando otras variables.\nConsidere los siguientes cuatro modelos:\n\n\\(m_1\\): temp3pm ~ temp9am;\n\\(m_2\\): temp3pm ~ location;\n\\(m_3\\): temp3pm ~ temp9am + location;\n\\(m_4\\): temp3pm ~ ..\n\n\nAjuste cada uno de los modelos y construya gráficas para mostrar los parámetros obtenidos.\nRealice pruebas predictivas a posteriori para comparar los modelos.\nCompare los ELPD de cada modelo utilizando LOO. \n\n\n\n\n\n\nParque Nacional Uluṟu-Kata Tjuṯa en Uluru, Australia. Foto de Snowscat en Unsplash\n\n\n\n\n📌 Pingüinos\nConsidere el dataset de pingüinos de Palmer (penguins) y los siguientes modelos:\n\n\\(m_1\\): body_mass_g ~ flipper_length_mm;\n\\(m_2\\): body_mass_g ~ species;\n\\(m_3\\): body_mass_g ~ flipper_length_mm + species;\n\\(m_4\\): body_mass_g ~ flipper_length_mm + species + flipper_length_mm:species;\n\\(m_5\\): body_mass_g ~ flipper_length_mm + bill_length_mm + bill_depth_mm.\n\n\nAjuste cada uno de los modelos y construya gráficas para mostrar los parámetros obtenidos.\nRealice pruebas predictivas a posteriori para comparar los modelos.\nCompare los ELPD de cada modelo utilizando LOO. \n\n📌 De tal palo…\nEl dataset child_iq contiene información de los resultados de tests de coeficiente intelectual de niños de 3 años, educación de la madre, y edad de la madre cuando dio a luz.\n\nAjuste un modelo de regresión del puntaje del bebé a los 3 años en función de la edad de la madre.\nAjsute ahora un modelo que incluya la educación de la madre.\nConstruya gráficas para mostrar los parámetros obtenidos.\nRealice pruebas predictivas a posteriori para comparar los modelos.\nCompare los ELPD de cada modelo utilizando LOO.\n\n\n📌 Ingresos\nEl dataset earnings contiene los resultados de la encuesta realizada por Ross sobre Trabajo, Familia y Bienestar.\n\nAjuste un modelo que prediga ingreso en función de altura e interprete los parámetros.\n¿Qué transformación sería necesaria para interpretar el intercepto como el ingreso promedio de una persona con altura promedio?\nAjuste un nuevo modelo utilizando la transformación propuesta en el punto anterior y compare los posteriors de los coeficientes.\n\n📌 !Kung\nLos !Kung son un pueblo que habita en el desierto de Kalahari entre Botsuana, Namibia y Angola. Hablan la lengua !Kung, que se destaca por su amplio uso de consonantes clic (chasquido consonántico). El !K del nombre ǃKung es un sonido como cuando sale un corcho de una botella.\nEl archivo Howell1 contiene datos de un censo parcial realizado por Dobe Howell acerca de la población !Kung.\nConsidere un modelo de altura en función del peso.\n\nDetermine e interprete las distribuciones a posteriori de los parámetros.\nConstruya un gráfico de altura en función del peso, incluya las observaciones de los individuos, la recta de regresión MAP, el intervalo del 80% para la media y y el intervalo del 80% para la altura predicha.\nRealice predicciones para individuos cuyos pesos son: 46.95, 43.72, 64.78, 32.59 y 54.63. Calcule la altura esperada y el intervalo del 89%.\n\n📌 Zorros urbanos\nConsidere del conjunto de datos sobre zorros urbanos (foxes). Ajuste tres modelos:\n\n\\(m_1\\): weight ~ area;\n\\(m_2\\): weight ~ groupsize;\n\\(m_3\\): weight ~ area + groupsize.\n\n\nPara los modelos \\(m_1\\) y \\(m_2\\), represente gráficamente los resultados, incluyendo la recta de regresión MAP, su intervalo del 89% y el intervalo de predicción del 89%. ¿Es alguna de las dos variables importantes para predecir la masa de un zorro?\nRepresentar gráficamente las predicciones del modelo para cada predictor, dejando el otro constante en su valor medio. ¿Qué puede decirse sobre la importancia de las variables para predecir la masa de un zorro?\n\n📌 Un prior informativo marca la diferencia\nConsidere el conjunto de datos sobre belleza y proporción de sexos (sexratio) . Estos datos provienen de un estudio de adolescentes estadounidenses cuyo atractivo en una escala de cinco puntos fue evaluado por entrevistadores en una encuesta cara a cara. Años más tarde, muchos de estos encuestados tuvieron hijos y se registraron ciertos atributos entre los cuales se incluyó el sexo. El objetivo del análisis es comparar la proporción de sexos de los hijos según la belleza de los padres. Para ello considere el siguiente modelo de regresión:\n\\[\n\\begin{aligned}\n\\text{pf}_i &\\sim \\text{Normal}(\\mu_i, \\sigma^2) \\\\\n\\mu_i &= \\beta_0 + \\beta_1 \\text{belleza}_i \\\\\n\\end{aligned}\n\\]\nDonde \\(\\text{pf}\\) representa la proporción de bebés de sexo femenino y \\(\\text{belleza}\\) representa el grupo de belleza de los padres.\n\nAjuste el modelo utilizando mínimos cuadrados.\nAjuste el modelo con {RStan} y priors uniformes.\nCompare el ajuste de ambos modelos.\nExplore los priors utilizados por {RStan} y la distribución predictiva a priori. ¿Qué puede concluir?\nProponga distribuciones a priori informativas.\nAjuste el modelo utilizando {RStan} y los priors informativos.\nCompare el resultado con los obtenidos anteriormente y concluya.\n\n📌 ¡A la pesca de priors!\nEl conjunto de datos fish-market contiene mediciones morfológicas realizadas sobre pescados de diferentes especies. El objetivo es construir un modelo de regresión lineal que permita predecir el peso de los pescados en base a sus otros atributos.\nUno de los modelos propuestos es el siguiente:\n\\[\n\\begin{aligned}\n\\log(\\text{Weight}_i) &\\sim \\text{Normal}(\\mu_i, \\sigma) \\\\\n\\mu_i  &= \\beta_{0, j[i]} + \\beta_{1, j[i]} \\log(\\text{Length1}_i)\n\\end{aligned}\n\\]\n\\(\\text{Weight}_i\\) es el peso del i-ésimo pescado en gramos y \\(\\text{Length1}_i\\) es la longitud del i-ésimo pescado en centímetros. La letra \\(j\\) indexa las especies de los pescados. Por lo tanto, en este modelo cada especie tiene su propio intercepto y pendiente.\n\nImplemente el modelo utilizando {RStan} y los siguientes priors no informativos: \\[\n\\begin{aligned}\n\\beta_{0, j[i]} &\\sim \\text{Normal}(0, 10) \\\\\n\\beta_{1, j[i]} &\\sim \\text{Normal}(0, 5)\n\\end{aligned}\n\\]\nObtenga y visualice la distribución predictiva a priori.\nElabore un gráfico y describa la función de densidad a priori de los parámetros \\(\\beta_{0, j[i]}\\) y \\(\\beta_{1, j[i]}\\).\nProponga priors más adecuados en base a la interpretación de los parámetros del modelo y la información que tenga del problema.\nNuevamente, obtenga y visualice la distribución predictiva a priori y compare con el resultado obtenido anteriormente.\nAjuste el modelo, obtenga la distribución predictiva a posteriori y grafíquela.\n\n\n\n\n\n\nEspecies de peces de todas variedades y tamaños.\n\n\n\n\nEn búsqueda del modelo adecuado\nContinuando con los datos del ejercicio anterior, El objetivo es construir un modelo de regresión lineal que permita predecir el peso de los pescados en base a sus otros atributos.\nConsidere los siguientes modelos\n\n\\(m_1\\): log(Weight) ~ 0 + Species;\n\\(m_2\\): log(Weight) ~ 0 + Species + log(Length1);\n\\(m_3\\): log(Weight) ~ 0 + Species + log(Length1):Species;\n\\(m_4\\): log(Weight) ~ 0 + Species + log(Length1):Species + log(Height);\n\\(m_5\\): log(Weight) ~ 0 + Species + log(Length1):Species + log(Height):Species.\n\n\nAjuste cada uno de los modelos.\nEstime el ELPD de cada modelo utilizando LOO y seleccione el modelo más adecuado de acuerdo a este criterio.\nExplique el resultado.\n\nComparación de modelos\nSe recopilaron datos (mesquite) con el fin de desarrollar un método para estimar la producción total (biomasa) de hojas de mesquite utilizando parámetros fácilmente medibles de la planta, antes de que se realice la cosecha real. Se tomaron dos conjuntos separados de mediciones, uno en un grupo de 26 arbustos de mesquite y otro en un grupo diferente de 20 arbustos de mesquite medidos en un momento diferente del año. Todos los datos se obtuvieron en la misma ubicación geográfica, pero ninguno constituyó una muestra estrictamente aleatoria. La variable de resultado es el peso total (en gramos) de material fotosintético obtenido de la cosecha real del arbusto. Las variables de entrada son:\n\n\n\n\n\n\n\nNombre\nDescripción\n\n\n\n\ndiam1\nDiámetro de la copa medido a lo largo del eje más largo del arbusto (metros)\n\n\ndiam2\nDiámetro de la copa medido a lo largo del eje más corto (metros)\n\n\ncanopy_height\nAltura de la copa\n\n\ntotal_height\nAltura total del arbusto\n\n\ndensity\nNúmero de tallos primarios por planta\n\n\ngroup\nGrupo de mediciones (0 para el primer grupo, 1 para el segundo)\n\n\n\n\nRealice un análisis exploratorio de los datos.\nAjuste el modelo weight ~ diam1 + diam2 + canopy_height + total_height + density + group.\nExplore y describa el posterior.\nEstime ELPD mediante PSIS-CV con la función loo(), analice los valores de las estimaciones del parámetro \\(k\\) de la distribución generalizada de Pareto y otros valores de la salida que crea relevant, ¿qué puede concluir?\nEstime ELPD mediante K-fold cross validation con \\(K=10\\). Compare la estimación con el resultado obtenido mediante PSIS-CV y concluya.\nAjuste el modelo transformando todas las variables numéricas con la función logarítmica. ¿Cómo afecta esta transformación la interpretación de los coeficientes?\nEstime ELPD mediante PSIS-CV con la función loo(). Concluya acerca de la estabilidad del cómputo. ¿Es posible comparar la la estimación con la obtenida en el inciso iv? ¿Por qué?\nCon ambos modelos, obtenga y grafique la distribución predictiva a posteriori comparándola con los datos observados ¿Cuál de los modelos representa mejor a los datos?\n\n\n\n\n\n\nUn árbol de mesquite.Foto de Sergei Bogomyakov, Alamy Stock Photo.\n\n\n\n\n\n\n🧩 Secundarios en Portugal\nSe cuenta con un conjunto de datos sobre 343 estudiantes de secundaria de Portugal (portugal) y se desea predecir la calificación final en matemáticas del último año en base a un gran número de predictores potencialmente relevantes.\nEl listado de variables se compone por: escuela del estudiante, sexo del estudiante, edad del estudiante, tipo de domicilio del estudiante, tamaño de la familia, estado de convivencia de los padres, educación de la madre, educación del padre, tiempo de viaje del hogar a la escuela, tiempo de estudio semanal, número de fracasos escolares pasados, apoyo educativo adicional, clases pagadas adicionales dentro de la materia del curso, actividades extracurriculares, si el estudiante asistió a una guardería, si el estudiante desea cursar estudios superiores, acceso a Internet en el hogar, si el estudiante tiene una relación romántica, calidad de las relaciones familiares, tiempo libre después de la escuela, si el estudiante sale con amigos, consumo de alcohol entre semana, consumo de alcohol los fines de semana, estado de salud actual y número de ausencias escolares.\nPriors débilmente informativos\n\nAjuste un modelo de regresión lineal utilizando todos los predictores luego de estandarizarlos y con los siguientes priors: \\[\n\\begin{aligned}\n\\beta_k &\\sim \\text{Normal}(0, 2.5) \\\\\n\\sigma  &\\sim \\text{Exponential}(1 / \\text{std}(y))\n\\end{aligned}\n\\]\nElabore un gráfico para visualizar los posteriors marginales y compárelos. ¿Qué puede concluir acerca de su incertidumbre?\nCalcule y compare la mediana del \\(R^2\\) bayesiano y del \\(R^2\\) calculado mediante LOO ¿Qué conclusión puede extraer de esta comparación?\n¿Cuál es el número efectivo de parámetros según LOO? ¿Qué indica?\nObtenga muestras del prior y del posterior del \\(R^2\\) bayesiano, compárelos utilizando una visualización y concluya considerando la elección de los priors débilmente informativos sobre \\(\\beta_k\\) y \\(\\sigma\\).\n\nPriors alternativos (I)\nSi se asume que muchos predictores pueden tener poca relevancia, se pueden escalar los priors independientes para que la suma de la varianza de los priors se encuentre alrededor de un valor razonable. En este caso, se cuenta con 26 predictores y se podría suponer que la proporción de la varianza explicada por los predictores está alrededor de 0.3. Entonces, un enfoque simple consiste en asignar priors independientes a los coeficientes de regresión con media 0 y desviación estándar \\(\\sqrt{0.3/26}\\text{sd}(y)\\) y un prior exponencial con media \\(\\sqrt{0.7}\\text{sd}(y)\\) para \\(\\sigma\\).\n\nAjuste el modelo nuevamente utilizando los siguientes priors: \\[\n\\begin{aligned}\n\\beta_k &\\sim \\text{Normal}(0, \\sqrt{\\frac{0.3}{26}}\\text{sd}(y)) \\\\\n\\sigma  &\\sim \\text{Exponential}(1 / \\sqrt{0.7}\\text{sd}(y))\n\\end{aligned}\n\\]\nExplore la distribución a priori sobre \\(R^2\\) y compárela con la distribución obtenida con los priors débilmente informativos.\nCalcule ELPD mediante LOO y compare este modelo con el anterior.\nElabore un gráfico para visualizar los posteriors marginales. Compare este resultado con el obtenido con los priors débilmente informativos.\n\nPriors alternativos (II)\nOtra alternativa es asumir que solo algunos de los predictores tienen alta relevancia y que el resto de los predictores tienen una relevancia insignificante. Una posibilidad para modelar bajo este supuesto es el horseshoe prior regularizado1. Este prior utiliza distribuciones normales independientes con media 0 y varianza \\(\\tau^2\\lambda_k^2\\) para los coeficientes de regresión \\(\\beta_k\\) y se describe a continuación:\n\\[\n\\begin{aligned}\n\\beta_k   &\\sim \\text{Normal}(0, \\tau^2\\tilde{\\lambda}_k^2) \\\\\n\\tilde{\\lambda}_k^2 &= \\frac{c^2\\lambda_k^2}{c^2 + \\tau^2\\lambda_k^2} \\\\\nc & = \\sqrt{c'} \\text{SS} \\\\\nc' &\\sim \\text{InvGamma}(0.5 \\cdot SDF, 0.5 \\cdot SDF) \\\\\n\\lambda_k &\\sim \\text{StudentT}^+(\\text{df} = 1, \\mu = 0, \\sigma = 1 ) \\\\\n\\tau      &\\sim \\text{StudentT}^+(\\text{df} = 1, \\mu = 0, \\sigma = \\text{GS})\n\\end{aligned}\n\\]\ncon \\[\n\\begin{aligned}\n\\text{GS} = \\frac{p_0}{p - p_0} \\frac{\\sigma}{\\sqrt{n}} \\\\\n\\text{SS} = \\sqrt{\\frac{0.3}{p_0}} \\text{sd}(y) \\\\\n\\text{SDF} = 4\n\\end{aligned}\n\\]\ndonde \\(\\text{GS}\\), \\(\\text{SS}\\) y \\(\\text{SDF}\\) representan global scale, slab scale y slab degrees of freedom, respectivamente. Además, \\(p\\) representa la cantidad de predictores, 26, y \\(p_0\\) la cantidad de predictores que se espera que sean relevantes.\nIntuitivamente, el parámetro global \\(\\tau\\) empuja todos los \\(\\beta_k\\) hacia el 0, mientras que los parámetros locales \\(\\lambda_k\\) contribuyen a que algunos de los \\(\\beta_k\\) escapen del 0.\n\nUtilice \\(p_0 = 6\\) para ajustar el modelo con todos los predictores y grafique y analice los posteriors marginales.\nCompare este modelo con los ajustados anteriormente en base a sus ELPD estimados con LOO y concluya.\n\nPriors débilmente informativos con menos predictores\nAjuste el modelo de regresión con un subconjunto de predictores que crea conveniente y los priors débilmente informativos que se utilizaron inicialmente.\n\nVisualice los posteriors marginales.\nNuevamente, calcule y compare la mediana del \\(R^2\\) bayesiano y del \\(R^2\\) calculado mediante.\nCompare este modelo con el ajustado anteriormente en base a sus ELPD estimados con LOO y concluya sobre la capacidad predictiva de este modelo.",
    "crumbs": [
      "Práctica",
      "Unidad 4"
    ]
  },
  {
    "objectID": "practica/practica_04.html#footnotes",
    "href": "practica/practica_04.html#footnotes",
    "title": "Práctica - Unidad 4",
    "section": "Notas",
    "text": "Notas\n\n\nEl nombre horseshoe (herradura) no se relaciona con la forma de la función de densidad en sí, sino con la forma del prior implícito para los coeficientes de shrinkage (contracción) aplicados a cada parámetro.↩︎",
    "crumbs": [
      "Práctica",
      "Unidad 4"
    ]
  },
  {
    "objectID": "practica/practica_03.html",
    "href": "practica/practica_03.html",
    "title": "Práctica - Unidad 3",
    "section": "",
    "text": "Esta sección contiene una lista exhaustiva de ejercicios que requieren el uso de herramientas computacionales para resolver problemas que involucran una variedad de cálculos, como el cálculo de probabilidades y el cálculo de integrales. Se vuelve indispensable el uso de R y se promueve el uso de buenas prácticas de programación científica, como el uso de funciones compartimentar los componentes de un programa.\n\n📌 ¡A calcular probabilidades! (I)\nSea \\(X \\sim \\text{Normal}(\\mu=3, \\sigma=1.2)\\).\n\nElabore un gráfico que permita visualizar la función de densidad de probabilidad de \\(X\\).\n¿Cuál es la probabilidad de que \\(X\\) sea menor a 2.5?\n¿Cuál es la probabilidad de que \\(X\\) sea mayor a 4?\n¿Cuál es la probabilidad de que \\(X\\) sea mayor 2 y menor 3?\n\n📌 ¡A calcular probabilidades! (II)\nSea \\(X \\sim \\text{Beta}(\\alpha=10, \\beta=2)\\).\n\nElabore un gráfico que permita visualizar la función de densidad de probabilidad de \\(X\\).\n¿Cuál es la probabilidad de que \\(X\\) sea menor a 0.5?\n¿Cuál es la probabilidad de que \\(X\\) sea mayor a 0.8?\n¿Cuál es la probabilidad de que \\(X\\) sea mayor 0.25 y menor 0.75?\n\n📌 Magia blanca: Obtener probabilidades mediante simulación\nResponda los dos puntos anteriores sin evaluar la función de densidad ni la función de distribución de las variables aleatorias mencionadas. Para eso genere muestras que provengan de las correspondientes distribuciones y utilícelas para responder las preguntas mencionadas. Reflexione sobre las ventajas y desventajas de utilizar un enfoque basado en la simulación para resolver problemas.\n📌 Media y varianza de una variable aleatoria\nUna variable aleatoria \\(X\\) toma valores en el conjunto \\(\\{2, 4, 6, 8, 10\\}\\) con igual probabilidad. Encuentre la media y el desvío estándar de las variables \\(X\\) e \\(Y = 2X + 1\\). \n📌 Probabilidades a posteriori\nEn un problema determinado la distribución a posteriori de la parámetro de inteŕes \\(\\alpha\\) es \\(\\Gamma(k=3, \\theta=1.5)\\), donde \\(k\\) es el parámetro de forma y \\(\\theta\\) es el parámetro de escala. Calcule la probabilidad de que \\(\\alpha^2\\) sea mayor a 10. \n📌 Probabilidades con dos variables aleatorias\nSean \\(X\\) e \\(Y\\) dos variables aleatorias independientes con distribución uniforme en el intervalo \\([0, 1]\\).\n\n¿Cuál es la probabilidad de que \\(X \\le Y\\)?\nGrafique los puntos muestreados coloreando de acuerdo a si la muestra satisface el evento antes mencionado o no.\n\nTe veo en la fotocopiadora\nDos estudiantes de estadística deciden encontrarse en la fotocopiadora de la Facultad entre las 10 y las 11 de la mañana, eligiendo el tiempo de llegada al azar. La estudiante A esperará 10 minutos luego de llegar. Si el estudiante B no llega en ese intervalo, se irá. Lo mismo hace el estudiante B, pero este decide esperar 14 minutos. ¿Cuál es la probabilidad de que se produzca el encuentro en la fotocopiadora entre la estudiante A y el estudiante B?\n🧩 Armando celulares en Tierra del Fuego\nUna máquina que se utiliza para ensamblar teléfonos celulares en una fábrica en Tierra del Fuego cuenta con tres componentes críticos para su funcionamiento. Ante una falla en cualquiera de estos componentes, la máquina se detiene. Las probabilidades de que estos elementos operen correctamente durante un día cualquiera son \\(p_1 = 0.8\\), \\(p_2 = 0.9\\) y \\(p_3 = 0.7\\). Responda las siguientes preguntas utilizando técnicas de simulación:\n\n¿Cuál es la probabilidad de que la máquina falle en el primer día de uso?\n¿Cuál es la probabilidad de que la máquina siga funcionando luego de 10 días?\n¿Cuál es la probabilidad de que la máquina falle en el día 7 de uso?\nSea \\(T=\\) Cantidad de días que la máquina funciona ininterrumpidamente. Grafique la función de densidad de probabilidad de \\(T\\).\n\nBolas infinitas\nEste tuit propone un problema muy interesante. Una urna contiene una bola azul y una amarilla. Se elije una bola al azar y se la vuelve a colocar junto con otra bola adicional del mismo color. Se repite este proceso indefinidamente. ¿Qué ocurre con la proporción de bolas azules en la urna a medida que repetimos más y más veces?\n\nTiende a 1/2.\nTiende a 0 ó a 1.\nNo se estabiliza.\nNinguna de las anteriores.\n\nEscriba un programa en R para responder esta pregunta utilizando simulaciones. Genere gráficos que faciliten la comprensión del resultado. \n📌 Estimando el valor de \\(\\pi\\)\nImagine un círculo de radio \\(r\\) y un cuadrado de lado \\(2r\\), ambos centrados en el mismo punto, que de manera arbitraria puede ser el punto \\((0, 0)\\). Obtenga muestras de una distribución uniforme en el plano \\((x, y)\\), cuyo dominio está acotado por el cuadrado antes mencionado. Para cada muestra extraida, determine si se encuentra dentro del círculo o no – todos las muestras se encontrarán dentro del cuadrado. Utilice esta información para estimar el valor de \\(\\pi\\).\n\n\n\n\n\n\n\n\n\nAlgunos datos útiles\n\nArea de un círculo: \\(\\pi \\cdot r^2\\).\nArea de un cuadrado: \\(a^2\\), donde \\(a\\) es la longitud del lado.\n\nLos puntos uniformes\nSe seleccionan dos puntos de manera uniforme e independiente dentro de un círculo. ¿Cuál es la probabilidad de que la distancia entre dos puntos sea menor al radio?\n\nResuelva el problema utilizando R.\nElabore una visualización que facilite la comunicación de los resultados.\n\n\n\n\n\n\n\nAyuda\n\n\n\n\n\nUna forma de simular puntos con distribución uniforme dentro de un círculo de radio \\(r\\) es la siguiente:\n\nElegir un ángulo \\(\\theta\\) al azar, uniformemente en \\([0, 2\\pi)\\).\nElegir una distancia \\(\\delta\\) al centro del círculo, de manera adecuada para garantizar uniformidad en el área.\n\nFinalmente, se convierten las coordenadas polares a cartesianas con: \\[\n\\begin{aligned}\nx &= \\delta \\times \\cos(\\theta) \\\\\ny &= \\delta \\times \\sin(\\theta)\n\\end{aligned}\n\\]\nNota : Para que los puntos queden uniformemente distribuidos en el área del círculo, \\(\\delta\\) no debe elegirse uniformemente en \\([0, r]\\), sino como \\(\\delta = r \\times \\sqrt{U}\\), donde \\(U \\sim \\text{Uniforme}(0, 1)\\).\n# Cantidad de puntos\nn &lt;- 1000\n\n# Radio del circulo\nradio &lt;- 3\n\n# Ángulo del punto\ntheta &lt;- runif(n, 0, 2 * pi)\n\n# Distancia al centro de coordenadas\ndelta &lt;- radio * sqrt(runif(n, 0, 1))\n\n\n\nSobre el histórico 7 a 1 del 2014\nEn la Copa del Mundo de la FIFA 2014, Alemania jugó contra Brasil en la semifinal. Los alemanes hicieron el primer gol a los 11 minutos y el segundo a los 23. Asuma que el tiempo entre goles sigue una distribución exponencial. Elija una distribución a priori para el tiempo entre goles (puede ser conjugada o no). En ese momento del partido,\n\n¿Cuál es la distribución a posteriori del tiempo entre goles de Alemania?\n¿Cuántos goles cabría esperar que Alemania hiciera al finalizar los 90 minutos?\n¿Cuál era la probabilidad de que Alemania hiciera más de 5 goles (cosa que ocurrió)? \n\n💻 ¿Tendré que esperar mucho?\nEl tiempo que un empleado de recursos humanos demora en hacer una entrevista tiene distribución exponencial con media 30 minutos. Los tiempos de duración de cada entrevista se pueden considerar independientes entre sí. Las entrevistas a postulantes para un trabajo están programadas cada 15 minutos, comenzando desde las 8. Es válido considerar que todos los postulantes llegan puntuales a su entrevista. Cuando la persona del turno de las 8:15 llega a la oficina\n\n¿Cuál es la probabilidad de que tenga que esperar antes de ser entrevistada?\n¿Cuál es el horario esperado al que terminará su entrevista? \n\n🧩 ¡Qué casualidad!\nDos personas se conocen en la fila de embarque para un vuelo en un avión Airbus A330-300. Considere que el Airbus A330-300 tiene 30 filas de 2-4-2 asientos.\n\n¿Cuál es la probabilidad de que tengan asientos en la misma fila?\n¿Cuál es la probabilidad de que estén sentados en asientos adyacentes?\n\n✍️ 💻 📌 El Problema de Monty Hall\nEl Problema de Monty Hall es un problema de probabilidad basado en un juego del concurso televisivo estadounidense “Trato hecho”. En este problema, el concursante debe elegir una puerta entre tres, todas cerradas. El premio consiste en llevarse lo que se encuentra detrás de la elegida. Se sabe con certeza que tras una de ellas se oculta un automóvil, y tras las otras dos hay cabras. Una vez que el concursante haya elegido una puerta y comunicado su elección a los presentes, el presentador, que sabe lo que hay detrás de cada puerta, abrirá una de las otras dos en la que haya una cabra. A continuación, le da la opción al concursante de cambiar, si lo desea, de puerta (tiene dos opciones). ¿Debe el concursante mantener su elección original o escoger la otra puerta? ¿Hay alguna diferencia? Resuelva este ejercicio utilizando simulaciones. \n\n\n\n\n\nLas 3 puertas del problema de Monty Hall\n\n\n\n\n💻 Que los cumplan feliz\nBasándose en el siguiente tuit y conociendo el problema del cumpleaños (¿cuántas personas debe haber en una habitación para que la probabilidad de que dos de ellas cumplan años el mismo día sea mayor a X%?) construir un gráfico similar al del tuit donde se grafique la probabilidad de que haya \\(n\\) personas que cumplan años el mismo día para \\(K\\) personas presentes en la habitación.\n💻 Qué suerte, ¿no?\nPrevio a la final de la Copa América 2021, los jugadores de la Selección Argentina se reúnen en la habitación del hotel como se describe en este tuit.\n\n¿Cuál es la probabilidad de que un jugador adivine una de diez cartas?\n¿Cuál es la probabilidad de que tres de ellos adivinen una de diez cartas?\n\nEl álbum del Campeón\nEl álbum oficial del Mundial de Fútbol de Qatar 2022 consta de 638 figuritas. Cada paquete trae cinco figuritas.\n\nComprando cinco paquetes, ¿cuál es la probabilidad de tener a Messi?\nComprando cinco paquetes, ¿cuál es la probabilidad de sacar a Messi repetido?\n¿Cuántos paquetes se necesitan, en promedio, para completar el álbum?\nSi a una persona le faltan diez figuritas para completar el álbum, ¿cuántos paquetes tiene que comprar para asegurarse de lograrlo?\n\n💻 ¿Que tán raras son estas secuencias raras?\nSi se arroja una moneda \\(n\\) veces, ¿cuál es la probabilidad de que no haya secuencias de \\(k\\) caras?\n📌 Un viaje por el elevador\n¿Cuál es la probabilidad de que tres personas en un ascensor con doce pisos presionen para ir a tres pisos consecutivos? ¿Qué supuestos realiza para resolver el problema? Escríbalos en una lista de manera explícita.\nLa vida es muy corta como para perderla ordenando medias\nUn cajón contiene 10 pares de medias. No hay dos pares iguales. Por fiaca, el dueño de las medias no las agrupa después de lavarlas y simplemente las pone en el cajón. Al momento de necesitar un par de medias, saca una tras una hasta que se forma un par. En promedio, ¿cuántas medias sacará hasta encontrar un par? \n¿Vale la pena hacer un ensayo clínico a gran escala?\nDados los resultados de un estudio piloto, la probabilidad a posteriori de que la droga desarrollada por tu compañía sea mas efectiva que el tratamiento actual es \\(\\theta \\in [0, 1]\\). Tu compañía está considerando realizar un ensayo clínico a gran escala para confirmar que la droga que desarrollan es de hecho mejor. El costo del estudio es $X. Si la droga es mejor, la probabilidad de que esto se confirme en el ensayo es del 80%. Si la droga no es mejor, hay una probabilidad del 5% de que el estudio confirme que es mejor. Si el ensayo sugiere que tu droga es mejor, ganarás $cX. ¿Para qué valores de \\(\\theta\\) y \\(c\\) tiene sentido realizar el estudio? \nEl problema de concordancia\nResuelva el problema de concordancia de de Montmort presentado en la Práctica 0 utilizando simulaciones. \nEl problema de los sobres\nResuelva el problema de los dos sobres presentado en la Práctica 0 utilizando simulaciones.\n🧩 Integración por muestreo\nCalcule las siguientes integrales utilizando muestras.\n\n\\(\\displaystyle \\int_{-\\infty}^{\\infty}{\\frac{x^2}{\\sqrt{2\\pi}}\\exp \\left(-\\frac{x^2}{2} \\right)dx}\\)\n\\(\\displaystyle \\int_{1}^{\\infty}{\\frac{x^3}{\\sqrt{2\\pi}}\\exp \\left(-\\frac{x^2}{2} \\right)dx}\\)\n\\(\\displaystyle \\int_{1}^{\\infty}{\\frac{x^6}{\\sqrt{2\\pi}}\\exp \\left(-\\frac{x^2 - 4x}{2} \\right)dx}\\)\n\\(\\displaystyle \\int_{1}^{10}{x^6\\frac{e^{-x^4/2}}{\\sqrt{2\\pi}}dx}\\) \n\n¿Cuál es la distribución de muestreo aproximada al usar muestreo independiente para evaluar integrales?",
    "crumbs": [
      "Práctica",
      "Unidad 3"
    ]
  },
  {
    "objectID": "practica/practica_03.html#métodos-computacionales",
    "href": "practica/practica_03.html#métodos-computacionales",
    "title": "Práctica - Unidad 3",
    "section": "",
    "text": "Esta sección contiene una lista exhaustiva de ejercicios que requieren el uso de herramientas computacionales para resolver problemas que involucran una variedad de cálculos, como el cálculo de probabilidades y el cálculo de integrales. Se vuelve indispensable el uso de R y se promueve el uso de buenas prácticas de programación científica, como el uso de funciones compartimentar los componentes de un programa.\n\n📌 ¡A calcular probabilidades! (I)\nSea \\(X \\sim \\text{Normal}(\\mu=3, \\sigma=1.2)\\).\n\nElabore un gráfico que permita visualizar la función de densidad de probabilidad de \\(X\\).\n¿Cuál es la probabilidad de que \\(X\\) sea menor a 2.5?\n¿Cuál es la probabilidad de que \\(X\\) sea mayor a 4?\n¿Cuál es la probabilidad de que \\(X\\) sea mayor 2 y menor 3?\n\n📌 ¡A calcular probabilidades! (II)\nSea \\(X \\sim \\text{Beta}(\\alpha=10, \\beta=2)\\).\n\nElabore un gráfico que permita visualizar la función de densidad de probabilidad de \\(X\\).\n¿Cuál es la probabilidad de que \\(X\\) sea menor a 0.5?\n¿Cuál es la probabilidad de que \\(X\\) sea mayor a 0.8?\n¿Cuál es la probabilidad de que \\(X\\) sea mayor 0.25 y menor 0.75?\n\n📌 Magia blanca: Obtener probabilidades mediante simulación\nResponda los dos puntos anteriores sin evaluar la función de densidad ni la función de distribución de las variables aleatorias mencionadas. Para eso genere muestras que provengan de las correspondientes distribuciones y utilícelas para responder las preguntas mencionadas. Reflexione sobre las ventajas y desventajas de utilizar un enfoque basado en la simulación para resolver problemas.\n📌 Media y varianza de una variable aleatoria\nUna variable aleatoria \\(X\\) toma valores en el conjunto \\(\\{2, 4, 6, 8, 10\\}\\) con igual probabilidad. Encuentre la media y el desvío estándar de las variables \\(X\\) e \\(Y = 2X + 1\\). \n📌 Probabilidades a posteriori\nEn un problema determinado la distribución a posteriori de la parámetro de inteŕes \\(\\alpha\\) es \\(\\Gamma(k=3, \\theta=1.5)\\), donde \\(k\\) es el parámetro de forma y \\(\\theta\\) es el parámetro de escala. Calcule la probabilidad de que \\(\\alpha^2\\) sea mayor a 10. \n📌 Probabilidades con dos variables aleatorias\nSean \\(X\\) e \\(Y\\) dos variables aleatorias independientes con distribución uniforme en el intervalo \\([0, 1]\\).\n\n¿Cuál es la probabilidad de que \\(X \\le Y\\)?\nGrafique los puntos muestreados coloreando de acuerdo a si la muestra satisface el evento antes mencionado o no.\n\nTe veo en la fotocopiadora\nDos estudiantes de estadística deciden encontrarse en la fotocopiadora de la Facultad entre las 10 y las 11 de la mañana, eligiendo el tiempo de llegada al azar. La estudiante A esperará 10 minutos luego de llegar. Si el estudiante B no llega en ese intervalo, se irá. Lo mismo hace el estudiante B, pero este decide esperar 14 minutos. ¿Cuál es la probabilidad de que se produzca el encuentro en la fotocopiadora entre la estudiante A y el estudiante B?\n🧩 Armando celulares en Tierra del Fuego\nUna máquina que se utiliza para ensamblar teléfonos celulares en una fábrica en Tierra del Fuego cuenta con tres componentes críticos para su funcionamiento. Ante una falla en cualquiera de estos componentes, la máquina se detiene. Las probabilidades de que estos elementos operen correctamente durante un día cualquiera son \\(p_1 = 0.8\\), \\(p_2 = 0.9\\) y \\(p_3 = 0.7\\). Responda las siguientes preguntas utilizando técnicas de simulación:\n\n¿Cuál es la probabilidad de que la máquina falle en el primer día de uso?\n¿Cuál es la probabilidad de que la máquina siga funcionando luego de 10 días?\n¿Cuál es la probabilidad de que la máquina falle en el día 7 de uso?\nSea \\(T=\\) Cantidad de días que la máquina funciona ininterrumpidamente. Grafique la función de densidad de probabilidad de \\(T\\).\n\nBolas infinitas\nEste tuit propone un problema muy interesante. Una urna contiene una bola azul y una amarilla. Se elije una bola al azar y se la vuelve a colocar junto con otra bola adicional del mismo color. Se repite este proceso indefinidamente. ¿Qué ocurre con la proporción de bolas azules en la urna a medida que repetimos más y más veces?\n\nTiende a 1/2.\nTiende a 0 ó a 1.\nNo se estabiliza.\nNinguna de las anteriores.\n\nEscriba un programa en R para responder esta pregunta utilizando simulaciones. Genere gráficos que faciliten la comprensión del resultado. \n📌 Estimando el valor de \\(\\pi\\)\nImagine un círculo de radio \\(r\\) y un cuadrado de lado \\(2r\\), ambos centrados en el mismo punto, que de manera arbitraria puede ser el punto \\((0, 0)\\). Obtenga muestras de una distribución uniforme en el plano \\((x, y)\\), cuyo dominio está acotado por el cuadrado antes mencionado. Para cada muestra extraida, determine si se encuentra dentro del círculo o no – todos las muestras se encontrarán dentro del cuadrado. Utilice esta información para estimar el valor de \\(\\pi\\).\n\n\n\n\n\n\n\n\n\nAlgunos datos útiles\n\nArea de un círculo: \\(\\pi \\cdot r^2\\).\nArea de un cuadrado: \\(a^2\\), donde \\(a\\) es la longitud del lado.\n\nLos puntos uniformes\nSe seleccionan dos puntos de manera uniforme e independiente dentro de un círculo. ¿Cuál es la probabilidad de que la distancia entre dos puntos sea menor al radio?\n\nResuelva el problema utilizando R.\nElabore una visualización que facilite la comunicación de los resultados.\n\n\n\n\n\n\n\nAyuda\n\n\n\n\n\nUna forma de simular puntos con distribución uniforme dentro de un círculo de radio \\(r\\) es la siguiente:\n\nElegir un ángulo \\(\\theta\\) al azar, uniformemente en \\([0, 2\\pi)\\).\nElegir una distancia \\(\\delta\\) al centro del círculo, de manera adecuada para garantizar uniformidad en el área.\n\nFinalmente, se convierten las coordenadas polares a cartesianas con: \\[\n\\begin{aligned}\nx &= \\delta \\times \\cos(\\theta) \\\\\ny &= \\delta \\times \\sin(\\theta)\n\\end{aligned}\n\\]\nNota : Para que los puntos queden uniformemente distribuidos en el área del círculo, \\(\\delta\\) no debe elegirse uniformemente en \\([0, r]\\), sino como \\(\\delta = r \\times \\sqrt{U}\\), donde \\(U \\sim \\text{Uniforme}(0, 1)\\).\n# Cantidad de puntos\nn &lt;- 1000\n\n# Radio del circulo\nradio &lt;- 3\n\n# Ángulo del punto\ntheta &lt;- runif(n, 0, 2 * pi)\n\n# Distancia al centro de coordenadas\ndelta &lt;- radio * sqrt(runif(n, 0, 1))\n\n\n\nSobre el histórico 7 a 1 del 2014\nEn la Copa del Mundo de la FIFA 2014, Alemania jugó contra Brasil en la semifinal. Los alemanes hicieron el primer gol a los 11 minutos y el segundo a los 23. Asuma que el tiempo entre goles sigue una distribución exponencial. Elija una distribución a priori para el tiempo entre goles (puede ser conjugada o no). En ese momento del partido,\n\n¿Cuál es la distribución a posteriori del tiempo entre goles de Alemania?\n¿Cuántos goles cabría esperar que Alemania hiciera al finalizar los 90 minutos?\n¿Cuál era la probabilidad de que Alemania hiciera más de 5 goles (cosa que ocurrió)? \n\n💻 ¿Tendré que esperar mucho?\nEl tiempo que un empleado de recursos humanos demora en hacer una entrevista tiene distribución exponencial con media 30 minutos. Los tiempos de duración de cada entrevista se pueden considerar independientes entre sí. Las entrevistas a postulantes para un trabajo están programadas cada 15 minutos, comenzando desde las 8. Es válido considerar que todos los postulantes llegan puntuales a su entrevista. Cuando la persona del turno de las 8:15 llega a la oficina\n\n¿Cuál es la probabilidad de que tenga que esperar antes de ser entrevistada?\n¿Cuál es el horario esperado al que terminará su entrevista? \n\n🧩 ¡Qué casualidad!\nDos personas se conocen en la fila de embarque para un vuelo en un avión Airbus A330-300. Considere que el Airbus A330-300 tiene 30 filas de 2-4-2 asientos.\n\n¿Cuál es la probabilidad de que tengan asientos en la misma fila?\n¿Cuál es la probabilidad de que estén sentados en asientos adyacentes?\n\n✍️ 💻 📌 El Problema de Monty Hall\nEl Problema de Monty Hall es un problema de probabilidad basado en un juego del concurso televisivo estadounidense “Trato hecho”. En este problema, el concursante debe elegir una puerta entre tres, todas cerradas. El premio consiste en llevarse lo que se encuentra detrás de la elegida. Se sabe con certeza que tras una de ellas se oculta un automóvil, y tras las otras dos hay cabras. Una vez que el concursante haya elegido una puerta y comunicado su elección a los presentes, el presentador, que sabe lo que hay detrás de cada puerta, abrirá una de las otras dos en la que haya una cabra. A continuación, le da la opción al concursante de cambiar, si lo desea, de puerta (tiene dos opciones). ¿Debe el concursante mantener su elección original o escoger la otra puerta? ¿Hay alguna diferencia? Resuelva este ejercicio utilizando simulaciones. \n\n\n\n\n\nLas 3 puertas del problema de Monty Hall\n\n\n\n\n💻 Que los cumplan feliz\nBasándose en el siguiente tuit y conociendo el problema del cumpleaños (¿cuántas personas debe haber en una habitación para que la probabilidad de que dos de ellas cumplan años el mismo día sea mayor a X%?) construir un gráfico similar al del tuit donde se grafique la probabilidad de que haya \\(n\\) personas que cumplan años el mismo día para \\(K\\) personas presentes en la habitación.\n💻 Qué suerte, ¿no?\nPrevio a la final de la Copa América 2021, los jugadores de la Selección Argentina se reúnen en la habitación del hotel como se describe en este tuit.\n\n¿Cuál es la probabilidad de que un jugador adivine una de diez cartas?\n¿Cuál es la probabilidad de que tres de ellos adivinen una de diez cartas?\n\nEl álbum del Campeón\nEl álbum oficial del Mundial de Fútbol de Qatar 2022 consta de 638 figuritas. Cada paquete trae cinco figuritas.\n\nComprando cinco paquetes, ¿cuál es la probabilidad de tener a Messi?\nComprando cinco paquetes, ¿cuál es la probabilidad de sacar a Messi repetido?\n¿Cuántos paquetes se necesitan, en promedio, para completar el álbum?\nSi a una persona le faltan diez figuritas para completar el álbum, ¿cuántos paquetes tiene que comprar para asegurarse de lograrlo?\n\n💻 ¿Que tán raras son estas secuencias raras?\nSi se arroja una moneda \\(n\\) veces, ¿cuál es la probabilidad de que no haya secuencias de \\(k\\) caras?\n📌 Un viaje por el elevador\n¿Cuál es la probabilidad de que tres personas en un ascensor con doce pisos presionen para ir a tres pisos consecutivos? ¿Qué supuestos realiza para resolver el problema? Escríbalos en una lista de manera explícita.\nLa vida es muy corta como para perderla ordenando medias\nUn cajón contiene 10 pares de medias. No hay dos pares iguales. Por fiaca, el dueño de las medias no las agrupa después de lavarlas y simplemente las pone en el cajón. Al momento de necesitar un par de medias, saca una tras una hasta que se forma un par. En promedio, ¿cuántas medias sacará hasta encontrar un par? \n¿Vale la pena hacer un ensayo clínico a gran escala?\nDados los resultados de un estudio piloto, la probabilidad a posteriori de que la droga desarrollada por tu compañía sea mas efectiva que el tratamiento actual es \\(\\theta \\in [0, 1]\\). Tu compañía está considerando realizar un ensayo clínico a gran escala para confirmar que la droga que desarrollan es de hecho mejor. El costo del estudio es $X. Si la droga es mejor, la probabilidad de que esto se confirme en el ensayo es del 80%. Si la droga no es mejor, hay una probabilidad del 5% de que el estudio confirme que es mejor. Si el ensayo sugiere que tu droga es mejor, ganarás $cX. ¿Para qué valores de \\(\\theta\\) y \\(c\\) tiene sentido realizar el estudio? \nEl problema de concordancia\nResuelva el problema de concordancia de de Montmort presentado en la Práctica 0 utilizando simulaciones. \nEl problema de los sobres\nResuelva el problema de los dos sobres presentado en la Práctica 0 utilizando simulaciones.\n🧩 Integración por muestreo\nCalcule las siguientes integrales utilizando muestras.\n\n\\(\\displaystyle \\int_{-\\infty}^{\\infty}{\\frac{x^2}{\\sqrt{2\\pi}}\\exp \\left(-\\frac{x^2}{2} \\right)dx}\\)\n\\(\\displaystyle \\int_{1}^{\\infty}{\\frac{x^3}{\\sqrt{2\\pi}}\\exp \\left(-\\frac{x^2}{2} \\right)dx}\\)\n\\(\\displaystyle \\int_{1}^{\\infty}{\\frac{x^6}{\\sqrt{2\\pi}}\\exp \\left(-\\frac{x^2 - 4x}{2} \\right)dx}\\)\n\\(\\displaystyle \\int_{1}^{10}{x^6\\frac{e^{-x^4/2}}{\\sqrt{2\\pi}}dx}\\) \n\n¿Cuál es la distribución de muestreo aproximada al usar muestreo independiente para evaluar integrales?",
    "crumbs": [
      "Práctica",
      "Unidad 3"
    ]
  },
  {
    "objectID": "practica/practica_03.html#aproximación-de-una-distribución-mediante-una-grilla",
    "href": "practica/practica_03.html#aproximación-de-una-distribución-mediante-una-grilla",
    "title": "Práctica - Unidad 3",
    "section": "💻 Aproximación de una distribución mediante una grilla",
    "text": "💻 Aproximación de una distribución mediante una grilla\nEn esta parte de la práctica se comienza a utilizar técnicas computacionales que se asocian directamente a la práctica de la estadística bayesiana. Los problemas tienen como objetivo la familiarización con el uso práctico de estas técnicas y la comprensión de sus características principales.\n\n📌 Aproximación de grilla\nSe tiene un experimento binomial donde \\(n=80\\) y se observan \\(y=7\\) éxitos. Considere que el prior de la probabilidad de éxito \\(\\theta\\) es \\(\\text{Beta}(2, 10)\\).\n\nObtenga la distribución a posteriori de \\(\\theta\\) de manera analítica y grafíquela.\nObtenga la distribución a posteriori de \\(\\theta\\) utilizando el método de la grilla en base a una grilla de 10 puntos y dibuje la curva obtenida en el gráfico creado anteriormente.\nRepita el proceso del punto anterior utilizando una grilla de 100 puntos.\nConcluya sobre la fidelidad de las aproximaciones. ¿Considera que es necesario utilizar una grilla más densa? ¿Cuáles serían las ventajas y desventajas?\n\n📌 Cálculo de probabilidades en base a la grilla (I)\nEn base al posterior obtenido en el ejercicio anterior mediante el método de la grilla calcule las siguientes probabilidades\n\n\\(P(\\theta &lt; 0.7)\\).\n\\(P(\\theta &gt; 0.05)\\).\n\\(P(0.05 &lt; \\theta &lt; 0.15)\\).\n\nDe ser necesario, obtenga el posterior mediante una grilla de mayor densidad.\n📌 Cálculo de probabilidades en base a la grilla (II)\nUtilice los valores de la grilla y sus probabilidades a posteriori para obtener muestras del posterior y calcular las mismas probabilidades que en el ejercicio anterior en base a muestras. Ayuda: Para obtener muestras utilice la función sample().\n🧩📌 Aproximación de grilla en 2 dimensiones\nSean dos variables aleatorias continuas \\(X\\) e \\(Y\\) tales que \\((X, Y) \\in \\mathbb{R}^2\\), y sea el siguiente modelo de regresión lineal simple:\n\\[\n\\begin{aligned}\n\\alpha &\\sim \\text{Normal}(0, 1.5) \\\\\n\\beta  &\\sim \\text{Normal}(0, 2) \\\\\nY      &\\sim \\text{Normal}(\\alpha + \\beta X, 0.8)\n\\end{aligned}\n\\]\n\nObtenga el posterior conjunto del vector de parámetros \\([\\alpha, \\beta]^T\\) mediante el método de la grilla. Elabore un gráfico que permita visualizar esta distribución.\nObtenga el posterior marginal de \\(\\alpha\\) y grafíquelo.\nObtenga el posterior marginal de \\(\\beta\\) y grafíquelo.\nCalcule la probabilidad de que el intercepto sea mayor a 0.95.\nCalcule la probabilidad de que la pendiente sea menor a -2.\n\nPara responder las consignas utilice los datos simulados que se obtienen con el siguiente bloque de código:\n\nset.seed(121195)\nalpha &lt;- 1\nbeta &lt;- -2\nsigma &lt;- 0.8\nn &lt;- 80\nx &lt;- rnorm(n)\ny &lt;- rnorm(n, alpha + beta * x, sigma)\ndf &lt;- data.frame(x = x, y = y)\n\nBonus: ¿Cómo podría responder las consignas (ii)-(v) utilizando muestras del posterior?\n📌 Escalando la aproximación de la grilla\nSuponga que se tiene que estimar un posterior utilizando la aproximación mediante una grilla de 200 puntos en cada dimensión. Calcule cúantas veces se tiene que evaluar el posterior en cada uno de los siguientes escenarios:\n\n1 dimensión.\n2 dimensiones.\n3 dimensiones.\n5 dimensiones.\n10 dimensiones.\n\nConcluya sobre las ventajas y desventajas de la aproximación de la grilla teniendo en cuenta sus características conforme se incrementa el número de dimensiones del posterior.\nBenchmark de la aproximación de la grilla\nEl siguiente bloque de código define una función llamada create_and_eval_grid() que evalúa la función de densidad normal en una cantidad arbitraria dimensiones. El argumento dimension_n indica la dimensionalidad de la distribución normal, y grid_n indica la cantidad de puntos en la grilla de cada dimensión. Debajo, se utiliza la función mark() del paquete {bench} para comparar el desempeño de la función create_and_eval_grid() con diferentes números de dimensiones.\n\ncreate_and_eval_grid &lt;- function(dimension_n, grid_n = 100) {\n    grid &lt;- seq(-3, 3, length.out = grid_n)\n    grids &lt;- replicate(dimension_n, grid, simplify = FALSE)\n    df &lt;- expand.grid(grids, KEEP.OUT.ATTRS = FALSE)\n    Mu &lt;- rep(0, dimension_n)\n    Sigma &lt;- diag(dimension_n)\n    mvtnorm::dmvnorm(df, mean = Mu, sigma = Sigma)\n}\nbench::mark(\n    create_and_eval_grid(1),\n    create_and_eval_grid(2),\n    check = FALSE,\n    max_iterations = 500\n)\n\nModifique el código brindado para evaluar la función de densidad en hasta un máximo de 10 dimensiones. Concluya sobre el tiempo de ejecución, el consumo de memoria, y otras cantidades que se encuentren en la salida y crea adecuadas para el análisis.",
    "crumbs": [
      "Práctica",
      "Unidad 3"
    ]
  },
  {
    "objectID": "practica/practica_03.html#sec-mh",
    "href": "practica/practica_03.html#sec-mh",
    "title": "Práctica - Unidad 3",
    "section": "💻 Metropolis-Hastings",
    "text": "💻 Metropolis-Hastings\n\n\nEn esta sección, se profundiza en la práctica de uno de los algoritmos fundamentales de la inferencia estadística bayesiana: el algoritmo de Metropolis-Hastings.\n\n📌 Muestreo utilizando el algoritmo de Metropolis-Hastings (I)\nUse el algoritmo de Metropolis-Hastings y una distribución de propuesta \\(\\text{Normal}(0, 0.1)\\) para obtener 5000 muestras de las siguientes distribuciones de probabilidad:\n\n\\(\\text{Normal}(\\mu = 3, \\sigma = 6)\\).\n\\(\\text{StudentT}(\\nu = 5)\\).\n\\(\\frac{2}{3}\\text{Normal}(\\mu = 0, \\sigma = 0.5) + \\frac{1}{3}\\text{Normal}(\\mu = 3, \\sigma = 2)\\).\n\nGrafique las distribuciones obtenidas utilizando un histograma o una estimación de densidad y superponga la función de densidad verdadera para realizar una comparación. Concluya sobre la similitud de las mismas y la aptitud de la distribución de propuesta utilizada.\n📌 Simplificando el algoritmo de Metropolis-Hastings\nLa distribución de propuesta utilizada en el ejercicio anterior goza de una propiedad que permite simplificar el algoritmo de Metropolis-Hastings.\n\n¿Cuál es esta propiedad?\n¿Qué simplificación se puede hacer?\n¿Qué nombre recibe la versión simplificada del algoritmo?\nImplemente la versión simplificada del algoritmo de Metropolis-Hastings y obtenga nuevamente 5000 muestras para las distribuciones presentadas en el ejercicio anterior utilizando la nueva implementación.\n\n📌 Muestreo utilizando el algoritmo de Metropolis-Hastings (II)\nUse el algoritmo de Metropolis-Hastings y una distribución de propuesta que crea conveniente para obtener 5000 muestras de las siguientes distribuciones de probabilidad:\n\n\\(\\text{Beta}(\\alpha=4, \\beta=8)\\).\n\\(\\text{Gamma}(\\alpha = 3, \\beta = 0.5)\\).\n\\(\\frac{1}{2}\\text{Beta}(\\alpha=10, \\beta=3) + \\frac{1}{2}\\text{Beta}(\\alpha=3, \\beta=10)\\).\n\nRealice un análisis similar al realizado en el Ejercicio 1.\n¡A jugar con la distribución de propuesta!\nSuponga que se desea obtener muestras de una distribución \\(\\text{Normal}(4, 1)\\) utilizando Metropolis-Hastings y la siguiente distribución de propuesta:\n\\[\n\\mu' \\mid \\mu \\sim \\text{Uniforme}(\\mu - w, \\mu + w)\n\\]\nObtenga \\(n=5000\\) muestras con \\(w \\in \\{0.01, 1, 100\\}\\) y compute la probabilidad de aceptación. Luego, para cada \\(w\\), grafique la distribución obtenida y visualice la cadena de Markov utilizando un traceplot.\n\n¿Cómo se relaciona \\(w\\) con el desempeño del muestreo?\n¿Cómo se relaciona \\(w\\) con la probabilidad de aceptación? Justifique su respuesta utilizando la ecuación del criterio de aceptación.\n\n📌 Modelo Normal-Normal\nSe desea estudiar el tiempo promedio que los estudiantes de estadística dedican por semana a la materia Estadística Bayesiana. Para eso se propone utilizar un modelo Normal-Normal, con \\(\\sigma=1.2\\) conocido.\n\nElija una distribución a priori para el parámetro \\(\\mu\\).\nDescriba el modelo matemáticamente.\nDetermine una distribución de propuesta adecuada para este problema. Explique.\nObtenga 2000 muestras del posterior de \\(\\mu\\). Ajuste los parámetros de la distribución de propuesta hasta que los resultados se vean adecuados.\nGrafique un histograma de las muestras obtenidas y concluya sobre el desempeño de la aproximación.\n\nPara resolver este problema utilice los datos que se leen con el siguiente codigo:\n\nurl &lt;- paste0(\n    \"https://raw.githubusercontent.com/estadisticaunr/\",\n    \"estadistica-bayesiana/main/datos/tiempo-estudio-eb.csv\"\n)\ndf_estudio &lt;- readr::read_csv(url)\n\n📌 Modelo Beta-Binomial\nSe desea estimar el posterior del parámetro \\(\\pi\\) en el siguiente modelo Beta-Binomial:\n\\[\n\\begin{aligned}\nY &\\sim \\text{Binomial}(n, \\pi) \\\\\n\\pi &\\sim \\text{Beta}(2, 3)\n\\end{aligned}\n\\]\nSe observan \\(n=10\\) ensayos de Bernoulli y se registran \\(y=3\\) éxitos. Determine una distribución de propuesta adecuada y obtenga muestras de la distribución a posteriori del parámetro \\(\\pi\\) utilizando el algoritmo de Metropolis-Hastings.\nDe ser necesario, ajuste los parámetros de la distribución de propuesta.\n📌 Modelo Poisson\nEn el ejercicio ¡Ostras! ¡Estoy haciendo inferencia bayesiana! de la Práctica 1 se reportó que la cantidad de especies marinas bivalvas descubiertas cada año entre 2010 y 2015 fue 64, 13, 33, 18, 30 y 20.\nSea \\(Y_t\\) la cantidad de especies descubiertas en el año \\(2009 + t\\) (e.g. \\(Y_1 = 64\\) es el conteo para el año 2010) y el siguiente modelo:\n\\[\n\\begin{aligned}\nY_t        &\\sim \\text{Poisson}(\\lambda_t) \\\\\n\\lambda_t  &= \\exp (\\alpha + \\beta t) \\\\\n\\alpha     &\\sim \\text{Normal}(0, 10^2) \\\\\n\\beta      &\\sim \\text{Normal}(0, 10^2)\n\\end{aligned}\n\\]\nAjuste el modelo utilizando Metropolis-Hastings y verifique la convergencia de las cadenas de Markov. \n📌 Metropolis-Hastings multivariado\nUse el algoritmo de Metropolis Hastings para obtener 1000 muestras de la distribución \\(\\text{MVN}(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma})\\) donde\n\\[\n\\begin{array}{cc}\n    \\boldsymbol{\\mu} = \\begin{bmatrix}1.2 \\\\ 0.8 \\end{bmatrix}, &\n    \\boldsymbol{\\Sigma} = \\begin{bmatrix}3 & 0.2 \\\\ 0.2 & 2 \\end{bmatrix}\n\\end{array}\n\\]\nUtilice una distribución de propuesta \\(\\text{MVN}(\\boldsymbol{0}_2, \\boldsymbol{I}_2\\sigma)\\) con \\(\\sigma = 0.2\\).\n📌 Metropolis-Hastings para regresión\nRepita el ejercicio Aproximación de grilla en 2 dimensiones pero utilice Metropolis-Hastings para obtener muestras del posterior en vez del método de la grilla.",
    "crumbs": [
      "Práctica",
      "Unidad 3"
    ]
  },
  {
    "objectID": "practica/practica_03.html#diagnósticos",
    "href": "practica/practica_03.html#diagnósticos",
    "title": "Práctica - Unidad 3",
    "section": "Diagnósticos",
    "text": "Diagnósticos\nEl uso de algoritmos de MCMC provee de un gran poder que conlleva una gran responsabilidad. En los ejercicios de esta sección ya se cuenta con un posterior y se busca evaluar la fiabilidad de las muestras utilizando diferentes medidas de diagnóstico análiticas y gráficas.\n\n📌 Describir medidas de diagnóstico (I)\nEn sus propias palabras, explique que son \\(\\text{ESS}\\), \\(\\hat{R}\\) y \\(\\text{MCSE}\\). Considere:\n\n¿Qué miden?\n¿Qué potencial problema de MCMC detectan? \n\n📌 Describir medidas de diagnóstico (II)\nEn sus propias palabras, explique por qué las técnicas de estimación del posterior basadas en MCMC necesitan diagnósticos de convergencia. En particular, contraste estos con los métodos conjugados descritos en la Unidad 2 que no necesitan esos diagnósticos. ¿Qué es diferente entre los dos métodos de inferencia? \n📌 Problemitas de MCMC\nPara cada escenario de simulación mediante MCMC descrito a continuación, explique cómo el escenario podría afectar la aproximación del posterior.\n\nLa cadena se mezcla muy lentamente.\nLa cadena presenta alta auto-correlación.\nLa cadena tiende a quedarse “trabada”. \n\n📌 Vamos de paseo\nElabore traceplots que le permitan visualizar la traza de la cadena de Markov utilizada en el ejercicio Modelo Normal-Normal de la sección Metropolis-Hastings.\nLuego, repita el ejercicio utilizando 4 cadenas independientes y grafique sus trazas en un mismo gráfico. ¿Qué puede concluir sobre la convergencia y la mezcla de las cadenas?\n📌 Primeros pasitos con \\(\\hat{R}\\)\nRepita lo realizado en el ejercicio Modelo Beta-Binomial de la sección Metropolis-Hastings utilizando 4 cadenas independientes. Luego:\n\nCalcule la varianza intra-cadenas \\(W\\).\nCalcule la varianza entre-cadenas \\(B\\).\nCalcule \\(\\hat{R}\\) y concluya sobre el resultado obtenido.\n\n\n\\(\\hat{R}\\) para un muestreo independiente\nEn un determinado problema se encuentra que el posterior del parámetro \\(\\pi\\) de un modelo con verosimilitud binomial está dado por\n\\[\n\\pi \\mid \\boldsymbol{y} \\sim \\text{Beta}(12, 6)\n\\]\n\nObtenga 4 conjuntos independientes de 1000 muestras independientes de esta distribución.\nCalcule \\(W\\), \\(B\\), y \\(\\hat{R}\\) considerando que cada conjunto representa una cadena.\nExplique el resultado de \\(\\hat{R}\\).\n\nBonus\n\n¿Se puede concluir que cada uno de los muestreos realizados representan realizaciones de una cadena de Markov? ¿Por qué?\n¿Por qué no fue necesario descartar un conjunto de muestras de warm-up?\n\n📌 Función de autocorrelación\nUtilice las 4 cadenas obtenidas en el ejercicio Primeros pasitos con \\(\\hat{R}\\) y grafique la función de autocorrelación y calcule el coeficiente de autocorrelación utilizando un rezago unitario. Concluya sobre la dependencia entre las muestras obtenidas.\n📌 Tamaño de muestra efectivo\nUtilice las muestras obtenidas en el ejercicio anterior para calcular el tamaño de muestra efectivo.\nExperimentos con el tamaño de muestra efectivo\nSuponga una distribución \\(\\text{Normal}(0, 1^2)\\). Obtenga \\(n\\) muestras independientes utilizando rnorm() y \\(n\\) muestras dependientes utilizando Metropolis-Hastings con \\(n \\in \\{10, 50, 100, 500, 1000, 10000\\}\\).\n\nCalcule el temaño de muestra efectivo en todos los escenarios simulados.\nDescriba el comportamiento del tamaño de muestra efectivo conforme se incrementa el número de muestras.\n¿Por qué se observan los comportamientos descritos?\n\n📌 ¡Rompan todo! Un caso simulado\nLos siguientes gráficos muestran las trazas y las distribuciones que resultan al obtener muestras de un posterior utilizando dos cadenas de Markov independientes. Estos muestreos fueron realizados de manera que presenten algunos problemas. Describa cuales son los problemas que puede observar en los siguientes gráficos y explique por qué no utilizaría estas muestras para obtener conclusiones sobre el posterior.\n\n\n\n\n\nGráfico 1\n\n\n\n\n\n\n\n\n\nGráfico 2\n\n\n\n\n\n\n\n\n\nGráfico 3\n\n\n\n\n\n\n\n\n\nGráfico 4\n\n\n\n\n📌 ¡Rompan todo! Un caso real\nAl igual que en el ejercicio anterior, se presentan traceplots donde el muestreo del posterior presenta problemas. En este caso, los gráficos que se observan fueron obtenidos en el marco de un problema real, y se usan 4 cadenas en vez de 2. Nuevamente, describa cuales son los problemas que puede observar en los siguientes gráficos y explique por qué no utilizaría estas muestras para obtener conclusiones sobre el posterior.\n\n\n\n\n\nGráfico 1\n\n\n\n\n\n\n\n\n\nGráfico 2\n\n\n\n\nAdemás, responda:\n\n¿Cuál gráfico se asocia a un mayor tamaño de muestra efectivo?\n¿Qué grafico muestra peor mezcla entre cadenas?\n¿En qué gráfico se puede observar que las cadenas convergen a la misma distribución?",
    "crumbs": [
      "Práctica",
      "Unidad 3"
    ]
  },
  {
    "objectID": "practica/practica_03.html#programación-probabilística---stan-y-rstan",
    "href": "practica/practica_03.html#programación-probabilística---stan-y-rstan",
    "title": "Práctica - Unidad 3",
    "section": "Programación probabilística - Stan y RStan",
    "text": "Programación probabilística - Stan y RStan\nEn esta sección se comienza a utilizar Stan para realizar los cálculos relacionados a la inferencia bayesiana. Stan es uno de los lenguajes de programación probabilística mas potentes y populares en la actualidad. Los ejercicios contienen modelos estadísticos que deben ser resueltos con la interface de Stan a R, {RStan}.\n\n📌 MCMC con RStan: Precalentamiento (I)\nUtilice la información proporcionada para definir la estructura del modelo bayesiano utilizando {RStan}. No es necesario ejecutar nada, solo necesita proporcionar el código correcto.\n\n\\(Y \\mid \\pi \\sim \\text{Binomial}(\\pi, 20)\\) con \\(\\pi \\sim \\text{Beta}(1, 1)\\).\n\\(Y \\mid \\lambda \\sim \\text{Poisson}(\\lambda)\\) con \\(\\lambda \\sim \\text{Gamma}(4, 2)\\).\n\\(Y \\mid \\mu \\sim \\text{Normal}(\\mu, 1^2)\\) con \\(\\mu \\sim \\text{Normal}(0, 10^2)\\). \n\n📌 MCMC con RStan: Precalentamiento (II)\nUtilice la información proporcionada para (1) definir la estructura del modelo bayesiano y (2) obtener muestras del posterior utilizando {RStan}. No es necesario ejecutar nada, solo necesita proporcionar el código correcto.\n\n\\(Y \\mid \\pi \\sim \\text{Binomial}(\\pi, 20)\\) con \\(\\pi \\sim \\text{Beta}(1, 1)\\) e \\(y = 12\\).\n\\(Y \\mid \\lambda \\sim \\text{Poisson}(\\lambda)\\) con \\(\\lambda \\sim \\text{Gamma}(4, 2)\\) e \\(y = 3\\).\n\\(Y \\mid \\mu \\sim \\text{Normal}(\\mu, 1^2)\\) con \\(\\mu \\sim \\text{Normal}(0, 10^2)\\) e \\(y = 12.2\\). \n\n📌 Modelo Beta-Binomial con RStan (I)\nConsidere el modelo Beta-Binomial para \\(\\pi\\) con \\(Y \\mid \\pi \\sim \\text{Binomial}(\\pi, n)\\) y \\(\\pi \\sim \\text{Beta}(3, 8)\\). Suponga que en \\(n = 10\\) ensayos independientes observa \\(y = 2\\) éxitos.\n\nObtenga muestras del posterior de \\(\\pi\\) con {RStan} utilizando 3 cadenas y 12000 iteraciones por cadena.\nGrafique la traza de cada una de las tres cadenas.\n¿Cuál es el rango de valores en el eje x del traceplot? ¿Por qué el valor máximo de este rango no es 12000?\nCree un gráfico que permita visualizar la función de densidad de los valores obtenidos con cada una de las tres cadenas.\nUtilizando lo estudiado en la Unidad 2, especifique el posterior de \\(\\pi\\). ¿Cómo se compara con la aproximación mediante MCMC?\n\n📌 Modelo Beta-Binomial con RStan (II)\nRepita el ejercicio anterior utilizando \\(\\pi \\sim \\text{Beta}(4, 3)\\), donde observa \\(y = 4\\) éxitos en \\(n = 12\\) ensayos independientes.\n📌 Modelo Gamma-Poisson con RStan (I)\nConsidere el modelo Gamma-Poisson para \\(\\lambda\\) con \\(Y_i \\mid \\lambda \\sim \\text{Poisson}(\\lambda)\\) y \\(\\lambda \\sim \\text{Gamma}(20, 5)\\). Suponga que cuenta con \\(n = 3\\) observaciones independientes \\((y_1, y_2, y_3) = (0, 1, 0)\\)\n\nObtenga muestras del posterior de \\(\\lambda\\) con {RStan} utilizando 4 cadenas y 10000 iteraciones por cadena.\nGrafique la traza y la función de densidad de cada una de las tres cadenas.\nA partir del gráfico de la función de densidad, ¿cuáles suelen ser, a posteriori, los valores más probables de \\(\\lambda\\)?\nUtilizando lo estudiado en la Unidad 2, especifique el posterior de \\(\\lambda\\). ¿Cómo se compara con la aproximación mediante MCMC?\n\n📌 Modelo Gamma-Poisson con RStan (II)\nRepita el ejercicio anterior utilizando el prior \\(\\lambda \\sim \\text{Gamma}(5, 5)\\).\n📌 Modelo Normal-Normal con RStan (I)\nRepita los mismos pasos del ejercicio Modelo Gamma-Poisson con Rstan (I) pero considere el modelo Normal-Normal para \\(\\mu\\) con \\(Y_i \\mid \\mu \\sim \\text{Normal}(\\mu, 1.3^2)\\) y \\(\\mu \\sim \\text{Normal}(10, 1.2^2)\\). Suponga que cuenta con \\(n = 4\\) observaciones independientes \\((y_1, y_2, y_3, y_4) = (7.1, 8.9, 8.4, 8.6)\\)\n📌 Modelo Normal-Normal con RStan (II)\nRepita el ejercicio anterior con el modelo Normal-Normal pero ahora considere \\(Y_i \\mid \\mu \\sim \\text{Normal}(\\mu, 8^2)\\) y \\(\\mu \\sim \\text{Normal}(-15, 2^2)\\). Suponga que en \\(n = 5\\) observaciones independientes observa \\((y_1, y_2, y_3, y_4, y_5) = (−10.1, 5.5, 0.1,−1.4, 11.5)\\)\nUn modelo que es un poco ¿complicado?\nConsidere el siguiente modelo: \\[\n\\begin{aligned}\n\\text{peso}_i &\\sim \\text{Normal}(\\mu_i, \\sigma^2) \\\\\n\\mu_i         &= \\theta_1 + \\theta_2 + \\text{edad}_i^{\\theta_3} \\\\\n\\theta_1      &\\sim \\text{Normal}(0, 100^2) \\\\\n\\theta_2      &\\sim \\text{Uniforme}(0, 20000) \\\\\n\\theta_3      &\\sim \\text{Normal}(0, 1) \\\\\n\\sigma^2      &\\sim \\text{InvGamma}(0.01, 0.01)\n\\end{aligned}\n\\]\ny los siguientes datos:\n\nedad &lt;- c(2, 15, 14, 16, 18, 22, 28)\npeso &lt;- c(29.9, 1761, 1807, 2984, 3230, 5040, 5654)\nn &lt;- length(edad)\ndata_list &lt;- data.frame(peso = peso, edad = edad, n = n)\n\n\nAjuste el modelo utilizando {RStan}.\nObtenga una visualización de la edad versus el peso junto con una curva que indique la media a posteriori de \\(\\mu_i\\) para evaluar si el modelo ajusta bien.\nEstudie la convergencia de las cadenas de Markov.\nMencione tres medidas que podría tomar para mejorar la convergencia.",
    "crumbs": [
      "Práctica",
      "Unidad 3"
    ]
  },
  {
    "objectID": "practica/practica_03.html#otros",
    "href": "practica/practica_03.html#otros",
    "title": "Práctica - Unidad 3",
    "section": "Otros",
    "text": "Otros\n\n📌 El peso de los pescados\nUna compañía pesquera de Comodoro Rivadavia se encuentra probando un nuevo método para estimar el peso de los peces que extrae del Mar Argentino. El objetivo de este método es obtener una estimación lo suficientemente buena del peso de cada pescado sin tener que pesarlos uno por uno, ya que es un proceso costoso en tiempo y labor. Para eso, seleccionaron una muestra de pescados, los pesaron y les midieron ciertos aspectos morfológicos (ancho, alto y largo). En el futuro, esperan recolectar estas mismas medidas morfológicas mediante una cámara especializada y utilizar un modelo para estimar el peso.\nEl modelo propuesto por el equipo de investigación es el siguiente:\n\\[\n\\begin{aligned}\n\\log(\\text{Peso}_i) &\\sim \\text{Normal}(\\mu_i, \\sigma) \\\\\n\\mu_i               &= \\beta_0 + \\beta_1 \\log(\\text{Largo}_i) \\\\\n\\sigma              &\\sim \\text{Gamma}(k, \\theta)\n\\end{aligned}\n\\]\nEl peso se encuentra medido en gramos y la longitud en centímetros. El equipo provee las muestras que obtuvieron del posterior. Las mismas se pueden leer en R utilizando el siguiente bloque de código.\n\nurl &lt;- paste0(\n    \"https://raw.githubusercontent.com/estadisticaunr/\",\n    \"estadistica-bayesiana/main/datos/fish-market-posterior.csv\"\n)\ndf_posterior &lt;- readr::read_csv(url)\nhead(df_posterior)\n\n# A tibble: 6 × 3\n  intercepto pendiente sigma\n       &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;\n1      -4.44      3.08 0.408\n2      -4.30      3.05 0.431\n3      -4.49      3.12 0.433\n4      -4.04      2.96 0.341\n5      -4.76      3.18 0.413\n6      -4.65      3.15 0.350\n\nAnalice de manera gráfica y analítica los posteriors marginales de los parámetros del modelo. Realice las transformaciones de parámetros que crea conveniente para facilitar la comprensión del análisis.\nConsidere un pescado cuya longitud es de 30 centímetros.\n\nObtenga y grafique la distribución a posteriori del peso medio.\nObtenga y grafique la distribución predictiva a posteriori del peso.\nInterprete los resultados.\n\nGrafique la curva de regresión junto a una banda de credibilidad del 95% en el plano de las variables originales y en el plano de las variables transformadas.\nAgregue a los gráficos anteriores una banda de credibilidad del 95% para la distribución predictiva a posteriori. Interprete los resultados.",
    "crumbs": [
      "Práctica",
      "Unidad 3"
    ]
  },
  {
    "objectID": "recursos/distribuciones/distribuciones.html",
    "href": "recursos/distribuciones/distribuciones.html",
    "title": "Distribuciones de probabilidad",
    "section": "",
    "text": "\\[\nX \\sim \\text{Normal}(\\mu, \\sigma)\n\\]\n\\[\np(x \\mid \\mu, \\sigma) = \\frac{1}{\\sqrt{2\\pi} \\sigma}e^{-\\frac{(x - \\mu) ^ 2}{2\\sigma^2}}\n\\]\n\\[\nP(X \\le x) = \\int_{-\\infty}^{x}{p(x | \\mu, \\sigma) dx}\n\\]\no también\n\\[\nP(X \\le x) = \\frac{1}{2} \\left[1 + \\text{erf}\\left(\\frac{x - \\mu}{\\sigma\\sqrt{2}} \\right) \\right]\n\\]\ncon\n\\[\n\\text{erf}(x) = \\frac{2}{\\sqrt{\\pi}}\\int_0^{x} {e^{-t^2}dt}\n\\]\n\n\\(X \\in \\mathbb{R}\\)\n\\(\\mu \\in \\mathbb{R}\\)\n\\(\\sigma &gt; 0\\)\n\\(\\mathbb{E}(X) = \\mu\\)\n\\(\\mathbb{V}(X) = \\sigma^2\\)\n\n\n\n\n\\[\nX \\sim \\text{StudentT}(\\nu)\n\\]\n\\[\np(x \\mid \\nu) =\n    \\frac{\\Gamma(\\frac{\\nu + 1}{2})}{\\Gamma(\\frac{\\nu}{2})}\n    \\left(\\frac{1}{\\pi\\nu}\\right) ^ {\\frac{1}{2}}\n    \\left[1 + \\frac{x^2}{\\nu}\\right]^{-\\frac{\\nu + 1}{2}}\n\\]\n\\[\nP(X \\le x) = \\frac{1}{2} + x \\Gamma\\left(\\frac{\\nu + 1}{2}\\right)\n             \\frac{{}_2F_1\\left(\\frac{1}{2}, \\frac{v + 1}{2}, \\frac{3}{2}, \\frac{-x^2}{\\nu} \\right)}\n             {\\sqrt{\\pi \\nu} \\Gamma(\\frac{\\nu}{2})}\n\\]\ndonde \\({}_2F_1\\) es la función hipergeométrica.\n\n\\(X \\in \\mathbb{R}\\)\n\\(\\nu &gt; 0\\)\n\\(\\mathbb{E}(X) = 0\\) si \\(\\nu &gt; 1\\)\n\\(\\mathbb{V}(X) = \\nu / (\\nu - 2)\\) si \\(\\nu &gt; 2\\)\n\n\n\n\n\n\n\\[\nX \\sim \\text{Gamma}(k, \\theta)\n\\]\n\\[\np(x \\mid k, \\theta) = \\frac{1}{\\Gamma(k)\\theta^k}x^{k-1}e^{-\\frac{x}{\\theta}}\n\\]\n\\[\nP(X \\le x) = \\frac{1}{\\Gamma(k)} \\gamma \\left(k, \\frac{x}{\\theta}\\right)\n\\]\n\n\\(X &gt; 0\\)\n\\(k &gt; 0\\)\n\\(\\theta &gt; 0\\)\n\\(\\mathbb{E}(X) = k\\theta\\)\n\\(\\mathbb{V}(X) = k\\theta^2\\)\n\n\n\n\n\\[\nX \\sim \\text{Gamma}(\\alpha, \\beta)\n\\]\n\\[\np(x | \\alpha, \\beta) = \\frac{1}{\\Gamma(\\alpha)}x^{\\alpha-1}e^{-\\beta x}\\beta^\\alpha\n\\]\n\\[\nP(X \\le x) = \\frac{1}{\\Gamma(\\alpha)} \\gamma(\\alpha, \\beta x)\n\\]\n\n\\(X &gt; 0\\)\n\\(\\alpha &gt; 0\\)\n\\(\\beta &gt; 0\\)\n\\(\\mathbb{E}(X) = \\alpha/\\beta\\)\n\\(\\mathbb{V}(X) = \\alpha/\\beta^2\\)\n\n\n\n\n\n\\[\nX \\sim \\text{Exponencial}(\\lambda)\n\\]\n\\[\np(x \\mid \\lambda) = \\lambda e^{-\\lambda x}\n\\]\n\\[\nP(X \\le x) = 1 - e^{-\\lambda x}\n\\]\n\n\\(X &gt; 0\\)\n\\(\\lambda &gt; 0\\)\n\\(\\mathbb{E}(X) = 1 / \\lambda\\)\n\\(\\mathbb{V}(X) = 1 / \\lambda ^ 2\\)\n\nEs un caso particular de \\(\\text{Gamma}(\\alpha, \\beta)\\) con \\(\\alpha = 1\\) y \\(\\beta = \\lambda\\)\n\n\n\n\\[\nX \\sim \\text{Beta}(a, b)\n\\]\n\\[\np(x \\mid a, b) = \\frac{x^{a-1} (1-x)^{b-1}}{B(a, b)}\n\\]\n\\[\nB(a, b) = \\int_0^1 x^{a-1} (1-x)^{b-1} dx = \\frac{\\Gamma(a)\\Gamma(b)}{\\Gamma(a+b)}\n\\]\n\\[\n\\Gamma(x) = \\int_0^\\infty u^{x-1} e^{-u} du\n\\]\n\n\\(X \\in (0, 1)\\)\n\\(\\displaystyle \\mathbb{E}(X) = \\frac{a}{a + b}\\)\n\\(\\displaystyle \\mathbb{V}(X) = \\frac{ab}{(a + b) ^ 2 (a + b + 1)}\\)\n\n\n\n\n\\[\nX \\sim \\text{Binomial}(\\theta, n)\n\\]\n\\[\np(x \\mid \\theta, n) = {n \\choose x} \\theta^x (1 - \\theta)^{(n - x)}\n\\]\n\\[\nP(X \\le x) = \\sum_{i = 0} ^ {x} {n \\choose i} \\theta^i (1 - \\theta)^{(n - i)}\n\\]\n\n\\(X \\in \\{0, 1, 2, \\cdots, n\\}\\)\n\\(\\theta \\in [0, 1]\\)\n\\(n \\in \\{0, 1, 2, \\cdots \\}\\)\n\\(\\mathbb{E}(X) = n \\theta\\)\n\\(\\mathbb{V}(X) = n \\theta (1 - \\theta)\\)\n\n\n\n\n\\[\nX \\sim \\text{Poisson}(\\lambda)\n\\]\n\\[\np(x \\mid \\lambda) = \\frac{\\lambda^x e^{-\\lambda}}{x!}\n\\]\n\\[\nP(X \\le x) = e^{-\\lambda} \\sum_{i = 0} ^ {x} \\frac{\\lambda^i}{i!}\n\\]\n\n\\(X \\in \\{0, 1, 2, \\cdots\\}\\)\n\\(\\lambda &gt; 0\\)\n\\(\\mathbb{E}(X) = \\lambda\\)\n\\(\\mathbb{V}(X) = \\lambda\\)\n\n\n\n\n\\[\nX \\sim \\text{BinomialNegativa}(r, p)\n\\]\n\\[\np(x \\mid k, p)= \\binom{x + r - 1}{x}(1 - p)^x p^r\n\\]\n\n\\(X \\in \\{0, 1, 2, \\cdots \\}\\)\n\\(r \\in \\{1, 2, 3, \\cdots \\}\\)\n\\(p \\in [0, 1]\\)\n\\(\\mathbb{E}(X) = r(1 - p) / p\\)\n\\(\\mathbb{V}(X) = r(1 - p) / p^2\\)",
    "crumbs": [
      "Recursos",
      "Distribuciones"
    ]
  },
  {
    "objectID": "recursos/distribuciones/distribuciones.html#normal",
    "href": "recursos/distribuciones/distribuciones.html#normal",
    "title": "Distribuciones de probabilidad",
    "section": "",
    "text": "\\[\nX \\sim \\text{Normal}(\\mu, \\sigma)\n\\]\n\\[\np(x \\mid \\mu, \\sigma) = \\frac{1}{\\sqrt{2\\pi} \\sigma}e^{-\\frac{(x - \\mu) ^ 2}{2\\sigma^2}}\n\\]\n\\[\nP(X \\le x) = \\int_{-\\infty}^{x}{p(x | \\mu, \\sigma) dx}\n\\]\no también\n\\[\nP(X \\le x) = \\frac{1}{2} \\left[1 + \\text{erf}\\left(\\frac{x - \\mu}{\\sigma\\sqrt{2}} \\right) \\right]\n\\]\ncon\n\\[\n\\text{erf}(x) = \\frac{2}{\\sqrt{\\pi}}\\int_0^{x} {e^{-t^2}dt}\n\\]\n\n\\(X \\in \\mathbb{R}\\)\n\\(\\mu \\in \\mathbb{R}\\)\n\\(\\sigma &gt; 0\\)\n\\(\\mathbb{E}(X) = \\mu\\)\n\\(\\mathbb{V}(X) = \\sigma^2\\)",
    "crumbs": [
      "Recursos",
      "Distribuciones"
    ]
  },
  {
    "objectID": "recursos/distribuciones/distribuciones.html#t-student",
    "href": "recursos/distribuciones/distribuciones.html#t-student",
    "title": "Distribuciones de probabilidad",
    "section": "",
    "text": "\\[\nX \\sim \\text{StudentT}(\\nu)\n\\]\n\\[\np(x \\mid \\nu) =\n    \\frac{\\Gamma(\\frac{\\nu + 1}{2})}{\\Gamma(\\frac{\\nu}{2})}\n    \\left(\\frac{1}{\\pi\\nu}\\right) ^ {\\frac{1}{2}}\n    \\left[1 + \\frac{x^2}{\\nu}\\right]^{-\\frac{\\nu + 1}{2}}\n\\]\n\\[\nP(X \\le x) = \\frac{1}{2} + x \\Gamma\\left(\\frac{\\nu + 1}{2}\\right)\n             \\frac{{}_2F_1\\left(\\frac{1}{2}, \\frac{v + 1}{2}, \\frac{3}{2}, \\frac{-x^2}{\\nu} \\right)}\n             {\\sqrt{\\pi \\nu} \\Gamma(\\frac{\\nu}{2})}\n\\]\ndonde \\({}_2F_1\\) es la función hipergeométrica.\n\n\\(X \\in \\mathbb{R}\\)\n\\(\\nu &gt; 0\\)\n\\(\\mathbb{E}(X) = 0\\) si \\(\\nu &gt; 1\\)\n\\(\\mathbb{V}(X) = \\nu / (\\nu - 2)\\) si \\(\\nu &gt; 2\\)",
    "crumbs": [
      "Recursos",
      "Distribuciones"
    ]
  },
  {
    "objectID": "recursos/distribuciones/distribuciones.html#gamma",
    "href": "recursos/distribuciones/distribuciones.html#gamma",
    "title": "Distribuciones de probabilidad",
    "section": "",
    "text": "\\[\nX \\sim \\text{Gamma}(k, \\theta)\n\\]\n\\[\np(x \\mid k, \\theta) = \\frac{1}{\\Gamma(k)\\theta^k}x^{k-1}e^{-\\frac{x}{\\theta}}\n\\]\n\\[\nP(X \\le x) = \\frac{1}{\\Gamma(k)} \\gamma \\left(k, \\frac{x}{\\theta}\\right)\n\\]\n\n\\(X &gt; 0\\)\n\\(k &gt; 0\\)\n\\(\\theta &gt; 0\\)\n\\(\\mathbb{E}(X) = k\\theta\\)\n\\(\\mathbb{V}(X) = k\\theta^2\\)\n\n\n\n\n\\[\nX \\sim \\text{Gamma}(\\alpha, \\beta)\n\\]\n\\[\np(x | \\alpha, \\beta) = \\frac{1}{\\Gamma(\\alpha)}x^{\\alpha-1}e^{-\\beta x}\\beta^\\alpha\n\\]\n\\[\nP(X \\le x) = \\frac{1}{\\Gamma(\\alpha)} \\gamma(\\alpha, \\beta x)\n\\]\n\n\\(X &gt; 0\\)\n\\(\\alpha &gt; 0\\)\n\\(\\beta &gt; 0\\)\n\\(\\mathbb{E}(X) = \\alpha/\\beta\\)\n\\(\\mathbb{V}(X) = \\alpha/\\beta^2\\)",
    "crumbs": [
      "Recursos",
      "Distribuciones"
    ]
  },
  {
    "objectID": "recursos/distribuciones/distribuciones.html#exponencial",
    "href": "recursos/distribuciones/distribuciones.html#exponencial",
    "title": "Distribuciones de probabilidad",
    "section": "",
    "text": "\\[\nX \\sim \\text{Exponencial}(\\lambda)\n\\]\n\\[\np(x \\mid \\lambda) = \\lambda e^{-\\lambda x}\n\\]\n\\[\nP(X \\le x) = 1 - e^{-\\lambda x}\n\\]\n\n\\(X &gt; 0\\)\n\\(\\lambda &gt; 0\\)\n\\(\\mathbb{E}(X) = 1 / \\lambda\\)\n\\(\\mathbb{V}(X) = 1 / \\lambda ^ 2\\)\n\nEs un caso particular de \\(\\text{Gamma}(\\alpha, \\beta)\\) con \\(\\alpha = 1\\) y \\(\\beta = \\lambda\\)",
    "crumbs": [
      "Recursos",
      "Distribuciones"
    ]
  },
  {
    "objectID": "recursos/distribuciones/distribuciones.html#beta",
    "href": "recursos/distribuciones/distribuciones.html#beta",
    "title": "Distribuciones de probabilidad",
    "section": "",
    "text": "\\[\nX \\sim \\text{Beta}(a, b)\n\\]\n\\[\np(x \\mid a, b) = \\frac{x^{a-1} (1-x)^{b-1}}{B(a, b)}\n\\]\n\\[\nB(a, b) = \\int_0^1 x^{a-1} (1-x)^{b-1} dx = \\frac{\\Gamma(a)\\Gamma(b)}{\\Gamma(a+b)}\n\\]\n\\[\n\\Gamma(x) = \\int_0^\\infty u^{x-1} e^{-u} du\n\\]\n\n\\(X \\in (0, 1)\\)\n\\(\\displaystyle \\mathbb{E}(X) = \\frac{a}{a + b}\\)\n\\(\\displaystyle \\mathbb{V}(X) = \\frac{ab}{(a + b) ^ 2 (a + b + 1)}\\)",
    "crumbs": [
      "Recursos",
      "Distribuciones"
    ]
  },
  {
    "objectID": "recursos/distribuciones/distribuciones.html#binomial",
    "href": "recursos/distribuciones/distribuciones.html#binomial",
    "title": "Distribuciones de probabilidad",
    "section": "",
    "text": "\\[\nX \\sim \\text{Binomial}(\\theta, n)\n\\]\n\\[\np(x \\mid \\theta, n) = {n \\choose x} \\theta^x (1 - \\theta)^{(n - x)}\n\\]\n\\[\nP(X \\le x) = \\sum_{i = 0} ^ {x} {n \\choose i} \\theta^i (1 - \\theta)^{(n - i)}\n\\]\n\n\\(X \\in \\{0, 1, 2, \\cdots, n\\}\\)\n\\(\\theta \\in [0, 1]\\)\n\\(n \\in \\{0, 1, 2, \\cdots \\}\\)\n\\(\\mathbb{E}(X) = n \\theta\\)\n\\(\\mathbb{V}(X) = n \\theta (1 - \\theta)\\)",
    "crumbs": [
      "Recursos",
      "Distribuciones"
    ]
  },
  {
    "objectID": "recursos/distribuciones/distribuciones.html#poisson",
    "href": "recursos/distribuciones/distribuciones.html#poisson",
    "title": "Distribuciones de probabilidad",
    "section": "",
    "text": "\\[\nX \\sim \\text{Poisson}(\\lambda)\n\\]\n\\[\np(x \\mid \\lambda) = \\frac{\\lambda^x e^{-\\lambda}}{x!}\n\\]\n\\[\nP(X \\le x) = e^{-\\lambda} \\sum_{i = 0} ^ {x} \\frac{\\lambda^i}{i!}\n\\]\n\n\\(X \\in \\{0, 1, 2, \\cdots\\}\\)\n\\(\\lambda &gt; 0\\)\n\\(\\mathbb{E}(X) = \\lambda\\)\n\\(\\mathbb{V}(X) = \\lambda\\)",
    "crumbs": [
      "Recursos",
      "Distribuciones"
    ]
  },
  {
    "objectID": "recursos/distribuciones/distribuciones.html#binomial-negativa",
    "href": "recursos/distribuciones/distribuciones.html#binomial-negativa",
    "title": "Distribuciones de probabilidad",
    "section": "",
    "text": "\\[\nX \\sim \\text{BinomialNegativa}(r, p)\n\\]\n\\[\np(x \\mid k, p)= \\binom{x + r - 1}{x}(1 - p)^x p^r\n\\]\n\n\\(X \\in \\{0, 1, 2, \\cdots \\}\\)\n\\(r \\in \\{1, 2, 3, \\cdots \\}\\)\n\\(p \\in [0, 1]\\)\n\\(\\mathbb{E}(X) = r(1 - p) / p\\)\n\\(\\mathbb{V}(X) = r(1 - p) / p^2\\)",
    "crumbs": [
      "Recursos",
      "Distribuciones"
    ]
  },
  {
    "objectID": "recursos/codigo/09_bolas_infinitas.html",
    "href": "recursos/codigo/09_bolas_infinitas.html",
    "title": "09 - Bolas infinitas",
    "section": "",
    "text": "El siguiente programa sirve para responder al ejercicio Bolas infinitas de la Práctica 3.\n\nlibrary(ggplot2)\n\nParte 1\nSe ejecutan 10000 pasos de este proceso una sola vez.\n\n# Inicializar urna con una bola azul y otra amarilla\nurna &lt;- c(\"azul\", \"amarillo\")\n\n# Realizar 10000 pasos del proceso\nfor (i in seq_len(10000)) {\n    # Seleccionar uno de los elementos de la urna\n    muestra &lt;- sample(urna, 1)\n\n# Agregar otro elemento igual al extraido\n    urna &lt;- c(urna, muestra)\n\n    # Cada 500 iteraciones imprimir la proporción de bolas azules\n    if (i %% 500 == 0) {\n        cat(\"Proporcion de bolas azules\", round(mean(urna == \"azul\"), 4), \"\\n\")\n    }\n}\n\nProporcion de bolas azules 0.755 \nProporcion de bolas azules 0.7305 \nProporcion de bolas azules 0.731 \nProporcion de bolas azules 0.7313 \nProporcion de bolas azules 0.735 \nProporcion de bolas azules 0.7348 \nProporcion de bolas azules 0.739 \nProporcion de bolas azules 0.7376 \nProporcion de bolas azules 0.7399 \nProporcion de bolas azules 0.7425 \nProporcion de bolas azules 0.7445 \nProporcion de bolas azules 0.7439 \nProporcion de bolas azules 0.7456 \nProporcion de bolas azules 0.7475 \nProporcion de bolas azules 0.7462 \nProporcion de bolas azules 0.7463 \nProporcion de bolas azules 0.7444 \nProporcion de bolas azules 0.7431 \nProporcion de bolas azules 0.7418 \nProporcion de bolas azules 0.7427 \n\n# Mostrar la proporción de bolas azules\nmean(urna == \"azul\")\n\n[1] 0.7426515\n\n\nParte 2\nSe ejecutan 1000 pasos de este proceso diez veces.\n\n# Crear vector vacío que contendrá las proporciones de azules a medida que\n# se realizan pasos del experimento.\nproporciones &lt;- numeric(0)\n\n# Realizar 10 iteraciones independientes (utilizando 10 bolsas independientes)\nfor (j in 1:10) {\n    urna &lt;- c(\"azul\", \"amarillo\")\n    for (i in 1:1000) {\n        muestra &lt;- sample(urna, 1)\n        urna &lt;- c(urna, muestra)\n    }\n    # Calcular la proporción de azules conforme se avanza en el experimento\n    proporcion &lt;- cumsum(urna == \"azul\") / seq_along(urna)\n\n    # Descartar el primer valor, que siempre es 0.5\n    proporcion &lt;- proporcion[2:length(proporcion)]\n\n    # Guardar el vector producido en el vector 'proporciones'\n    proporciones &lt;- c(proporciones, proporcion)\n}\n\n# Generar un data.frame para graficar con ggplot2\n# x: Los pasos del experimento\n# y: Las proporciones conforme se realizan pasos del experimento\n# prueba: Indica la prueba. Sirve para graficar una línea para cada prueba\ndatos &lt;- data.frame(\n    x = rep(seq_along(proporcion), 10),\n    y = proporciones,\n    prueba = as.factor(rep(1:10, each = length(proporcion)))\n)\n\nggplot(datos) +\n    geom_line(aes(x = x, y = y, color = prueba)) +\n    theme_bw() +\n    theme(\n        panel.grid.minor = element_blank()\n    )\n\n\n\n\n\n\n\n\n¿Conclusión?",
    "crumbs": [
      "Recursos",
      "Código",
      "09 - Bolas infinitas"
    ]
  },
  {
    "objectID": "recursos/codigo/01_froot_loops_grilla.html",
    "href": "recursos/codigo/01_froot_loops_grilla.html",
    "title": "01 - Liberen al Tucán Sam",
    "section": "",
    "text": "En este recurso se muestra como usar R para resolver el ejercicio de los Froot Loops hecho durante la primera clase.\n\nlibrary(ggplot2)\nlibrary(patchwork)\n\ncolores &lt;- c(\"#f08533\", \"#3b78b0\", \"#d1352c\")\n\nEn base al prior elicitado grupalmente en clase:\n\n# Determinar grilla de puntos\npi_grid &lt;- seq(0, 1, length.out = 11)\nprint(pi_grid)\n\n [1] 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0\n\n# Especificar prior\n# Tenemos $1000 y los dividimos en los diferentes valores de \"pi\"\nprior_ &lt;- c(0, 50, 150, 600, 150, 50, 0, 0, 0, 0, 0)\nprior &lt;- prior_ / sum(prior_)\n\n# Recolectar datos\ncantidad_de_cereales &lt;- 29 # n\ncantidad_de_cereales_amarillos &lt;- 7 # y\n\n# Calcular verosimilitud para cada valor de \"pi\" en la grilla\nlikelihood &lt;- dbinom(\n  cantidad_de_cereales_amarillos,\n  cantidad_de_cereales,\n  pi_grid\n)\n\n# Obtener posterior\nposterior_ &lt;- prior * likelihood\nposterior &lt;- posterior_ / sum(posterior_) # normalización\n\n# Graficar prior\nplt_prior &lt;- data.frame(x = pi_grid, y = prior) |&gt;\n  ggplot() +\n  geom_segment(aes(x = x, xend = x, y = 0, yend = y), color = colores[1], linewidth = 0.8) +\n  geom_point(aes(x = x, y = y), , color = colores[1], size = 2.4) +\n  scale_x_continuous(breaks = pi_grid) +\n  labs(\n    x = expression(pi),\n    y = expression(p ~ \"(\" ~ pi ~ \")\"),\n    title = \"Distribución a priori\"\n  ) +\n  theme_bw() +\n  theme(panel.grid.minor = element_blank())\n\n# Graficar verosimilitud\nplt_likelihood &lt;- data.frame(x = pi_grid, y = likelihood) |&gt;\n  ggplot() +\n  geom_segment(aes(x = x, xend = x, y = 0, yend = y), color = colores[2], linewidth = 0.8) +\n  geom_point(aes(x = x, y = y), , color = colores[2], size = 2.4) +\n  scale_x_continuous(breaks = pi_grid) +\n  labs(\n    x = expression(pi),\n    y = expression(p ~ \"(y | \" ~ pi ~ \")\"),\n    title = \"Función de verosimilitud\"\n  ) +\n  theme_bw() +\n  theme(panel.grid.minor = element_blank())\n\n# Graficar posterior\nplt_posterior &lt;- data.frame(x = pi_grid, y = posterior) |&gt;\n  ggplot() +\n  geom_segment(aes(x = x, xend = x, y = 0, yend = y), color = colores[3], linewidth = 0.8) +\n  geom_point(aes(x = x, y = y), , color = colores[3], size = 2.4) +\n  scale_x_continuous(breaks = pi_grid) +\n  labs(\n    x = expression(pi),\n    y = expression(p ~ \"(\" ~ pi ~ \" | y)\"),\n    title = \"Distribución a posteriori\"\n  ) +\n  theme_bw() +\n  theme(panel.grid.minor = element_blank())\n\n# Concatenar graficos\nplt_prior | plt_likelihood | plt_posterior\n\n\n\n\n\n\n\n\nEn base a un prior beta:\n\n# Determinar grilla de puntos\npi_grid &lt;- seq(0, 1, length.out = 200)\n\n# Obtener prior\nprior_ &lt;- dbeta(pi_grid, 3, 7)\nprior &lt;- prior_ / sum(prior_)\n\n# Calcular verosimilitud para cada valor de \"pi\"\nlikelihood &lt;- dbinom(\n  cantidad_de_cereales_amarillos,\n  cantidad_de_cereales,\n  pi_grid\n)\n\n# Obtener posterior\nposterior_ &lt;- prior * likelihood\nposterior &lt;- posterior_ / sum(posterior_)\n\n# Graficar prior\nplt_prior &lt;- data.frame(x = pi_grid, y = prior) |&gt;\n  ggplot() +\n  geom_line(aes(x = x, y = y), color = colores[1], linewidth = 1) +\n  labs(\n    x = expression(pi),\n    y = expression(p ~ \"(\" ~ pi ~ \")\"),\n    title = \"Distribución a priori\"\n  ) +\n  theme_bw() +\n  theme(panel.grid.minor = element_blank())\n\n# Graficar verosimilitud\nplt_likelihood &lt;- data.frame(x = pi_grid, y = likelihood) |&gt;\n  ggplot() +\n  geom_line(aes(x = x, y = y), color = colores[2], linewidth = 1) +\n  labs(\n    x = expression(pi),\n    y = expression(p ~ \"(y | \" ~ pi ~ \")\"),\n    title = \"Función de verosimilitud\"\n  ) +\n  theme_bw() +\n  theme(panel.grid.minor = element_blank())\n\n# Graficar posterior\nplt_posterior &lt;- data.frame(x = pi_grid, y = posterior) |&gt;\n  ggplot() +\n  geom_line(aes(x = x, y = y), color = colores[3], linewidth = 1) +\n  labs(\n    x = expression(pi),\n    y = expression(p ~ \"(\" ~ pi ~ \" | y)\"), \n    title = \"Distribución a posteriori\"\n  ) +\n  theme_bw() +\n  theme(panel.grid.minor = element_blank())\n\n# Concatenar graficos\nplt_prior | plt_likelihood | plt_posterior\n\n\n\n\n\n\n\n\nPreguntas\n\n¿Qué pasa con el prior si incrementamos el \\(n\\)?\n¿Qué pasa con el posterior si incrementamos el \\(n\\)?",
    "crumbs": [
      "Recursos",
      "Código",
      "01 - Liberen al Tucán Sam"
    ]
  },
  {
    "objectID": "recursos/codigo/07_puntos_uniformes.html",
    "href": "recursos/codigo/07_puntos_uniformes.html",
    "title": "07 - Los puntos uniformes",
    "section": "",
    "text": "Se explica como obtener muestras de una distribución uniforme en un círculo. Esto sirve de puntapié inicial para hacer el ejercicio Los puntos uniformes de la Práctica 3.\nEl programa muestrea los puntos en coordenadas polares. Primero se muestrean los ángulos y luego las distancias al origen. Finalmente se convierten estos puntos al sistema de coordenadas del plano cartesiano y se grafican.\n\nlibrary(ggplot2)\n\n# Cantidad de puntos\nn &lt;- 2000\n\n# Radio del circulo\nradio &lt;- 3\n\n# Muestrear ángulos\ntheta &lt;- runif(n, 0, 2 * pi)\n\n# Muestrear distancia al origen\nr &lt;- radio * sqrt(runif(n, 0, 1))\n\n# Convertir coordenadas al plano cartesiano\nx &lt;- r * cos(theta)\ny &lt;- r * sin(theta)\n\n# Graficar\ncentro &lt;- c(0, 0)\nangulos &lt;- seq(0, 2 * pi, length.out = 512)\nx_circulo &lt;- centro[1] + radio * cos(angulos)\ny_circulo &lt;- centro[2] + radio * sin(angulos)\n\ndata.frame(x = x, y = y) |&gt;\n    ggplot() +\n    geom_point(aes(x = x, y = y), alpha = 0.5, color = \"grey50\") +\n    geom_path(\n        aes(x = x, y = y),\n        linewidth = 1.25,\n        color = \"#3b78b0\",\n        data = data.frame(x = x_circulo, y = y_circulo)\n    ) +\n    theme_bw() +\n    theme(\n        panel.grid.major = element_blank(),\n        panel.grid.minor = element_blank(),\n    )",
    "crumbs": [
      "Recursos",
      "Código",
      "07 - Los puntos uniformes"
    ]
  },
  {
    "objectID": "recursos/codigo/04_diferentes_posteriors.html",
    "href": "recursos/codigo/04_diferentes_posteriors.html",
    "title": "04 - Diferentes observaciones, diferentes posteriors",
    "section": "",
    "text": "Este programa obtiene la distribución a posteriori para los diferentes trabajadores del ejercicio Diferentes observaciones, diferentes posteriors de la Práctica 2.\n\nlibrary(ggplot2)\n\ncolores &lt;- c(\"#f08533\", \"#3b78b0\", \"#d1352c\")\n\n# Crear grilla para los valores de \"pi\"\ngrid_n &lt;- 200\npi_grid &lt;- seq(0, 1, length.out = grid_n)\n\n# Hiperparámetros del prior\na_prior &lt;- 4\nb_prior &lt;- 3\n\n# Cantidad de éxitos y ensayos para cada trabajador\ny_1 &lt;- 0\nN_1 &lt;- 1\n\ny_2 &lt;- 3\nN_2 &lt;- 10\n\ny_3 &lt;- 20\nN_3 &lt;- 100\n\n# Hiperparámetros del posterior, para cada trabajador\na_posterior_1 &lt;- a_prior + y_1\nb_posterior_1 &lt;- b_prior + N_1 - y_1\n\na_posterior_2 &lt;- a_prior + y_2\nb_posterior_2 &lt;- b_prior + N_2 - y_2\n\na_posterior_3 &lt;- a_prior + y_3\nb_posterior_3 &lt;- b_prior + N_3 - y_3\n\n# Evaluar la funcion de densidad del posterior de cada trabajador\n# en cada uno de los puntos de \"pi_grid\"\nposterior_1 &lt;- dbeta(pi_grid, a_posterior_1, b_posterior_1)\nposterior_2 &lt;- dbeta(pi_grid, a_posterior_2, b_posterior_2)\nposterior_3 &lt;- dbeta(pi_grid, a_posterior_3, b_posterior_3)\n\n# Crear un data.frame, necesario para trabajar con ggplot2\ndatos &lt;- data.frame(\n    p = rep(pi_grid, times = 3),\n    posterior = c(posterior_1, posterior_2, posterior_3),\n    trabajador = rep(c(\"T1\", \"T2\", \"T3\"), each = grid_n)\n)\n\n# Crear el grafico con ggplot2 con los siguientes mapeos\n# * Los valores del eje horizontal \"x\" salen de \"pi\"\n# * Los valores de la altura en el eje vertical \"y\" salen de \"posterior\"\n# * Los colores se mapean a cada uno de los valores de \"trabajador\"\n# * Las areas tienen un color de relleno distinto para cada valor en \"trabajador\"\nggplot(datos, aes(x = p, y = posterior, color = trabajador)) +\n    geom_line() +\n    geom_area(aes(fill = trabajador), alpha = 0.4, position = \"identity\") +\n    scale_color_manual(values = colores) +\n    scale_fill_manual(values = colores) +\n    labs(x = expression(pi), y = expression(\"p(\" ~ pi ~ \"| y)\")) +\n    theme_bw() +\n    theme(\n        panel.grid.major = element_blank(),\n        panel.grid.minor = element_blank(),\n    )",
    "crumbs": [
      "Recursos",
      "Código",
      "04 - Diferentes observaciones, diferentes posteriors"
    ]
  },
  {
    "objectID": "recursos/codigo/03_quien_domina_el_posterior.html",
    "href": "recursos/codigo/03_quien_domina_el_posterior.html",
    "title": "03 - ¿Quién domina el posterior?",
    "section": "",
    "text": "Este programa obtiene la distribución a posteriori para los diferentes casos del ejercicio ¿Quién domina el posterior? de la Práctica 2.\nCreamos dos funciones para simplificar los bloques de código en cada caso. La primera, generar_datos(), recibe una grilla para \\(\\pi\\), los valores de \\(p(\\pi)\\), \\(p(y \\mid \\pi)\\) y \\(p(\\pi, \\mid y)\\) en cada valor de la grilla, y devuelve un data.frame que permite graficar las 3 curvas con {ggplot2}. La segunda, generar_grafico(), simplemente toma el data.frame generado por generar_datos() y produce la visualización.\n\nlibrary(ggplot2)\n\ngrid_n &lt;- 200\npi_grid &lt;- seq(0, 1, length.out = grid_n)\n\ngenerar_datos &lt;- function(pi_grid, pi_prior, pi_likelihood, pi_posterior) {\n    grid_n &lt;- length(pi_grid)\n\n    datos &lt;- data.frame(\n        x = rep(pi_grid, times = 3),\n        y = c(pi_prior, pi_likelihood, pi_posterior),\n1        grupo = factor(\n            rep(c(\"prior\", \"likelihood\", \"posterior\"), each = grid_n),\n            levels = c(\"prior\", \"likelihood\", \"posterior\"),\n            ordered = TRUE\n        )\n    )\n    return(datos)\n}\n\ngenerar_grafico &lt;- function(datos) {\n    colores &lt;- c(\"#f08533\", \"#3b78b0\", \"#d1352c\")\n\n    plt &lt;- ggplot(datos) +\n        geom_line(aes(x = x, y = y, color = grupo), linewidth = 1) +\n        scale_color_manual(values = colores) +\n        labs(x = expression(pi), y = NULL) +\n        facet_wrap(~ grupo, scales = \"free_y\") +\n        theme_bw() +\n        theme(\n            panel.grid.major = element_blank(),\n            panel.grid.minor = element_blank(),\n            axis.ticks.y = element_blank(),\n            axis.text.y = element_blank(),\n            legend.position = \"none\"\n        )\n    return(plt)\n}\n\n\n1\n\nSe usa un factor() ordenado para indicarle a {ggplot2} que primero se ubica el prior, luego el likelihood, y finalmente el posterior.\n\n\n\n\nCaso i\n\na_prior &lt;- 1\nb_prior &lt;- 4\ny &lt;- 8\nN &lt;- 10\n\npi_prior &lt;- dbeta(pi_grid, a_prior, b_prior)\npi_likelihood &lt;- dbinom(y, N, pi_grid)\npi_posterior &lt;- dbeta(pi_grid, a_prior + y, b_prior + N - y)\n\ndatos &lt;- generar_datos(pi_grid, pi_prior, pi_likelihood, pi_posterior)\ngenerar_grafico(datos)\n\n\n\n\n\n\n\n\nCaso ii\n\na_prior &lt;- 20\nb_prior &lt;- 3\ny &lt;- 0\nN &lt;- 1\n\npi_prior &lt;- dbeta(pi_grid, a_prior, b_prior)\npi_likelihood &lt;- dbinom(y, N, pi_grid)\npi_posterior &lt;- dbeta(pi_grid, a_prior + y, b_prior + N - y)\n\ndatos &lt;- generar_datos(pi_grid, pi_prior, pi_likelihood, pi_posterior)\ngenerar_grafico(datos)\n\n\n\n\n\n\n\n\nCaso iii\n\na_prior &lt;- 4\nb_prior &lt;- 2\ny &lt;- 1\nN &lt;- 3\n\npi_prior &lt;- dbeta(pi_grid, a_prior, b_prior)\npi_likelihood &lt;- dbinom(y, N, pi_grid)\npi_posterior &lt;- dbeta(pi_grid, a_prior + y, b_prior + N - y)\n\ndatos &lt;- generar_datos(pi_grid, pi_prior, pi_likelihood, pi_posterior)\ngenerar_grafico(datos)\n\n\n\n\n\n\n\n\nCaso iv\n\na_prior &lt;- 3\nb_prior &lt;- 10\ny &lt;- 10\nN &lt;- 13\n\npi_prior &lt;- dbeta(pi_grid, a_prior, b_prior)\npi_likelihood &lt;- dbinom(y, N, pi_grid)\npi_posterior &lt;- dbeta(pi_grid, a_prior + y, b_prior + N - y)\n\ndatos &lt;- generar_datos(pi_grid, pi_prior, pi_likelihood, pi_posterior)\ngenerar_grafico(datos)\n\n\n\n\n\n\n\n\nCaso v\n\na_prior &lt;- 20\nb_prior &lt;- 2\ny &lt;- 10\nN &lt;- 200\n\npi_prior &lt;- dbeta(pi_grid, a_prior, b_prior)\npi_likelihood &lt;- dbinom(y, N, pi_grid)\npi_posterior &lt;- dbeta(pi_grid, a_prior + y, b_prior + N - y)\n\ndatos &lt;- generar_datos(pi_grid, pi_prior, pi_likelihood, pi_posterior)\ngenerar_grafico(datos)",
    "crumbs": [
      "Recursos",
      "Código",
      "03 - ¿Quién domina el _posterior_?"
    ]
  },
  {
    "objectID": "recursos/codigo/11_rwmh_1d.html",
    "href": "recursos/codigo/11_rwmh_1d.html",
    "title": "11 - Metropolis-Hastings en 1 dimensión",
    "section": "",
    "text": "En este recurso se implementa el algoritmo de Metropolis-Hastings para obtener muestras de distribuciones de variables aleatorias unidimensionales utilizando una distribución de propuesta normal. Se muestra como usar el algoritmo con ejercicio Muestreo utilizando el algoritmo de Metropolis-Hastings (I) de la Práctica 3.\n\nFunciones auxiliares\n\nlibrary(ggplot2)\n\ngraficar_distribucion &lt;- function(x_muestras, x_grid, p_grid) {\n    # Graficar histograma de las muestras con la curva de densidad teórica superpuesta.\n    plt &lt;- ggplot() +\n        geom_histogram(\n            aes(x = x, y = after_stat(density)),\n            fill = \"grey60\",\n            bins = 50,\n            data = data.frame(x = x_muestras)\n        ) +\n        geom_line(\n            aes(x = x, y = y),\n            color = \"#3b78b0\",\n            linewidth = 1,\n            data = data.frame(x = x_grid, y = p_grid)\n        ) +\n        labs(y = NULL) +\n        theme_bw() +\n        theme(\n            panel.grid.major = element_blank(),\n            axis.ticks.y = element_blank(),\n            axis.text.y = element_blank(),\n            legend.position = \"none\"\n        )\n\n    return(plt)\n}\n\ngraficar_traza &lt;- function(x_muestras) {\n    # Generar un _traceplot_ para una cadena de Markov.\n    plt &lt;- data.frame(x = seq_along(x_muestras), y = x_muestras) |&gt;\n        ggplot() +\n        geom_line(aes(x = x, y = y), color = \"grey30\") +\n        labs(x = \"Paso\", y = \"x\") +\n        theme_bw()\n        theme(\n            panel.grid.major = element_blank(),\n            axis.ticks.y = element_blank(),\n            axis.text.y = element_blank(),\n            legend.position = \"none\"\n        )\n\n    return(plt)\n}\n\nset.seed(12) # para que los resultados sean reproducibles\n\n\n\nImplementación\n\nmetropolis_hastings_normal &lt;- function(n, x, p, sigma, verbose = FALSE) {\n    # Algortimo de Metropolis Hastings en una dimensión.\n    # La distribución de propuesta es normal.\n    #\n    # Parámetros\n    #  ------------------------------------------------------------------------\n    # | n        | Cantidad de muestras a obtener.                             |\n    # | x        | Posición inicial del algoritmo.                             |\n    # | p        | Función de densidad objetivo, normalizada o sin normalizar. |\n    # | sigma    | Desvío estándar de la distribución de propuesta normal.     |\n    # | verbose  | Indica si se muestran mensajes con información del          |\n    # |            algoritmo en cada paso.                                     |\n    #  ------------------------------------------------------------------------\n    #\n    # Salida\n    #  ------------------------------------------------------------------------\n    # | muestras | Vector con las muestras obtenidas.                          |\n    #  ------------------------------------------------------------------------\n\n    muestras &lt;- numeric(n)\n    muestras[1] &lt;- x\n\n    # Iterar desde i = 1 hasta i = n - 1\n    for (i in seq_len(n - 1)) {\n        # Paso 1: Proponer un nuevo valor\n        x_actual &lt;- muestras[i]\n        x_propuesto &lt;- rnorm(1, mean = x_actual, sd = sigma)\n\n        # Paso 2: Calcular probabilidad de aceptación\n        p_actual &lt;- p(x_actual)\n        p_propuesto &lt;- p(x_propuesto)\n\n        # Corrección en base a la densidad de la distribución de propuesta\n        q_propuesto &lt;- dnorm(x_propuesto, mean = x_actual, sd = sigma)  # q(x_propuesto | x_actual)\n        q_actual &lt;- dnorm(x_actual, mean = x_propuesto, sd = sigma)     # q(x_actual | x_propuesto)\n\n        alpha &lt;- min(1, (p_propuesto / p_actual) * (q_actual / q_propuesto))\n\n        # Paso 3: Dedicir si se acepta el valor propuesto\n        u &lt;- runif(1)\n        aceptar &lt;- u &lt; alpha\n\n        # Guardar posición\n        if (aceptar) {\n            muestras[i + 1] &lt;- x_propuesto\n        } else {\n            muestras[i + 1] &lt;- x_actual\n        }\n\n        # Si 'verbose' es TRUE, mostrar valores de variables relevantes\n        if (verbose) {\n            cat(\"-----------------------\\n\")\n            cat(\"x_actual: \", x_actual, \"x_propuesto\", x_propuesto, \"\\n\")\n            cat(\"p_actual: \", p_actual, \"p_propuesto\", p_propuesto, \"\\n\")\n            cat(\"q_propuesto: \", q_propuesto, \"q_actual\", q_actual, \"\\n\")\n            cat(\"alpha:\", alpha, \"u:\", u, \"aceptar:\", aceptar, \"\\n\")\n        }\n    }\n\n    return(muestras)\n}\n\n\n\nNormal\n\\[\nX \\sim \\text{Normal}(\\mu = 3, \\sigma = 6)\n\\]\n\ndensidad &lt;- function(x) dnorm(x, mean = 3, sd = 6)\nx_grid &lt;- seq(-15, 21, length.out = 500)\np_grid &lt;- densidad(x_grid)\n\n\nPropuesta: \\(\\sigma = 0.1\\)\n\nmuestras &lt;- metropolis_hastings_normal(n = 5000, x = 2, p = densidad, sigma = 0.1)\ngraficar_distribucion(x_muestras = muestras, x_grid = x_grid, p_grid = p_grid)\n\n\n\n\n\n\n\n\n\ngraficar_traza(x_muestras = muestras)\n\n\n\n\n\n\n\n\n\n\nPropuesta: \\(\\sigma = 2\\)\n\nmuestras &lt;- metropolis_hastings_normal(n = 5000, x = 2, p = densidad, sigma = 2)\ngraficar_distribucion(x_muestras = muestras, x_grid = x_grid, p_grid = p_grid)\n\n\n\n\n\n\n\n\n\ngraficar_traza(x_muestras = muestras)\n\n\n\n\n\n\n\n\n\n\n\nT de Student\n\\[\nX \\sim \\text{StudentT}(\\nu = 5)\n\\]\n\ndensidad &lt;- function(x) dt(x, df = 5)\nx_grid &lt;- seq(-6, 6, length.out = 500)\np_grid &lt;- densidad(x_grid)\n\n\nPropuesta: \\(\\sigma = 0.1\\)\n\nmuestras &lt;- metropolis_hastings_normal(n = 5000, x = 0, p = densidad, sigma = 0.1)\ngraficar_distribucion(x_muestras = muestras, x_grid = x_grid, p_grid = p_grid)\n\n\n\n\n\n\n\n\n\ngraficar_traza(x_muestras = muestras)\n\n\n\n\n\n\n\n\n\n\nPropuesta: \\(\\sigma = 1\\)\n\nmuestras &lt;- metropolis_hastings_normal(n = 5000, x = 0, p = densidad, sigma = 1)\ngraficar_distribucion(x_muestras = muestras, x_grid = x_grid, p_grid = p_grid)\n\n\n\n\n\n\n\n\n\ngraficar_traza(x_muestras = muestras)\n\n\n\n\n\n\n\n\n\n\n\nMezcla de normales\n\\[\nX \\sim \\frac{2}{3}\\text{Normal}(\\mu = 0, \\sigma = 0.5) + \\frac{1}{3}\\text{Normal}(\\mu = 3, \\sigma = 2)\n\\]\n\ndensidad &lt;- function(x) {\n   (2 / 3) * dnorm(x, mean = 0, sd = 0.5) + (1 / 3) * dnorm(x, mean = 3, sd = 2)\n}\nx_grid &lt;- seq(-5, 10, length.out = 500)\np_grid &lt;- densidad(x_grid)\n\n\nPropuesta: \\(\\sigma = 0.1\\)\n\nmuestras &lt;- metropolis_hastings_normal(n = 5000, x = 0, p = densidad, sigma = 0.1)\ngraficar_distribucion(x_muestras = muestras, x_grid = x_grid, p_grid = p_grid)\n\n\n\n\n\n\n\n\n\ngraficar_traza(x_muestras = muestras)\n\n\n\n\n\n\n\n\n\n\nPropuesta: \\(\\sigma = 1.2\\)\n\nmuestras &lt;- metropolis_hastings_normal(n = 5000, x = 0, p = densidad, sigma = 1.2)\ngraficar_distribucion(x_muestras = muestras, x_grid = x_grid, p_grid = p_grid)\n\n\n\n\n\n\n\n\n\ngraficar_traza(x_muestras = muestras)\n\n\n\n\n\n\n\n\n\n\n\nComentarios\nEn ninguno de los casos resulta adecuado el desvío estándar inicialmente asignado a la distribución de propuesta. El valor inicial de \\(\\sigma\\) es bajo y las cadenas se desplazan lentamente por el soporte de \\(X\\). Los traceplots muestran que las muestras obtenidas con \\(\\sigma=0.1\\) presentan una alta autocorrelación. Además, al comparar las muestras con las densidades teóricas, se observa una notable discrepancia.\nEn cambio, en todos los ejemplos, bastó con incrementar el valor de \\(\\sigma\\) para que la distribución de propuesta generara saltos más largos. Las distribuciones empíricas obtenidas se ajustan mejor a las teóricas, y los traceplots evidencian una autocorrelación considerablemente menor.\n\n\nBonus\n¿Cómo se ve la trayectoria cuando el punto inicial no se encuentra en el typical set de la distribución?\n\nEjemplo 1\n\ndensidad &lt;- function(x) dnorm(x, mean = 3, sd = 6)\nx_grid &lt;- seq(-15, 21, length.out = 500)\np_grid &lt;- densidad(x_grid)\n\n\nmuestras &lt;- metropolis_hastings_normal(n = 5000, x = 60, p = densidad, sigma = 3)\ngraficar_distribucion(x_muestras = muestras, x_grid = x_grid, p_grid = p_grid)\n\n\n\n\n\n\n\n\n\ngraficar_traza(x_muestras = muestras)\n\n\n\n\n\n\n\n\n\n\nEjemplo 2\n\ndensidad &lt;- function(x) {\n   (2 / 3) * dnorm(x, mean = 0, sd = 0.5) + (1 / 3) * dnorm(x, mean = 3, sd = 2)\n}\nx_grid &lt;- seq(-5, 10, length.out = 500)\np_grid &lt;- densidad(x_grid)\n\n\nmuestras &lt;- metropolis_hastings_normal(n = 5000, x = -20, p = densidad, sigma = 1)\ngraficar_distribucion(x_muestras = muestras, x_grid = x_grid, p_grid = p_grid)\n\n\n\n\n\n\n\n\n\ngraficar_traza(x_muestras = muestras)",
    "crumbs": [
      "Recursos",
      "Código",
      "11 - Metropolis-Hastings en 1 dimensión"
    ]
  },
  {
    "objectID": "recursos/codigo/12_rwmh_2d.html",
    "href": "recursos/codigo/12_rwmh_2d.html",
    "title": "12 - Metropolis-Hastings en 2 dimensiones",
    "section": "",
    "text": "El siguiente código es de utilidad para resolver con R el ejercicio Metropolis-Hastings multivariado de la Práctica 3.\n\nlibrary(ggplot2)\nlibrary(mvtnorm)\n\n# Cantidad de muestras a obtener\nn &lt;- 5000\n\n# Parámetros de la distribución a muestrear\nMu_objetivo &lt;- c(1.2, 0.8)\nSigma_objetivo &lt;- matrix(c(3, 0.2, 0.2, 2), ncol = 2)\n\n# Matriz de dimensión (n, 2) que va a contener las muestras\nmuestras &lt;- matrix(NA, nrow = n, ncol = 2)\n\n# El punto inicial es el (0, 0)\nmuestras[1, ] &lt;- c(0, 0)\n\n# Matriz de varianza de la distribución de propuesta\nSigma_propuesta &lt;- diag(2) * 0.2\n\nfor (i in 2:n) {\n    # Proponer un nuevo valor\n    propuesta &lt;- rmvnorm(1, mean = muestras[i - 1, ], sigma = Sigma_propuesta)\n\n    # Evaluar la función de densidad en el valor actual y en el propuesto\n    f_propuesta &lt;- dmvnorm(propuesta, Mu_objetivo, Sigma_objetivo)\n    f_actual &lt;- dmvnorm(muestras[i - 1, ], Mu_objetivo, Sigma_objetivo)\n\n    # Calcular probabilidad de aceptación\n    alpha &lt;- min(c(1, f_propuesta / f_actual))\n\n    # Determinar aceptación de propuesta\n    aceptar &lt;- rbinom(1, 1, alpha)\n\n    # Seleccionar nueva muestras\n    if (aceptar) {\n        muestras[i, ] &lt;- propuesta\n    } else {\n        muestras[i, ] &lt;- muestras[i - 1, ]\n    }\n}\n\n# Obtener las muestras como data.frame\ndf &lt;- as.data.frame(muestras)\ncolnames(df) &lt;- c(\"y1\", \"y2\")\ndf$x &lt;- 1:n\n\n# Graficar muestras en el plano\nggplot(df) +\n    geom_point(aes(x = y1, y = y2), alpha = 0.6, color = \"grey30\") +\n    theme_bw() +\n    theme(\n        panel.grid.minor = element_blank()\n    )\n\n\n\n\n\n\n\n# Obtener una visualización de la densidad empírica\nggplot(df, aes(x = y1, y = y2)) +\n    stat_density2d(\n        geom = \"raster\",\n        aes(fill = after_stat(density)),\n        contour = FALSE\n    ) +\n    scale_x_continuous(expand = c(0, 0)) +\n    scale_y_continuous(expand = c(0, 0)) +\n    scale_fill_viridis_c()\n\n\n\n\n\n\n\n# Graficar las trazas de ambas variables\ntidyr::pivot_longer(df, c(\"y1\", \"y2\"), names_to = \"variable\") |&gt;\n    ggplot() +\n    geom_line(aes(x = x, y = value, color = variable)) +\n    scale_color_manual(values = c(\"#f08533\", \"#3b78b0\")) +\n    facet_wrap(~ variable, ncol = 1) +\n    theme_bw() +\n    theme(\n        panel.grid.minor = element_blank(),\n        legend.position = \"none\"\n    )",
    "crumbs": [
      "Recursos",
      "Código",
      "12 - Metropolis-Hastings en 2 dimensiones"
    ]
  },
  {
    "objectID": "recursos/codigo/05_likelihood_poisson.html",
    "href": "recursos/codigo/05_likelihood_poisson.html",
    "title": "05 - Función de verosimilitud Poisson",
    "section": "",
    "text": "Este programa grafica la función de verosimilitud cuando se obtiene una muestra de \\(n\\) realizaciones independientes de una distribución Poisson. Se utiliza en el ejercicio El modelo Gamma-Poisson de la Práctica 2.\n\\[\n\\begin{array}{lcc}\nX \\sim \\text{Poisson}(\\lambda), & \\lambda &gt; 0, & X \\in \\{0, 1, 2, \\cdots \\}\n\\end{array}\n\\]\n\\[\np(x \\mid \\lambda) = \\frac{e^{-\\lambda} \\lambda^x}{x!}\n\\]\n\\[\n\\ell(\\theta \\mid x_1, x_2, \\cdots, x_n) = p(\\mathbf{x} \\mid \\theta)\n    = \\prod_{i=1}^{n} p(x_i \\mid \\lambda)\n    = \\prod_{i=1}^{n} \\frac{e^{-\\lambda} \\lambda^{x_i}}{x_i!}\n    = \\frac{e^{- n \\lambda} \\lambda^{\\sum_i x_i}}{\\prod_i x_i!}\n\\]\n\nlibrary(ggplot2)\n\n# Cantidad de mensajes observados\nmensajes &lt;- c(7, 3, 8, 9, 10, 12)\n\nOpción 1: Escribir la función de verosimilitud analíticamente\n\nn &lt;- length(mensajes)\ntotal &lt;- sum(mensajes)\nproducto_factoriales &lt;- prod(factorial(mensajes))\n\n# Grilla de valores para 'lambda'\n# Teoréticamente, el soporte del parámetro es (0, infty). Lo acotamos en 20.\nlambda &lt;- seq(0, 20, length.out = 200)\n\n# Cálculo de la verosimilitud\nverosimilitud &lt;- exp(-n * lambda) * lambda ^ total / producto_factoriales\n\n# Visualización de la verosimilitud para los valores de lambda en la grilla\ndata.frame(x = lambda, y = verosimilitud) |&gt;\n    ggplot() +\n    geom_line(aes(x = x, y = y), linewidth = 1, color = \"#3b78b0\") +\n    labs(x = expression(lambda), y = expression(\"p(y | \" ~ lambda ~ \")\")) +\n    theme_bw() +\n    theme(\n        panel.grid.major = element_blank(),\n        panel.grid.minor = element_blank(),\n    )\n\n\n\n\n\n\n\n\nOpción 2: Utilizar dpois\n\nmensajes_matriz &lt;- matrix(rep(mensajes, 200), nrow = 200, byrow = TRUE)\n1pmf &lt;- dpois(mensajes_matriz, lambda)\n2verosimilitud &lt;- apply(pmf, 1, prod)\n\ndata.frame(x = lambda, y = verosimilitud) |&gt;\n    ggplot() +\n    geom_line(aes(x = x, y = y), linewidth = 1, color = \"#3b78b0\") +\n    labs(x = expression(lambda), y = expression(\"p(y | \" ~ lambda ~ \")\")) +\n    theme_bw() +\n    theme(\n        panel.grid.major = element_blank(),\n        panel.grid.minor = element_blank(),\n    )\n\n\n1\n\nEvaluar la función de masa de probabilidad de la distribución Poisson en cada uno de los valores observados utilizando una grilla de valores de \\(\\lambda\\).\n\n2\n\nPara cada valor de \\(\\lambda\\), multiplicar todas las evaluaciones realizadas.",
    "crumbs": [
      "Recursos",
      "Código",
      "05 - Función de verosimilitud Poisson"
    ]
  },
  {
    "objectID": "recursos/software/01_instalar_r.html",
    "href": "recursos/software/01_instalar_r.html",
    "title": "Instalar R",
    "section": "",
    "text": "El primer paso es ir al sitio https://r-project.org/ y hacer clic en donde dice CRAN.\n\n\n\n\n\n\n\n\n\nUna vez allí, aparecen mirrors (réplicas o copias) de la red CRAN donde se almacenan múltiples versiones de R, paquetes y otras dependencias. Se recomienda seleccionar la réplica Cloud o Argentina.\n\n\n\n\n\n\n\n\n\nAquí se incluyen diferentes enlaces que proveen el instalador de R para diferentes sistemas operativos. En el caso de encontrarse en Windows, hay que cliquear en Download R for Windows.\n\n\n\n\n\n\n\n\n\nLa siguiente página muestra diferentes subdirectorios. Para instalar R base hay que seleccionar el que dice base. Notar que también aparece RTools, un programa utilitario que será necesario más adelante.\n\n\n\n\n\n\n\n\n\nEn esta sección, aparece por defecto el enlace para descargar la última versión de R. Al momento de realizar esta guía la versión R 4.4.0 se encuentra recién lanzada, por lo que se recomienda utilizar la última versión de la serie 4.3. Para encontrar el enlace de descarga de versiones anteriores hay que hacer clic en Previous releases.\n\n\n\n\n\n\n\n\n\nAllí se encuentran enlaces para descargar versiones anteriores de R, junto a su fecha de publicación. Se recomienda seleccionar la version 4.3.3.\n\n\n\n\n\n\n\n\n\nLuego se presenta un listado de archivos disponibles en el directorio. Hay que seleccionar el instalador, que es el archivo que termina en .exe, y esto comienza la descarga del mismo.\n\n\n\n\n\n\n\n\n\nAl finalizar la descarga, se tiene que encontrar un archivo como el que se muestra debajo. Simplemente hay que hacer doble clic para ejecutar el instalador.\n\n\n\n\n\n\n\n\n\nLa instalación es sencilla y solamente hay que hacer seleccionar Next hasta finalizar. Las capturas a continuación se incluyen para mostrar las opciones seleccionadas en los diferentes pasos de la instalación.",
    "crumbs": [
      "Recursos",
      "Instalación de software",
      "Instalar R"
    ]
  },
  {
    "objectID": "recursos/software/04_instalar_rstan.html",
    "href": "recursos/software/04_instalar_rstan.html",
    "title": "Instalar RStan",
    "section": "",
    "text": "Si bien los pasos a continuación pueden funcionar con múltiples versiones de R, en esta guía se asume que se tiene instalado R 4.3.3, RTools 4.3 y RStudio (aunque este último no sea estrictamente necesario).\nNo se recomienda utilizar las versiones 4.0, 4.1 y 4.2 de R para trabajar con RStan en Windows debido a la cantidad de problemas que han reportado en el pasado.\nPrevio a instalar RStan, se debe verificar que RTools se encuentra instalado y R tiene acceso al mismo. Una forma de verificarlo es ejecutando Sys.which(\"make\"), como se explica en Instalar RTools.\n\n\n\n\n\n\n\n\n\nDado que RTools funciona en el sistema, instalar RStan es tan sencillo como correr install.packages(\"rstan\").\n\n\n\n\n\n\n\n\n\nComienza un proceso de descarga de dependencias (ya compiladas):\n\n\n\n\n\n\n\n\n\ny luego se instalan y verifican las mismas:\n\n\n\n\n\n\n\n\n\nUna vez finalizado este proceso, se puede ejecutar library(\"rstan\"). Es de esperar que aparezcan varios mensajes, pero no hay que alarmarse ya que suelen ser a modo informativo. Lo más relevante son las versiones de RStan y Stan.\n\n\n\n\n\n\n\n\n\nFinalmente, la mejor forma de determinar si RStan funciona correctamente es obteniendo muestras de un posterior. En las capturas debajo se utiliza el siguiente bloque de código:\n\nlibrary(\"rstan\")\n\nN &lt;- 20\ny &lt;- 4\n\nmodel_beta1_stan &lt;- \"\ndata {\n  int N;     \n  int Y; \n}\nparameters {\n  real&lt;lower=0, upper=1&gt; pi;\n}\nmodel {\n  pi ~ beta(2,2); // prior\n  Y ~ binomial(N, pi);  // likelihood\n}\"\n\nmodel_beta1 &lt;- stan_model(model_code = model_beta1_stan)\n\ndata_list &lt;- list(Y = y, N = N)\n\nmodel_beta1_fit &lt;- sampling(\n  object = model_beta1, \n  data = data_list, \n  chains = 2, \n  iter = 500,\n  warmup = 100\n)\n\nEn el caso de que RStan haya funcionado correctamente, los mensajes en la consola serán similares a los que se muestran a continuación:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSi se quiere obtener más información sobre la instalación de RStan se puede consultar la wiki del repositorio de RStan o el foro de Stan donde se discuten múltiples temas.",
    "crumbs": [
      "Recursos",
      "Instalación de software",
      "Instalar RStan"
    ]
  },
  {
    "objectID": "recursos/software/index.html",
    "href": "recursos/software/index.html",
    "title": "Instalación de software",
    "section": "",
    "text": "En este curso se utiliza el lenguaje de programación probabilística Stan mediante su interface RStan.\nLa instalación de RStan en sistemas operativos basados en Linux es relativamente sencilla. Pero no lo es así en el sistema operativo Windows. En este último caso se necesita tener instalado un conjunto de herramientas conocido como RTools.\nEn las guías debajo se explica como instalar R, RTools, RStudio y RStan en una computadora que utiliza Windows.\nInstalar RInstalar RToolsInstalar RStudioInstalar RStan",
    "crumbs": [
      "Recursos",
      "Instalación de software"
    ]
  },
  {
    "objectID": "info/enlaces_utiles.html",
    "href": "info/enlaces_utiles.html",
    "title": "Enlaces útiles",
    "section": "",
    "text": "Awesome Bayesian Statistics. Es un listado de recursos en línea (y gratuitos!) relacionados al mundo de la Estadística Bayesiana."
  },
  {
    "objectID": "info/calendario.html",
    "href": "info/calendario.html",
    "title": "Calendario",
    "section": "",
    "text": "Warning in attr(x, \"align\"): 'xfun::attr()' is deprecated.\nUse 'xfun::attr2()' instead.\nSee help(\"Deprecated\")\n\n\nWarning in attr(x, \"format\"): 'xfun::attr()' is deprecated.\nUse 'xfun::attr2()' instead.\nSee help(\"Deprecated\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nSemana\nFecha\nUnidad\nTemas\nLectura sugerida\nOtras actividades\n\n\n\n\n1\n17 de marzo\n1, 2\n• Presentación de la materia• Probabilidad• Regla de Bayes• Inferencia bayesiana• Distribución a priori, función de verosimilitud y distribución a posteriori• Modelos conjugados\n• McElreath (2020): Capítulos 1 y 2• Kruschke (2014): Capítulos 1, 4, 5 y 6• Johnson, Ott, y Dogucu (2022): Capítulos 1, 3, 4, 5 y 8\n\n\n\n2\n24 de marzo\n1, 2\n• Modelos conjugados• Modelos de varios parámetros\n• Johnson, Ott, y Dogucu (2022): Capítulos 1, 3, 4, 5 y 8• Kruschke (2014): Capítulos 1, 5 y 6• McElreath (2020): Capítulo 2• Gelman et al. (2013): Capítulo 3\n\n\n\n3\n31 de marzo\n2\n• Nociones de Teoria de Decisión Bayesiana\n\nPresentación TP1 (4-abr)\n\n\n4\n7 de abril\n2, 3\n• Limitaciones del enfoque analítico• Simulaciones• Aproximación mediante grilla• Introducción al cómputo bayesiano• Markov-chain Montecarlo• Metropolis-Hastings\n• Johnson, Ott, y Dogucu (2022): Capítulos 6 y 7• McElreath (2020): Capítulo 3• Kruschke (2014): Capítulo 7\n\n\n\n5\n14 de abril\n3\n• Metropolis-Hastings• Diagnósticos• Hamiltonian Montecarlo\n• Johnson, Ott, y Dogucu (2022): Capítulo 7• Kruschke (2014): Capítulo 7\nEntrega TP1 (18-abr)\n\n\n6\n21 de abril\n3\n• Programación probabilística• Stan• Diagnósticos• Visualizaciones• Hamiltonian Montecarlo• Modelos lineales• Paquetes brms y rstanarm\n• Kruschke (2014): Capítulo 14• Lambert (2018): Capítulo 15• Johnson, Ott, y Dogucu (2022): Capítulo 9• McElreath (2020): Capítulo 4• Gelman y Hill (2006): Capítulos 3 y 4• Gelman, Hill, y Vehtari (2021): Capítulos 6, 7 y 8\n\n\n\n7\n28 de abril\n3\n• Modelos lineales• Paquetes brms y rstanarm• Trabajo TP2\n• Johnson, Ott, y Dogucu (2022): Capítulo 9• McElreath (2020): Capítulo 4• Gelman y Hill (2006): Capítulos 3 y 4• Gelman, Hill, y Vehtari (2021): Capítulos 6, 7 y 8\nPresentación TP2 (30-abr)\n\n\n8\n5 de mayo\n4\n• Validación interna de modelos• Pruebas predictivas a posteriori\n• Gelman, Hill, y Vehtari (2021): Capítulo 11• Johnson, Ott, y Dogucu (2022): Capítulo 10• McElreath (2020): Capítulo 7\n\n\n\n9\n12 de mayo\n4\n• Regularización• Selección de modelos• Criterios de información• Validación cruzada• Sobreajuste y subajuste• Repaso\n• Gelman, Hill, y Vehtari (2021): Capítulo 11• Johnson, Ott, y Dogucu (2022): Capítulo 10• McElreath (2020): Capítulo 7\nEntrega TP2 (14-may)\n\n\n10\n19 de mayo\n5\n• Parcial• Trabajo TP3\n\nParcial (21-may)  Presentación TP3 (23-may)\n\n\n11\n26 de mayo\n5\n• Regresión logística• Regresión Poisson\n• Gelman, Hill, y Vehtari (2021): Capítulos 13 y 15• Gelman y Hill (2006): Capítulos 5 y 6• Johnson, Ott, y Dogucu (2022): Capítulos 12 y 13\n\n\n\n12\n2 de junio\n5\n• Enfoque multinivel• Modelos jerárquicos• Shrinkage de parámetros\n• Kruschke (2014): Capítulo 9• Johnson, Ott, y Dogucu (2022): Capítulos 15 y 16• McElreath (2020): Capítulo 13• Gelman y Hill (2006): Capítulo 11\nRecuperatorio (2-jun)  Entrega TP3 (4-jun)  Presentación TP Final (4-jun)\n\n\n13\n9 de junio\n5\n• Modelos lineales jerárquicos• Variación en el intercepto• Variación en la pendiente• Problemas de estimación\n• Johnson, Ott, y Dogucu (2022): Capítulo 17• Gelman y Hill (2006): Capítulos 12 y 13\n\n\n\n14\n16 de junio\n\n• Trabajo TP Final\n\n\n\n\n15\n23 de junio\n\n\n\nEntrega TP Final + Defensa oral\n\n\n\n\n\n\n\n\n\n\nReferencias\n\nGelman, Andrew, John B. Carlin, Hal S. Stern, David B. Dunson, Aki Vehtari, y Donald B. Rubin. 2013. Bayesian Data Analysis. 3rd edition. Chapman; Hall/CRC.\n\n\nGelman, Andrew, y Jennifer Hill. 2006. Data Analysis Using Regression and Multilevel-Hierarchical Models. 1st edition. Cambridge University Press.\n\n\nGelman, Andrew, Jennifer Hill, y Aki Vehtari. 2021. Regression and Other Stories. 1st edition. Cambridge University Press. https://users.aalto.fi/~ave/ROS.pdf.\n\n\nJohnson, Alicia A., Miles Q. Ott, y Mine Dogucu. 2022. Bayes Rules! An Introduction to Bayesian Modeling. 1st edition. Chapman; Hall/CRC. https://www.bayesrulesbook.com/.\n\n\nKruschke, John. 2014. Doing Bayesian Data Analysis: A Tutorial with R, JAGS, and Stan. 2nd edition. Academic Press.\n\n\nLambert, Ben. 2018. A Student’s Guide to Bayesian Statistics. 1st edition. SAGE Publications Ltd.\n\n\nMcElreath, Richard. 2020. Statistical Rethinking: A Bayesian Course with Examples in R and Stan. 2nd edition. Chapman; Hall/CRC.",
    "crumbs": [
      "Información",
      "Calendario"
    ]
  },
  {
    "objectID": "info/aprobacion.html",
    "href": "info/aprobacion.html",
    "title": "Condiciones de aprobación",
    "section": "",
    "text": "Instancias de evaluación\n\n📝 Parcial – escrito e individual, se aprueba con 6, hay una instancia de recuperación;\n💻💻💻 Trabajos prácticos cortos – grupales (hasta tres personas), se hacen por fuera del horario de clase, se entrega informe;\n📈 Trabajo práctico final – realización grupal, presentación oral individual y defensa.\n\n\n\nCondiciones de aprobación\n\nPromoción\n\nLas y los estudiantes que hayan aprobado el parcial o su recuperatorio (con nota \\(P\\)), los tres trabajos prácticos cortos (con promedio simple \\(T\\)) y hayan entregado el trabajo práctico final, accederán a una instancia de evaluación oral donde se discutirá el trabajo práctico final y se evaluarán de manera general todos los contenidos conceptuales de la asignatura; esta instancia puede incluir la realización de algunas actividades en computadora (el trabajo práctico final y la instancia oral tendrán nota \\(O\\)).\nLa nota final se obtendrá según \\(0.3\\ T + 0.4\\ P + 0.3\\ O\\)\n\nRegularidad\n\nQuienes no alcancen la condición de promoción podrán quedar en condición de estudiante regular si aprueban el parcial o su recuperatorio y aprueban al menos dos de los trabajos prácticos cortos. Para alcanzar la aprobación de la asignatura, los y las estudiantes en condición de regulares deberán presentar el trabajo final en una mesa de examen y aprobar una instancia oral de defensa del trabajo y de evaluación integral de los contenidos de la materia. Esta instancia puede incluir la realización de algunas actividades en computadora.\n\nLibres\n\nAquellas personas que no alcancen la promoción de la asignatura ni la condición de estudiante regular quedarán en condición de estudiante libre. Los y las estudiantes en condición de libre deberán presentar un trabajo práctico y rendir un examen teórico-práctico sobre la totalidad de los temas de la asignatura.",
    "crumbs": [
      "Información",
      "Condiciones de aprobación"
    ]
  },
  {
    "objectID": "info/faq.html",
    "href": "info/faq.html",
    "title": "Preguntas frecuentes",
    "section": "",
    "text": "Preguntas frecuentes\n\nPertenezo al plan 2003 de la carrera, ¿puedo participar de las clases?\nSi, podés asistir a las clases como oyente y también realizar los trabajos prácticos. Sin embargo, los profesores no se comprometen a realizar devoluciones o correcciones sobre los trabajos de estudiantes que asistan en calidad de oyente."
  },
  {
    "objectID": "info/programa.html",
    "href": "info/programa.html",
    "title": "Programa",
    "section": "",
    "text": "Fundamentación\nLa Estadística Bayesiana es un enfoque de la inferencia estadística que se basa en utilizar probabilidades para representar el conocimiento disponible sobre el conjunto de parámetros de un modelo y actualizar esa información utilizando la Regla de Bayes a partir de la observación de un conjunto de datos. El conocimiento inicial se representa con una distribución de probabilidad a priori, la información contenida en los datos observados se modeliza con una función de verosimilitud y ambas fuentes de información se combinan para obtener una distribución de probabilidad a posteriori. La información a posteriori puede ser utilizada para extraer conclusiones sobre el fenómeno en estudio y realizar predicciones sobre datos no observados o eventos futuros.\nLos métodos bayesianos requieren, salvo en casos muy simples, una complejidad computacional que resultaba inalcanzable hace algunos años. Gracias a desarrollos revolucionarios en el ámbito de la computación, la principal barrera para la implementación de los modelos bayesianos desapareció y la utilización de estos se ha incrementado masivamente en múltiples campos científicos. Esta creciente popularidad se debe a que la inferencia bayesiana brinda un marco teórico consistente que permite la incorporación de información a priori, el desarrollo de un aprendizaje secuencial, la obtención de inferencias y predicciones en forma de distribuciones de probabilidad, el tratamiento de datos faltantes, los análisis con pocos datos, entre otras ventajas.\n\n\nObjetivos\nQue quienes cursen la materia logren:\n\nentender las características y los conceptos fundamentales de la Estadística Bayesiana;\ndescribir las características principales de la Estadística Bayesiana;\ncomprender la complejidad analítica de la inferencia bayesiana y la necesidad de la utilización de un enfoque computacional para superar estas dificultades;\nser capaces de aplicar métodos bayesianos a problemas reales utilizando software específico; e\ninterpretar los resultados del proceso de análisis bayesiano de datos.\n\n\n\nContenidos\n\nUnidad 1: Introducción y Fundamentos de la Estadística Bayesiana\n\nProbabilidad para cuantificar la incertidumbre. Modelos de probabilidad. Regla de Bayes. Inferencia bayesiana. Distribución a priori, función de verosimilitud, distribución a posteriori.\n\nUnidad 2: Inferencia Bayesiana\n\nModelos de distribuciones conjugadas. Modelos de un parámetro. Modelo beta-binomial. Enfoque intuitivo. Distribución a posteriori como compromiso entre la verosimilitud y la distribución a priori. Razonamiento secuencial. Modelo normal-normal. Modelo gamma–Poisson. Modelos de varios parámetros. Modelo normal – normal-gamma-inversa. Modelo Dirichlet–multinomial.\nElección de distribuciones a priori: no informativas (impropias, de Jeffrey) y débilmente informativas. Medidas de resumen de la distribución a posteriori. Intervalos de credibilidad. Distribución predictiva a posteriori. Nociones de teoría de la decisión bayesiana. Riesgo bayesiano. Estimador de Bayes.\n\nUnidad 3: Métodos Computacionales\n\nLimitaciones del enfoque analítico: cálculo de probabilidades y determinación de la distribución a posteriori. Soluciones: análisis de datos simulados y aproximación de grilla. Introducción al cómputo bayesiano. Nociones básicas de métodos de cadenas de Markov – Montecarlo (MCMC). Algoritmo de Metropolis–Hastings. Montecarlo Hamiltoniano. Diagnóstico de métodos MCMC.\nProgramación probabilística. Alternativas. Sintaxis de modelos. Ejemplos. Diagnóstico. Medidas de resumen a partir de las cadenas obtenidas. Visualizaciones.\n\nUnidad 4: Modelos Lineales\n\nModelos lineales. Elección de distribuciones a priori. Regularización. Diagnóstico de modelos. Predicciones basadas en distribuciones de probabilidad. Pruebas predictivas a priori y a posteriori. Densidad predictiva a posteriori logarítmica evaluada punto a punto (lppd). Deviance. Criterios de información: AIC, BIC, WAIC. Validacion cruzada. Sobreajuste y subajuste. Validación cruzada utilizando muestreo por importancia mediante suavizado Pareto (PSIS-CV).\n\nUnidad 5: Modelos Avanzados\n\nRegresión logística. Regresión Poisson. Comparación de grupos. Modelos de variable latente. Formulación gráfica. Análisis de sensibilidad.\nEl enfoque multinivel: modelos jerárquicos. Modelo beta-binomial jerárquico. Shrinkage de parámetros. Variación en el intercepto. Variación en la pendiente. Pooling de estimaciones. Problemas de estimación.",
    "crumbs": [
      "Información",
      "Programa"
    ]
  },
  {
    "objectID": "info/bibliografia.html",
    "href": "info/bibliografia.html",
    "title": "Bibliografía",
    "section": "",
    "text": "Bibliografía principal\n\nJohnson, Ott, y Dogucu (2022) McElreath (2020) Gelman y Hill (2006) Kruschke (2014) Reich y Ghosh (2019)\n\n\n\nGelman, Andrew, y Jennifer Hill. 2006. Data Analysis Using Regression and Multilevel-Hierarchical Models. 1st edition. Cambridge University Press.\n\n\nJohnson, Alicia A., Miles Q. Ott, y Mine Dogucu. 2022. Bayes Rules! An Introduction to Bayesian Modeling. 1st edition. Chapman; Hall/CRC. https://www.bayesrulesbook.com/.\n\n\nKruschke, John. 2014. Doing Bayesian Data Analysis: A Tutorial with R, JAGS, and Stan. 2nd edition. Academic Press.\n\n\nMcElreath, Richard. 2020. Statistical Rethinking: A Bayesian Course with Examples in R and Stan. 2nd edition. Chapman; Hall/CRC.\n\n\nReich, Brian J., y Sujit K. Ghosh. 2019. Bayesian Statistical Methods. 1st edition. Chapman; Hall/CRC.\n\n\n\n\nBibliografía complementaria\n\nGelman et al. (2013), Gelman, Hill, y Vehtari (2021), Downey (2021), Lee y Wagenmakers (2014), Davidson-Pilon (2015), Nicenboim, Schad, y Vasishth (2022), Barr (2021), Carlin y Louis (2008), Hoff (2009), MacKay (2003), Lambert (2018), Murphy (2022), Murphy (2023), Bishop (2006), Martin, Kumar, y Lao (2021), Theoridis (2020), Clyde et al. (2022), Ma, Kording, y Goldreich (2022)\n\n\n\n\n\n\n\n\nBarr, Dale J. 2021. Learning statistical models through simulation in R: An interactive textbook. 1st edition. https://psyteachr.github.io/stat-models-v1/.\n\n\nBishop, Christopher M. 2006. Pattern Recognition and Machine Learning. 1st edition. Springer.\n\n\nCarlin, Bradley P., y Thomas A. Louis. 2008. Bayesian Methods for Data Analysis. 3rd edition. Chapman; Hall/CRC.\n\n\nClyde, Merlise, Mine Çetinkaya-Rundel, Colin Rundel, David Banks, Christine Chai, y Lizzy Huang. 2022. An Introduction to Bayesian Thinking. 1st edition. https://statswithr.github.io/book/.\n\n\nDavidson-Pilon, Cameron. 2015. Bayesian Methods for Hackers: Probabilistic Programming and Bayesian Inference. 1st edition. Addison-Wesley Data; Analytics Series.\n\n\nDowney, Allen B. 2021. Think Bayes: Bayesian Statistics in Python. 2nd edition. O’Reilly Media. http://allendowney.github.io/ThinkBayes2/.\n\n\nGelman, Andrew, John B. Carlin, Hal S. Stern, David B. Dunson, Aki Vehtari, y Donald B. Rubin. 2013. Bayesian Data Analysis. 3rd edition. Chapman; Hall/CRC.\n\n\nGelman, Andrew, Jennifer Hill, y Aki Vehtari. 2021. Regression and Other Stories. 1st edition. Cambridge University Press. https://users.aalto.fi/~ave/ROS.pdf.\n\n\nHoff, Peter D. 2009. A First Course in Bayesian Statistical Methods. 1st edition. Springer.\n\n\nLambert, Ben. 2018. A Student’s Guide to Bayesian Statistics. 1st edition. SAGE Publications Ltd.\n\n\nLee, Michael D., y Eric-Jan Wagenmakers. 2014. Bayesian Cognitive Modeling: A Practical Course. 1st edition. Cambridge University Press.\n\n\nMa, Wei Ji, Konrad P. Kording, y Daniel Goldreich. 2022. Bayesian Models of Perception and Action: An Introduction. 3rd edition. http://www.cns.nyu.edu/malab/bayesianbook.html.\n\n\nMacKay, David J. C. 2003. Information Theory, Inference and Learning Algorithms. 1st edition. Cambridge University Press.\n\n\nMartin, Osvaldo A., Ravin Kumar, y Junpeng Lao. 2021. Bayesian Modeling and Computation in Python. 1st edition. Chapman; Hall/CRC.\n\n\nMurphy, Kevin P. 2022. Probabilistic Machine Learning: An Introduction. 1st edition. The MIT Press. https://probml.ai/.\n\n\nMurphy, Kevin P. 2023. Probabilistic Machine Learning: Advanced Topics. 1st edition. The MIT Press. https://probml.ai/.\n\n\nNicenboim, Bruno, Daniel Schad, y Shravan Vasishth. 2022. An Introduction to Bayesian Data Analysis for Cognitive Science. https://vasishth.github.io/bayescogsci/book/.\n\n\nTheoridis, Sergios. 2020. Machine Learning: A Bayesian and Optimization Perspective. 2nd edition. Academic Press.",
    "crumbs": [
      "Información",
      "Bibliografia"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Estadística Bayesiana",
    "section": "",
    "text": "Licenciatura en Estadística\n   Facultad de Ciencias Económicas y Estadística (UNR)\n   1° Cuatrimestre 2025\n   Mié – 7:00 a 11:00   |   Vie – 7:00 a 9:00"
  },
  {
    "objectID": "index.html#profesores",
    "href": "index.html#profesores",
    "title": "Estadística Bayesiana",
    "section": "Profesores",
    "text": "Profesores\n\n\nNacho Evangelista\n\n   ignacio.evangelista@fcecon.unr.edu.ar\n   Consultas: Lunes 15:00\n\n\n\nTomás Capretto\n\n   tomas.capretto@fcecon.unr.edu.ar\n   Consultas: Martes 16:00"
  },
  {
    "objectID": "recursos/software/02_instalar_rtools.html",
    "href": "recursos/software/02_instalar_rtools.html",
    "title": "Instalar RTools",
    "section": "",
    "text": "RTools es un conjunto esencial de herramientas que incluye compiladores, paquetes y utilidades necesarias para compilar tanto paquetes de R como código fuente en diversos lenguajes de bajo nivel, como C, C++ y Fortran, en computadoras con el sistema operativo Windows.\nEn el contexto de la materia, RTools adquiere una relevancia significativa al posibilitar el uso de Stan en Windows, ya que este requiere la compilación de programas en C++ para obtener muestras del posterior de un modelo.\nEn la misma página donde se seleccionó la opción base para descargar R base, se encuentra el enlace para continuar con la descarga de RTools.\n\n\n\n\n\n\n\n\n\nAl igual que R, RTools también cuenta con diferentes versiones. Es muy importante tener presente que la versión de RTools que se utilice debe corresponderse con la versión de R que se tenga instalada. Si se instaló R 4.3.x, de acuerdo a la guía Instalar R, se debe seleccionar RTools 4.3.\n\n\n\n\n\n\n\n\n\nA continuación, se muestra una página con abundante información sobre RTools. El enlace para realizar la descarga se encuentra en Rtools43 installer.\n\n\n\n\n\n\n\n\n\nCuando finaliza la descarga se tiene que encontrar un archivo como el que se muestra debajo. Nuevamente, es solo cuestión de hacer doble clic sobre el mismo y seleccionar Next e Install hasta finalizar.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPara comprobar si RTools fue instalado en una ruta identificada por R, se puede correr Sys.which(\"make\") en el intérprete de R y esto tiene que devolver una cadena con la ruta donde se encuentra el programa make (que en Windows es provista por RTools).",
    "crumbs": [
      "Recursos",
      "Instalación de software",
      "Instalar RTools"
    ]
  },
  {
    "objectID": "recursos/software/03_instalar_rstudio.html",
    "href": "recursos/software/03_instalar_rstudio.html",
    "title": "Instalar RStudio",
    "section": "",
    "text": "La descarga e instalación de RStudio en Windows es sencilla. Para descargar RStudio hay que ir a la página de descargas de RStudio de Posit https://posit.co/download/rstudio-desktop/ y seleccionar el botón que descarga RStudio para Windows.\n\n\n\n\n\n\n\n\n\nUna vez que se tiene el instalador descargado, hay que ejecutarlo haciendo doble clic y seleccionar Next e Install hasta finalizar.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLa primera vez que se abre, RStudio requiere que se especifique la instalación de R a utilizar. Al menos que se cuente con múltiples instalaciones y se quiera usar una en particular, es recomendable utilizar la opción seleccionada por defecto.",
    "crumbs": [
      "Recursos",
      "Instalación de software",
      "Instalar RStudio"
    ]
  },
  {
    "objectID": "recursos/codigo/index.html",
    "href": "recursos/codigo/index.html",
    "title": "Instalación de Software",
    "section": "",
    "text": "Esta sección contiene scripts de R que son de utilidad para el curso.\n01 - Liberen al Tucán Sam02 - ¡Ostras! ¡Estoy haciendo inferencia bayesiana!03 - ¿Quién domina el posterior?04 - Diferentes observaciones, diferentes posteriors05 - Función de verosimilitud Poisson06 - Propiedades frecuentistas de inferencias bayesianas07 - Los puntos uniformes08 - Te veo en la fotocopiadora09 - Bolas infinitas10 - Aproximación de grilla en 2 dimensiones11 - Metropolis-Hastings en 1 dimensión12 - Metropolis-Hastings en 2 dimensiones13 - Hamiltonian Monte Carlo para normal bivariada14 - Medidas de diagnóstico MCMC15 - Introducción a RStan14 - Regresión lineal con {RStan}",
    "crumbs": [
      "Recursos",
      "Código"
    ]
  },
  {
    "objectID": "recursos/codigo/10_grid_2d.html",
    "href": "recursos/codigo/10_grid_2d.html",
    "title": "10 - Aproximación de grilla en 2 dimensiones",
    "section": "",
    "text": "El siguiente programa muestra como se puede resolver en R el ejercicio Aproximación de grilla en 2 dimensiones de la Práctica 3.\n\nlibrary(dplyr)\nlibrary(ggplot2)\n\nset.seed(121195)\n\n# Generación de datos\nalpha &lt;- 1\nbeta &lt;- -2\nsigma &lt;- 0.8\nn &lt;- 80\nx &lt;- rnorm(n)\ny &lt;- rnorm(n, alpha + beta * x, sigma)\ndf &lt;- data.frame(x = x, y = y)\n\n# Crear grilla para alfa y beta\ngrid_a &lt;- seq(0.5, 1.5, length.out = 50)\ngrid_b &lt;- seq(-2.5, -1.5, length.out = 50)\n\n# Crear todas las combinaciones entre los valores de las dos grillas\ngrid_df &lt;- expand.grid(grid_a, grid_b)\n\n# Utilizar nombres indicativos\nnames(grid_df) &lt;- c(\"a\", \"b\")\n\n# Crear vectores que van a contener los valores de la\n# función de verosimilitud y del posterior\nlikelihood &lt;- numeric(nrow(grid_df))\nposterior &lt;- numeric(nrow(grid_df))\n\n# Calcular la función de verosimilitud en cada punto\nfor (i in seq_along(likelihood)) {\n    likelihood[i] &lt;- prod(dnorm(y, grid_df$a[i] + grid_df$b[i] * x, sigma))\n}\n# Calcular el posterior en cada punto\nposterior &lt;- (\n    likelihood\n    * dnorm(grid_df$a, mean = 0, sd = 1.5) # alpha ~ Normal(0, 1.5)\n    * dnorm(grid_df$b, mean = 0, sd = 2)   # beta ~ Normal(0, 2)\n)\n\n# Escalar el posterior para que sea propio\nposterior &lt;- posterior / sum(posterior)\n\n# Incorporar likelihood y posterior al data frame\ngrid_df$likelihood &lt;- likelihood\ngrid_df$posterior &lt;- posterior\n\n# Graficar con ggplot\nggplot(grid_df, aes(x = a, y = b)) +\n    geom_raster(aes(fill = posterior)) +\n    stat_contour(aes(z = posterior), col = \"white\", bins = 5) +\n    geom_point(x = alpha, y = beta, color = \"black\", fill = \"red\", size = 3, pch = 21) +\n    labs(x = expression(alpha), y = expression(beta)) +\n    viridis::scale_fill_viridis() +\n    guides(fill = guide_colourbar(barheight = unit(4.2, \"in\"))) +\n    scale_x_continuous(expand = c(0, 0)) +\n    scale_y_continuous(expand = c(0, 0))\n\n\n\n\nDistribución a posteriori evaluada en una grilla de 50×50 puntos. El punto indica los valores de \\(\\alpha\\) y \\(\\beta\\) utilizados para generar los datos.\n\n\n\n\n\nposterior_a_df &lt;- grid_df |&gt;\n    group_by(a) |&gt;\n    summarise(p = sum(posterior))\n\nggplot(posterior_a_df) +\n    geom_segment(aes(x = a, xend = a, y = 0, yend = p), color = \"#d1352c\") +\n    geom_point(aes(x = a, y = p), color = \"#d1352c\") +\n    labs(x = expression(alpha), y = expression(\"p(\" ~ alpha ~ \" | y)\")) +\n    theme_bw() +\n    theme(\n        panel.grid.major = element_blank(),\n        panel.grid.minor = element_blank(),\n    )\n\n\n\n\nDistribución marginal de \\(\\alpha\\).\n\n\n\n\n\nposterior_b_df &lt;- grid_df |&gt;\n    group_by(b) |&gt;\n    summarise(p = sum(posterior))\n\nggplot(posterior_b_df) +\n    geom_segment(aes(x = b, xend = b, y = 0, yend = p), color = \"#d1352c\") +\n    geom_point(aes(x = b, y = p), color = \"#d1352c\") +\n    labs(x = expression(beta), y = expression(\"p(\" ~ beta ~ \" | y)\")) +\n    theme_bw() +\n    theme(\n        panel.grid.major = element_blank(),\n        panel.grid.minor = element_blank(),\n    )\n\n\n\n\nDistribución marginal de \\(\\beta\\).\n\n\n\n\n\\(P(\\alpha &gt; 0.95)\\)\n\nsum(posterior_a_df$p[posterior_a_df$a &gt; 0.95])\n\n[1] 0.7292537\n\n\n\\(P(\\beta &lt; -2)\\)\n\nsum(posterior_b_df$p[posterior_b_df$b &lt; -2])\n\n[1] 0.1204088",
    "crumbs": [
      "Recursos",
      "Código",
      "10 - Aproximación de grilla en 2 dimensiones"
    ]
  },
  {
    "objectID": "recursos/codigo/15_intro_rstan.html",
    "href": "recursos/codigo/15_intro_rstan.html",
    "title": "15 - Introducción a RStan",
    "section": "",
    "text": "A continuación se presenta una introducción a Stan, a través de su interfaz con R: {rstan}\nLa programación probabilística es una herramienta para describir modelos estadísticos (probabilísticos) y operar con ellos. Se trata de usar herramientas de programación para realizar inferencias estadísticas.",
    "crumbs": [
      "Recursos",
      "Código",
      "15 - Introducción a RStan"
    ]
  },
  {
    "objectID": "recursos/codigo/15_intro_rstan.html#stan-y-rstan",
    "href": "recursos/codigo/15_intro_rstan.html#stan-y-rstan",
    "title": "15 - Introducción a RStan",
    "section": "Stan y RStan",
    "text": "Stan y RStan\n¿Qué es Stan?\n\nStan es un lenguaje de programación probabilístico imperativo\nEstá desarrollado sobre C++\nLos programas de Stan definen un modelo probabilístico (datos + parámetros) y realizan inferencias sobre él\nOpen source\n\n¿Qué es RStan?\n\nRStan es una interfaz de Stan en R\nPermite compilar y ejecutar modelos de Stan directamente en R\nAdemás, incluye la clase stanfit y funciones para operar con ella",
    "crumbs": [
      "Recursos",
      "Código",
      "15 - Introducción a RStan"
    ]
  },
  {
    "objectID": "recursos/codigo/15_intro_rstan.html#pasos-a-seguir",
    "href": "recursos/codigo/15_intro_rstan.html#pasos-a-seguir",
    "title": "15 - Introducción a RStan",
    "section": "Pasos a seguir",
    "text": "Pasos a seguir\n\nEscribir un programa en Stan (archivo .stan o string en una variable)\nUsando la función rstan::stan_model, generar el código C\\(++\\), compilarlo y generar un objeto stanmodel\nUsando la función rstan::sampling, obtener muestras del posterior dado el stanmodel y los datos. Las muestras quedan en un objeto stanfit\nProcesar el stan_fit y sacar conclusiones\n\nEstructura de un modelo en Stan\n\nmodelo &lt;- \"\ndata {\n\n}\ntransformed data {\n\n}\nparameters {\n\n}\ntransformed parameters {\n\n}\nmodel {\n\n}\ngenerated quantities {\n\n}\"\n\n\nlibrary(rstan)\n\nLoading required package: StanHeaders\n\n\n\nrstan version 2.32.6 (Stan version 2.32.2)\n\n\nFor execution on a local, multicore CPU with excess RAM we recommend calling\noptions(mc.cores = parallel::detectCores()).\nTo avoid recompilation of unchanged Stan programs, we recommend calling\nrstan_options(auto_write = TRUE)\nFor within-chain threading using `reduce_sum()` or `map_rect()` Stan functions,\nchange `threads_per_chain` option:\nrstan_options(threads_per_chain = 1)\n\nlibrary(tidybayes)\nlibrary(ggdist)\nlibrary(ggplot2)\n\n\nN &lt;- 20\ny &lt;- 4\n\nmodel_beta1_stan &lt;- \"\ndata {\n  int N;\n  int Y;\n}\nparameters {\n  real&lt;lower=0, upper=1&gt; pi;\n}\nmodel {\n  pi ~ beta(2,2); // prior\n  Y ~ binomial(N, pi);  // likelihood\n}\"\n\n# No olvidar el final de línea ;\n# Todas las variables tienen que estar declaradas\n\n\nmodel_beta1 &lt;- stan_model(model_code = model_beta1_stan)\n\ndata_list &lt;- list(Y=y, N=N)\n\nmodel_beta1_fit &lt;- sampling(object=model_beta1,\n                            data=data_list,\n                            chains = 3,\n                            iter = 2000,\n                            warmup = 100)\n\n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 5e-06 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.05 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: WARNING: There aren't enough warmup iterations to fit the\nChain 1:          three stages of adaptation as currently configured.\nChain 1:          Reducing each adaptation stage to 15%/75%/10% of\nChain 1:          the given number of warmup iterations:\nChain 1:            init_buffer = 15\nChain 1:            adapt_window = 75\nChain 1:            term_buffer = 10\nChain 1: \nChain 1: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 1: Iteration:  101 / 2000 [  5%]  (Sampling)\nChain 1: Iteration:  300 / 2000 [ 15%]  (Sampling)\nChain 1: Iteration:  500 / 2000 [ 25%]  (Sampling)\nChain 1: Iteration:  700 / 2000 [ 35%]  (Sampling)\nChain 1: Iteration:  900 / 2000 [ 45%]  (Sampling)\nChain 1: Iteration: 1100 / 2000 [ 55%]  (Sampling)\nChain 1: Iteration: 1300 / 2000 [ 65%]  (Sampling)\nChain 1: Iteration: 1500 / 2000 [ 75%]  (Sampling)\nChain 1: Iteration: 1700 / 2000 [ 85%]  (Sampling)\nChain 1: Iteration: 1900 / 2000 [ 95%]  (Sampling)\nChain 1: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 0 seconds (Warm-up)\nChain 1:                0.008 seconds (Sampling)\nChain 1:                0.008 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 1e-06 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.01 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: WARNING: There aren't enough warmup iterations to fit the\nChain 2:          three stages of adaptation as currently configured.\nChain 2:          Reducing each adaptation stage to 15%/75%/10% of\nChain 2:          the given number of warmup iterations:\nChain 2:            init_buffer = 15\nChain 2:            adapt_window = 75\nChain 2:            term_buffer = 10\nChain 2: \nChain 2: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 2: Iteration:  101 / 2000 [  5%]  (Sampling)\nChain 2: Iteration:  300 / 2000 [ 15%]  (Sampling)\nChain 2: Iteration:  500 / 2000 [ 25%]  (Sampling)\nChain 2: Iteration:  700 / 2000 [ 35%]  (Sampling)\nChain 2: Iteration:  900 / 2000 [ 45%]  (Sampling)\nChain 2: Iteration: 1100 / 2000 [ 55%]  (Sampling)\nChain 2: Iteration: 1300 / 2000 [ 65%]  (Sampling)\nChain 2: Iteration: 1500 / 2000 [ 75%]  (Sampling)\nChain 2: Iteration: 1700 / 2000 [ 85%]  (Sampling)\nChain 2: Iteration: 1900 / 2000 [ 95%]  (Sampling)\nChain 2: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 0 seconds (Warm-up)\nChain 2:                0.008 seconds (Sampling)\nChain 2:                0.008 seconds (Total)\nChain 2: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 3).\nChain 3: \nChain 3: Gradient evaluation took 1e-06 seconds\nChain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.01 seconds.\nChain 3: Adjust your expectations accordingly!\nChain 3: \nChain 3: \nChain 3: WARNING: There aren't enough warmup iterations to fit the\nChain 3:          three stages of adaptation as currently configured.\nChain 3:          Reducing each adaptation stage to 15%/75%/10% of\nChain 3:          the given number of warmup iterations:\nChain 3:            init_buffer = 15\nChain 3:            adapt_window = 75\nChain 3:            term_buffer = 10\nChain 3: \nChain 3: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 3: Iteration:  101 / 2000 [  5%]  (Sampling)\nChain 3: Iteration:  300 / 2000 [ 15%]  (Sampling)\nChain 3: Iteration:  500 / 2000 [ 25%]  (Sampling)\nChain 3: Iteration:  700 / 2000 [ 35%]  (Sampling)\nChain 3: Iteration:  900 / 2000 [ 45%]  (Sampling)\nChain 3: Iteration: 1100 / 2000 [ 55%]  (Sampling)\nChain 3: Iteration: 1300 / 2000 [ 65%]  (Sampling)\nChain 3: Iteration: 1500 / 2000 [ 75%]  (Sampling)\nChain 3: Iteration: 1700 / 2000 [ 85%]  (Sampling)\nChain 3: Iteration: 1900 / 2000 [ 95%]  (Sampling)\nChain 3: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 3: \nChain 3:  Elapsed Time: 0 seconds (Warm-up)\nChain 3:                0.007 seconds (Sampling)\nChain 3:                0.007 seconds (Total)\nChain 3: \n\nmodel_beta1_fit\n\nInference for Stan model: anon_model.\n3 chains, each with iter=2000; warmup=100; thin=1; \npost-warmup draws per chain=1900, total post-warmup draws=5700.\n\n       mean se_mean   sd   2.5%    25%    50%    75%  97.5% n_eff Rhat\npi     0.25    0.00 0.09   0.10   0.19   0.24   0.31   0.44  2137    1\nlp__ -14.00    0.01 0.72 -16.07 -14.14 -13.72 -13.55 -13.50  2532    1\n\nSamples were drawn using NUTS(diag_e) at Wed May 21 11:15:38 2025.\nFor each parameter, n_eff is a crude measure of effective sample size,\nand Rhat is the potential scale reduction factor on split chains (at \nconvergence, Rhat=1).\n\n\n\nmodel_beta1_fit@model_pars\n\n[1] \"pi\"   \"lp__\"\n\n\n\nlist_of_draws &lt;- extract(model_beta1_fit,pars=\"pi\")\n\nstr(list_of_draws)\n\nList of 1\n $ pi: num [1:5700(1d)] 0.304 0.11 0.138 0.213 0.248 ...\n  ..- attr(*, \"dimnames\")=List of 1\n  .. ..$ iterations: NULL\n\ndim(list_of_draws$pi)\n\n[1] 5700\n\nhead(list_of_draws$pi)\n\n[1] 0.3037579 0.1097705 0.1376805 0.2129005 0.2484328 0.2081411\n\nmean(list_of_draws$pi&lt;0.6)\n\n[1] 0.9996491\n\n\n\nggplot2::qplot(list_of_draws$pi)\n\n\n\n\nDistribución a posteriori de \\(\\pi\\).\n\n\n\n\n\narray_of_draws &lt;- as.array(model_beta1_fit, pars=\"pi\")\n\n\nbayesplot::mcmc_trace(array_of_draws,pars=\"pi\")\n\n\n\n\nTraceplot de \\(\\pi\\).\n\n\n\n\n\nbayesplot::mcmc_hist_by_chain(array_of_draws,pars=\"pi\")\n\n\n\n\nHistograma para cada cadena de \\(\\pi\\).\n\n\n\n\n\nbayesplot::mcmc_dens_chains(array_of_draws,pars=\"pi\")\n\n\n\n\nEstimación de densidad para cada cadena de \\(\\pi\\).\n\n\n\n\n\ndf_of_draws &lt;- as.data.frame(model_beta1_fit,pars=\"pi\")\n\n#usando el paquete tidybayes + ggdist\nmodel_beta1_fit |&gt;\n  spread_draws(pi) |&gt;\n  ggplot(aes(x=pi)) +\n  stat_histinterval()\n\n\n\n\nDistribución a posteriori de \\(\\pi\\).",
    "crumbs": [
      "Recursos",
      "Código",
      "15 - Introducción a RStan"
    ]
  },
  {
    "objectID": "recursos/codigo/02_ostras.html",
    "href": "recursos/codigo/02_ostras.html",
    "title": "02 - ¡Ostras! ¡Estoy haciendo inferencia bayesiana!",
    "section": "",
    "text": "El siguiente programa muestra diferentes alternativas para obtener la densidad de la distribución a posteriori en el ejercicio ¡Ostras! ¡Estoy haciendo inferencia bayesiana! de la Práctica 1.\nPara graficar la densidad a posteriori de \\(\\lambda\\) basta con ver que: \\[\n\\begin{array}{lr}\np(\\lambda \\mid \\boldsymbol{y})\n  \\propto p(\\lambda) p(\\boldsymbol{y} \\mid \\lambda)\n  \\propto p(\\boldsymbol{y} \\mid \\lambda),\n& \\lambda \\in (0, 100)\n\\end{array}\n\\]\npor ser el prior sobre \\(\\lambda\\) uniforme. Es decir, la forma del posterior queda completamente determinada por la forma de la función de verosimilitud.\nTener presente: \\[\n\\begin{aligned}\np(\\boldsymbol{y} \\mid \\lambda) &= \\prod_{i=1}^n p(y_i \\mid \\lambda) \\\\\np(\\boldsymbol{y} \\mid \\lambda) &= \\prod_{i=1}^n {\\frac{e^{-\\lambda}\\lambda^{y_i}}{y_i!}} \\\\\n& = \\frac{e^{-n\\lambda}\\lambda^{\\sum_i y_i}}{\\prod_{i=1}^{n}{y_i!}} \\\\\n\\log p(\\boldsymbol{y} \\mid \\lambda) &= \\sum_{i=1}^{n} \\log(p(y_i \\mid \\lambda))\n\\end{aligned}\n\\]\n\nlibrary(ggplot2)\n\ncolores &lt;- c(\"#f08533\", \"#3b78b0\", \"#d1352c\")\n\n# Valores observados de \"Y\"\ny &lt;- c(64, 13, 33, 18, 30, 20)\n\n# Altura de la densidad a priori uniforme\nprior &lt;- rep(1 / 100, 400)\n\n# Grilla con los valorse de lambda\nlambda_grid &lt;- seq(0, 100, length.out = 400)\n\nOpción 1: Replicar la fórmula de la función de verosimilitud en una función de R\n\nf_likelihood &lt;- function(lambda, y_vector) {\n  n &lt;- length(y_vector)\n  y_suma &lt;- sum(y_vector)\n  denominador &lt;- prod(factorial(y_vector))\n  (exp(- n * lambda) * lambda ^ y_suma) / denominador\n}\n\nlikelihood &lt;- f_likelihood(lambda_grid, y)\nposterior_ &lt;- prior * likelihood # ¿Es necesario multiplicar por 'prior'?\nposterior_\n\n  [1]  0.000000e+00 8.341450e-313 7.104056e-260 3.488800e-229 1.344921e-207\n  [6] 5.316241e-191 1.468223e-177 2.693061e-166 1.258167e-156 3.562976e-148\n [11] 1.105533e-140 5.733178e-134 6.787109e-128 2.324156e-122 2.767353e-117\n [16] 1.325696e-112 2.873971e-108 3.104267e-104 1.809183e-100  6.081954e-97\n [21]  1.247863e-93  1.639788e-90  1.438521e-87  8.732918e-85  3.785569e-82\n [26]  1.204418e-79  2.881624e-77  5.297329e-75  7.627146e-73  8.749872e-71\n [31]  8.122076e-69  6.185727e-67  3.914092e-65  2.081334e-63  9.397953e-62\n [36]  3.637672e-60  1.217538e-58  3.551973e-57  9.098475e-56  2.060222e-54\n [41]  4.149713e-53  7.478107e-52  1.212173e-50  1.776239e-49  2.363842e-48\n [46]  2.869426e-47  3.189974e-46  3.260149e-45  3.073864e-44  2.682700e-43\n [51]  2.173984e-42  1.640637e-41  1.156224e-40  7.629166e-40  4.724840e-39\n [56]  2.752848e-38  1.512230e-37  7.848722e-37  3.856409e-36  1.797148e-35\n [61]  7.957451e-35  3.353418e-34  1.347173e-33  5.167090e-33  1.894912e-32\n [66]  6.653603e-32  2.239884e-31  7.238448e-31  2.248240e-30  6.719236e-30\n [71]  1.934454e-29  5.370553e-29  1.439272e-28  3.726947e-28  9.333742e-28\n [76]  2.262769e-27  5.314719e-27  1.210418e-26  2.675170e-26  5.741957e-26\n [81]  1.197791e-25  2.430089e-25  4.798220e-25  9.226574e-25  1.728934e-24\n [86]  3.159075e-24  5.631721e-24  9.800977e-24  1.666033e-23  2.767673e-23\n [91]  4.495581e-23  7.143525e-23  1.110973e-22  1.691846e-22  2.523943e-22\n [96]  3.690191e-22  5.289958e-22  7.438185e-22  1.026279e-21  1.389998e-21\n[101]  1.848737e-21  2.415490e-21  3.101393e-21  3.914514e-21  4.858608e-21\n[106]  5.931961e-21  7.126440e-21  8.426877e-21  9.810887e-21  1.124921e-20\n[111]  1.270659e-20  1.414317e-20  1.551637e-20  1.678304e-20  1.790181e-20\n[116]  1.883539e-20  1.955269e-20  2.003058e-20  2.025504e-20  2.022183e-20\n[121]  1.993649e-20  1.941370e-20  1.867617e-20  1.775307e-20  1.667820e-20\n[126]  1.548806e-20  1.421993e-20  1.291006e-20  1.159221e-20  1.029641e-20\n[131]  9.048144e-21  7.867903e-21  6.771014e-21  5.767820e-21  4.864064e-21\n[136]  4.061454e-21  3.358320e-21  2.750318e-21  2.231130e-21  1.793113e-21\n[141]  1.427871e-21  1.126747e-21  8.812014e-22  6.831092e-22  5.249581e-22\n[146]  3.999734e-22  3.021764e-22  2.263932e-22  1.682243e-22  1.239895e-22\n[151]  9.065646e-23  6.576228e-23  4.733305e-23  3.380696e-23  2.396323e-23\n[156]  1.685873e-23  1.177299e-23  8.161536e-24  5.617212e-24  3.838602e-24\n[161]  2.604758e-24  1.755262e-24  1.174720e-24  7.808739e-25  5.156059e-25\n[166]  3.382054e-25  2.203960e-25  1.426992e-25  9.180523e-26  5.869137e-26\n[171]  3.728846e-26  2.354505e-26  1.477683e-26  9.218273e-27  5.716563e-27\n[176]  3.524253e-27  2.160105e-27  1.316397e-27  7.976853e-28  4.806581e-28\n[181]  2.880237e-28  1.716462e-28  1.017374e-28  5.997816e-29  3.517201e-29\n[186]  2.051719e-29  1.190639e-29  6.873967e-30  3.948428e-30  2.256596e-30\n[191]  1.283274e-30  7.261782e-31  4.089292e-31  2.291691e-31  1.278170e-31\n[196]  7.095239e-32  3.920239e-32  2.155985e-32  1.180285e-32  6.432150e-33\n[201]  3.489582e-33  1.884768e-33  1.013513e-33  5.426324e-34  2.892720e-34\n[206]  1.535499e-34  8.116204e-35  4.272035e-35  2.239301e-35  1.168970e-35\n[211]  6.077494e-36  3.146971e-36  1.623022e-36  8.337508e-37  4.266228e-37\n[216]  2.174523e-37           Inf           Inf           Inf           Inf\n[221]           Inf           Inf           Inf           Inf           Inf\n[226]           Inf           Inf           Inf           Inf           Inf\n[231]           Inf           Inf           Inf           Inf           Inf\n[236]           Inf           Inf           Inf           Inf           Inf\n[241]           Inf           Inf           Inf           Inf           Inf\n[246]           Inf           Inf           Inf           Inf           Inf\n[251]           Inf           Inf           Inf           Inf           Inf\n[256]           Inf           Inf           Inf           Inf           Inf\n[261]           Inf           Inf           Inf           Inf           Inf\n[266]           Inf           Inf           Inf           Inf           Inf\n[271]           Inf           Inf           Inf           Inf           Inf\n[276]           Inf           Inf           Inf           Inf           Inf\n[281]           Inf           Inf           Inf           Inf           Inf\n[286]           Inf           Inf           Inf           Inf           Inf\n[291]           Inf           Inf           Inf           Inf           Inf\n[296]           Inf           Inf           Inf           Inf           Inf\n[301]           Inf           Inf           Inf           Inf           Inf\n[306]           Inf           Inf           Inf           Inf           Inf\n[311]           Inf           Inf           Inf           Inf           Inf\n[316]           Inf           Inf           Inf           Inf           Inf\n[321]           Inf           Inf           Inf           Inf           Inf\n[326]           Inf           Inf           Inf           Inf           Inf\n[331]           Inf           Inf           Inf           Inf           Inf\n[336]           Inf           Inf           Inf           Inf           Inf\n[341]           Inf           Inf           Inf           Inf           Inf\n[346]           Inf           Inf           Inf           Inf           Inf\n[351]           Inf           Inf           Inf           Inf           Inf\n[356]           Inf           Inf           Inf           Inf           Inf\n[361]           Inf           Inf           Inf           Inf           Inf\n[366]           Inf           Inf           Inf           Inf           Inf\n[371]           Inf           Inf           Inf           Inf           Inf\n[376]           Inf           Inf           Inf           Inf           Inf\n[381]           Inf           Inf           Inf           Inf           Inf\n[386]           Inf           Inf           Inf           Inf           Inf\n[391]           Inf           Inf           Inf           Inf           Inf\n[396]           Inf           Inf           Inf           Inf           Inf\n\n\nLa densidad a posteriori no normalizada resulta Inf en algunos casos. Esto indica que el computo no es estable. Veamos otras alternativas.\nOpción 2: Utilizar la funcion dpois de R para evaluar la función de masa de probabilidad en cada observación y luego obtener la función de verosimilitud multiplicando estos resultados.\n\nf_likelihood &lt;- function(lambda, y_vector) {\n  output &lt;- numeric(length(lambda))\n  for (i in seq_along(output)) {\n    output[i] &lt;- prod(dpois(y_vector, lambda[i]))\n  }\n  output\n}\n\nlikelihood &lt;- f_likelihood(lambda_grid, y)\nposterior_ &lt;- prior * likelihood\narea &lt;- integrate(f_likelihood, lower = 0, upper = 100, y_vector = y)$value\nposterior &lt;- posterior_ / area\n\ndf &lt;- data.frame(\n  grupo = factor(\n    rep(c(\"prior\", \"likelihood\", \"posterior\"), each = 400),\n    levels = c(\"prior\", \"likelihood\", \"posterior\"),\n    ordered = TRUE\n  ),\n  lambda = rep(lambda_grid, 3),\n  valor = c(prior, likelihood, posterior)\n)\n\nggplot(df) +\n  geom_line(aes(x = lambda, y = valor, color = grupo), linewidth = 1) +\n  scale_color_manual(values = colores) +\n  labs(x = expression(lambda), y = NULL) +\n  facet_wrap(~ grupo, scales = \"free_y\") +\n  theme_bw() +\n    theme(\n        panel.grid.major = element_blank(),\n        panel.grid.minor = element_blank(),\n        legend.position = \"none\"\n    )\n\n\n\n\n\n\n\n\nOpción 3: Utilizar la funcion dpois de R, pero hacer cuentas en escalas logaritmica.\n\nf_likelihood &lt;- function(lambda, y_vector) {\n  output &lt;- numeric(length(lambda))\n  for (i in seq_along(output)) {\n    # El producto se transforma en suma\n    output[i] &lt;- sum(dpois(y_vector, lambda[i], log = TRUE))\n  }\n  # Volver a la escala original\n  exp(output)\n}\n\nlikelihood &lt;- f_likelihood(lambda_grid, y)\nposterior_ &lt;- prior * likelihood\narea &lt;- integrate(f_likelihood, lower = 0, upper = 100, y_vector = y)$value\nposterior &lt;- posterior_ / area\n\ndf &lt;- data.frame(\n  grupo = factor(\n    rep(c(\"prior\", \"likelihood\", \"posterior\"), each = 400),\n    levels = c(\"prior\", \"likelihood\", \"posterior\"),\n    ordered = TRUE\n  ),\n  lambda = rep(lambda_grid, 3),\n  valor = c(prior, likelihood, posterior)\n)\n\nggplot(df) +\n  geom_line(aes(x = lambda, y = valor, color = grupo), linewidth = 1) +\n  scale_color_manual(values = colores) +\n  labs(x = expression(lambda), y = NULL) +\n  facet_wrap(~ grupo, scales = \"free_y\") +\n  theme_bw() +\n    theme(\n        panel.grid.major = element_blank(),\n        panel.grid.minor = element_blank(),\n        legend.position = \"none\"\n    )\n\n\n\n\n\n\n\n\nPregunta\n\n¿Qué explica que los valores del eje vertical para el likelihood sean tan distintos del resto?",
    "crumbs": [
      "Recursos",
      "Código",
      "02 - ¡Ostras! ¡Estoy haciendo inferencia bayesiana!"
    ]
  },
  {
    "objectID": "recursos/codigo/14_medidas_diagnostico_mcmc.html",
    "href": "recursos/codigo/14_medidas_diagnostico_mcmc.html",
    "title": "14 - Medidas de diagnóstico MCMC",
    "section": "",
    "text": "Este recurso muestra cómo evaluar la convergencia y eficiencia de simulaciones MCMC con múltiples cadenas utilizando el algoritmo de Metropolis-Hastings. Se usa un modelo con verosimilitud normal y varianza conocida, donde el objetivo es obtener el posterior de la media. Se construyen traceplots, se calcula el estadístico \\(\\hat{R}\\) de Gelman-Rubin para diagnosticar la convergencia, y se calcula el tamaño efectivo de muestra \\(N_{\\text{eff}}\\) para evaluar la la cantidad de información contenida en las muestras.\nLa siguiente celda de código contiene funciones utilitarias.\nCódigo\nlibrary(dplyr)\nlibrary(ggplot2)\n\ngraficar_trazas &lt;- function(df_muestras) {\n    plt &lt;- ggplot(df_muestras) +\n        geom_line(aes(x = paso, y = valor, group = cadena, color = cadena)) +\n        scale_color_manual(values = c(\"#107591\", \"#00c0bf\", \"#f69a48\", \"#fdcd49\")) +\n        labs(x = \"Paso\", y = \"x\") +\n        theme_bw()\n        theme(\n            panel.grid.minor = element_blank()\n        )\n\n    return(plt)\n}\n\ncrear_df_muestras &lt;- function(muestras) {\n    # Muestras es una lista de vectores.\n    # Hay tantos vectores como cadenas.\n\n    n_cadenas &lt;- length(muestras)\n    n_muestras &lt;- length(muestras[[1]])\n\n    df &lt;- data.frame(\n        cadena = as.factor(rep(seq_len(n_cadenas), each = n_muestras)),\n        paso = rep(seq_len(n_muestras), n_cadenas),\n        valor = unlist(muestras)\n    )\n\n    return(df)\n}\n\nmetropolis_hastings &lt;- function(n, x, p, sigma) {\n    # Algortimo de Metropolis Hastings en una dimensión con propuesta normal.\n    #\n    # Parámetros\n    #  ------------------------------------------------------------------------\n    # | n        | Cantidad de muestras a obtener.                             |\n    # | x        | Posición inicial del algoritmo.                             |\n    # | p        | Función de densidad objetivo, normalizada o sin normalizar. |\n    # | sigma    | Desvío estándar de la distribución de propuesta normal.     |\n    #  ------------------------------------------------------------------------\n\n    muestras &lt;- numeric(n)\n    muestras[1] &lt;- x\n\n    for (i in seq_len(n - 1)) {\n        x_actual &lt;- muestras[i]\n        x_propuesto &lt;- rnorm(1, mean = x_actual, sd = sigma)\n\n        p_actual &lt;- p(x_actual)\n        p_propuesto &lt;- p(x_propuesto)\n\n        if (runif(1) &lt; (p_propuesto / p_actual)) {\n            muestras[i + 1] &lt;- x_propuesto\n        } else {\n            muestras[i + 1] &lt;- x_actual\n        }\n    }\n\n    return(muestras)\n}\n\nset.seed(1211)",
    "crumbs": [
      "Recursos",
      "Código",
      "14 - Medidas de diagnóstico MCMC"
    ]
  },
  {
    "objectID": "recursos/codigo/14_medidas_diagnostico_mcmc.html#diagnósticos",
    "href": "recursos/codigo/14_medidas_diagnostico_mcmc.html#diagnósticos",
    "title": "14 - Medidas de diagnóstico MCMC",
    "section": "Diagnósticos",
    "text": "Diagnósticos\n\nDiagnóstico \\(\\hat{R}\\) de Gelman-Rubin\nEl valor de \\(\\hat{R}\\) se utiliza para evaluar la convergencia de las cadenas de MCMC. Un valor cercano a 1 sugiere que las cadenas se han mezclado adecuadamente y convergido a la misma distribución objetivo. En cambio, valores mayores a 1 (en la práctica, 1.01 o más) indican que las cadenas aún no han convergido y/o no se han mezclado correctamente.\nEl estadístico de Gelman-Rubin se define: \\[\n\\hat{R} = \\sqrt{\\frac{\\frac{S-1}{S} W + \\frac{1}{S}  B}{W}}\n\\]\ndonde: \\[\n\\begin{aligned}\nB &= \\frac{S}{M - 1} \\sum_{m = 1}^{M} \\left(\\bar{\\theta}_{\\cdot m} - \\bar{\\theta}_{\\cdot \\cdot}\\right)^2 \\\\\nW &= \\frac{1}{M} \\sum_{m = 1}^{M} s^2_m\n\\end{aligned}\n\\]\ncon: \\[\n\\begin{aligned}\n\\bar{\\theta}_{\\cdot m} &= \\frac{1}{S} \\sum_{s=1}^S \\theta_{sm}  && \\text{Promedio de la cadena } m \\\\\n\\bar{\\theta}_{\\cdot \\cdot} &= \\frac{1}{M} \\sum_{m=1}^M \\bar{\\theta}_{\\cdot m}  && \\text{Promedio global} \\\\\ns^2_m &= \\frac{1}{S-1} \\sum_{s=1}^S \\left(\\theta_{sm} - \\bar{\\theta}_{\\cdot m}\\right)^2 && \\text{Varianza de la cadena } m\n\\end{aligned}\n\\]\nAdemás:\n\n\\(S\\) es la cantidad de muestras en cada cadena.\n\\(M\\) es la cantidad de cadenas.\n\\(\\theta_{sm}\\) es un valor de \\(\\theta\\) muestreado en el paso \\(s\\) de la cadena \\(m\\).\n\nEn R, definimos:\n\ncalcular_W &lt;- function(muestras) {\n    # muestras: matriz de dimensión (n_muestras, n_cadenas)\n    return(mean(apply(muestras, 2, var)))\n}\n\ncalcular_B &lt;- function(muestras) {\n    # muestras: matriz de dimensión (n_muestras, n_cadenas)\n    S &lt;- nrow(muestras) # cantidad de muestras\n    M &lt;- ncol(muestras) # cantidad de cadenas\n    media_cadenas &lt;- apply(muestras, 2, mean)\n    media_global &lt;- mean(media_cadenas)\n    diferencias_cuadrado &lt;- (media_cadenas - media_global) ^ 2\n    return(S / (M - 1) * sum(diferencias_cuadrado))\n}\n\ncalcular_R &lt;- function(muestras) {\n    # muestras: matriz de dimensión (n_muestras, n_cadenas)\n    S &lt;- nrow(muestras)\n    W &lt;- calcular_W(muestras)\n    B &lt;- calcular_B(muestras)\n    numerador &lt;- ((S - 1) / S) * W + B / S\n    return(sqrt(numerador / W))\n}\n\n\n\nTamaño efectivo de muestra \\(N_\\text{eff}\\)\nEl número efectivo de muestras, \\(N_{\\text{eff}}\\), estima a cuántas muestras independientes equivalen las muestras autocorrelacionadas obtenidas mediante algoritmos de MCMC, en términos de la incertidumbre de las estimaciones.\nDebido a la autocorrelación entre muestras, este valor suele ser menor que la cantidad total de muestras generadas.\nUn \\(N_{\\text{eff}}\\) alto indica baja autocorrelación y, por lo tanto, una incertidumbre similar a la que se obtendría con muestras independientes. En cambio, un valor bajo refleja alta autocorrelación y mayor incertidumbre respecto de la que se tendría con un muestreo independiente.\nEs una medida complementaria a \\(\\hat{R}\\).\nSe define: \\[\nN_{\\text{eff}} = \\frac{S}{1 + 2 \\sum_{k=1}^\\infty \\text{ACF}(k)}\n\\]\ndonde \\(\\text{ACF}(k)\\) es la función de autocorrelación para un rezago \\(k\\).\nNotar que la suma infinita en el denominador comienza en \\(k = 1\\) (y no en \\(k = 0\\), donde \\(\\text{ACF}(0) = 1\\)).\nEn la práctica, una regla común para truncar la suma de autocorrelaciones es detenerla en el primer valor de \\(k\\) tal que \\(\\text{ACF}(k) &lt; 0.05\\).\n\ncalcular_n_eff &lt;- function(muestras) {\n    # muestras: vector de longitud 'n_muestras'\n    autocorrelacion &lt;- acf(muestras, lag = Inf, plot = FALSE)$acf\n    limite &lt;- which(autocorrelacion &lt; 0.05)[1]\n    numerador &lt;- length(muestras)\n    denominador &lt;- 1 + 2 * sum(autocorrelacion[2:limite])\n    return(numerador / denominador)\n}",
    "crumbs": [
      "Recursos",
      "Código",
      "14 - Medidas de diagnóstico MCMC"
    ]
  },
  {
    "objectID": "recursos/codigo/14_medidas_diagnostico_mcmc.html#modelo",
    "href": "recursos/codigo/14_medidas_diagnostico_mcmc.html#modelo",
    "title": "14 - Medidas de diagnóstico MCMC",
    "section": "Modelo",
    "text": "Modelo\nSe plantea un modelo normal de media desconocida y varianza conocida. Para \\(\\mu\\), se utiliza un prior normal. \\[\n\\begin{aligned}\nY_i &\\sim \\text{Normal}(\\mu, \\sigma = 1.2) \\\\\n\\mu &\\sim \\text{Normal}(6, 1.8^2)\n\\end{aligned}\n\\]\nSe cuenta con un vector de observaciones \\(\\boldsymbol{y}\\):\n\ny &lt;- c(\n    5.8, 7.58, 8.55, 4.44, 7.76, 7.86, 6.56, 6.59, 6.57, 6.18,\n    6.68, 6.05, 6.32, 7.33, 8.4, 7.12, 6.64, 6.16, 6.25, 10.15\n)\n\nY la densidad a posteriori sin normalizar es:\n\\[\n\\begin{aligned}\np(\\mu \\mid \\boldsymbol{y}) &\\propto p_\\boldsymbol{y}(\\boldsymbol{y} \\mid \\mu, \\sigma = 1.2) p_\\mu(\\mu) \\\\\n&\\propto \\prod_{i=1}^{N} p_y(y_i \\mid \\mu, \\sigma = 1.2) p_\\mu(\\mu)\n\\end{aligned}\n\\]\ndonde tanto \\(p_y\\) como \\(p_\\mu\\) son densidades normales.\nEn R:\n\n# Función que depende de parámetros y datos\nposterior_no_normalizado &lt;- function(mu, y) {\n    exp(sum(dnorm(y, mean = mu, sd = 1.2, log = FALSE)) + dnorm(mu, mean = 6, sd = 1.8, log = TRUE))\n}\n\n# Función que depende de parámetros\np &lt;- purrr::partial(posterior_no_normalizado, y = y)\n\n# Se comprueba que ambas calculan la misma cantidad\ncat(posterior_no_normalizado(4, y) == p(4))\n\nTRUE",
    "crumbs": [
      "Recursos",
      "Código",
      "14 - Medidas de diagnóstico MCMC"
    ]
  },
  {
    "objectID": "recursos/codigo/14_medidas_diagnostico_mcmc.html#aplicación",
    "href": "recursos/codigo/14_medidas_diagnostico_mcmc.html#aplicación",
    "title": "14 - Medidas de diagnóstico MCMC",
    "section": "Aplicación",
    "text": "Aplicación\nEn todos los casos se utilizan 4 cadenas de 5000 muestras cada una. En cada caso, se utilizan diferentes desvíos estándar para la distribución de propuesta normal algoritmo de Metropolis-Hastings. Además, se visualizan los traceplots y se calculan los estadísticos \\(\\hat{R}\\) y \\(N_{\\text{eff}}\\).\n\nCaso 1: \\(\\sigma=0.05\\)\n\nmuestras &lt;- list()\nn_muestras &lt;- 5000\nn_cadenas &lt;- 4\nsigma &lt;- 0.05\n\n# Se llama a la función `metropolis_hastings` y se guardan los resultados en una lista.\n# La posición inicial de las cadenas está dada por un valor al azar de una normal estándar.\nfor (j in seq_len(n_cadenas)) {\n    muestras[[j]] &lt;- metropolis_hastings(\n        n = n_muestras,\n        x = rnorm(1),\n        p = p,\n        sigma = sigma\n    )\n}\n\nSe crea un data.frame que contiene las muestras para las diferentes cadenas. Lo necesitamos para crear el traceplot.\n\ndf_muestras &lt;- crear_df_muestras(muestras)\nhead(df_muestras)\ntail(df_muestras)\n\n  cadena paso      valor\n1      1    1 -0.3219005\n2      1    2 -0.2798418\n3      1    3 -0.2761195\n4      1    4 -0.3383172\n5      1    5 -0.3503256\n6      1    6 -0.3807455\n      cadena paso    valor\n19995      4 4995 7.351506\n19996      4 4996 7.410347\n19997      4 4997 7.451080\n19998      4 4998 7.372025\n19999      4 4999 7.359707\n20000      4 5000 7.366110\n\n\n\ngraficar_trazas(df_muestras)\n\n\n\n\n\n\n\n\nAhora, se ponen las muestras de todas las cadenas en una matriz de dimensión (n_muestras, n_cadenas).\n\nmuestras_matriz &lt;- matrix(df_muestras$valor, ncol = n_cadenas, byrow = FALSE)\nhead(muestras_matriz)\n\n           [,1]     [,2]     [,3]      [,4]\n[1,] -0.3219005 1.987686 1.570537 0.9665045\n[2,] -0.2798418 1.986047 1.570537 0.9670591\n[3,] -0.2761195 1.995807 1.630264 0.9506234\n[4,] -0.3383172 1.980003 1.743239 1.0431927\n[5,] -0.3503256 2.013825 1.755004 0.9951937\n[6,] -0.3807455 2.059922 1.721614 0.9276723\n\n\n\nR_hat &lt;- calcular_R(muestras_matriz)\nn_effs &lt;- apply(muestras_matriz, 2, calcular_n_eff) # Para cada columna, calcular N_eff\nn_eff &lt;- sum(n_effs) # Sumar los N_eff de todas las cadenas\n\ncat(\n    \"R_hat: \", R_hat, \"\\n\",\n    \"N_eff por cadena: \", paste(round(n_effs, 4), collapse = \", \"), \"\\n\",\n    \"N_eff: \", round(n_eff, 4), \"\\n\",\n    sep = \"\"\n)\n\nR_hat: 1.276305\nN_eff por cadena: 2.956, 3.2387, 4.9391, 4.1391\nN_eff: 15.273\n\n\nEn el traceplot, las cadenas se mueven lentamente, lo que indica una alta autocorrelación. Además, no se mezclan y tampoco parecen converger a ninguna distribución objetivo. El valor de \\(\\hat{R}\\) es considerablemente mayor a 1, lo que también señala falta de convergencia y resulta consistente con lo observado en el traceplot. Además, el tamaño efectivo de muestra en cada cadena es muy bajo, y en consecuencia, el tamaño efectivo de muestra total también lo es, lo cual es coherente con cadenas que presentan una autocorrelación muy alta.\nAsí, se concluye que no se pueden utilizar las muestras para realizar inferencia. Decidimos incrementar el desvío estándar de la distribución de propuesta.\n\n\nCaso 2: \\(\\sigma=0.15\\)\n\nmuestras &lt;- list()\nn_muestras &lt;- 5000\nn_cadenas &lt;- 4\nsigma &lt;- 0.15\n\nfor (j in seq_len(n_cadenas)) {\n    muestras[[j]] &lt;- metropolis_hastings(\n        n = n_muestras,\n        x = rnorm(1),\n        p = p,\n        sigma = sigma\n    )\n}\n\ndf_muestras &lt;- crear_df_muestras(muestras)\n\n\ngraficar_trazas(df_muestras)\n\n\n\n\n\n\n\n\n\nmuestras_matriz &lt;- matrix(df_muestras$valor, ncol = n_cadenas, byrow = FALSE)\nR_hat &lt;- calcular_R(muestras_matriz)\nn_effs &lt;- apply(muestras_matriz, 2, calcular_n_eff)\nn_eff &lt;- sum(n_effs)\n\ncat(\n    \"R_hat: \", R_hat, \"\\n\",\n    \"N_eff por cadena: \", paste(round(n_effs, 4), collapse = \", \"), \"\\n\",\n    \"N_eff: \", round(n_eff, 4), \"\\n\",\n    sep = \"\"\n)\n\nR_hat: 1.00457\nN_eff por cadena: 14.3766, 13.8293, 21.2161, 20.2053\nN_eff: 69.6272\n\n\nEn este caso, las cadenas parecen converger a una distribución objetivo común. Sin embargo, el movimiento todavía resulta lento, lo que es consistente con cadenas que presentan alta autocorrelación. El valor de \\(\\hat{R}\\) ahora es prácticamente igual a 1, en línea con la mezcla que se observa. A pesar de esto, el tamaño efectivo de muestra sigue siendo considerablemente bajo: las 20 000 muestras obtenidas tienen un poder de estimación equivalente al de aproximadamente 70 muestras independientes.\n\n\nCaso 3: \\(\\sigma=1\\)\n\nmuestras &lt;- list()\nn_muestras &lt;- 5000\nn_cadenas &lt;- 4\nsigma &lt;- 1\n\nfor (j in seq_len(n_cadenas)) {\n    muestras[[j]] &lt;- metropolis_hastings(\n        n = n_muestras,\n        x = rnorm(1),\n        p = p,\n        sigma = sigma\n    )\n}\n\ndf_muestras &lt;- crear_df_muestras(muestras)\n\n\ngraficar_trazas(df_muestras)\n\n\n\n\n\n\n\n\n\nmuestras_matriz &lt;- matrix(df_muestras$valor, ncol = n_cadenas, byrow = FALSE)\nR_hat &lt;- calcular_R(muestras_matriz)\nn_effs &lt;- apply(muestras_matriz, 2, calcular_n_eff)\nn_eff &lt;- sum(n_effs)\n\ncat(\n    \"R_hat: \", R_hat, \"\\n\",\n    \"N_eff por cadena: \", paste(round(n_effs, 4), collapse = \", \"), \"\\n\",\n    \"N_eff: \", round(n_eff, 4), \"\\n\",\n    sep = \"\"\n)\n\nR_hat: 1.000995\nN_eff por cadena: 727.1356, 243.6087, 361.1435, 678.9349\nN_eff: 2010.823\n\n\nFinalmente, las cadenas convergen, se mezclan y parecen generar muestras representativas de la distribución objetivo. Esto se refleja en el traceplot, donde el patrón no se distingue del de un ruido blanco una vez que las cadenas convergen; en el valor de \\(\\hat{R}\\), que es prácticamente igual a 1; y en el tamaño efectivo de muestra, que ahora alcanza aproximadamente 2000.\nAhora sí, se tiene confianza en que se pueden realizar inferencias válidas sobre \\(\\mu\\) a partir de las muestras obtenidas mediante Metropolis-Hastings.\n\n\nCaso 4: \\(\\sigma=1\\) con descarte de muestras\nA continuación se muestra cómo descartar un conjunto inicial de muestras. Estas corresponden al período en que la cadena aún se encuentra convergiendo. Dado que esas muestras no provienen de la distribución objetivo, es recomendable eliminarlas. En este caso, se descartan las primeras 1000 muestras de cada cadena.\n\nmuestras &lt;- list()\nn_muestras &lt;- 5000\nn_cadenas &lt;- 4\nsigma &lt;- 1\n\nfor (j in seq_len(n_cadenas)) {\n    muestras[[j]] &lt;- metropolis_hastings(\n        n = n_muestras,\n        x = rnorm(1),\n        p = p,\n        sigma = sigma\n    )\n}\n\ndf_muestras &lt;- crear_df_muestras(muestras) |&gt; filter(paso &gt; 1000)\n\n\ngraficar_trazas(df_muestras)\n\n\n\n\n\n\n\n\n\nmuestras_matriz &lt;- matrix(df_muestras$valor, ncol = n_cadenas, byrow = FALSE)\nR_hat &lt;- calcular_R(muestras_matriz)\nn_effs &lt;- apply(muestras_matriz, 2, calcular_n_eff)\nn_eff &lt;- sum(n_effs)\n\ncat(\n    \"R_hat: \", R_hat, \"\\n\",\n    \"N_eff por cadena: \", paste(round(n_effs, 4), collapse = \", \"), \"\\n\",\n    \"N_eff: \", round(n_eff, 4), \"\\n\",\n    sep = \"\"\n)\n\nR_hat: 1.000537\nN_eff por cadena: 644.2362, 569.0011, 545.1367, 709.0444\nN_eff: 2467.418\n\n\nTodos los diagnósticos son tan buenos como los obtenidos en el punto anterior, lo que indica que es seguro realizar inferencias sobre \\(\\mu\\) a partir de las muestras obtenidas. Además, vale la pena destacar que el tamaño efectivo de muestra ha aumentado, lo cual se debe al descarte de las muestras iniciales, que presentan mayor autocorrelación.\nPara finalizar, se visualiza la distribución a posteriori de \\(\\mu\\) mediante un histograma, y se lo resume utilizando la media y un intervalo de credibilidad central del 94%.\n\nggplot(df_muestras) +\n    geom_histogram(aes(x = valor), bins = 30, fill = \"#f08533\", alpha = 0.8) +\n    labs(x = expression(mu), y = NULL) +\n    theme_bw() +\n    theme(\n        panel.grid.minor = element_blank(),\n        axis.ticks.y = element_blank(),\n        axis.text.y = element_blank()\n    )\n\n\n\n\n\n\n\nmedia &lt;- mean(df_muestras$valor)\nic &lt;- quantile(df_muestras$valor, c(0.03, 0.97))\ncat(\n    \"Media: \", media, \"\\n\",\n    \"94% IC: \", paste(round(ic, 4), collapse = \" - \"), \"\\n\",\n    sep = \"\"\n)\n\nMedia: 6.668306\n94% IC: 5.2196 - 8.1212",
    "crumbs": [
      "Recursos",
      "Código",
      "14 - Medidas de diagnóstico MCMC"
    ]
  },
  {
    "objectID": "recursos/codigo/14_medidas_diagnostico_mcmc.html#comentarios",
    "href": "recursos/codigo/14_medidas_diagnostico_mcmc.html#comentarios",
    "title": "14 - Medidas de diagnóstico MCMC",
    "section": "Comentarios",
    "text": "Comentarios\nNo existe una forma única de “calcular” (en realidad, estimar) los estadísticos \\(\\hat{R}\\) y \\(N_{\\text{eff}}\\). Para más información se puede consultar la bibliografía de la materia, o la documentación de Stan sobre estos temas.",
    "crumbs": [
      "Recursos",
      "Código",
      "14 - Medidas de diagnóstico MCMC"
    ]
  },
  {
    "objectID": "recursos/codigo/06_cobertura_ic.html",
    "href": "recursos/codigo/06_cobertura_ic.html",
    "title": "06 - Propiedades frecuentistas de inferencias bayesianas",
    "section": "",
    "text": "El siguiente programa sirve para responder al ejercicio Propiedades frecuentistas de inferencias bayesianas de la Práctica 2.\n\nlibrary(ggplot2)\n\nset.seed(1234)\n\n# Definir cantidad de repeticiones\nreps_n &lt;- 10000\n\n# Definir valores de 'n'\nn_vector &lt;- c(1, 5, 10, 25)\n\n# Definir valores de 'theta'\ntheta_vector &lt;- seq(0.05, 0.5, by = 0.05)\n\n# Definir hiperparámetros del prior\na_prior &lt;- 0.5\nb_prior &lt;- 0.5\n\n# Crear matriz para almacenar las coberturas empíricas\ncoberturas &lt;- matrix(\n    nrow = length(n_vector),\n    ncol = length(theta_vector)\n)\n\nfor (i in seq_along(n_vector)) {\n    for (j in seq_along(theta_vector)) {\n        n &lt;- n_vector[i]\n        theta &lt;- theta_vector[j]\n        y_rvs &lt;- rbinom(reps_n, n, theta)\n        a_posterior &lt;- a_prior + y_rvs\n        b_posterior &lt;- b_prior + (n - y_rvs)\n        ic_lower &lt;- qbeta(0.025, a_posterior, b_posterior)\n        ic_upper &lt;- qbeta(0.975, a_posterior, b_posterior)\n        cobertura &lt;- mean(theta &gt; ic_lower & theta &lt; ic_upper)\n        cat(\"n=\", n, \"theta=\", theta, \"cobertura=\", cobertura, \"\\n\")\n        coberturas[i, j] &lt;- cobertura\n    }\n}\n\nn= 1 theta= 0.05 cobertura= 0.9523 \nn= 1 theta= 0.1 cobertura= 0.9007 \nn= 1 theta= 0.15 cobertura= 1 \nn= 1 theta= 0.2 cobertura= 1 \nn= 1 theta= 0.25 cobertura= 1 \nn= 1 theta= 0.3 cobertura= 1 \nn= 1 theta= 0.35 cobertura= 1 \nn= 1 theta= 0.4 cobertura= 1 \nn= 1 theta= 0.45 cobertura= 1 \nn= 1 theta= 0.5 cobertura= 1 \nn= 5 theta= 0.05 cobertura= 0.9766 \nn= 5 theta= 0.1 cobertura= 0.9901 \nn= 5 theta= 0.15 cobertura= 0.9744 \nn= 5 theta= 0.2 cobertura= 0.9383 \nn= 5 theta= 0.25 cobertura= 0.9834 \nn= 5 theta= 0.3 cobertura= 0.968 \nn= 5 theta= 0.35 cobertura= 0.9464 \nn= 5 theta= 0.4 cobertura= 0.9074 \nn= 5 theta= 0.45 cobertura= 0.9296 \nn= 5 theta= 0.5 cobertura= 0.9381 \nn= 10 theta= 0.05 cobertura= 0.9893 \nn= 10 theta= 0.1 cobertura= 0.9863 \nn= 10 theta= 0.15 cobertura= 0.9517 \nn= 10 theta= 0.2 cobertura= 0.9678 \nn= 10 theta= 0.25 cobertura= 0.9273 \nn= 10 theta= 0.3 cobertura= 0.926 \nn= 10 theta= 0.35 cobertura= 0.9642 \nn= 10 theta= 0.4 cobertura= 0.9428 \nn= 10 theta= 0.45 cobertura= 0.9503 \nn= 10 theta= 0.5 cobertura= 0.9781 \nn= 25 theta= 0.05 cobertura= 0.9643 \nn= 25 theta= 0.1 cobertura= 0.8903 \nn= 25 theta= 0.15 cobertura= 0.9551 \nn= 25 theta= 0.2 cobertura= 0.9534 \nn= 25 theta= 0.25 cobertura= 0.9376 \nn= 25 theta= 0.3 cobertura= 0.9471 \nn= 25 theta= 0.35 cobertura= 0.9419 \nn= 25 theta= 0.4 cobertura= 0.9414 \nn= 25 theta= 0.45 cobertura= 0.9553 \nn= 25 theta= 0.5 cobertura= 0.9602 \n\n# Crear combinaciones entre los valores de 'n' y 'theta'\ndatos &lt;- as.data.frame(expand.grid(n_vector, theta_vector))\n\n# Asignar nombres de columnas mas claros\ncolnames(datos) &lt;- c(\"n\", \"theta\")\n\n# Transformar matriz en vector y guardar como columna del data frame\ndatos$cobertura &lt;- as.vector(coberturas)\n\n# Graficar con ggplot2\nggplot(datos, aes(x = theta, y = cobertura)) +\n    geom_line(color = \"#3b78b0\") +\n    geom_point(color = \"#3b78b0\") +\n    geom_hline(yintercept = 0.95, linetype = \"dashed\") +\n    labs(x = expression(theta)) +\n    facet_wrap(~ n, labeller = label_both) +\n    theme_bw() +\n    theme(\n        panel.grid.minor = element_blank()\n    )",
    "crumbs": [
      "Recursos",
      "Código",
      "06 - Propiedades frecuentistas de inferencias bayesianas"
    ]
  },
  {
    "objectID": "recursos/codigo/16_regresion_lineal_rstan.html",
    "href": "recursos/codigo/16_regresion_lineal_rstan.html",
    "title": "14 - Regresión lineal con {RStan}",
    "section": "",
    "text": "El siguiente programa muestra el código necesario para con R los ejercicios Mi primer regresión bayesiana y Mejorando mi regresión bayesiana de la Práctica 4.\nEn primer lugar, se cargan librerías necesarias.\n\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(readr)\nlibrary(rstan)\n\nLuego se leen y visualizan los datos.\n\n# Leer los datos desde el repositorio\ndf_sales &lt;- read_csv(\n    \"https://raw.githubusercontent.com/estadisticaunr/estadistica-bayesiana/main/datos/sales.csv\"\n)\n\n# Explorar los datos\nggplot(df_sales) +\n  geom_point(aes(x = x, y = y), alpha = 0.6, size = 2) +\n  labs(x = \"Publicidad ($)\", y = \"Ventas ($)\")\n\n\n\n\n\n\n\n\nSe crea el siguiente modelo de regresión lineal bayesiana: \\[\n\\begin{aligned}\n\\text{ventas}_i &\\sim \\text{Normal}(\\mu_i, \\sigma^2) \\\\\n\\mu_i &= \\beta_0 + \\beta_1 \\text{publicidad}_i \\\\\n\\end{aligned}\n\\]\ndonde los parámetros \\(\\beta_0\\), \\(\\beta_1\\) y \\(\\sigma\\) siguen distribuciones a priori uniformes. El programa de Stan para implementar el modelo es el siguiente:\n\ndata {\n  int&lt;lower=0&gt; N;  // Cantidad de observaciones\n  vector[N] x;     // Valores de la variable predictora\n  vector[N] y;     // Valores de la variable respuesta\n}\nparameters {\n  real beta0;           // Intercepto\n  real beta1;           // Pendiente\n  real&lt;lower=0&gt; sigma;  // Desvio estándar del error\n}\nmodel {\n  y ~ normal(beta0 + beta1 * x, sigma);\n}\n\nAhora se crea una lista con los datos para el modelo y se obtienen muestras del posterior utilizando la función stan().\n\nstan_data &lt;- list(\n  N = nrow(df_sales), # Cantidad de observaciones\n  x = df_sales$x,     # Publicidad\n  y = df_sales$y      # Ventas\n)\n\nruta_modelo_1 &lt;- here::here(\n  \"recursos\", \"codigo\", \"stan\", \"regresion_lineal\", \"01_modelo.stan\"\n)\n\n# Pasamos la ubicación del archivo con el codigo del modelo en Stan\nmodelo_1 &lt;- stan_model(file = ruta_modelo_1)\n\nmodelo_1_fit &lt;- sampling(\n  modelo_1,             # El modelo\n  data = stan_data,     # Datos\n  chains = 4,           # Cantidad de cadenas\n  seed = 1211,          # Para que el resultado sea reproducible\n  refresh = 1,          # Mostrar mensajes del sampler (1: si, 0: no)\n)\n\n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 1.2e-05 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.12 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 1: Iteration:    2 / 2000 [  0%]  (Warmup)\nChain 1: Iteration:    3 / 2000 [  0%]  (Warmup)\nChain 1: Iteration:    4 / 2000 [  0%]  (Warmup)\nChain 1: Iteration:    5 / 2000 [  0%]  (Warmup)\nChain 1: Iteration:    6 / 2000 [  0%]  (Warmup)\nChain 1: Iteration:    7 / 2000 [  0%]  (Warmup)\nChain 1: Iteration:    8 / 2000 [  0%]  (Warmup)\nChain 1: Iteration:    9 / 2000 [  0%]  (Warmup)\nChain 1: Iteration:   10 / 2000 [  0%]  (Warmup)\nChain 1: Iteration:   11 / 2000 [  0%]  (Warmup)\nChain 1: Iteration:   12 / 2000 [  0%]  (Warmup)\nChain 1: Iteration:   13 / 2000 [  0%]  (Warmup)\nChain 1: Iteration:   14 / 2000 [  0%]  (Warmup)\nChain 1: Iteration:   15 / 2000 [  0%]  (Warmup)\nChain 1: Iteration:   16 / 2000 [  0%]  (Warmup)\nChain 1: Iteration:   17 / 2000 [  0%]  (Warmup)\nChain 1: Iteration:   18 / 2000 [  0%]  (Warmup)\nChain 1: Iteration:   19 / 2000 [  0%]  (Warmup)\nChain 1: Iteration:   20 / 2000 [  1%]  (Warmup)\nChain 1: Iteration:   21 / 2000 [  1%]  (Warmup)\nChain 1: Iteration:   22 / 2000 [  1%]  (Warmup)\nChain 1: Iteration:   23 / 2000 [  1%]  (Warmup)\nChain 1: Iteration:   24 / 2000 [  1%]  (Warmup)\nChain 1: Iteration:   25 / 2000 [  1%]  (Warmup)\nChain 1: Iteration:   26 / 2000 [  1%]  (Warmup)\nChain 1: Iteration:   27 / 2000 [  1%]  (Warmup)\nChain 1: Iteration:   28 / 2000 [  1%]  (Warmup)\nChain 1: Iteration:   29 / 2000 [  1%]  (Warmup)\nChain 1: Iteration:   30 / 2000 [  1%]  (Warmup)\nChain 1: Iteration:   31 / 2000 [  1%]  (Warmup)\nChain 1: Iteration:   32 / 2000 [  1%]  (Warmup)\nChain 1: Iteration:   33 / 2000 [  1%]  (Warmup)\nChain 1: Iteration:   34 / 2000 [  1%]  (Warmup)\nChain 1: Iteration:   35 / 2000 [  1%]  (Warmup)\nChain 1: Iteration:   36 / 2000 [  1%]  (Warmup)\nChain 1: Iteration:   37 / 2000 [  1%]  (Warmup)\nChain 1: Iteration:   38 / 2000 [  1%]  (Warmup)\nChain 1: Iteration:   39 / 2000 [  1%]  (Warmup)\nChain 1: Iteration:   40 / 2000 [  2%]  (Warmup)\nChain 1: Iteration:   41 / 2000 [  2%]  (Warmup)\nChain 1: Iteration:   42 / 2000 [  2%]  (Warmup)\nChain 1: Iteration:   43 / 2000 [  2%]  (Warmup)\nChain 1: Iteration:   44 / 2000 [  2%]  (Warmup)\nChain 1: Iteration:   45 / 2000 [  2%]  (Warmup)\nChain 1: Iteration:   46 / 2000 [  2%]  (Warmup)\nChain 1: Iteration:   47 / 2000 [  2%]  (Warmup)\nChain 1: Iteration:   48 / 2000 [  2%]  (Warmup)\nChain 1: Iteration:   49 / 2000 [  2%]  (Warmup)\nChain 1: Iteration:   50 / 2000 [  2%]  (Warmup)\nChain 1: Iteration:   51 / 2000 [  2%]  (Warmup)\nChain 1: Iteration:   52 / 2000 [  2%]  (Warmup)\nChain 1: Iteration:   53 / 2000 [  2%]  (Warmup)\nChain 1: Iteration:   54 / 2000 [  2%]  (Warmup)\nChain 1: Iteration:   55 / 2000 [  2%]  (Warmup)\nChain 1: Iteration:   56 / 2000 [  2%]  (Warmup)\nChain 1: Iteration:   57 / 2000 [  2%]  (Warmup)\nChain 1: Iteration:   58 / 2000 [  2%]  (Warmup)\nChain 1: Iteration:   59 / 2000 [  2%]  (Warmup)\nChain 1: Iteration:   60 / 2000 [  3%]  (Warmup)\nChain 1: Iteration:   61 / 2000 [  3%]  (Warmup)\nChain 1: Iteration:   62 / 2000 [  3%]  (Warmup)\nChain 1: Iteration:   63 / 2000 [  3%]  (Warmup)\nChain 1: Iteration:   64 / 2000 [  3%]  (Warmup)\nChain 1: Iteration:   65 / 2000 [  3%]  (Warmup)\nChain 1: Iteration:   66 / 2000 [  3%]  (Warmup)\nChain 1: Iteration:   67 / 2000 [  3%]  (Warmup)\nChain 1: Iteration:   68 / 2000 [  3%]  (Warmup)\nChain 1: Iteration:   69 / 2000 [  3%]  (Warmup)\nChain 1: Iteration:   70 / 2000 [  3%]  (Warmup)\nChain 1: Iteration:   71 / 2000 [  3%]  (Warmup)\nChain 1: Iteration:   72 / 2000 [  3%]  (Warmup)\nChain 1: Iteration:   73 / 2000 [  3%]  (Warmup)\nChain 1: Iteration:   74 / 2000 [  3%]  (Warmup)\nChain 1: Iteration:   75 / 2000 [  3%]  (Warmup)\nChain 1: Iteration:   76 / 2000 [  3%]  (Warmup)\nChain 1: Iteration:   77 / 2000 [  3%]  (Warmup)\nChain 1: Iteration:   78 / 2000 [  3%]  (Warmup)\nChain 1: Iteration:   79 / 2000 [  3%]  (Warmup)\nChain 1: Iteration:   80 / 2000 [  4%]  (Warmup)\nChain 1: Iteration:   81 / 2000 [  4%]  (Warmup)\nChain 1: Iteration:   82 / 2000 [  4%]  (Warmup)\nChain 1: Iteration:   83 / 2000 [  4%]  (Warmup)\nChain 1: Iteration:   84 / 2000 [  4%]  (Warmup)\nChain 1: Iteration:   85 / 2000 [  4%]  (Warmup)\nChain 1: Iteration:   86 / 2000 [  4%]  (Warmup)\nChain 1: Iteration:   87 / 2000 [  4%]  (Warmup)\nChain 1: Iteration:   88 / 2000 [  4%]  (Warmup)\nChain 1: Iteration:   89 / 2000 [  4%]  (Warmup)\nChain 1: Iteration:   90 / 2000 [  4%]  (Warmup)\nChain 1: Iteration:   91 / 2000 [  4%]  (Warmup)\nChain 1: Iteration:   92 / 2000 [  4%]  (Warmup)\nChain 1: Iteration:   93 / 2000 [  4%]  (Warmup)\nChain 1: Iteration:   94 / 2000 [  4%]  (Warmup)\nChain 1: Iteration:   95 / 2000 [  4%]  (Warmup)\nChain 1: Iteration:   96 / 2000 [  4%]  (Warmup)\nChain 1: Iteration:   97 / 2000 [  4%]  (Warmup)\nChain 1: Iteration:   98 / 2000 [  4%]  (Warmup)\nChain 1: Iteration:   99 / 2000 [  4%]  (Warmup)\nChain 1: Iteration:  100 / 2000 [  5%]  (Warmup)\nChain 1: Iteration:  101 / 2000 [  5%]  (Warmup)\nChain 1: Iteration:  102 / 2000 [  5%]  (Warmup)\nChain 1: Iteration:  103 / 2000 [  5%]  (Warmup)\nChain 1: Iteration:  104 / 2000 [  5%]  (Warmup)\nChain 1: Iteration:  105 / 2000 [  5%]  (Warmup)\nChain 1: Iteration:  106 / 2000 [  5%]  (Warmup)\nChain 1: Iteration:  107 / 2000 [  5%]  (Warmup)\nChain 1: Iteration:  108 / 2000 [  5%]  (Warmup)\nChain 1: Iteration:  109 / 2000 [  5%]  (Warmup)\nChain 1: Iteration:  110 / 2000 [  5%]  (Warmup)\nChain 1: Iteration:  111 / 2000 [  5%]  (Warmup)\nChain 1: Iteration:  112 / 2000 [  5%]  (Warmup)\nChain 1: Iteration:  113 / 2000 [  5%]  (Warmup)\nChain 1: Iteration:  114 / 2000 [  5%]  (Warmup)\nChain 1: Iteration:  115 / 2000 [  5%]  (Warmup)\nChain 1: Iteration:  116 / 2000 [  5%]  (Warmup)\nChain 1: Iteration:  117 / 2000 [  5%]  (Warmup)\nChain 1: Iteration:  118 / 2000 [  5%]  (Warmup)\nChain 1: Iteration:  119 / 2000 [  5%]  (Warmup)\nChain 1: Iteration:  120 / 2000 [  6%]  (Warmup)\nChain 1: Iteration:  121 / 2000 [  6%]  (Warmup)\nChain 1: Iteration:  122 / 2000 [  6%]  (Warmup)\nChain 1: Iteration:  123 / 2000 [  6%]  (Warmup)\nChain 1: Iteration:  124 / 2000 [  6%]  (Warmup)\nChain 1: Iteration:  125 / 2000 [  6%]  (Warmup)\nChain 1: Iteration:  126 / 2000 [  6%]  (Warmup)\nChain 1: Iteration:  127 / 2000 [  6%]  (Warmup)\nChain 1: Iteration:  128 / 2000 [  6%]  (Warmup)\nChain 1: Iteration:  129 / 2000 [  6%]  (Warmup)\nChain 1: Iteration:  130 / 2000 [  6%]  (Warmup)\nChain 1: Iteration:  131 / 2000 [  6%]  (Warmup)\nChain 1: Iteration:  132 / 2000 [  6%]  (Warmup)\nChain 1: Iteration:  133 / 2000 [  6%]  (Warmup)\nChain 1: Iteration:  134 / 2000 [  6%]  (Warmup)\nChain 1: Iteration:  135 / 2000 [  6%]  (Warmup)\nChain 1: Iteration:  136 / 2000 [  6%]  (Warmup)\nChain 1: Iteration:  137 / 2000 [  6%]  (Warmup)\nChain 1: Iteration:  138 / 2000 [  6%]  (Warmup)\nChain 1: Iteration:  139 / 2000 [  6%]  (Warmup)\nChain 1: Iteration:  140 / 2000 [  7%]  (Warmup)\nChain 1: Iteration:  141 / 2000 [  7%]  (Warmup)\nChain 1: Iteration:  142 / 2000 [  7%]  (Warmup)\nChain 1: Iteration:  143 / 2000 [  7%]  (Warmup)\nChain 1: Iteration:  144 / 2000 [  7%]  (Warmup)\nChain 1: Iteration:  145 / 2000 [  7%]  (Warmup)\nChain 1: Iteration:  146 / 2000 [  7%]  (Warmup)\nChain 1: Iteration:  147 / 2000 [  7%]  (Warmup)\nChain 1: Iteration:  148 / 2000 [  7%]  (Warmup)\nChain 1: Iteration:  149 / 2000 [  7%]  (Warmup)\nChain 1: Iteration:  150 / 2000 [  7%]  (Warmup)\nChain 1: Iteration:  151 / 2000 [  7%]  (Warmup)\nChain 1: Iteration:  152 / 2000 [  7%]  (Warmup)\nChain 1: Iteration:  153 / 2000 [  7%]  (Warmup)\nChain 1: Iteration:  154 / 2000 [  7%]  (Warmup)\nChain 1: Iteration:  155 / 2000 [  7%]  (Warmup)\nChain 1: Iteration:  156 / 2000 [  7%]  (Warmup)\nChain 1: Iteration:  157 / 2000 [  7%]  (Warmup)\nChain 1: Iteration:  158 / 2000 [  7%]  (Warmup)\nChain 1: Iteration:  159 / 2000 [  7%]  (Warmup)\nChain 1: Iteration:  160 / 2000 [  8%]  (Warmup)\nChain 1: Iteration:  161 / 2000 [  8%]  (Warmup)\nChain 1: Iteration:  162 / 2000 [  8%]  (Warmup)\nChain 1: Iteration:  163 / 2000 [  8%]  (Warmup)\nChain 1: Iteration:  164 / 2000 [  8%]  (Warmup)\nChain 1: Iteration:  165 / 2000 [  8%]  (Warmup)\nChain 1: Iteration:  166 / 2000 [  8%]  (Warmup)\nChain 1: Iteration:  167 / 2000 [  8%]  (Warmup)\nChain 1: Iteration:  168 / 2000 [  8%]  (Warmup)\nChain 1: Iteration:  169 / 2000 [  8%]  (Warmup)\nChain 1: Iteration:  170 / 2000 [  8%]  (Warmup)\nChain 1: Iteration:  171 / 2000 [  8%]  (Warmup)\nChain 1: Iteration:  172 / 2000 [  8%]  (Warmup)\nChain 1: Iteration:  173 / 2000 [  8%]  (Warmup)\nChain 1: Iteration:  174 / 2000 [  8%]  (Warmup)\nChain 1: Iteration:  175 / 2000 [  8%]  (Warmup)\nChain 1: Iteration:  176 / 2000 [  8%]  (Warmup)\nChain 1: Iteration:  177 / 2000 [  8%]  (Warmup)\nChain 1: Iteration:  178 / 2000 [  8%]  (Warmup)\nChain 1: Iteration:  179 / 2000 [  8%]  (Warmup)\nChain 1: Iteration:  180 / 2000 [  9%]  (Warmup)\nChain 1: Iteration:  181 / 2000 [  9%]  (Warmup)\nChain 1: Iteration:  182 / 2000 [  9%]  (Warmup)\nChain 1: Iteration:  183 / 2000 [  9%]  (Warmup)\nChain 1: Iteration:  184 / 2000 [  9%]  (Warmup)\nChain 1: Iteration:  185 / 2000 [  9%]  (Warmup)\nChain 1: Iteration:  186 / 2000 [  9%]  (Warmup)\nChain 1: Iteration:  187 / 2000 [  9%]  (Warmup)\nChain 1: Iteration:  188 / 2000 [  9%]  (Warmup)\nChain 1: Iteration:  189 / 2000 [  9%]  (Warmup)\nChain 1: Iteration:  190 / 2000 [  9%]  (Warmup)\nChain 1: Iteration:  191 / 2000 [  9%]  (Warmup)\nChain 1: Iteration:  192 / 2000 [  9%]  (Warmup)\nChain 1: Iteration:  193 / 2000 [  9%]  (Warmup)\nChain 1: Iteration:  194 / 2000 [  9%]  (Warmup)\nChain 1: Iteration:  195 / 2000 [  9%]  (Warmup)\nChain 1: Iteration:  196 / 2000 [  9%]  (Warmup)\nChain 1: Iteration:  197 / 2000 [  9%]  (Warmup)\nChain 1: Iteration:  198 / 2000 [  9%]  (Warmup)\nChain 1: Iteration:  199 / 2000 [  9%]  (Warmup)\nChain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 1: Iteration:  201 / 2000 [ 10%]  (Warmup)\nChain 1: Iteration:  202 / 2000 [ 10%]  (Warmup)\nChain 1: Iteration:  203 / 2000 [ 10%]  (Warmup)\nChain 1: Iteration:  204 / 2000 [ 10%]  (Warmup)\nChain 1: Iteration:  205 / 2000 [ 10%]  (Warmup)\nChain 1: Iteration:  206 / 2000 [ 10%]  (Warmup)\nChain 1: Iteration:  207 / 2000 [ 10%]  (Warmup)\nChain 1: Iteration:  208 / 2000 [ 10%]  (Warmup)\nChain 1: Iteration:  209 / 2000 [ 10%]  (Warmup)\nChain 1: Iteration:  210 / 2000 [ 10%]  (Warmup)\nChain 1: Iteration:  211 / 2000 [ 10%]  (Warmup)\nChain 1: Iteration:  212 / 2000 [ 10%]  (Warmup)\nChain 1: Iteration:  213 / 2000 [ 10%]  (Warmup)\nChain 1: Iteration:  214 / 2000 [ 10%]  (Warmup)\nChain 1: Iteration:  215 / 2000 [ 10%]  (Warmup)\nChain 1: Iteration:  216 / 2000 [ 10%]  (Warmup)\nChain 1: Iteration:  217 / 2000 [ 10%]  (Warmup)\nChain 1: Iteration:  218 / 2000 [ 10%]  (Warmup)\nChain 1: Iteration:  219 / 2000 [ 10%]  (Warmup)\nChain 1: Iteration:  220 / 2000 [ 11%]  (Warmup)\nChain 1: Iteration:  221 / 2000 [ 11%]  (Warmup)\nChain 1: Iteration:  222 / 2000 [ 11%]  (Warmup)\nChain 1: Iteration:  223 / 2000 [ 11%]  (Warmup)\nChain 1: Iteration:  224 / 2000 [ 11%]  (Warmup)\nChain 1: Iteration:  225 / 2000 [ 11%]  (Warmup)\nChain 1: Iteration:  226 / 2000 [ 11%]  (Warmup)\nChain 1: Iteration:  227 / 2000 [ 11%]  (Warmup)\nChain 1: Iteration:  228 / 2000 [ 11%]  (Warmup)\nChain 1: Iteration:  229 / 2000 [ 11%]  (Warmup)\nChain 1: Iteration:  230 / 2000 [ 11%]  (Warmup)\nChain 1: Iteration:  231 / 2000 [ 11%]  (Warmup)\nChain 1: Iteration:  232 / 2000 [ 11%]  (Warmup)\nChain 1: Iteration:  233 / 2000 [ 11%]  (Warmup)\nChain 1: Iteration:  234 / 2000 [ 11%]  (Warmup)\nChain 1: Iteration:  235 / 2000 [ 11%]  (Warmup)\nChain 1: Iteration:  236 / 2000 [ 11%]  (Warmup)\nChain 1: Iteration:  237 / 2000 [ 11%]  (Warmup)\nChain 1: Iteration:  238 / 2000 [ 11%]  (Warmup)\nChain 1: Iteration:  239 / 2000 [ 11%]  (Warmup)\nChain 1: Iteration:  240 / 2000 [ 12%]  (Warmup)\nChain 1: Iteration:  241 / 2000 [ 12%]  (Warmup)\nChain 1: Iteration:  242 / 2000 [ 12%]  (Warmup)\nChain 1: Iteration:  243 / 2000 [ 12%]  (Warmup)\nChain 1: Iteration:  244 / 2000 [ 12%]  (Warmup)\nChain 1: Iteration:  245 / 2000 [ 12%]  (Warmup)\nChain 1: Iteration:  246 / 2000 [ 12%]  (Warmup)\nChain 1: Iteration:  247 / 2000 [ 12%]  (Warmup)\nChain 1: Iteration:  248 / 2000 [ 12%]  (Warmup)\nChain 1: Iteration:  249 / 2000 [ 12%]  (Warmup)\nChain 1: Iteration:  250 / 2000 [ 12%]  (Warmup)\nChain 1: Iteration:  251 / 2000 [ 12%]  (Warmup)\nChain 1: Iteration:  252 / 2000 [ 12%]  (Warmup)\nChain 1: Iteration:  253 / 2000 [ 12%]  (Warmup)\nChain 1: Iteration:  254 / 2000 [ 12%]  (Warmup)\nChain 1: Iteration:  255 / 2000 [ 12%]  (Warmup)\nChain 1: Iteration:  256 / 2000 [ 12%]  (Warmup)\nChain 1: Iteration:  257 / 2000 [ 12%]  (Warmup)\nChain 1: Iteration:  258 / 2000 [ 12%]  (Warmup)\nChain 1: Iteration:  259 / 2000 [ 12%]  (Warmup)\nChain 1: Iteration:  260 / 2000 [ 13%]  (Warmup)\nChain 1: Iteration:  261 / 2000 [ 13%]  (Warmup)\nChain 1: Iteration:  262 / 2000 [ 13%]  (Warmup)\nChain 1: Iteration:  263 / 2000 [ 13%]  (Warmup)\nChain 1: Iteration:  264 / 2000 [ 13%]  (Warmup)\nChain 1: Iteration:  265 / 2000 [ 13%]  (Warmup)\nChain 1: Iteration:  266 / 2000 [ 13%]  (Warmup)\nChain 1: Iteration:  267 / 2000 [ 13%]  (Warmup)\nChain 1: Iteration:  268 / 2000 [ 13%]  (Warmup)\nChain 1: Iteration:  269 / 2000 [ 13%]  (Warmup)\nChain 1: Iteration:  270 / 2000 [ 13%]  (Warmup)\nChain 1: Iteration:  271 / 2000 [ 13%]  (Warmup)\nChain 1: Iteration:  272 / 2000 [ 13%]  (Warmup)\nChain 1: Iteration:  273 / 2000 [ 13%]  (Warmup)\nChain 1: Iteration:  274 / 2000 [ 13%]  (Warmup)\nChain 1: Iteration:  275 / 2000 [ 13%]  (Warmup)\nChain 1: Iteration:  276 / 2000 [ 13%]  (Warmup)\nChain 1: Iteration:  277 / 2000 [ 13%]  (Warmup)\nChain 1: Iteration:  278 / 2000 [ 13%]  (Warmup)\nChain 1: Iteration:  279 / 2000 [ 13%]  (Warmup)\nChain 1: Iteration:  280 / 2000 [ 14%]  (Warmup)\nChain 1: Iteration:  281 / 2000 [ 14%]  (Warmup)\nChain 1: Iteration:  282 / 2000 [ 14%]  (Warmup)\nChain 1: Iteration:  283 / 2000 [ 14%]  (Warmup)\nChain 1: Iteration:  284 / 2000 [ 14%]  (Warmup)\nChain 1: Iteration:  285 / 2000 [ 14%]  (Warmup)\nChain 1: Iteration:  286 / 2000 [ 14%]  (Warmup)\nChain 1: Iteration:  287 / 2000 [ 14%]  (Warmup)\nChain 1: Iteration:  288 / 2000 [ 14%]  (Warmup)\nChain 1: Iteration:  289 / 2000 [ 14%]  (Warmup)\nChain 1: Iteration:  290 / 2000 [ 14%]  (Warmup)\nChain 1: Iteration:  291 / 2000 [ 14%]  (Warmup)\nChain 1: Iteration:  292 / 2000 [ 14%]  (Warmup)\nChain 1: Iteration:  293 / 2000 [ 14%]  (Warmup)\nChain 1: Iteration:  294 / 2000 [ 14%]  (Warmup)\nChain 1: Iteration:  295 / 2000 [ 14%]  (Warmup)\nChain 1: Iteration:  296 / 2000 [ 14%]  (Warmup)\nChain 1: Iteration:  297 / 2000 [ 14%]  (Warmup)\nChain 1: Iteration:  298 / 2000 [ 14%]  (Warmup)\nChain 1: Iteration:  299 / 2000 [ 14%]  (Warmup)\nChain 1: Iteration:  300 / 2000 [ 15%]  (Warmup)\nChain 1: Iteration:  301 / 2000 [ 15%]  (Warmup)\nChain 1: Iteration:  302 / 2000 [ 15%]  (Warmup)\nChain 1: Iteration:  303 / 2000 [ 15%]  (Warmup)\nChain 1: Iteration:  304 / 2000 [ 15%]  (Warmup)\nChain 1: Iteration:  305 / 2000 [ 15%]  (Warmup)\nChain 1: Iteration:  306 / 2000 [ 15%]  (Warmup)\nChain 1: Iteration:  307 / 2000 [ 15%]  (Warmup)\nChain 1: Iteration:  308 / 2000 [ 15%]  (Warmup)\nChain 1: Iteration:  309 / 2000 [ 15%]  (Warmup)\nChain 1: Iteration:  310 / 2000 [ 15%]  (Warmup)\nChain 1: Iteration:  311 / 2000 [ 15%]  (Warmup)\nChain 1: Iteration:  312 / 2000 [ 15%]  (Warmup)\nChain 1: Iteration:  313 / 2000 [ 15%]  (Warmup)\nChain 1: Iteration:  314 / 2000 [ 15%]  (Warmup)\nChain 1: Iteration:  315 / 2000 [ 15%]  (Warmup)\nChain 1: Iteration:  316 / 2000 [ 15%]  (Warmup)\nChain 1: Iteration:  317 / 2000 [ 15%]  (Warmup)\nChain 1: Iteration:  318 / 2000 [ 15%]  (Warmup)\nChain 1: Iteration:  319 / 2000 [ 15%]  (Warmup)\nChain 1: Iteration:  320 / 2000 [ 16%]  (Warmup)\nChain 1: Iteration:  321 / 2000 [ 16%]  (Warmup)\nChain 1: Iteration:  322 / 2000 [ 16%]  (Warmup)\nChain 1: Iteration:  323 / 2000 [ 16%]  (Warmup)\nChain 1: Iteration:  324 / 2000 [ 16%]  (Warmup)\nChain 1: Iteration:  325 / 2000 [ 16%]  (Warmup)\nChain 1: Iteration:  326 / 2000 [ 16%]  (Warmup)\nChain 1: Iteration:  327 / 2000 [ 16%]  (Warmup)\nChain 1: Iteration:  328 / 2000 [ 16%]  (Warmup)\nChain 1: Iteration:  329 / 2000 [ 16%]  (Warmup)\nChain 1: Iteration:  330 / 2000 [ 16%]  (Warmup)\nChain 1: Iteration:  331 / 2000 [ 16%]  (Warmup)\nChain 1: Iteration:  332 / 2000 [ 16%]  (Warmup)\nChain 1: Iteration:  333 / 2000 [ 16%]  (Warmup)\nChain 1: Iteration:  334 / 2000 [ 16%]  (Warmup)\nChain 1: Iteration:  335 / 2000 [ 16%]  (Warmup)\nChain 1: Iteration:  336 / 2000 [ 16%]  (Warmup)\nChain 1: Iteration:  337 / 2000 [ 16%]  (Warmup)\nChain 1: Iteration:  338 / 2000 [ 16%]  (Warmup)\nChain 1: Iteration:  339 / 2000 [ 16%]  (Warmup)\nChain 1: Iteration:  340 / 2000 [ 17%]  (Warmup)\nChain 1: Iteration:  341 / 2000 [ 17%]  (Warmup)\nChain 1: Iteration:  342 / 2000 [ 17%]  (Warmup)\nChain 1: Iteration:  343 / 2000 [ 17%]  (Warmup)\nChain 1: Iteration:  344 / 2000 [ 17%]  (Warmup)\nChain 1: Iteration:  345 / 2000 [ 17%]  (Warmup)\nChain 1: Iteration:  346 / 2000 [ 17%]  (Warmup)\nChain 1: Iteration:  347 / 2000 [ 17%]  (Warmup)\nChain 1: Iteration:  348 / 2000 [ 17%]  (Warmup)\nChain 1: Iteration:  349 / 2000 [ 17%]  (Warmup)\nChain 1: Iteration:  350 / 2000 [ 17%]  (Warmup)\nChain 1: Iteration:  351 / 2000 [ 17%]  (Warmup)\nChain 1: Iteration:  352 / 2000 [ 17%]  (Warmup)\nChain 1: Iteration:  353 / 2000 [ 17%]  (Warmup)\nChain 1: Iteration:  354 / 2000 [ 17%]  (Warmup)\nChain 1: Iteration:  355 / 2000 [ 17%]  (Warmup)\nChain 1: Iteration:  356 / 2000 [ 17%]  (Warmup)\nChain 1: Iteration:  357 / 2000 [ 17%]  (Warmup)\nChain 1: Iteration:  358 / 2000 [ 17%]  (Warmup)\nChain 1: Iteration:  359 / 2000 [ 17%]  (Warmup)\nChain 1: Iteration:  360 / 2000 [ 18%]  (Warmup)\nChain 1: Iteration:  361 / 2000 [ 18%]  (Warmup)\nChain 1: Iteration:  362 / 2000 [ 18%]  (Warmup)\nChain 1: Iteration:  363 / 2000 [ 18%]  (Warmup)\nChain 1: Iteration:  364 / 2000 [ 18%]  (Warmup)\nChain 1: Iteration:  365 / 2000 [ 18%]  (Warmup)\nChain 1: Iteration:  366 / 2000 [ 18%]  (Warmup)\nChain 1: Iteration:  367 / 2000 [ 18%]  (Warmup)\nChain 1: Iteration:  368 / 2000 [ 18%]  (Warmup)\nChain 1: Iteration:  369 / 2000 [ 18%]  (Warmup)\nChain 1: Iteration:  370 / 2000 [ 18%]  (Warmup)\nChain 1: Iteration:  371 / 2000 [ 18%]  (Warmup)\nChain 1: Iteration:  372 / 2000 [ 18%]  (Warmup)\nChain 1: Iteration:  373 / 2000 [ 18%]  (Warmup)\nChain 1: Iteration:  374 / 2000 [ 18%]  (Warmup)\nChain 1: Iteration:  375 / 2000 [ 18%]  (Warmup)\nChain 1: Iteration:  376 / 2000 [ 18%]  (Warmup)\nChain 1: Iteration:  377 / 2000 [ 18%]  (Warmup)\nChain 1: Iteration:  378 / 2000 [ 18%]  (Warmup)\nChain 1: Iteration:  379 / 2000 [ 18%]  (Warmup)\nChain 1: Iteration:  380 / 2000 [ 19%]  (Warmup)\nChain 1: Iteration:  381 / 2000 [ 19%]  (Warmup)\nChain 1: Iteration:  382 / 2000 [ 19%]  (Warmup)\nChain 1: Iteration:  383 / 2000 [ 19%]  (Warmup)\nChain 1: Iteration:  384 / 2000 [ 19%]  (Warmup)\nChain 1: Iteration:  385 / 2000 [ 19%]  (Warmup)\nChain 1: Iteration:  386 / 2000 [ 19%]  (Warmup)\nChain 1: Iteration:  387 / 2000 [ 19%]  (Warmup)\nChain 1: Iteration:  388 / 2000 [ 19%]  (Warmup)\nChain 1: Iteration:  389 / 2000 [ 19%]  (Warmup)\nChain 1: Iteration:  390 / 2000 [ 19%]  (Warmup)\nChain 1: Iteration:  391 / 2000 [ 19%]  (Warmup)\nChain 1: Iteration:  392 / 2000 [ 19%]  (Warmup)\nChain 1: Iteration:  393 / 2000 [ 19%]  (Warmup)\nChain 1: Iteration:  394 / 2000 [ 19%]  (Warmup)\nChain 1: Iteration:  395 / 2000 [ 19%]  (Warmup)\nChain 1: Iteration:  396 / 2000 [ 19%]  (Warmup)\nChain 1: Iteration:  397 / 2000 [ 19%]  (Warmup)\nChain 1: Iteration:  398 / 2000 [ 19%]  (Warmup)\nChain 1: Iteration:  399 / 2000 [ 19%]  (Warmup)\nChain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 1: Iteration:  401 / 2000 [ 20%]  (Warmup)\nChain 1: Iteration:  402 / 2000 [ 20%]  (Warmup)\nChain 1: Iteration:  403 / 2000 [ 20%]  (Warmup)\nChain 1: Iteration:  404 / 2000 [ 20%]  (Warmup)\nChain 1: Iteration:  405 / 2000 [ 20%]  (Warmup)\nChain 1: Iteration:  406 / 2000 [ 20%]  (Warmup)\nChain 1: Iteration:  407 / 2000 [ 20%]  (Warmup)\nChain 1: Iteration:  408 / 2000 [ 20%]  (Warmup)\nChain 1: Iteration:  409 / 2000 [ 20%]  (Warmup)\nChain 1: Iteration:  410 / 2000 [ 20%]  (Warmup)\nChain 1: Iteration:  411 / 2000 [ 20%]  (Warmup)\nChain 1: Iteration:  412 / 2000 [ 20%]  (Warmup)\nChain 1: Iteration:  413 / 2000 [ 20%]  (Warmup)\nChain 1: Iteration:  414 / 2000 [ 20%]  (Warmup)\nChain 1: Iteration:  415 / 2000 [ 20%]  (Warmup)\nChain 1: Iteration:  416 / 2000 [ 20%]  (Warmup)\nChain 1: Iteration:  417 / 2000 [ 20%]  (Warmup)\nChain 1: Iteration:  418 / 2000 [ 20%]  (Warmup)\nChain 1: Iteration:  419 / 2000 [ 20%]  (Warmup)\nChain 1: Iteration:  420 / 2000 [ 21%]  (Warmup)\nChain 1: Iteration:  421 / 2000 [ 21%]  (Warmup)\nChain 1: Iteration:  422 / 2000 [ 21%]  (Warmup)\nChain 1: Iteration:  423 / 2000 [ 21%]  (Warmup)\nChain 1: Iteration:  424 / 2000 [ 21%]  (Warmup)\nChain 1: Iteration:  425 / 2000 [ 21%]  (Warmup)\nChain 1: Iteration:  426 / 2000 [ 21%]  (Warmup)\nChain 1: Iteration:  427 / 2000 [ 21%]  (Warmup)\nChain 1: Iteration:  428 / 2000 [ 21%]  (Warmup)\nChain 1: Iteration:  429 / 2000 [ 21%]  (Warmup)\nChain 1: Iteration:  430 / 2000 [ 21%]  (Warmup)\nChain 1: Iteration:  431 / 2000 [ 21%]  (Warmup)\nChain 1: Iteration:  432 / 2000 [ 21%]  (Warmup)\nChain 1: Iteration:  433 / 2000 [ 21%]  (Warmup)\nChain 1: Iteration:  434 / 2000 [ 21%]  (Warmup)\nChain 1: Iteration:  435 / 2000 [ 21%]  (Warmup)\nChain 1: Iteration:  436 / 2000 [ 21%]  (Warmup)\nChain 1: Iteration:  437 / 2000 [ 21%]  (Warmup)\nChain 1: Iteration:  438 / 2000 [ 21%]  (Warmup)\nChain 1: Iteration:  439 / 2000 [ 21%]  (Warmup)\nChain 1: Iteration:  440 / 2000 [ 22%]  (Warmup)\nChain 1: Iteration:  441 / 2000 [ 22%]  (Warmup)\nChain 1: Iteration:  442 / 2000 [ 22%]  (Warmup)\nChain 1: Iteration:  443 / 2000 [ 22%]  (Warmup)\nChain 1: Iteration:  444 / 2000 [ 22%]  (Warmup)\nChain 1: Iteration:  445 / 2000 [ 22%]  (Warmup)\nChain 1: Iteration:  446 / 2000 [ 22%]  (Warmup)\nChain 1: Iteration:  447 / 2000 [ 22%]  (Warmup)\nChain 1: Iteration:  448 / 2000 [ 22%]  (Warmup)\nChain 1: Iteration:  449 / 2000 [ 22%]  (Warmup)\nChain 1: Iteration:  450 / 2000 [ 22%]  (Warmup)\nChain 1: Iteration:  451 / 2000 [ 22%]  (Warmup)\nChain 1: Iteration:  452 / 2000 [ 22%]  (Warmup)\nChain 1: Iteration:  453 / 2000 [ 22%]  (Warmup)\nChain 1: Iteration:  454 / 2000 [ 22%]  (Warmup)\nChain 1: Iteration:  455 / 2000 [ 22%]  (Warmup)\nChain 1: Iteration:  456 / 2000 [ 22%]  (Warmup)\nChain 1: Iteration:  457 / 2000 [ 22%]  (Warmup)\nChain 1: Iteration:  458 / 2000 [ 22%]  (Warmup)\nChain 1: Iteration:  459 / 2000 [ 22%]  (Warmup)\nChain 1: Iteration:  460 / 2000 [ 23%]  (Warmup)\nChain 1: Iteration:  461 / 2000 [ 23%]  (Warmup)\nChain 1: Iteration:  462 / 2000 [ 23%]  (Warmup)\nChain 1: Iteration:  463 / 2000 [ 23%]  (Warmup)\nChain 1: Iteration:  464 / 2000 [ 23%]  (Warmup)\nChain 1: Iteration:  465 / 2000 [ 23%]  (Warmup)\nChain 1: Iteration:  466 / 2000 [ 23%]  (Warmup)\nChain 1: Iteration:  467 / 2000 [ 23%]  (Warmup)\nChain 1: Iteration:  468 / 2000 [ 23%]  (Warmup)\nChain 1: Iteration:  469 / 2000 [ 23%]  (Warmup)\nChain 1: Iteration:  470 / 2000 [ 23%]  (Warmup)\nChain 1: Iteration:  471 / 2000 [ 23%]  (Warmup)\nChain 1: Iteration:  472 / 2000 [ 23%]  (Warmup)\nChain 1: Iteration:  473 / 2000 [ 23%]  (Warmup)\nChain 1: Iteration:  474 / 2000 [ 23%]  (Warmup)\nChain 1: Iteration:  475 / 2000 [ 23%]  (Warmup)\nChain 1: Iteration:  476 / 2000 [ 23%]  (Warmup)\nChain 1: Iteration:  477 / 2000 [ 23%]  (Warmup)\nChain 1: Iteration:  478 / 2000 [ 23%]  (Warmup)\nChain 1: Iteration:  479 / 2000 [ 23%]  (Warmup)\nChain 1: Iteration:  480 / 2000 [ 24%]  (Warmup)\nChain 1: Iteration:  481 / 2000 [ 24%]  (Warmup)\nChain 1: Iteration:  482 / 2000 [ 24%]  (Warmup)\nChain 1: Iteration:  483 / 2000 [ 24%]  (Warmup)\nChain 1: Iteration:  484 / 2000 [ 24%]  (Warmup)\nChain 1: Iteration:  485 / 2000 [ 24%]  (Warmup)\nChain 1: Iteration:  486 / 2000 [ 24%]  (Warmup)\nChain 1: Iteration:  487 / 2000 [ 24%]  (Warmup)\nChain 1: Iteration:  488 / 2000 [ 24%]  (Warmup)\nChain 1: Iteration:  489 / 2000 [ 24%]  (Warmup)\nChain 1: Iteration:  490 / 2000 [ 24%]  (Warmup)\nChain 1: Iteration:  491 / 2000 [ 24%]  (Warmup)\nChain 1: Iteration:  492 / 2000 [ 24%]  (Warmup)\nChain 1: Iteration:  493 / 2000 [ 24%]  (Warmup)\nChain 1: Iteration:  494 / 2000 [ 24%]  (Warmup)\nChain 1: Iteration:  495 / 2000 [ 24%]  (Warmup)\nChain 1: Iteration:  496 / 2000 [ 24%]  (Warmup)\nChain 1: Iteration:  497 / 2000 [ 24%]  (Warmup)\nChain 1: Iteration:  498 / 2000 [ 24%]  (Warmup)\nChain 1: Iteration:  499 / 2000 [ 24%]  (Warmup)\nChain 1: Iteration:  500 / 2000 [ 25%]  (Warmup)\nChain 1: Iteration:  501 / 2000 [ 25%]  (Warmup)\nChain 1: Iteration:  502 / 2000 [ 25%]  (Warmup)\nChain 1: Iteration:  503 / 2000 [ 25%]  (Warmup)\nChain 1: Iteration:  504 / 2000 [ 25%]  (Warmup)\nChain 1: Iteration:  505 / 2000 [ 25%]  (Warmup)\nChain 1: Iteration:  506 / 2000 [ 25%]  (Warmup)\nChain 1: Iteration:  507 / 2000 [ 25%]  (Warmup)\nChain 1: Iteration:  508 / 2000 [ 25%]  (Warmup)\nChain 1: Iteration:  509 / 2000 [ 25%]  (Warmup)\nChain 1: Iteration:  510 / 2000 [ 25%]  (Warmup)\nChain 1: Iteration:  511 / 2000 [ 25%]  (Warmup)\nChain 1: Iteration:  512 / 2000 [ 25%]  (Warmup)\nChain 1: Iteration:  513 / 2000 [ 25%]  (Warmup)\nChain 1: Iteration:  514 / 2000 [ 25%]  (Warmup)\nChain 1: Iteration:  515 / 2000 [ 25%]  (Warmup)\nChain 1: Iteration:  516 / 2000 [ 25%]  (Warmup)\nChain 1: Iteration:  517 / 2000 [ 25%]  (Warmup)\nChain 1: Iteration:  518 / 2000 [ 25%]  (Warmup)\nChain 1: Iteration:  519 / 2000 [ 25%]  (Warmup)\nChain 1: Iteration:  520 / 2000 [ 26%]  (Warmup)\nChain 1: Iteration:  521 / 2000 [ 26%]  (Warmup)\nChain 1: Iteration:  522 / 2000 [ 26%]  (Warmup)\nChain 1: Iteration:  523 / 2000 [ 26%]  (Warmup)\nChain 1: Iteration:  524 / 2000 [ 26%]  (Warmup)\nChain 1: Iteration:  525 / 2000 [ 26%]  (Warmup)\nChain 1: Iteration:  526 / 2000 [ 26%]  (Warmup)\nChain 1: Iteration:  527 / 2000 [ 26%]  (Warmup)\nChain 1: Iteration:  528 / 2000 [ 26%]  (Warmup)\nChain 1: Iteration:  529 / 2000 [ 26%]  (Warmup)\nChain 1: Iteration:  530 / 2000 [ 26%]  (Warmup)\nChain 1: Iteration:  531 / 2000 [ 26%]  (Warmup)\nChain 1: Iteration:  532 / 2000 [ 26%]  (Warmup)\nChain 1: Iteration:  533 / 2000 [ 26%]  (Warmup)\nChain 1: Iteration:  534 / 2000 [ 26%]  (Warmup)\nChain 1: Iteration:  535 / 2000 [ 26%]  (Warmup)\nChain 1: Iteration:  536 / 2000 [ 26%]  (Warmup)\nChain 1: Iteration:  537 / 2000 [ 26%]  (Warmup)\nChain 1: Iteration:  538 / 2000 [ 26%]  (Warmup)\nChain 1: Iteration:  539 / 2000 [ 26%]  (Warmup)\nChain 1: Iteration:  540 / 2000 [ 27%]  (Warmup)\nChain 1: Iteration:  541 / 2000 [ 27%]  (Warmup)\nChain 1: Iteration:  542 / 2000 [ 27%]  (Warmup)\nChain 1: Iteration:  543 / 2000 [ 27%]  (Warmup)\nChain 1: Iteration:  544 / 2000 [ 27%]  (Warmup)\nChain 1: Iteration:  545 / 2000 [ 27%]  (Warmup)\nChain 1: Iteration:  546 / 2000 [ 27%]  (Warmup)\nChain 1: Iteration:  547 / 2000 [ 27%]  (Warmup)\nChain 1: Iteration:  548 / 2000 [ 27%]  (Warmup)\nChain 1: Iteration:  549 / 2000 [ 27%]  (Warmup)\nChain 1: Iteration:  550 / 2000 [ 27%]  (Warmup)\nChain 1: Iteration:  551 / 2000 [ 27%]  (Warmup)\nChain 1: Iteration:  552 / 2000 [ 27%]  (Warmup)\nChain 1: Iteration:  553 / 2000 [ 27%]  (Warmup)\nChain 1: Iteration:  554 / 2000 [ 27%]  (Warmup)\nChain 1: Iteration:  555 / 2000 [ 27%]  (Warmup)\nChain 1: Iteration:  556 / 2000 [ 27%]  (Warmup)\nChain 1: Iteration:  557 / 2000 [ 27%]  (Warmup)\nChain 1: Iteration:  558 / 2000 [ 27%]  (Warmup)\nChain 1: Iteration:  559 / 2000 [ 27%]  (Warmup)\nChain 1: Iteration:  560 / 2000 [ 28%]  (Warmup)\nChain 1: Iteration:  561 / 2000 [ 28%]  (Warmup)\nChain 1: Iteration:  562 / 2000 [ 28%]  (Warmup)\nChain 1: Iteration:  563 / 2000 [ 28%]  (Warmup)\nChain 1: Iteration:  564 / 2000 [ 28%]  (Warmup)\nChain 1: Iteration:  565 / 2000 [ 28%]  (Warmup)\nChain 1: Iteration:  566 / 2000 [ 28%]  (Warmup)\nChain 1: Iteration:  567 / 2000 [ 28%]  (Warmup)\nChain 1: Iteration:  568 / 2000 [ 28%]  (Warmup)\nChain 1: Iteration:  569 / 2000 [ 28%]  (Warmup)\nChain 1: Iteration:  570 / 2000 [ 28%]  (Warmup)\nChain 1: Iteration:  571 / 2000 [ 28%]  (Warmup)\nChain 1: Iteration:  572 / 2000 [ 28%]  (Warmup)\nChain 1: Iteration:  573 / 2000 [ 28%]  (Warmup)\nChain 1: Iteration:  574 / 2000 [ 28%]  (Warmup)\nChain 1: Iteration:  575 / 2000 [ 28%]  (Warmup)\nChain 1: Iteration:  576 / 2000 [ 28%]  (Warmup)\nChain 1: Iteration:  577 / 2000 [ 28%]  (Warmup)\nChain 1: Iteration:  578 / 2000 [ 28%]  (Warmup)\nChain 1: Iteration:  579 / 2000 [ 28%]  (Warmup)\nChain 1: Iteration:  580 / 2000 [ 29%]  (Warmup)\nChain 1: Iteration:  581 / 2000 [ 29%]  (Warmup)\nChain 1: Iteration:  582 / 2000 [ 29%]  (Warmup)\nChain 1: Iteration:  583 / 2000 [ 29%]  (Warmup)\nChain 1: Iteration:  584 / 2000 [ 29%]  (Warmup)\nChain 1: Iteration:  585 / 2000 [ 29%]  (Warmup)\nChain 1: Iteration:  586 / 2000 [ 29%]  (Warmup)\nChain 1: Iteration:  587 / 2000 [ 29%]  (Warmup)\nChain 1: Iteration:  588 / 2000 [ 29%]  (Warmup)\nChain 1: Iteration:  589 / 2000 [ 29%]  (Warmup)\nChain 1: Iteration:  590 / 2000 [ 29%]  (Warmup)\nChain 1: Iteration:  591 / 2000 [ 29%]  (Warmup)\nChain 1: Iteration:  592 / 2000 [ 29%]  (Warmup)\nChain 1: Iteration:  593 / 2000 [ 29%]  (Warmup)\nChain 1: Iteration:  594 / 2000 [ 29%]  (Warmup)\nChain 1: Iteration:  595 / 2000 [ 29%]  (Warmup)\nChain 1: Iteration:  596 / 2000 [ 29%]  (Warmup)\nChain 1: Iteration:  597 / 2000 [ 29%]  (Warmup)\nChain 1: Iteration:  598 / 2000 [ 29%]  (Warmup)\nChain 1: Iteration:  599 / 2000 [ 29%]  (Warmup)\nChain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 1: Iteration:  601 / 2000 [ 30%]  (Warmup)\nChain 1: Iteration:  602 / 2000 [ 30%]  (Warmup)\nChain 1: Iteration:  603 / 2000 [ 30%]  (Warmup)\nChain 1: Iteration:  604 / 2000 [ 30%]  (Warmup)\nChain 1: Iteration:  605 / 2000 [ 30%]  (Warmup)\nChain 1: Iteration:  606 / 2000 [ 30%]  (Warmup)\nChain 1: Iteration:  607 / 2000 [ 30%]  (Warmup)\nChain 1: Iteration:  608 / 2000 [ 30%]  (Warmup)\nChain 1: Iteration:  609 / 2000 [ 30%]  (Warmup)\nChain 1: Iteration:  610 / 2000 [ 30%]  (Warmup)\nChain 1: Iteration:  611 / 2000 [ 30%]  (Warmup)\nChain 1: Iteration:  612 / 2000 [ 30%]  (Warmup)\nChain 1: Iteration:  613 / 2000 [ 30%]  (Warmup)\nChain 1: Iteration:  614 / 2000 [ 30%]  (Warmup)\nChain 1: Iteration:  615 / 2000 [ 30%]  (Warmup)\nChain 1: Iteration:  616 / 2000 [ 30%]  (Warmup)\nChain 1: Iteration:  617 / 2000 [ 30%]  (Warmup)\nChain 1: Iteration:  618 / 2000 [ 30%]  (Warmup)\nChain 1: Iteration:  619 / 2000 [ 30%]  (Warmup)\nChain 1: Iteration:  620 / 2000 [ 31%]  (Warmup)\nChain 1: Iteration:  621 / 2000 [ 31%]  (Warmup)\nChain 1: Iteration:  622 / 2000 [ 31%]  (Warmup)\nChain 1: Iteration:  623 / 2000 [ 31%]  (Warmup)\nChain 1: Iteration:  624 / 2000 [ 31%]  (Warmup)\nChain 1: Iteration:  625 / 2000 [ 31%]  (Warmup)\nChain 1: Iteration:  626 / 2000 [ 31%]  (Warmup)\nChain 1: Iteration:  627 / 2000 [ 31%]  (Warmup)\nChain 1: Iteration:  628 / 2000 [ 31%]  (Warmup)\nChain 1: Iteration:  629 / 2000 [ 31%]  (Warmup)\nChain 1: Iteration:  630 / 2000 [ 31%]  (Warmup)\nChain 1: Iteration:  631 / 2000 [ 31%]  (Warmup)\nChain 1: Iteration:  632 / 2000 [ 31%]  (Warmup)\nChain 1: Iteration:  633 / 2000 [ 31%]  (Warmup)\nChain 1: Iteration:  634 / 2000 [ 31%]  (Warmup)\nChain 1: Iteration:  635 / 2000 [ 31%]  (Warmup)\nChain 1: Iteration:  636 / 2000 [ 31%]  (Warmup)\nChain 1: Iteration:  637 / 2000 [ 31%]  (Warmup)\nChain 1: Iteration:  638 / 2000 [ 31%]  (Warmup)\nChain 1: Iteration:  639 / 2000 [ 31%]  (Warmup)\nChain 1: Iteration:  640 / 2000 [ 32%]  (Warmup)\nChain 1: Iteration:  641 / 2000 [ 32%]  (Warmup)\nChain 1: Iteration:  642 / 2000 [ 32%]  (Warmup)\nChain 1: Iteration:  643 / 2000 [ 32%]  (Warmup)\nChain 1: Iteration:  644 / 2000 [ 32%]  (Warmup)\nChain 1: Iteration:  645 / 2000 [ 32%]  (Warmup)\nChain 1: Iteration:  646 / 2000 [ 32%]  (Warmup)\nChain 1: Iteration:  647 / 2000 [ 32%]  (Warmup)\nChain 1: Iteration:  648 / 2000 [ 32%]  (Warmup)\nChain 1: Iteration:  649 / 2000 [ 32%]  (Warmup)\nChain 1: Iteration:  650 / 2000 [ 32%]  (Warmup)\nChain 1: Iteration:  651 / 2000 [ 32%]  (Warmup)\nChain 1: Iteration:  652 / 2000 [ 32%]  (Warmup)\nChain 1: Iteration:  653 / 2000 [ 32%]  (Warmup)\nChain 1: Iteration:  654 / 2000 [ 32%]  (Warmup)\nChain 1: Iteration:  655 / 2000 [ 32%]  (Warmup)\nChain 1: Iteration:  656 / 2000 [ 32%]  (Warmup)\nChain 1: Iteration:  657 / 2000 [ 32%]  (Warmup)\nChain 1: Iteration:  658 / 2000 [ 32%]  (Warmup)\nChain 1: Iteration:  659 / 2000 [ 32%]  (Warmup)\nChain 1: Iteration:  660 / 2000 [ 33%]  (Warmup)\nChain 1: Iteration:  661 / 2000 [ 33%]  (Warmup)\nChain 1: Iteration:  662 / 2000 [ 33%]  (Warmup)\nChain 1: Iteration:  663 / 2000 [ 33%]  (Warmup)\nChain 1: Iteration:  664 / 2000 [ 33%]  (Warmup)\nChain 1: Iteration:  665 / 2000 [ 33%]  (Warmup)\nChain 1: Iteration:  666 / 2000 [ 33%]  (Warmup)\nChain 1: Iteration:  667 / 2000 [ 33%]  (Warmup)\nChain 1: Iteration:  668 / 2000 [ 33%]  (Warmup)\nChain 1: Iteration:  669 / 2000 [ 33%]  (Warmup)\nChain 1: Iteration:  670 / 2000 [ 33%]  (Warmup)\nChain 1: Iteration:  671 / 2000 [ 33%]  (Warmup)\nChain 1: Iteration:  672 / 2000 [ 33%]  (Warmup)\nChain 1: Iteration:  673 / 2000 [ 33%]  (Warmup)\nChain 1: Iteration:  674 / 2000 [ 33%]  (Warmup)\nChain 1: Iteration:  675 / 2000 [ 33%]  (Warmup)\nChain 1: Iteration:  676 / 2000 [ 33%]  (Warmup)\nChain 1: Iteration:  677 / 2000 [ 33%]  (Warmup)\nChain 1: Iteration:  678 / 2000 [ 33%]  (Warmup)\nChain 1: Iteration:  679 / 2000 [ 33%]  (Warmup)\nChain 1: Iteration:  680 / 2000 [ 34%]  (Warmup)\nChain 1: Iteration:  681 / 2000 [ 34%]  (Warmup)\nChain 1: Iteration:  682 / 2000 [ 34%]  (Warmup)\nChain 1: Iteration:  683 / 2000 [ 34%]  (Warmup)\nChain 1: Iteration:  684 / 2000 [ 34%]  (Warmup)\nChain 1: Iteration:  685 / 2000 [ 34%]  (Warmup)\nChain 1: Iteration:  686 / 2000 [ 34%]  (Warmup)\nChain 1: Iteration:  687 / 2000 [ 34%]  (Warmup)\nChain 1: Iteration:  688 / 2000 [ 34%]  (Warmup)\nChain 1: Iteration:  689 / 2000 [ 34%]  (Warmup)\nChain 1: Iteration:  690 / 2000 [ 34%]  (Warmup)\nChain 1: Iteration:  691 / 2000 [ 34%]  (Warmup)\nChain 1: Iteration:  692 / 2000 [ 34%]  (Warmup)\nChain 1: Iteration:  693 / 2000 [ 34%]  (Warmup)\nChain 1: Iteration:  694 / 2000 [ 34%]  (Warmup)\nChain 1: Iteration:  695 / 2000 [ 34%]  (Warmup)\nChain 1: Iteration:  696 / 2000 [ 34%]  (Warmup)\nChain 1: Iteration:  697 / 2000 [ 34%]  (Warmup)\nChain 1: Iteration:  698 / 2000 [ 34%]  (Warmup)\nChain 1: Iteration:  699 / 2000 [ 34%]  (Warmup)\nChain 1: Iteration:  700 / 2000 [ 35%]  (Warmup)\nChain 1: Iteration:  701 / 2000 [ 35%]  (Warmup)\nChain 1: Iteration:  702 / 2000 [ 35%]  (Warmup)\nChain 1: Iteration:  703 / 2000 [ 35%]  (Warmup)\nChain 1: Iteration:  704 / 2000 [ 35%]  (Warmup)\nChain 1: Iteration:  705 / 2000 [ 35%]  (Warmup)\nChain 1: Iteration:  706 / 2000 [ 35%]  (Warmup)\nChain 1: Iteration:  707 / 2000 [ 35%]  (Warmup)\nChain 1: Iteration:  708 / 2000 [ 35%]  (Warmup)\nChain 1: Iteration:  709 / 2000 [ 35%]  (Warmup)\nChain 1: Iteration:  710 / 2000 [ 35%]  (Warmup)\nChain 1: Iteration:  711 / 2000 [ 35%]  (Warmup)\nChain 1: Iteration:  712 / 2000 [ 35%]  (Warmup)\nChain 1: Iteration:  713 / 2000 [ 35%]  (Warmup)\nChain 1: Iteration:  714 / 2000 [ 35%]  (Warmup)\nChain 1: Iteration:  715 / 2000 [ 35%]  (Warmup)\nChain 1: Iteration:  716 / 2000 [ 35%]  (Warmup)\nChain 1: Iteration:  717 / 2000 [ 35%]  (Warmup)\nChain 1: Iteration:  718 / 2000 [ 35%]  (Warmup)\nChain 1: Iteration:  719 / 2000 [ 35%]  (Warmup)\nChain 1: Iteration:  720 / 2000 [ 36%]  (Warmup)\nChain 1: Iteration:  721 / 2000 [ 36%]  (Warmup)\nChain 1: Iteration:  722 / 2000 [ 36%]  (Warmup)\nChain 1: Iteration:  723 / 2000 [ 36%]  (Warmup)\nChain 1: Iteration:  724 / 2000 [ 36%]  (Warmup)\nChain 1: Iteration:  725 / 2000 [ 36%]  (Warmup)\nChain 1: Iteration:  726 / 2000 [ 36%]  (Warmup)\nChain 1: Iteration:  727 / 2000 [ 36%]  (Warmup)\nChain 1: Iteration:  728 / 2000 [ 36%]  (Warmup)\nChain 1: Iteration:  729 / 2000 [ 36%]  (Warmup)\nChain 1: Iteration:  730 / 2000 [ 36%]  (Warmup)\nChain 1: Iteration:  731 / 2000 [ 36%]  (Warmup)\nChain 1: Iteration:  732 / 2000 [ 36%]  (Warmup)\nChain 1: Iteration:  733 / 2000 [ 36%]  (Warmup)\nChain 1: Iteration:  734 / 2000 [ 36%]  (Warmup)\nChain 1: Iteration:  735 / 2000 [ 36%]  (Warmup)\nChain 1: Iteration:  736 / 2000 [ 36%]  (Warmup)\nChain 1: Iteration:  737 / 2000 [ 36%]  (Warmup)\nChain 1: Iteration:  738 / 2000 [ 36%]  (Warmup)\nChain 1: Iteration:  739 / 2000 [ 36%]  (Warmup)\nChain 1: Iteration:  740 / 2000 [ 37%]  (Warmup)\nChain 1: Iteration:  741 / 2000 [ 37%]  (Warmup)\nChain 1: Iteration:  742 / 2000 [ 37%]  (Warmup)\nChain 1: Iteration:  743 / 2000 [ 37%]  (Warmup)\nChain 1: Iteration:  744 / 2000 [ 37%]  (Warmup)\nChain 1: Iteration:  745 / 2000 [ 37%]  (Warmup)\nChain 1: Iteration:  746 / 2000 [ 37%]  (Warmup)\nChain 1: Iteration:  747 / 2000 [ 37%]  (Warmup)\nChain 1: Iteration:  748 / 2000 [ 37%]  (Warmup)\nChain 1: Iteration:  749 / 2000 [ 37%]  (Warmup)\nChain 1: Iteration:  750 / 2000 [ 37%]  (Warmup)\nChain 1: Iteration:  751 / 2000 [ 37%]  (Warmup)\nChain 1: Iteration:  752 / 2000 [ 37%]  (Warmup)\nChain 1: Iteration:  753 / 2000 [ 37%]  (Warmup)\nChain 1: Iteration:  754 / 2000 [ 37%]  (Warmup)\nChain 1: Iteration:  755 / 2000 [ 37%]  (Warmup)\nChain 1: Iteration:  756 / 2000 [ 37%]  (Warmup)\nChain 1: Iteration:  757 / 2000 [ 37%]  (Warmup)\nChain 1: Iteration:  758 / 2000 [ 37%]  (Warmup)\nChain 1: Iteration:  759 / 2000 [ 37%]  (Warmup)\nChain 1: Iteration:  760 / 2000 [ 38%]  (Warmup)\nChain 1: Iteration:  761 / 2000 [ 38%]  (Warmup)\nChain 1: Iteration:  762 / 2000 [ 38%]  (Warmup)\nChain 1: Iteration:  763 / 2000 [ 38%]  (Warmup)\nChain 1: Iteration:  764 / 2000 [ 38%]  (Warmup)\nChain 1: Iteration:  765 / 2000 [ 38%]  (Warmup)\nChain 1: Iteration:  766 / 2000 [ 38%]  (Warmup)\nChain 1: Iteration:  767 / 2000 [ 38%]  (Warmup)\nChain 1: Iteration:  768 / 2000 [ 38%]  (Warmup)\nChain 1: Iteration:  769 / 2000 [ 38%]  (Warmup)\nChain 1: Iteration:  770 / 2000 [ 38%]  (Warmup)\nChain 1: Iteration:  771 / 2000 [ 38%]  (Warmup)\nChain 1: Iteration:  772 / 2000 [ 38%]  (Warmup)\nChain 1: Iteration:  773 / 2000 [ 38%]  (Warmup)\nChain 1: Iteration:  774 / 2000 [ 38%]  (Warmup)\nChain 1: Iteration:  775 / 2000 [ 38%]  (Warmup)\nChain 1: Iteration:  776 / 2000 [ 38%]  (Warmup)\nChain 1: Iteration:  777 / 2000 [ 38%]  (Warmup)\nChain 1: Iteration:  778 / 2000 [ 38%]  (Warmup)\nChain 1: Iteration:  779 / 2000 [ 38%]  (Warmup)\nChain 1: Iteration:  780 / 2000 [ 39%]  (Warmup)\nChain 1: Iteration:  781 / 2000 [ 39%]  (Warmup)\nChain 1: Iteration:  782 / 2000 [ 39%]  (Warmup)\nChain 1: Iteration:  783 / 2000 [ 39%]  (Warmup)\nChain 1: Iteration:  784 / 2000 [ 39%]  (Warmup)\nChain 1: Iteration:  785 / 2000 [ 39%]  (Warmup)\nChain 1: Iteration:  786 / 2000 [ 39%]  (Warmup)\nChain 1: Iteration:  787 / 2000 [ 39%]  (Warmup)\nChain 1: Iteration:  788 / 2000 [ 39%]  (Warmup)\nChain 1: Iteration:  789 / 2000 [ 39%]  (Warmup)\nChain 1: Iteration:  790 / 2000 [ 39%]  (Warmup)\nChain 1: Iteration:  791 / 2000 [ 39%]  (Warmup)\nChain 1: Iteration:  792 / 2000 [ 39%]  (Warmup)\nChain 1: Iteration:  793 / 2000 [ 39%]  (Warmup)\nChain 1: Iteration:  794 / 2000 [ 39%]  (Warmup)\nChain 1: Iteration:  795 / 2000 [ 39%]  (Warmup)\nChain 1: Iteration:  796 / 2000 [ 39%]  (Warmup)\nChain 1: Iteration:  797 / 2000 [ 39%]  (Warmup)\nChain 1: Iteration:  798 / 2000 [ 39%]  (Warmup)\nChain 1: Iteration:  799 / 2000 [ 39%]  (Warmup)\nChain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 1: Iteration:  801 / 2000 [ 40%]  (Warmup)\nChain 1: Iteration:  802 / 2000 [ 40%]  (Warmup)\nChain 1: Iteration:  803 / 2000 [ 40%]  (Warmup)\nChain 1: Iteration:  804 / 2000 [ 40%]  (Warmup)\nChain 1: Iteration:  805 / 2000 [ 40%]  (Warmup)\nChain 1: Iteration:  806 / 2000 [ 40%]  (Warmup)\nChain 1: Iteration:  807 / 2000 [ 40%]  (Warmup)\nChain 1: Iteration:  808 / 2000 [ 40%]  (Warmup)\nChain 1: Iteration:  809 / 2000 [ 40%]  (Warmup)\nChain 1: Iteration:  810 / 2000 [ 40%]  (Warmup)\nChain 1: Iteration:  811 / 2000 [ 40%]  (Warmup)\nChain 1: Iteration:  812 / 2000 [ 40%]  (Warmup)\nChain 1: Iteration:  813 / 2000 [ 40%]  (Warmup)\nChain 1: Iteration:  814 / 2000 [ 40%]  (Warmup)\nChain 1: Iteration:  815 / 2000 [ 40%]  (Warmup)\nChain 1: Iteration:  816 / 2000 [ 40%]  (Warmup)\nChain 1: Iteration:  817 / 2000 [ 40%]  (Warmup)\nChain 1: Iteration:  818 / 2000 [ 40%]  (Warmup)\nChain 1: Iteration:  819 / 2000 [ 40%]  (Warmup)\nChain 1: Iteration:  820 / 2000 [ 41%]  (Warmup)\nChain 1: Iteration:  821 / 2000 [ 41%]  (Warmup)\nChain 1: Iteration:  822 / 2000 [ 41%]  (Warmup)\nChain 1: Iteration:  823 / 2000 [ 41%]  (Warmup)\nChain 1: Iteration:  824 / 2000 [ 41%]  (Warmup)\nChain 1: Iteration:  825 / 2000 [ 41%]  (Warmup)\nChain 1: Iteration:  826 / 2000 [ 41%]  (Warmup)\nChain 1: Iteration:  827 / 2000 [ 41%]  (Warmup)\nChain 1: Iteration:  828 / 2000 [ 41%]  (Warmup)\nChain 1: Iteration:  829 / 2000 [ 41%]  (Warmup)\nChain 1: Iteration:  830 / 2000 [ 41%]  (Warmup)\nChain 1: Iteration:  831 / 2000 [ 41%]  (Warmup)\nChain 1: Iteration:  832 / 2000 [ 41%]  (Warmup)\nChain 1: Iteration:  833 / 2000 [ 41%]  (Warmup)\nChain 1: Iteration:  834 / 2000 [ 41%]  (Warmup)\nChain 1: Iteration:  835 / 2000 [ 41%]  (Warmup)\nChain 1: Iteration:  836 / 2000 [ 41%]  (Warmup)\nChain 1: Iteration:  837 / 2000 [ 41%]  (Warmup)\nChain 1: Iteration:  838 / 2000 [ 41%]  (Warmup)\nChain 1: Iteration:  839 / 2000 [ 41%]  (Warmup)\nChain 1: Iteration:  840 / 2000 [ 42%]  (Warmup)\nChain 1: Iteration:  841 / 2000 [ 42%]  (Warmup)\nChain 1: Iteration:  842 / 2000 [ 42%]  (Warmup)\nChain 1: Iteration:  843 / 2000 [ 42%]  (Warmup)\nChain 1: Iteration:  844 / 2000 [ 42%]  (Warmup)\nChain 1: Iteration:  845 / 2000 [ 42%]  (Warmup)\nChain 1: Iteration:  846 / 2000 [ 42%]  (Warmup)\nChain 1: Iteration:  847 / 2000 [ 42%]  (Warmup)\nChain 1: Iteration:  848 / 2000 [ 42%]  (Warmup)\nChain 1: Iteration:  849 / 2000 [ 42%]  (Warmup)\nChain 1: Iteration:  850 / 2000 [ 42%]  (Warmup)\nChain 1: Iteration:  851 / 2000 [ 42%]  (Warmup)\nChain 1: Iteration:  852 / 2000 [ 42%]  (Warmup)\nChain 1: Iteration:  853 / 2000 [ 42%]  (Warmup)\nChain 1: Iteration:  854 / 2000 [ 42%]  (Warmup)\nChain 1: Iteration:  855 / 2000 [ 42%]  (Warmup)\nChain 1: Iteration:  856 / 2000 [ 42%]  (Warmup)\nChain 1: Iteration:  857 / 2000 [ 42%]  (Warmup)\nChain 1: Iteration:  858 / 2000 [ 42%]  (Warmup)\nChain 1: Iteration:  859 / 2000 [ 42%]  (Warmup)\nChain 1: Iteration:  860 / 2000 [ 43%]  (Warmup)\nChain 1: Iteration:  861 / 2000 [ 43%]  (Warmup)\nChain 1: Iteration:  862 / 2000 [ 43%]  (Warmup)\nChain 1: Iteration:  863 / 2000 [ 43%]  (Warmup)\nChain 1: Iteration:  864 / 2000 [ 43%]  (Warmup)\nChain 1: Iteration:  865 / 2000 [ 43%]  (Warmup)\nChain 1: Iteration:  866 / 2000 [ 43%]  (Warmup)\nChain 1: Iteration:  867 / 2000 [ 43%]  (Warmup)\nChain 1: Iteration:  868 / 2000 [ 43%]  (Warmup)\nChain 1: Iteration:  869 / 2000 [ 43%]  (Warmup)\nChain 1: Iteration:  870 / 2000 [ 43%]  (Warmup)\nChain 1: Iteration:  871 / 2000 [ 43%]  (Warmup)\nChain 1: Iteration:  872 / 2000 [ 43%]  (Warmup)\nChain 1: Iteration:  873 / 2000 [ 43%]  (Warmup)\nChain 1: Iteration:  874 / 2000 [ 43%]  (Warmup)\nChain 1: Iteration:  875 / 2000 [ 43%]  (Warmup)\nChain 1: Iteration:  876 / 2000 [ 43%]  (Warmup)\nChain 1: Iteration:  877 / 2000 [ 43%]  (Warmup)\nChain 1: Iteration:  878 / 2000 [ 43%]  (Warmup)\nChain 1: Iteration:  879 / 2000 [ 43%]  (Warmup)\nChain 1: Iteration:  880 / 2000 [ 44%]  (Warmup)\nChain 1: Iteration:  881 / 2000 [ 44%]  (Warmup)\nChain 1: Iteration:  882 / 2000 [ 44%]  (Warmup)\nChain 1: Iteration:  883 / 2000 [ 44%]  (Warmup)\nChain 1: Iteration:  884 / 2000 [ 44%]  (Warmup)\nChain 1: Iteration:  885 / 2000 [ 44%]  (Warmup)\nChain 1: Iteration:  886 / 2000 [ 44%]  (Warmup)\nChain 1: Iteration:  887 / 2000 [ 44%]  (Warmup)\nChain 1: Iteration:  888 / 2000 [ 44%]  (Warmup)\nChain 1: Iteration:  889 / 2000 [ 44%]  (Warmup)\nChain 1: Iteration:  890 / 2000 [ 44%]  (Warmup)\nChain 1: Iteration:  891 / 2000 [ 44%]  (Warmup)\nChain 1: Iteration:  892 / 2000 [ 44%]  (Warmup)\nChain 1: Iteration:  893 / 2000 [ 44%]  (Warmup)\nChain 1: Iteration:  894 / 2000 [ 44%]  (Warmup)\nChain 1: Iteration:  895 / 2000 [ 44%]  (Warmup)\nChain 1: Iteration:  896 / 2000 [ 44%]  (Warmup)\nChain 1: Iteration:  897 / 2000 [ 44%]  (Warmup)\nChain 1: Iteration:  898 / 2000 [ 44%]  (Warmup)\nChain 1: Iteration:  899 / 2000 [ 44%]  (Warmup)\nChain 1: Iteration:  900 / 2000 [ 45%]  (Warmup)\nChain 1: Iteration:  901 / 2000 [ 45%]  (Warmup)\nChain 1: Iteration:  902 / 2000 [ 45%]  (Warmup)\nChain 1: Iteration:  903 / 2000 [ 45%]  (Warmup)\nChain 1: Iteration:  904 / 2000 [ 45%]  (Warmup)\nChain 1: Iteration:  905 / 2000 [ 45%]  (Warmup)\nChain 1: Iteration:  906 / 2000 [ 45%]  (Warmup)\nChain 1: Iteration:  907 / 2000 [ 45%]  (Warmup)\nChain 1: Iteration:  908 / 2000 [ 45%]  (Warmup)\nChain 1: Iteration:  909 / 2000 [ 45%]  (Warmup)\nChain 1: Iteration:  910 / 2000 [ 45%]  (Warmup)\nChain 1: Iteration:  911 / 2000 [ 45%]  (Warmup)\nChain 1: Iteration:  912 / 2000 [ 45%]  (Warmup)\nChain 1: Iteration:  913 / 2000 [ 45%]  (Warmup)\nChain 1: Iteration:  914 / 2000 [ 45%]  (Warmup)\nChain 1: Iteration:  915 / 2000 [ 45%]  (Warmup)\nChain 1: Iteration:  916 / 2000 [ 45%]  (Warmup)\nChain 1: Iteration:  917 / 2000 [ 45%]  (Warmup)\nChain 1: Iteration:  918 / 2000 [ 45%]  (Warmup)\nChain 1: Iteration:  919 / 2000 [ 45%]  (Warmup)\nChain 1: Iteration:  920 / 2000 [ 46%]  (Warmup)\nChain 1: Iteration:  921 / 2000 [ 46%]  (Warmup)\nChain 1: Iteration:  922 / 2000 [ 46%]  (Warmup)\nChain 1: Iteration:  923 / 2000 [ 46%]  (Warmup)\nChain 1: Iteration:  924 / 2000 [ 46%]  (Warmup)\nChain 1: Iteration:  925 / 2000 [ 46%]  (Warmup)\nChain 1: Iteration:  926 / 2000 [ 46%]  (Warmup)\nChain 1: Iteration:  927 / 2000 [ 46%]  (Warmup)\nChain 1: Iteration:  928 / 2000 [ 46%]  (Warmup)\nChain 1: Iteration:  929 / 2000 [ 46%]  (Warmup)\nChain 1: Iteration:  930 / 2000 [ 46%]  (Warmup)\nChain 1: Iteration:  931 / 2000 [ 46%]  (Warmup)\nChain 1: Iteration:  932 / 2000 [ 46%]  (Warmup)\nChain 1: Iteration:  933 / 2000 [ 46%]  (Warmup)\nChain 1: Iteration:  934 / 2000 [ 46%]  (Warmup)\nChain 1: Iteration:  935 / 2000 [ 46%]  (Warmup)\nChain 1: Iteration:  936 / 2000 [ 46%]  (Warmup)\nChain 1: Iteration:  937 / 2000 [ 46%]  (Warmup)\nChain 1: Iteration:  938 / 2000 [ 46%]  (Warmup)\nChain 1: Iteration:  939 / 2000 [ 46%]  (Warmup)\nChain 1: Iteration:  940 / 2000 [ 47%]  (Warmup)\nChain 1: Iteration:  941 / 2000 [ 47%]  (Warmup)\nChain 1: Iteration:  942 / 2000 [ 47%]  (Warmup)\nChain 1: Iteration:  943 / 2000 [ 47%]  (Warmup)\nChain 1: Iteration:  944 / 2000 [ 47%]  (Warmup)\nChain 1: Iteration:  945 / 2000 [ 47%]  (Warmup)\nChain 1: Iteration:  946 / 2000 [ 47%]  (Warmup)\nChain 1: Iteration:  947 / 2000 [ 47%]  (Warmup)\nChain 1: Iteration:  948 / 2000 [ 47%]  (Warmup)\nChain 1: Iteration:  949 / 2000 [ 47%]  (Warmup)\nChain 1: Iteration:  950 / 2000 [ 47%]  (Warmup)\nChain 1: Iteration:  951 / 2000 [ 47%]  (Warmup)\nChain 1: Iteration:  952 / 2000 [ 47%]  (Warmup)\nChain 1: Iteration:  953 / 2000 [ 47%]  (Warmup)\nChain 1: Iteration:  954 / 2000 [ 47%]  (Warmup)\nChain 1: Iteration:  955 / 2000 [ 47%]  (Warmup)\nChain 1: Iteration:  956 / 2000 [ 47%]  (Warmup)\nChain 1: Iteration:  957 / 2000 [ 47%]  (Warmup)\nChain 1: Iteration:  958 / 2000 [ 47%]  (Warmup)\nChain 1: Iteration:  959 / 2000 [ 47%]  (Warmup)\nChain 1: Iteration:  960 / 2000 [ 48%]  (Warmup)\nChain 1: Iteration:  961 / 2000 [ 48%]  (Warmup)\nChain 1: Iteration:  962 / 2000 [ 48%]  (Warmup)\nChain 1: Iteration:  963 / 2000 [ 48%]  (Warmup)\nChain 1: Iteration:  964 / 2000 [ 48%]  (Warmup)\nChain 1: Iteration:  965 / 2000 [ 48%]  (Warmup)\nChain 1: Iteration:  966 / 2000 [ 48%]  (Warmup)\nChain 1: Iteration:  967 / 2000 [ 48%]  (Warmup)\nChain 1: Iteration:  968 / 2000 [ 48%]  (Warmup)\nChain 1: Iteration:  969 / 2000 [ 48%]  (Warmup)\nChain 1: Iteration:  970 / 2000 [ 48%]  (Warmup)\nChain 1: Iteration:  971 / 2000 [ 48%]  (Warmup)\nChain 1: Iteration:  972 / 2000 [ 48%]  (Warmup)\nChain 1: Iteration:  973 / 2000 [ 48%]  (Warmup)\nChain 1: Iteration:  974 / 2000 [ 48%]  (Warmup)\nChain 1: Iteration:  975 / 2000 [ 48%]  (Warmup)\nChain 1: Iteration:  976 / 2000 [ 48%]  (Warmup)\nChain 1: Iteration:  977 / 2000 [ 48%]  (Warmup)\nChain 1: Iteration:  978 / 2000 [ 48%]  (Warmup)\nChain 1: Iteration:  979 / 2000 [ 48%]  (Warmup)\nChain 1: Iteration:  980 / 2000 [ 49%]  (Warmup)\nChain 1: Iteration:  981 / 2000 [ 49%]  (Warmup)\nChain 1: Iteration:  982 / 2000 [ 49%]  (Warmup)\nChain 1: Iteration:  983 / 2000 [ 49%]  (Warmup)\nChain 1: Iteration:  984 / 2000 [ 49%]  (Warmup)\nChain 1: Iteration:  985 / 2000 [ 49%]  (Warmup)\nChain 1: Iteration:  986 / 2000 [ 49%]  (Warmup)\nChain 1: Iteration:  987 / 2000 [ 49%]  (Warmup)\nChain 1: Iteration:  988 / 2000 [ 49%]  (Warmup)\nChain 1: Iteration:  989 / 2000 [ 49%]  (Warmup)\nChain 1: Iteration:  990 / 2000 [ 49%]  (Warmup)\nChain 1: Iteration:  991 / 2000 [ 49%]  (Warmup)\nChain 1: Iteration:  992 / 2000 [ 49%]  (Warmup)\nChain 1: Iteration:  993 / 2000 [ 49%]  (Warmup)\nChain 1: Iteration:  994 / 2000 [ 49%]  (Warmup)\nChain 1: Iteration:  995 / 2000 [ 49%]  (Warmup)\nChain 1: Iteration:  996 / 2000 [ 49%]  (Warmup)\nChain 1: Iteration:  997 / 2000 [ 49%]  (Warmup)\nChain 1: Iteration:  998 / 2000 [ 49%]  (Warmup)\nChain 1: Iteration:  999 / 2000 [ 49%]  (Warmup)\nChain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 1: Iteration: 1002 / 2000 [ 50%]  (Sampling)\nChain 1: Iteration: 1003 / 2000 [ 50%]  (Sampling)\nChain 1: Iteration: 1004 / 2000 [ 50%]  (Sampling)\nChain 1: Iteration: 1005 / 2000 [ 50%]  (Sampling)\nChain 1: Iteration: 1006 / 2000 [ 50%]  (Sampling)\nChain 1: Iteration: 1007 / 2000 [ 50%]  (Sampling)\nChain 1: Iteration: 1008 / 2000 [ 50%]  (Sampling)\nChain 1: Iteration: 1009 / 2000 [ 50%]  (Sampling)\nChain 1: Iteration: 1010 / 2000 [ 50%]  (Sampling)\nChain 1: Iteration: 1011 / 2000 [ 50%]  (Sampling)\nChain 1: Iteration: 1012 / 2000 [ 50%]  (Sampling)\nChain 1: Iteration: 1013 / 2000 [ 50%]  (Sampling)\nChain 1: Iteration: 1014 / 2000 [ 50%]  (Sampling)\nChain 1: Iteration: 1015 / 2000 [ 50%]  (Sampling)\nChain 1: Iteration: 1016 / 2000 [ 50%]  (Sampling)\nChain 1: Iteration: 1017 / 2000 [ 50%]  (Sampling)\nChain 1: Iteration: 1018 / 2000 [ 50%]  (Sampling)\nChain 1: Iteration: 1019 / 2000 [ 50%]  (Sampling)\nChain 1: Iteration: 1020 / 2000 [ 51%]  (Sampling)\nChain 1: Iteration: 1021 / 2000 [ 51%]  (Sampling)\nChain 1: Iteration: 1022 / 2000 [ 51%]  (Sampling)\nChain 1: Iteration: 1023 / 2000 [ 51%]  (Sampling)\nChain 1: Iteration: 1024 / 2000 [ 51%]  (Sampling)\nChain 1: Iteration: 1025 / 2000 [ 51%]  (Sampling)\nChain 1: Iteration: 1026 / 2000 [ 51%]  (Sampling)\nChain 1: Iteration: 1027 / 2000 [ 51%]  (Sampling)\nChain 1: Iteration: 1028 / 2000 [ 51%]  (Sampling)\nChain 1: Iteration: 1029 / 2000 [ 51%]  (Sampling)\nChain 1: Iteration: 1030 / 2000 [ 51%]  (Sampling)\nChain 1: Iteration: 1031 / 2000 [ 51%]  (Sampling)\nChain 1: Iteration: 1032 / 2000 [ 51%]  (Sampling)\nChain 1: Iteration: 1033 / 2000 [ 51%]  (Sampling)\nChain 1: Iteration: 1034 / 2000 [ 51%]  (Sampling)\nChain 1: Iteration: 1035 / 2000 [ 51%]  (Sampling)\nChain 1: Iteration: 1036 / 2000 [ 51%]  (Sampling)\nChain 1: Iteration: 1037 / 2000 [ 51%]  (Sampling)\nChain 1: Iteration: 1038 / 2000 [ 51%]  (Sampling)\nChain 1: Iteration: 1039 / 2000 [ 51%]  (Sampling)\nChain 1: Iteration: 1040 / 2000 [ 52%]  (Sampling)\nChain 1: Iteration: 1041 / 2000 [ 52%]  (Sampling)\nChain 1: Iteration: 1042 / 2000 [ 52%]  (Sampling)\nChain 1: Iteration: 1043 / 2000 [ 52%]  (Sampling)\nChain 1: Iteration: 1044 / 2000 [ 52%]  (Sampling)\nChain 1: Iteration: 1045 / 2000 [ 52%]  (Sampling)\nChain 1: Iteration: 1046 / 2000 [ 52%]  (Sampling)\nChain 1: Iteration: 1047 / 2000 [ 52%]  (Sampling)\nChain 1: Iteration: 1048 / 2000 [ 52%]  (Sampling)\nChain 1: Iteration: 1049 / 2000 [ 52%]  (Sampling)\nChain 1: Iteration: 1050 / 2000 [ 52%]  (Sampling)\nChain 1: Iteration: 1051 / 2000 [ 52%]  (Sampling)\nChain 1: Iteration: 1052 / 2000 [ 52%]  (Sampling)\nChain 1: Iteration: 1053 / 2000 [ 52%]  (Sampling)\nChain 1: Iteration: 1054 / 2000 [ 52%]  (Sampling)\nChain 1: Iteration: 1055 / 2000 [ 52%]  (Sampling)\nChain 1: Iteration: 1056 / 2000 [ 52%]  (Sampling)\nChain 1: Iteration: 1057 / 2000 [ 52%]  (Sampling)\nChain 1: Iteration: 1058 / 2000 [ 52%]  (Sampling)\nChain 1: Iteration: 1059 / 2000 [ 52%]  (Sampling)\nChain 1: Iteration: 1060 / 2000 [ 53%]  (Sampling)\nChain 1: Iteration: 1061 / 2000 [ 53%]  (Sampling)\nChain 1: Iteration: 1062 / 2000 [ 53%]  (Sampling)\nChain 1: Iteration: 1063 / 2000 [ 53%]  (Sampling)\nChain 1: Iteration: 1064 / 2000 [ 53%]  (Sampling)\nChain 1: Iteration: 1065 / 2000 [ 53%]  (Sampling)\nChain 1: Iteration: 1066 / 2000 [ 53%]  (Sampling)\nChain 1: Iteration: 1067 / 2000 [ 53%]  (Sampling)\nChain 1: Iteration: 1068 / 2000 [ 53%]  (Sampling)\nChain 1: Iteration: 1069 / 2000 [ 53%]  (Sampling)\nChain 1: Iteration: 1070 / 2000 [ 53%]  (Sampling)\nChain 1: Iteration: 1071 / 2000 [ 53%]  (Sampling)\nChain 1: Iteration: 1072 / 2000 [ 53%]  (Sampling)\nChain 1: Iteration: 1073 / 2000 [ 53%]  (Sampling)\nChain 1: Iteration: 1074 / 2000 [ 53%]  (Sampling)\nChain 1: Iteration: 1075 / 2000 [ 53%]  (Sampling)\nChain 1: Iteration: 1076 / 2000 [ 53%]  (Sampling)\nChain 1: Iteration: 1077 / 2000 [ 53%]  (Sampling)\nChain 1: Iteration: 1078 / 2000 [ 53%]  (Sampling)\nChain 1: Iteration: 1079 / 2000 [ 53%]  (Sampling)\nChain 1: Iteration: 1080 / 2000 [ 54%]  (Sampling)\nChain 1: Iteration: 1081 / 2000 [ 54%]  (Sampling)\nChain 1: Iteration: 1082 / 2000 [ 54%]  (Sampling)\nChain 1: Iteration: 1083 / 2000 [ 54%]  (Sampling)\nChain 1: Iteration: 1084 / 2000 [ 54%]  (Sampling)\nChain 1: Iteration: 1085 / 2000 [ 54%]  (Sampling)\nChain 1: Iteration: 1086 / 2000 [ 54%]  (Sampling)\nChain 1: Iteration: 1087 / 2000 [ 54%]  (Sampling)\nChain 1: Iteration: 1088 / 2000 [ 54%]  (Sampling)\nChain 1: Iteration: 1089 / 2000 [ 54%]  (Sampling)\nChain 1: Iteration: 1090 / 2000 [ 54%]  (Sampling)\nChain 1: Iteration: 1091 / 2000 [ 54%]  (Sampling)\nChain 1: Iteration: 1092 / 2000 [ 54%]  (Sampling)\nChain 1: Iteration: 1093 / 2000 [ 54%]  (Sampling)\nChain 1: Iteration: 1094 / 2000 [ 54%]  (Sampling)\nChain 1: Iteration: 1095 / 2000 [ 54%]  (Sampling)\nChain 1: Iteration: 1096 / 2000 [ 54%]  (Sampling)\nChain 1: Iteration: 1097 / 2000 [ 54%]  (Sampling)\nChain 1: Iteration: 1098 / 2000 [ 54%]  (Sampling)\nChain 1: Iteration: 1099 / 2000 [ 54%]  (Sampling)\nChain 1: Iteration: 1100 / 2000 [ 55%]  (Sampling)\nChain 1: Iteration: 1101 / 2000 [ 55%]  (Sampling)\nChain 1: Iteration: 1102 / 2000 [ 55%]  (Sampling)\nChain 1: Iteration: 1103 / 2000 [ 55%]  (Sampling)\nChain 1: Iteration: 1104 / 2000 [ 55%]  (Sampling)\nChain 1: Iteration: 1105 / 2000 [ 55%]  (Sampling)\nChain 1: Iteration: 1106 / 2000 [ 55%]  (Sampling)\nChain 1: Iteration: 1107 / 2000 [ 55%]  (Sampling)\nChain 1: Iteration: 1108 / 2000 [ 55%]  (Sampling)\nChain 1: Iteration: 1109 / 2000 [ 55%]  (Sampling)\nChain 1: Iteration: 1110 / 2000 [ 55%]  (Sampling)\nChain 1: Iteration: 1111 / 2000 [ 55%]  (Sampling)\nChain 1: Iteration: 1112 / 2000 [ 55%]  (Sampling)\nChain 1: Iteration: 1113 / 2000 [ 55%]  (Sampling)\nChain 1: Iteration: 1114 / 2000 [ 55%]  (Sampling)\nChain 1: Iteration: 1115 / 2000 [ 55%]  (Sampling)\nChain 1: Iteration: 1116 / 2000 [ 55%]  (Sampling)\nChain 1: Iteration: 1117 / 2000 [ 55%]  (Sampling)\nChain 1: Iteration: 1118 / 2000 [ 55%]  (Sampling)\nChain 1: Iteration: 1119 / 2000 [ 55%]  (Sampling)\nChain 1: Iteration: 1120 / 2000 [ 56%]  (Sampling)\nChain 1: Iteration: 1121 / 2000 [ 56%]  (Sampling)\nChain 1: Iteration: 1122 / 2000 [ 56%]  (Sampling)\nChain 1: Iteration: 1123 / 2000 [ 56%]  (Sampling)\nChain 1: Iteration: 1124 / 2000 [ 56%]  (Sampling)\nChain 1: Iteration: 1125 / 2000 [ 56%]  (Sampling)\nChain 1: Iteration: 1126 / 2000 [ 56%]  (Sampling)\nChain 1: Iteration: 1127 / 2000 [ 56%]  (Sampling)\nChain 1: Iteration: 1128 / 2000 [ 56%]  (Sampling)\nChain 1: Iteration: 1129 / 2000 [ 56%]  (Sampling)\nChain 1: Iteration: 1130 / 2000 [ 56%]  (Sampling)\nChain 1: Iteration: 1131 / 2000 [ 56%]  (Sampling)\nChain 1: Iteration: 1132 / 2000 [ 56%]  (Sampling)\nChain 1: Iteration: 1133 / 2000 [ 56%]  (Sampling)\nChain 1: Iteration: 1134 / 2000 [ 56%]  (Sampling)\nChain 1: Iteration: 1135 / 2000 [ 56%]  (Sampling)\nChain 1: Iteration: 1136 / 2000 [ 56%]  (Sampling)\nChain 1: Iteration: 1137 / 2000 [ 56%]  (Sampling)\nChain 1: Iteration: 1138 / 2000 [ 56%]  (Sampling)\nChain 1: Iteration: 1139 / 2000 [ 56%]  (Sampling)\nChain 1: Iteration: 1140 / 2000 [ 57%]  (Sampling)\nChain 1: Iteration: 1141 / 2000 [ 57%]  (Sampling)\nChain 1: Iteration: 1142 / 2000 [ 57%]  (Sampling)\nChain 1: Iteration: 1143 / 2000 [ 57%]  (Sampling)\nChain 1: Iteration: 1144 / 2000 [ 57%]  (Sampling)\nChain 1: Iteration: 1145 / 2000 [ 57%]  (Sampling)\nChain 1: Iteration: 1146 / 2000 [ 57%]  (Sampling)\nChain 1: Iteration: 1147 / 2000 [ 57%]  (Sampling)\nChain 1: Iteration: 1148 / 2000 [ 57%]  (Sampling)\nChain 1: Iteration: 1149 / 2000 [ 57%]  (Sampling)\nChain 1: Iteration: 1150 / 2000 [ 57%]  (Sampling)\nChain 1: Iteration: 1151 / 2000 [ 57%]  (Sampling)\nChain 1: Iteration: 1152 / 2000 [ 57%]  (Sampling)\nChain 1: Iteration: 1153 / 2000 [ 57%]  (Sampling)\nChain 1: Iteration: 1154 / 2000 [ 57%]  (Sampling)\nChain 1: Iteration: 1155 / 2000 [ 57%]  (Sampling)\nChain 1: Iteration: 1156 / 2000 [ 57%]  (Sampling)\nChain 1: Iteration: 1157 / 2000 [ 57%]  (Sampling)\nChain 1: Iteration: 1158 / 2000 [ 57%]  (Sampling)\nChain 1: Iteration: 1159 / 2000 [ 57%]  (Sampling)\nChain 1: Iteration: 1160 / 2000 [ 58%]  (Sampling)\nChain 1: Iteration: 1161 / 2000 [ 58%]  (Sampling)\nChain 1: Iteration: 1162 / 2000 [ 58%]  (Sampling)\nChain 1: Iteration: 1163 / 2000 [ 58%]  (Sampling)\nChain 1: Iteration: 1164 / 2000 [ 58%]  (Sampling)\nChain 1: Iteration: 1165 / 2000 [ 58%]  (Sampling)\nChain 1: Iteration: 1166 / 2000 [ 58%]  (Sampling)\nChain 1: Iteration: 1167 / 2000 [ 58%]  (Sampling)\nChain 1: Iteration: 1168 / 2000 [ 58%]  (Sampling)\nChain 1: Iteration: 1169 / 2000 [ 58%]  (Sampling)\nChain 1: Iteration: 1170 / 2000 [ 58%]  (Sampling)\nChain 1: Iteration: 1171 / 2000 [ 58%]  (Sampling)\nChain 1: Iteration: 1172 / 2000 [ 58%]  (Sampling)\nChain 1: Iteration: 1173 / 2000 [ 58%]  (Sampling)\nChain 1: Iteration: 1174 / 2000 [ 58%]  (Sampling)\nChain 1: Iteration: 1175 / 2000 [ 58%]  (Sampling)\nChain 1: Iteration: 1176 / 2000 [ 58%]  (Sampling)\nChain 1: Iteration: 1177 / 2000 [ 58%]  (Sampling)\nChain 1: Iteration: 1178 / 2000 [ 58%]  (Sampling)\nChain 1: Iteration: 1179 / 2000 [ 58%]  (Sampling)\nChain 1: Iteration: 1180 / 2000 [ 59%]  (Sampling)\nChain 1: Iteration: 1181 / 2000 [ 59%]  (Sampling)\nChain 1: Iteration: 1182 / 2000 [ 59%]  (Sampling)\nChain 1: Iteration: 1183 / 2000 [ 59%]  (Sampling)\nChain 1: Iteration: 1184 / 2000 [ 59%]  (Sampling)\nChain 1: Iteration: 1185 / 2000 [ 59%]  (Sampling)\nChain 1: Iteration: 1186 / 2000 [ 59%]  (Sampling)\nChain 1: Iteration: 1187 / 2000 [ 59%]  (Sampling)\nChain 1: Iteration: 1188 / 2000 [ 59%]  (Sampling)\nChain 1: Iteration: 1189 / 2000 [ 59%]  (Sampling)\nChain 1: Iteration: 1190 / 2000 [ 59%]  (Sampling)\nChain 1: Iteration: 1191 / 2000 [ 59%]  (Sampling)\nChain 1: Iteration: 1192 / 2000 [ 59%]  (Sampling)\nChain 1: Iteration: 1193 / 2000 [ 59%]  (Sampling)\nChain 1: Iteration: 1194 / 2000 [ 59%]  (Sampling)\nChain 1: Iteration: 1195 / 2000 [ 59%]  (Sampling)\nChain 1: Iteration: 1196 / 2000 [ 59%]  (Sampling)\nChain 1: Iteration: 1197 / 2000 [ 59%]  (Sampling)\nChain 1: Iteration: 1198 / 2000 [ 59%]  (Sampling)\nChain 1: Iteration: 1199 / 2000 [ 59%]  (Sampling)\nChain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 1: Iteration: 1201 / 2000 [ 60%]  (Sampling)\nChain 1: Iteration: 1202 / 2000 [ 60%]  (Sampling)\nChain 1: Iteration: 1203 / 2000 [ 60%]  (Sampling)\nChain 1: Iteration: 1204 / 2000 [ 60%]  (Sampling)\nChain 1: Iteration: 1205 / 2000 [ 60%]  (Sampling)\nChain 1: Iteration: 1206 / 2000 [ 60%]  (Sampling)\nChain 1: Iteration: 1207 / 2000 [ 60%]  (Sampling)\nChain 1: Iteration: 1208 / 2000 [ 60%]  (Sampling)\nChain 1: Iteration: 1209 / 2000 [ 60%]  (Sampling)\nChain 1: Iteration: 1210 / 2000 [ 60%]  (Sampling)\nChain 1: Iteration: 1211 / 2000 [ 60%]  (Sampling)\nChain 1: Iteration: 1212 / 2000 [ 60%]  (Sampling)\nChain 1: Iteration: 1213 / 2000 [ 60%]  (Sampling)\nChain 1: Iteration: 1214 / 2000 [ 60%]  (Sampling)\nChain 1: Iteration: 1215 / 2000 [ 60%]  (Sampling)\nChain 1: Iteration: 1216 / 2000 [ 60%]  (Sampling)\nChain 1: Iteration: 1217 / 2000 [ 60%]  (Sampling)\nChain 1: Iteration: 1218 / 2000 [ 60%]  (Sampling)\nChain 1: Iteration: 1219 / 2000 [ 60%]  (Sampling)\nChain 1: Iteration: 1220 / 2000 [ 61%]  (Sampling)\nChain 1: Iteration: 1221 / 2000 [ 61%]  (Sampling)\nChain 1: Iteration: 1222 / 2000 [ 61%]  (Sampling)\nChain 1: Iteration: 1223 / 2000 [ 61%]  (Sampling)\nChain 1: Iteration: 1224 / 2000 [ 61%]  (Sampling)\nChain 1: Iteration: 1225 / 2000 [ 61%]  (Sampling)\nChain 1: Iteration: 1226 / 2000 [ 61%]  (Sampling)\nChain 1: Iteration: 1227 / 2000 [ 61%]  (Sampling)\nChain 1: Iteration: 1228 / 2000 [ 61%]  (Sampling)\nChain 1: Iteration: 1229 / 2000 [ 61%]  (Sampling)\nChain 1: Iteration: 1230 / 2000 [ 61%]  (Sampling)\nChain 1: Iteration: 1231 / 2000 [ 61%]  (Sampling)\nChain 1: Iteration: 1232 / 2000 [ 61%]  (Sampling)\nChain 1: Iteration: 1233 / 2000 [ 61%]  (Sampling)\nChain 1: Iteration: 1234 / 2000 [ 61%]  (Sampling)\nChain 1: Iteration: 1235 / 2000 [ 61%]  (Sampling)\nChain 1: Iteration: 1236 / 2000 [ 61%]  (Sampling)\nChain 1: Iteration: 1237 / 2000 [ 61%]  (Sampling)\nChain 1: Iteration: 1238 / 2000 [ 61%]  (Sampling)\nChain 1: Iteration: 1239 / 2000 [ 61%]  (Sampling)\nChain 1: Iteration: 1240 / 2000 [ 62%]  (Sampling)\nChain 1: Iteration: 1241 / 2000 [ 62%]  (Sampling)\nChain 1: Iteration: 1242 / 2000 [ 62%]  (Sampling)\nChain 1: Iteration: 1243 / 2000 [ 62%]  (Sampling)\nChain 1: Iteration: 1244 / 2000 [ 62%]  (Sampling)\nChain 1: Iteration: 1245 / 2000 [ 62%]  (Sampling)\nChain 1: Iteration: 1246 / 2000 [ 62%]  (Sampling)\nChain 1: Iteration: 1247 / 2000 [ 62%]  (Sampling)\nChain 1: Iteration: 1248 / 2000 [ 62%]  (Sampling)\nChain 1: Iteration: 1249 / 2000 [ 62%]  (Sampling)\nChain 1: Iteration: 1250 / 2000 [ 62%]  (Sampling)\nChain 1: Iteration: 1251 / 2000 [ 62%]  (Sampling)\nChain 1: Iteration: 1252 / 2000 [ 62%]  (Sampling)\nChain 1: Iteration: 1253 / 2000 [ 62%]  (Sampling)\nChain 1: Iteration: 1254 / 2000 [ 62%]  (Sampling)\nChain 1: Iteration: 1255 / 2000 [ 62%]  (Sampling)\nChain 1: Iteration: 1256 / 2000 [ 62%]  (Sampling)\nChain 1: Iteration: 1257 / 2000 [ 62%]  (Sampling)\nChain 1: Iteration: 1258 / 2000 [ 62%]  (Sampling)\nChain 1: Iteration: 1259 / 2000 [ 62%]  (Sampling)\nChain 1: Iteration: 1260 / 2000 [ 63%]  (Sampling)\nChain 1: Iteration: 1261 / 2000 [ 63%]  (Sampling)\nChain 1: Iteration: 1262 / 2000 [ 63%]  (Sampling)\nChain 1: Iteration: 1263 / 2000 [ 63%]  (Sampling)\nChain 1: Iteration: 1264 / 2000 [ 63%]  (Sampling)\nChain 1: Iteration: 1265 / 2000 [ 63%]  (Sampling)\nChain 1: Iteration: 1266 / 2000 [ 63%]  (Sampling)\nChain 1: Iteration: 1267 / 2000 [ 63%]  (Sampling)\nChain 1: Iteration: 1268 / 2000 [ 63%]  (Sampling)\nChain 1: Iteration: 1269 / 2000 [ 63%]  (Sampling)\nChain 1: Iteration: 1270 / 2000 [ 63%]  (Sampling)\nChain 1: Iteration: 1271 / 2000 [ 63%]  (Sampling)\nChain 1: Iteration: 1272 / 2000 [ 63%]  (Sampling)\nChain 1: Iteration: 1273 / 2000 [ 63%]  (Sampling)\nChain 1: Iteration: 1274 / 2000 [ 63%]  (Sampling)\nChain 1: Iteration: 1275 / 2000 [ 63%]  (Sampling)\nChain 1: Iteration: 1276 / 2000 [ 63%]  (Sampling)\nChain 1: Iteration: 1277 / 2000 [ 63%]  (Sampling)\nChain 1: Iteration: 1278 / 2000 [ 63%]  (Sampling)\nChain 1: Iteration: 1279 / 2000 [ 63%]  (Sampling)\nChain 1: Iteration: 1280 / 2000 [ 64%]  (Sampling)\nChain 1: Iteration: 1281 / 2000 [ 64%]  (Sampling)\nChain 1: Iteration: 1282 / 2000 [ 64%]  (Sampling)\nChain 1: Iteration: 1283 / 2000 [ 64%]  (Sampling)\nChain 1: Iteration: 1284 / 2000 [ 64%]  (Sampling)\nChain 1: Iteration: 1285 / 2000 [ 64%]  (Sampling)\nChain 1: Iteration: 1286 / 2000 [ 64%]  (Sampling)\nChain 1: Iteration: 1287 / 2000 [ 64%]  (Sampling)\nChain 1: Iteration: 1288 / 2000 [ 64%]  (Sampling)\nChain 1: Iteration: 1289 / 2000 [ 64%]  (Sampling)\nChain 1: Iteration: 1290 / 2000 [ 64%]  (Sampling)\nChain 1: Iteration: 1291 / 2000 [ 64%]  (Sampling)\nChain 1: Iteration: 1292 / 2000 [ 64%]  (Sampling)\nChain 1: Iteration: 1293 / 2000 [ 64%]  (Sampling)\nChain 1: Iteration: 1294 / 2000 [ 64%]  (Sampling)\nChain 1: Iteration: 1295 / 2000 [ 64%]  (Sampling)\nChain 1: Iteration: 1296 / 2000 [ 64%]  (Sampling)\nChain 1: Iteration: 1297 / 2000 [ 64%]  (Sampling)\nChain 1: Iteration: 1298 / 2000 [ 64%]  (Sampling)\nChain 1: Iteration: 1299 / 2000 [ 64%]  (Sampling)\nChain 1: Iteration: 1300 / 2000 [ 65%]  (Sampling)\nChain 1: Iteration: 1301 / 2000 [ 65%]  (Sampling)\nChain 1: Iteration: 1302 / 2000 [ 65%]  (Sampling)\nChain 1: Iteration: 1303 / 2000 [ 65%]  (Sampling)\nChain 1: Iteration: 1304 / 2000 [ 65%]  (Sampling)\nChain 1: Iteration: 1305 / 2000 [ 65%]  (Sampling)\nChain 1: Iteration: 1306 / 2000 [ 65%]  (Sampling)\nChain 1: Iteration: 1307 / 2000 [ 65%]  (Sampling)\nChain 1: Iteration: 1308 / 2000 [ 65%]  (Sampling)\nChain 1: Iteration: 1309 / 2000 [ 65%]  (Sampling)\nChain 1: Iteration: 1310 / 2000 [ 65%]  (Sampling)\nChain 1: Iteration: 1311 / 2000 [ 65%]  (Sampling)\nChain 1: Iteration: 1312 / 2000 [ 65%]  (Sampling)\nChain 1: Iteration: 1313 / 2000 [ 65%]  (Sampling)\nChain 1: Iteration: 1314 / 2000 [ 65%]  (Sampling)\nChain 1: Iteration: 1315 / 2000 [ 65%]  (Sampling)\nChain 1: Iteration: 1316 / 2000 [ 65%]  (Sampling)\nChain 1: Iteration: 1317 / 2000 [ 65%]  (Sampling)\nChain 1: Iteration: 1318 / 2000 [ 65%]  (Sampling)\nChain 1: Iteration: 1319 / 2000 [ 65%]  (Sampling)\nChain 1: Iteration: 1320 / 2000 [ 66%]  (Sampling)\nChain 1: Iteration: 1321 / 2000 [ 66%]  (Sampling)\nChain 1: Iteration: 1322 / 2000 [ 66%]  (Sampling)\nChain 1: Iteration: 1323 / 2000 [ 66%]  (Sampling)\nChain 1: Iteration: 1324 / 2000 [ 66%]  (Sampling)\nChain 1: Iteration: 1325 / 2000 [ 66%]  (Sampling)\nChain 1: Iteration: 1326 / 2000 [ 66%]  (Sampling)\nChain 1: Iteration: 1327 / 2000 [ 66%]  (Sampling)\nChain 1: Iteration: 1328 / 2000 [ 66%]  (Sampling)\nChain 1: Iteration: 1329 / 2000 [ 66%]  (Sampling)\nChain 1: Iteration: 1330 / 2000 [ 66%]  (Sampling)\nChain 1: Iteration: 1331 / 2000 [ 66%]  (Sampling)\nChain 1: Iteration: 1332 / 2000 [ 66%]  (Sampling)\nChain 1: Iteration: 1333 / 2000 [ 66%]  (Sampling)\nChain 1: Iteration: 1334 / 2000 [ 66%]  (Sampling)\nChain 1: Iteration: 1335 / 2000 [ 66%]  (Sampling)\nChain 1: Iteration: 1336 / 2000 [ 66%]  (Sampling)\nChain 1: Iteration: 1337 / 2000 [ 66%]  (Sampling)\nChain 1: Iteration: 1338 / 2000 [ 66%]  (Sampling)\nChain 1: Iteration: 1339 / 2000 [ 66%]  (Sampling)\nChain 1: Iteration: 1340 / 2000 [ 67%]  (Sampling)\nChain 1: Iteration: 1341 / 2000 [ 67%]  (Sampling)\nChain 1: Iteration: 1342 / 2000 [ 67%]  (Sampling)\nChain 1: Iteration: 1343 / 2000 [ 67%]  (Sampling)\nChain 1: Iteration: 1344 / 2000 [ 67%]  (Sampling)\nChain 1: Iteration: 1345 / 2000 [ 67%]  (Sampling)\nChain 1: Iteration: 1346 / 2000 [ 67%]  (Sampling)\nChain 1: Iteration: 1347 / 2000 [ 67%]  (Sampling)\nChain 1: Iteration: 1348 / 2000 [ 67%]  (Sampling)\nChain 1: Iteration: 1349 / 2000 [ 67%]  (Sampling)\nChain 1: Iteration: 1350 / 2000 [ 67%]  (Sampling)\nChain 1: Iteration: 1351 / 2000 [ 67%]  (Sampling)\nChain 1: Iteration: 1352 / 2000 [ 67%]  (Sampling)\nChain 1: Iteration: 1353 / 2000 [ 67%]  (Sampling)\nChain 1: Iteration: 1354 / 2000 [ 67%]  (Sampling)\nChain 1: Iteration: 1355 / 2000 [ 67%]  (Sampling)\nChain 1: Iteration: 1356 / 2000 [ 67%]  (Sampling)\nChain 1: Iteration: 1357 / 2000 [ 67%]  (Sampling)\nChain 1: Iteration: 1358 / 2000 [ 67%]  (Sampling)\nChain 1: Iteration: 1359 / 2000 [ 67%]  (Sampling)\nChain 1: Iteration: 1360 / 2000 [ 68%]  (Sampling)\nChain 1: Iteration: 1361 / 2000 [ 68%]  (Sampling)\nChain 1: Iteration: 1362 / 2000 [ 68%]  (Sampling)\nChain 1: Iteration: 1363 / 2000 [ 68%]  (Sampling)\nChain 1: Iteration: 1364 / 2000 [ 68%]  (Sampling)\nChain 1: Iteration: 1365 / 2000 [ 68%]  (Sampling)\nChain 1: Iteration: 1366 / 2000 [ 68%]  (Sampling)\nChain 1: Iteration: 1367 / 2000 [ 68%]  (Sampling)\nChain 1: Iteration: 1368 / 2000 [ 68%]  (Sampling)\nChain 1: Iteration: 1369 / 2000 [ 68%]  (Sampling)\nChain 1: Iteration: 1370 / 2000 [ 68%]  (Sampling)\nChain 1: Iteration: 1371 / 2000 [ 68%]  (Sampling)\nChain 1: Iteration: 1372 / 2000 [ 68%]  (Sampling)\nChain 1: Iteration: 1373 / 2000 [ 68%]  (Sampling)\nChain 1: Iteration: 1374 / 2000 [ 68%]  (Sampling)\nChain 1: Iteration: 1375 / 2000 [ 68%]  (Sampling)\nChain 1: Iteration: 1376 / 2000 [ 68%]  (Sampling)\nChain 1: Iteration: 1377 / 2000 [ 68%]  (Sampling)\nChain 1: Iteration: 1378 / 2000 [ 68%]  (Sampling)\nChain 1: Iteration: 1379 / 2000 [ 68%]  (Sampling)\nChain 1: Iteration: 1380 / 2000 [ 69%]  (Sampling)\nChain 1: Iteration: 1381 / 2000 [ 69%]  (Sampling)\nChain 1: Iteration: 1382 / 2000 [ 69%]  (Sampling)\nChain 1: Iteration: 1383 / 2000 [ 69%]  (Sampling)\nChain 1: Iteration: 1384 / 2000 [ 69%]  (Sampling)\nChain 1: Iteration: 1385 / 2000 [ 69%]  (Sampling)\nChain 1: Iteration: 1386 / 2000 [ 69%]  (Sampling)\nChain 1: Iteration: 1387 / 2000 [ 69%]  (Sampling)\nChain 1: Iteration: 1388 / 2000 [ 69%]  (Sampling)\nChain 1: Iteration: 1389 / 2000 [ 69%]  (Sampling)\nChain 1: Iteration: 1390 / 2000 [ 69%]  (Sampling)\nChain 1: Iteration: 1391 / 2000 [ 69%]  (Sampling)\nChain 1: Iteration: 1392 / 2000 [ 69%]  (Sampling)\nChain 1: Iteration: 1393 / 2000 [ 69%]  (Sampling)\nChain 1: Iteration: 1394 / 2000 [ 69%]  (Sampling)\nChain 1: Iteration: 1395 / 2000 [ 69%]  (Sampling)\nChain 1: Iteration: 1396 / 2000 [ 69%]  (Sampling)\nChain 1: Iteration: 1397 / 2000 [ 69%]  (Sampling)\nChain 1: Iteration: 1398 / 2000 [ 69%]  (Sampling)\nChain 1: Iteration: 1399 / 2000 [ 69%]  (Sampling)\nChain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 1: Iteration: 1401 / 2000 [ 70%]  (Sampling)\nChain 1: Iteration: 1402 / 2000 [ 70%]  (Sampling)\nChain 1: Iteration: 1403 / 2000 [ 70%]  (Sampling)\nChain 1: Iteration: 1404 / 2000 [ 70%]  (Sampling)\nChain 1: Iteration: 1405 / 2000 [ 70%]  (Sampling)\nChain 1: Iteration: 1406 / 2000 [ 70%]  (Sampling)\nChain 1: Iteration: 1407 / 2000 [ 70%]  (Sampling)\nChain 1: Iteration: 1408 / 2000 [ 70%]  (Sampling)\nChain 1: Iteration: 1409 / 2000 [ 70%]  (Sampling)\nChain 1: Iteration: 1410 / 2000 [ 70%]  (Sampling)\nChain 1: Iteration: 1411 / 2000 [ 70%]  (Sampling)\nChain 1: Iteration: 1412 / 2000 [ 70%]  (Sampling)\nChain 1: Iteration: 1413 / 2000 [ 70%]  (Sampling)\nChain 1: Iteration: 1414 / 2000 [ 70%]  (Sampling)\nChain 1: Iteration: 1415 / 2000 [ 70%]  (Sampling)\nChain 1: Iteration: 1416 / 2000 [ 70%]  (Sampling)\nChain 1: Iteration: 1417 / 2000 [ 70%]  (Sampling)\nChain 1: Iteration: 1418 / 2000 [ 70%]  (Sampling)\nChain 1: Iteration: 1419 / 2000 [ 70%]  (Sampling)\nChain 1: Iteration: 1420 / 2000 [ 71%]  (Sampling)\nChain 1: Iteration: 1421 / 2000 [ 71%]  (Sampling)\nChain 1: Iteration: 1422 / 2000 [ 71%]  (Sampling)\nChain 1: Iteration: 1423 / 2000 [ 71%]  (Sampling)\nChain 1: Iteration: 1424 / 2000 [ 71%]  (Sampling)\nChain 1: Iteration: 1425 / 2000 [ 71%]  (Sampling)\nChain 1: Iteration: 1426 / 2000 [ 71%]  (Sampling)\nChain 1: Iteration: 1427 / 2000 [ 71%]  (Sampling)\nChain 1: Iteration: 1428 / 2000 [ 71%]  (Sampling)\nChain 1: Iteration: 1429 / 2000 [ 71%]  (Sampling)\nChain 1: Iteration: 1430 / 2000 [ 71%]  (Sampling)\nChain 1: Iteration: 1431 / 2000 [ 71%]  (Sampling)\nChain 1: Iteration: 1432 / 2000 [ 71%]  (Sampling)\nChain 1: Iteration: 1433 / 2000 [ 71%]  (Sampling)\nChain 1: Iteration: 1434 / 2000 [ 71%]  (Sampling)\nChain 1: Iteration: 1435 / 2000 [ 71%]  (Sampling)\nChain 1: Iteration: 1436 / 2000 [ 71%]  (Sampling)\nChain 1: Iteration: 1437 / 2000 [ 71%]  (Sampling)\nChain 1: Iteration: 1438 / 2000 [ 71%]  (Sampling)\nChain 1: Iteration: 1439 / 2000 [ 71%]  (Sampling)\nChain 1: Iteration: 1440 / 2000 [ 72%]  (Sampling)\nChain 1: Iteration: 1441 / 2000 [ 72%]  (Sampling)\nChain 1: Iteration: 1442 / 2000 [ 72%]  (Sampling)\nChain 1: Iteration: 1443 / 2000 [ 72%]  (Sampling)\nChain 1: Iteration: 1444 / 2000 [ 72%]  (Sampling)\nChain 1: Iteration: 1445 / 2000 [ 72%]  (Sampling)\nChain 1: Iteration: 1446 / 2000 [ 72%]  (Sampling)\nChain 1: Iteration: 1447 / 2000 [ 72%]  (Sampling)\nChain 1: Iteration: 1448 / 2000 [ 72%]  (Sampling)\nChain 1: Iteration: 1449 / 2000 [ 72%]  (Sampling)\nChain 1: Iteration: 1450 / 2000 [ 72%]  (Sampling)\nChain 1: Iteration: 1451 / 2000 [ 72%]  (Sampling)\nChain 1: Iteration: 1452 / 2000 [ 72%]  (Sampling)\nChain 1: Iteration: 1453 / 2000 [ 72%]  (Sampling)\nChain 1: Iteration: 1454 / 2000 [ 72%]  (Sampling)\nChain 1: Iteration: 1455 / 2000 [ 72%]  (Sampling)\nChain 1: Iteration: 1456 / 2000 [ 72%]  (Sampling)\nChain 1: Iteration: 1457 / 2000 [ 72%]  (Sampling)\nChain 1: Iteration: 1458 / 2000 [ 72%]  (Sampling)\nChain 1: Iteration: 1459 / 2000 [ 72%]  (Sampling)\nChain 1: Iteration: 1460 / 2000 [ 73%]  (Sampling)\nChain 1: Iteration: 1461 / 2000 [ 73%]  (Sampling)\nChain 1: Iteration: 1462 / 2000 [ 73%]  (Sampling)\nChain 1: Iteration: 1463 / 2000 [ 73%]  (Sampling)\nChain 1: Iteration: 1464 / 2000 [ 73%]  (Sampling)\nChain 1: Iteration: 1465 / 2000 [ 73%]  (Sampling)\nChain 1: Iteration: 1466 / 2000 [ 73%]  (Sampling)\nChain 1: Iteration: 1467 / 2000 [ 73%]  (Sampling)\nChain 1: Iteration: 1468 / 2000 [ 73%]  (Sampling)\nChain 1: Iteration: 1469 / 2000 [ 73%]  (Sampling)\nChain 1: Iteration: 1470 / 2000 [ 73%]  (Sampling)\nChain 1: Iteration: 1471 / 2000 [ 73%]  (Sampling)\nChain 1: Iteration: 1472 / 2000 [ 73%]  (Sampling)\nChain 1: Iteration: 1473 / 2000 [ 73%]  (Sampling)\nChain 1: Iteration: 1474 / 2000 [ 73%]  (Sampling)\nChain 1: Iteration: 1475 / 2000 [ 73%]  (Sampling)\nChain 1: Iteration: 1476 / 2000 [ 73%]  (Sampling)\nChain 1: Iteration: 1477 / 2000 [ 73%]  (Sampling)\nChain 1: Iteration: 1478 / 2000 [ 73%]  (Sampling)\nChain 1: Iteration: 1479 / 2000 [ 73%]  (Sampling)\nChain 1: Iteration: 1480 / 2000 [ 74%]  (Sampling)\nChain 1: Iteration: 1481 / 2000 [ 74%]  (Sampling)\nChain 1: Iteration: 1482 / 2000 [ 74%]  (Sampling)\nChain 1: Iteration: 1483 / 2000 [ 74%]  (Sampling)\nChain 1: Iteration: 1484 / 2000 [ 74%]  (Sampling)\nChain 1: Iteration: 1485 / 2000 [ 74%]  (Sampling)\nChain 1: Iteration: 1486 / 2000 [ 74%]  (Sampling)\nChain 1: Iteration: 1487 / 2000 [ 74%]  (Sampling)\nChain 1: Iteration: 1488 / 2000 [ 74%]  (Sampling)\nChain 1: Iteration: 1489 / 2000 [ 74%]  (Sampling)\nChain 1: Iteration: 1490 / 2000 [ 74%]  (Sampling)\nChain 1: Iteration: 1491 / 2000 [ 74%]  (Sampling)\nChain 1: Iteration: 1492 / 2000 [ 74%]  (Sampling)\nChain 1: Iteration: 1493 / 2000 [ 74%]  (Sampling)\nChain 1: Iteration: 1494 / 2000 [ 74%]  (Sampling)\nChain 1: Iteration: 1495 / 2000 [ 74%]  (Sampling)\nChain 1: Iteration: 1496 / 2000 [ 74%]  (Sampling)\nChain 1: Iteration: 1497 / 2000 [ 74%]  (Sampling)\nChain 1: Iteration: 1498 / 2000 [ 74%]  (Sampling)\nChain 1: Iteration: 1499 / 2000 [ 74%]  (Sampling)\nChain 1: Iteration: 1500 / 2000 [ 75%]  (Sampling)\nChain 1: Iteration: 1501 / 2000 [ 75%]  (Sampling)\nChain 1: Iteration: 1502 / 2000 [ 75%]  (Sampling)\nChain 1: Iteration: 1503 / 2000 [ 75%]  (Sampling)\nChain 1: Iteration: 1504 / 2000 [ 75%]  (Sampling)\nChain 1: Iteration: 1505 / 2000 [ 75%]  (Sampling)\nChain 1: Iteration: 1506 / 2000 [ 75%]  (Sampling)\nChain 1: Iteration: 1507 / 2000 [ 75%]  (Sampling)\nChain 1: Iteration: 1508 / 2000 [ 75%]  (Sampling)\nChain 1: Iteration: 1509 / 2000 [ 75%]  (Sampling)\nChain 1: Iteration: 1510 / 2000 [ 75%]  (Sampling)\nChain 1: Iteration: 1511 / 2000 [ 75%]  (Sampling)\nChain 1: Iteration: 1512 / 2000 [ 75%]  (Sampling)\nChain 1: Iteration: 1513 / 2000 [ 75%]  (Sampling)\nChain 1: Iteration: 1514 / 2000 [ 75%]  (Sampling)\nChain 1: Iteration: 1515 / 2000 [ 75%]  (Sampling)\nChain 1: Iteration: 1516 / 2000 [ 75%]  (Sampling)\nChain 1: Iteration: 1517 / 2000 [ 75%]  (Sampling)\nChain 1: Iteration: 1518 / 2000 [ 75%]  (Sampling)\nChain 1: Iteration: 1519 / 2000 [ 75%]  (Sampling)\nChain 1: Iteration: 1520 / 2000 [ 76%]  (Sampling)\nChain 1: Iteration: 1521 / 2000 [ 76%]  (Sampling)\nChain 1: Iteration: 1522 / 2000 [ 76%]  (Sampling)\nChain 1: Iteration: 1523 / 2000 [ 76%]  (Sampling)\nChain 1: Iteration: 1524 / 2000 [ 76%]  (Sampling)\nChain 1: Iteration: 1525 / 2000 [ 76%]  (Sampling)\nChain 1: Iteration: 1526 / 2000 [ 76%]  (Sampling)\nChain 1: Iteration: 1527 / 2000 [ 76%]  (Sampling)\nChain 1: Iteration: 1528 / 2000 [ 76%]  (Sampling)\nChain 1: Iteration: 1529 / 2000 [ 76%]  (Sampling)\nChain 1: Iteration: 1530 / 2000 [ 76%]  (Sampling)\nChain 1: Iteration: 1531 / 2000 [ 76%]  (Sampling)\nChain 1: Iteration: 1532 / 2000 [ 76%]  (Sampling)\nChain 1: Iteration: 1533 / 2000 [ 76%]  (Sampling)\nChain 1: Iteration: 1534 / 2000 [ 76%]  (Sampling)\nChain 1: Iteration: 1535 / 2000 [ 76%]  (Sampling)\nChain 1: Iteration: 1536 / 2000 [ 76%]  (Sampling)\nChain 1: Iteration: 1537 / 2000 [ 76%]  (Sampling)\nChain 1: Iteration: 1538 / 2000 [ 76%]  (Sampling)\nChain 1: Iteration: 1539 / 2000 [ 76%]  (Sampling)\nChain 1: Iteration: 1540 / 2000 [ 77%]  (Sampling)\nChain 1: Iteration: 1541 / 2000 [ 77%]  (Sampling)\nChain 1: Iteration: 1542 / 2000 [ 77%]  (Sampling)\nChain 1: Iteration: 1543 / 2000 [ 77%]  (Sampling)\nChain 1: Iteration: 1544 / 2000 [ 77%]  (Sampling)\nChain 1: Iteration: 1545 / 2000 [ 77%]  (Sampling)\nChain 1: Iteration: 1546 / 2000 [ 77%]  (Sampling)\nChain 1: Iteration: 1547 / 2000 [ 77%]  (Sampling)\nChain 1: Iteration: 1548 / 2000 [ 77%]  (Sampling)\nChain 1: Iteration: 1549 / 2000 [ 77%]  (Sampling)\nChain 1: Iteration: 1550 / 2000 [ 77%]  (Sampling)\nChain 1: Iteration: 1551 / 2000 [ 77%]  (Sampling)\nChain 1: Iteration: 1552 / 2000 [ 77%]  (Sampling)\nChain 1: Iteration: 1553 / 2000 [ 77%]  (Sampling)\nChain 1: Iteration: 1554 / 2000 [ 77%]  (Sampling)\nChain 1: Iteration: 1555 / 2000 [ 77%]  (Sampling)\nChain 1: Iteration: 1556 / 2000 [ 77%]  (Sampling)\nChain 1: Iteration: 1557 / 2000 [ 77%]  (Sampling)\nChain 1: Iteration: 1558 / 2000 [ 77%]  (Sampling)\nChain 1: Iteration: 1559 / 2000 [ 77%]  (Sampling)\nChain 1: Iteration: 1560 / 2000 [ 78%]  (Sampling)\nChain 1: Iteration: 1561 / 2000 [ 78%]  (Sampling)\nChain 1: Iteration: 1562 / 2000 [ 78%]  (Sampling)\nChain 1: Iteration: 1563 / 2000 [ 78%]  (Sampling)\nChain 1: Iteration: 1564 / 2000 [ 78%]  (Sampling)\nChain 1: Iteration: 1565 / 2000 [ 78%]  (Sampling)\nChain 1: Iteration: 1566 / 2000 [ 78%]  (Sampling)\nChain 1: Iteration: 1567 / 2000 [ 78%]  (Sampling)\nChain 1: Iteration: 1568 / 2000 [ 78%]  (Sampling)\nChain 1: Iteration: 1569 / 2000 [ 78%]  (Sampling)\nChain 1: Iteration: 1570 / 2000 [ 78%]  (Sampling)\nChain 1: Iteration: 1571 / 2000 [ 78%]  (Sampling)\nChain 1: Iteration: 1572 / 2000 [ 78%]  (Sampling)\nChain 1: Iteration: 1573 / 2000 [ 78%]  (Sampling)\nChain 1: Iteration: 1574 / 2000 [ 78%]  (Sampling)\nChain 1: Iteration: 1575 / 2000 [ 78%]  (Sampling)\nChain 1: Iteration: 1576 / 2000 [ 78%]  (Sampling)\nChain 1: Iteration: 1577 / 2000 [ 78%]  (Sampling)\nChain 1: Iteration: 1578 / 2000 [ 78%]  (Sampling)\nChain 1: Iteration: 1579 / 2000 [ 78%]  (Sampling)\nChain 1: Iteration: 1580 / 2000 [ 79%]  (Sampling)\nChain 1: Iteration: 1581 / 2000 [ 79%]  (Sampling)\nChain 1: Iteration: 1582 / 2000 [ 79%]  (Sampling)\nChain 1: Iteration: 1583 / 2000 [ 79%]  (Sampling)\nChain 1: Iteration: 1584 / 2000 [ 79%]  (Sampling)\nChain 1: Iteration: 1585 / 2000 [ 79%]  (Sampling)\nChain 1: Iteration: 1586 / 2000 [ 79%]  (Sampling)\nChain 1: Iteration: 1587 / 2000 [ 79%]  (Sampling)\nChain 1: Iteration: 1588 / 2000 [ 79%]  (Sampling)\nChain 1: Iteration: 1589 / 2000 [ 79%]  (Sampling)\nChain 1: Iteration: 1590 / 2000 [ 79%]  (Sampling)\nChain 1: Iteration: 1591 / 2000 [ 79%]  (Sampling)\nChain 1: Iteration: 1592 / 2000 [ 79%]  (Sampling)\nChain 1: Iteration: 1593 / 2000 [ 79%]  (Sampling)\nChain 1: Iteration: 1594 / 2000 [ 79%]  (Sampling)\nChain 1: Iteration: 1595 / 2000 [ 79%]  (Sampling)\nChain 1: Iteration: 1596 / 2000 [ 79%]  (Sampling)\nChain 1: Iteration: 1597 / 2000 [ 79%]  (Sampling)\nChain 1: Iteration: 1598 / 2000 [ 79%]  (Sampling)\nChain 1: Iteration: 1599 / 2000 [ 79%]  (Sampling)\nChain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 1: Iteration: 1601 / 2000 [ 80%]  (Sampling)\nChain 1: Iteration: 1602 / 2000 [ 80%]  (Sampling)\nChain 1: Iteration: 1603 / 2000 [ 80%]  (Sampling)\nChain 1: Iteration: 1604 / 2000 [ 80%]  (Sampling)\nChain 1: Iteration: 1605 / 2000 [ 80%]  (Sampling)\nChain 1: Iteration: 1606 / 2000 [ 80%]  (Sampling)\nChain 1: Iteration: 1607 / 2000 [ 80%]  (Sampling)\nChain 1: Iteration: 1608 / 2000 [ 80%]  (Sampling)\nChain 1: Iteration: 1609 / 2000 [ 80%]  (Sampling)\nChain 1: Iteration: 1610 / 2000 [ 80%]  (Sampling)\nChain 1: Iteration: 1611 / 2000 [ 80%]  (Sampling)\nChain 1: Iteration: 1612 / 2000 [ 80%]  (Sampling)\nChain 1: Iteration: 1613 / 2000 [ 80%]  (Sampling)\nChain 1: Iteration: 1614 / 2000 [ 80%]  (Sampling)\nChain 1: Iteration: 1615 / 2000 [ 80%]  (Sampling)\nChain 1: Iteration: 1616 / 2000 [ 80%]  (Sampling)\nChain 1: Iteration: 1617 / 2000 [ 80%]  (Sampling)\nChain 1: Iteration: 1618 / 2000 [ 80%]  (Sampling)\nChain 1: Iteration: 1619 / 2000 [ 80%]  (Sampling)\nChain 1: Iteration: 1620 / 2000 [ 81%]  (Sampling)\nChain 1: Iteration: 1621 / 2000 [ 81%]  (Sampling)\nChain 1: Iteration: 1622 / 2000 [ 81%]  (Sampling)\nChain 1: Iteration: 1623 / 2000 [ 81%]  (Sampling)\nChain 1: Iteration: 1624 / 2000 [ 81%]  (Sampling)\nChain 1: Iteration: 1625 / 2000 [ 81%]  (Sampling)\nChain 1: Iteration: 1626 / 2000 [ 81%]  (Sampling)\nChain 1: Iteration: 1627 / 2000 [ 81%]  (Sampling)\nChain 1: Iteration: 1628 / 2000 [ 81%]  (Sampling)\nChain 1: Iteration: 1629 / 2000 [ 81%]  (Sampling)\nChain 1: Iteration: 1630 / 2000 [ 81%]  (Sampling)\nChain 1: Iteration: 1631 / 2000 [ 81%]  (Sampling)\nChain 1: Iteration: 1632 / 2000 [ 81%]  (Sampling)\nChain 1: Iteration: 1633 / 2000 [ 81%]  (Sampling)\nChain 1: Iteration: 1634 / 2000 [ 81%]  (Sampling)\nChain 1: Iteration: 1635 / 2000 [ 81%]  (Sampling)\nChain 1: Iteration: 1636 / 2000 [ 81%]  (Sampling)\nChain 1: Iteration: 1637 / 2000 [ 81%]  (Sampling)\nChain 1: Iteration: 1638 / 2000 [ 81%]  (Sampling)\nChain 1: Iteration: 1639 / 2000 [ 81%]  (Sampling)\nChain 1: Iteration: 1640 / 2000 [ 82%]  (Sampling)\nChain 1: Iteration: 1641 / 2000 [ 82%]  (Sampling)\nChain 1: Iteration: 1642 / 2000 [ 82%]  (Sampling)\nChain 1: Iteration: 1643 / 2000 [ 82%]  (Sampling)\nChain 1: Iteration: 1644 / 2000 [ 82%]  (Sampling)\nChain 1: Iteration: 1645 / 2000 [ 82%]  (Sampling)\nChain 1: Iteration: 1646 / 2000 [ 82%]  (Sampling)\nChain 1: Iteration: 1647 / 2000 [ 82%]  (Sampling)\nChain 1: Iteration: 1648 / 2000 [ 82%]  (Sampling)\nChain 1: Iteration: 1649 / 2000 [ 82%]  (Sampling)\nChain 1: Iteration: 1650 / 2000 [ 82%]  (Sampling)\nChain 1: Iteration: 1651 / 2000 [ 82%]  (Sampling)\nChain 1: Iteration: 1652 / 2000 [ 82%]  (Sampling)\nChain 1: Iteration: 1653 / 2000 [ 82%]  (Sampling)\nChain 1: Iteration: 1654 / 2000 [ 82%]  (Sampling)\nChain 1: Iteration: 1655 / 2000 [ 82%]  (Sampling)\nChain 1: Iteration: 1656 / 2000 [ 82%]  (Sampling)\nChain 1: Iteration: 1657 / 2000 [ 82%]  (Sampling)\nChain 1: Iteration: 1658 / 2000 [ 82%]  (Sampling)\nChain 1: Iteration: 1659 / 2000 [ 82%]  (Sampling)\nChain 1: Iteration: 1660 / 2000 [ 83%]  (Sampling)\nChain 1: Iteration: 1661 / 2000 [ 83%]  (Sampling)\nChain 1: Iteration: 1662 / 2000 [ 83%]  (Sampling)\nChain 1: Iteration: 1663 / 2000 [ 83%]  (Sampling)\nChain 1: Iteration: 1664 / 2000 [ 83%]  (Sampling)\nChain 1: Iteration: 1665 / 2000 [ 83%]  (Sampling)\nChain 1: Iteration: 1666 / 2000 [ 83%]  (Sampling)\nChain 1: Iteration: 1667 / 2000 [ 83%]  (Sampling)\nChain 1: Iteration: 1668 / 2000 [ 83%]  (Sampling)\nChain 1: Iteration: 1669 / 2000 [ 83%]  (Sampling)\nChain 1: Iteration: 1670 / 2000 [ 83%]  (Sampling)\nChain 1: Iteration: 1671 / 2000 [ 83%]  (Sampling)\nChain 1: Iteration: 1672 / 2000 [ 83%]  (Sampling)\nChain 1: Iteration: 1673 / 2000 [ 83%]  (Sampling)\nChain 1: Iteration: 1674 / 2000 [ 83%]  (Sampling)\nChain 1: Iteration: 1675 / 2000 [ 83%]  (Sampling)\nChain 1: Iteration: 1676 / 2000 [ 83%]  (Sampling)\nChain 1: Iteration: 1677 / 2000 [ 83%]  (Sampling)\nChain 1: Iteration: 1678 / 2000 [ 83%]  (Sampling)\nChain 1: Iteration: 1679 / 2000 [ 83%]  (Sampling)\nChain 1: Iteration: 1680 / 2000 [ 84%]  (Sampling)\nChain 1: Iteration: 1681 / 2000 [ 84%]  (Sampling)\nChain 1: Iteration: 1682 / 2000 [ 84%]  (Sampling)\nChain 1: Iteration: 1683 / 2000 [ 84%]  (Sampling)\nChain 1: Iteration: 1684 / 2000 [ 84%]  (Sampling)\nChain 1: Iteration: 1685 / 2000 [ 84%]  (Sampling)\nChain 1: Iteration: 1686 / 2000 [ 84%]  (Sampling)\nChain 1: Iteration: 1687 / 2000 [ 84%]  (Sampling)\nChain 1: Iteration: 1688 / 2000 [ 84%]  (Sampling)\nChain 1: Iteration: 1689 / 2000 [ 84%]  (Sampling)\nChain 1: Iteration: 1690 / 2000 [ 84%]  (Sampling)\nChain 1: Iteration: 1691 / 2000 [ 84%]  (Sampling)\nChain 1: Iteration: 1692 / 2000 [ 84%]  (Sampling)\nChain 1: Iteration: 1693 / 2000 [ 84%]  (Sampling)\nChain 1: Iteration: 1694 / 2000 [ 84%]  (Sampling)\nChain 1: Iteration: 1695 / 2000 [ 84%]  (Sampling)\nChain 1: Iteration: 1696 / 2000 [ 84%]  (Sampling)\nChain 1: Iteration: 1697 / 2000 [ 84%]  (Sampling)\nChain 1: Iteration: 1698 / 2000 [ 84%]  (Sampling)\nChain 1: Iteration: 1699 / 2000 [ 84%]  (Sampling)\nChain 1: Iteration: 1700 / 2000 [ 85%]  (Sampling)\nChain 1: Iteration: 1701 / 2000 [ 85%]  (Sampling)\nChain 1: Iteration: 1702 / 2000 [ 85%]  (Sampling)\nChain 1: Iteration: 1703 / 2000 [ 85%]  (Sampling)\nChain 1: Iteration: 1704 / 2000 [ 85%]  (Sampling)\nChain 1: Iteration: 1705 / 2000 [ 85%]  (Sampling)\nChain 1: Iteration: 1706 / 2000 [ 85%]  (Sampling)\nChain 1: Iteration: 1707 / 2000 [ 85%]  (Sampling)\nChain 1: Iteration: 1708 / 2000 [ 85%]  (Sampling)\nChain 1: Iteration: 1709 / 2000 [ 85%]  (Sampling)\nChain 1: Iteration: 1710 / 2000 [ 85%]  (Sampling)\nChain 1: Iteration: 1711 / 2000 [ 85%]  (Sampling)\nChain 1: Iteration: 1712 / 2000 [ 85%]  (Sampling)\nChain 1: Iteration: 1713 / 2000 [ 85%]  (Sampling)\nChain 1: Iteration: 1714 / 2000 [ 85%]  (Sampling)\nChain 1: Iteration: 1715 / 2000 [ 85%]  (Sampling)\nChain 1: Iteration: 1716 / 2000 [ 85%]  (Sampling)\nChain 1: Iteration: 1717 / 2000 [ 85%]  (Sampling)\nChain 1: Iteration: 1718 / 2000 [ 85%]  (Sampling)\nChain 1: Iteration: 1719 / 2000 [ 85%]  (Sampling)\nChain 1: Iteration: 1720 / 2000 [ 86%]  (Sampling)\nChain 1: Iteration: 1721 / 2000 [ 86%]  (Sampling)\nChain 1: Iteration: 1722 / 2000 [ 86%]  (Sampling)\nChain 1: Iteration: 1723 / 2000 [ 86%]  (Sampling)\nChain 1: Iteration: 1724 / 2000 [ 86%]  (Sampling)\nChain 1: Iteration: 1725 / 2000 [ 86%]  (Sampling)\nChain 1: Iteration: 1726 / 2000 [ 86%]  (Sampling)\nChain 1: Iteration: 1727 / 2000 [ 86%]  (Sampling)\nChain 1: Iteration: 1728 / 2000 [ 86%]  (Sampling)\nChain 1: Iteration: 1729 / 2000 [ 86%]  (Sampling)\nChain 1: Iteration: 1730 / 2000 [ 86%]  (Sampling)\nChain 1: Iteration: 1731 / 2000 [ 86%]  (Sampling)\nChain 1: Iteration: 1732 / 2000 [ 86%]  (Sampling)\nChain 1: Iteration: 1733 / 2000 [ 86%]  (Sampling)\nChain 1: Iteration: 1734 / 2000 [ 86%]  (Sampling)\nChain 1: Iteration: 1735 / 2000 [ 86%]  (Sampling)\nChain 1: Iteration: 1736 / 2000 [ 86%]  (Sampling)\nChain 1: Iteration: 1737 / 2000 [ 86%]  (Sampling)\nChain 1: Iteration: 1738 / 2000 [ 86%]  (Sampling)\nChain 1: Iteration: 1739 / 2000 [ 86%]  (Sampling)\nChain 1: Iteration: 1740 / 2000 [ 87%]  (Sampling)\nChain 1: Iteration: 1741 / 2000 [ 87%]  (Sampling)\nChain 1: Iteration: 1742 / 2000 [ 87%]  (Sampling)\nChain 1: Iteration: 1743 / 2000 [ 87%]  (Sampling)\nChain 1: Iteration: 1744 / 2000 [ 87%]  (Sampling)\nChain 1: Iteration: 1745 / 2000 [ 87%]  (Sampling)\nChain 1: Iteration: 1746 / 2000 [ 87%]  (Sampling)\nChain 1: Iteration: 1747 / 2000 [ 87%]  (Sampling)\nChain 1: Iteration: 1748 / 2000 [ 87%]  (Sampling)\nChain 1: Iteration: 1749 / 2000 [ 87%]  (Sampling)\nChain 1: Iteration: 1750 / 2000 [ 87%]  (Sampling)\nChain 1: Iteration: 1751 / 2000 [ 87%]  (Sampling)\nChain 1: Iteration: 1752 / 2000 [ 87%]  (Sampling)\nChain 1: Iteration: 1753 / 2000 [ 87%]  (Sampling)\nChain 1: Iteration: 1754 / 2000 [ 87%]  (Sampling)\nChain 1: Iteration: 1755 / 2000 [ 87%]  (Sampling)\nChain 1: Iteration: 1756 / 2000 [ 87%]  (Sampling)\nChain 1: Iteration: 1757 / 2000 [ 87%]  (Sampling)\nChain 1: Iteration: 1758 / 2000 [ 87%]  (Sampling)\nChain 1: Iteration: 1759 / 2000 [ 87%]  (Sampling)\nChain 1: Iteration: 1760 / 2000 [ 88%]  (Sampling)\nChain 1: Iteration: 1761 / 2000 [ 88%]  (Sampling)\nChain 1: Iteration: 1762 / 2000 [ 88%]  (Sampling)\nChain 1: Iteration: 1763 / 2000 [ 88%]  (Sampling)\nChain 1: Iteration: 1764 / 2000 [ 88%]  (Sampling)\nChain 1: Iteration: 1765 / 2000 [ 88%]  (Sampling)\nChain 1: Iteration: 1766 / 2000 [ 88%]  (Sampling)\nChain 1: Iteration: 1767 / 2000 [ 88%]  (Sampling)\nChain 1: Iteration: 1768 / 2000 [ 88%]  (Sampling)\nChain 1: Iteration: 1769 / 2000 [ 88%]  (Sampling)\nChain 1: Iteration: 1770 / 2000 [ 88%]  (Sampling)\nChain 1: Iteration: 1771 / 2000 [ 88%]  (Sampling)\nChain 1: Iteration: 1772 / 2000 [ 88%]  (Sampling)\nChain 1: Iteration: 1773 / 2000 [ 88%]  (Sampling)\nChain 1: Iteration: 1774 / 2000 [ 88%]  (Sampling)\nChain 1: Iteration: 1775 / 2000 [ 88%]  (Sampling)\nChain 1: Iteration: 1776 / 2000 [ 88%]  (Sampling)\nChain 1: Iteration: 1777 / 2000 [ 88%]  (Sampling)\nChain 1: Iteration: 1778 / 2000 [ 88%]  (Sampling)\nChain 1: Iteration: 1779 / 2000 [ 88%]  (Sampling)\nChain 1: Iteration: 1780 / 2000 [ 89%]  (Sampling)\nChain 1: Iteration: 1781 / 2000 [ 89%]  (Sampling)\nChain 1: Iteration: 1782 / 2000 [ 89%]  (Sampling)\nChain 1: Iteration: 1783 / 2000 [ 89%]  (Sampling)\nChain 1: Iteration: 1784 / 2000 [ 89%]  (Sampling)\nChain 1: Iteration: 1785 / 2000 [ 89%]  (Sampling)\nChain 1: Iteration: 1786 / 2000 [ 89%]  (Sampling)\nChain 1: Iteration: 1787 / 2000 [ 89%]  (Sampling)\nChain 1: Iteration: 1788 / 2000 [ 89%]  (Sampling)\nChain 1: Iteration: 1789 / 2000 [ 89%]  (Sampling)\nChain 1: Iteration: 1790 / 2000 [ 89%]  (Sampling)\nChain 1: Iteration: 1791 / 2000 [ 89%]  (Sampling)\nChain 1: Iteration: 1792 / 2000 [ 89%]  (Sampling)\nChain 1: Iteration: 1793 / 2000 [ 89%]  (Sampling)\nChain 1: Iteration: 1794 / 2000 [ 89%]  (Sampling)\nChain 1: Iteration: 1795 / 2000 [ 89%]  (Sampling)\nChain 1: Iteration: 1796 / 2000 [ 89%]  (Sampling)\nChain 1: Iteration: 1797 / 2000 [ 89%]  (Sampling)\nChain 1: Iteration: 1798 / 2000 [ 89%]  (Sampling)\nChain 1: Iteration: 1799 / 2000 [ 89%]  (Sampling)\nChain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 1: Iteration: 1801 / 2000 [ 90%]  (Sampling)\nChain 1: Iteration: 1802 / 2000 [ 90%]  (Sampling)\nChain 1: Iteration: 1803 / 2000 [ 90%]  (Sampling)\nChain 1: Iteration: 1804 / 2000 [ 90%]  (Sampling)\nChain 1: Iteration: 1805 / 2000 [ 90%]  (Sampling)\nChain 1: Iteration: 1806 / 2000 [ 90%]  (Sampling)\nChain 1: Iteration: 1807 / 2000 [ 90%]  (Sampling)\nChain 1: Iteration: 1808 / 2000 [ 90%]  (Sampling)\nChain 1: Iteration: 1809 / 2000 [ 90%]  (Sampling)\nChain 1: Iteration: 1810 / 2000 [ 90%]  (Sampling)\nChain 1: Iteration: 1811 / 2000 [ 90%]  (Sampling)\nChain 1: Iteration: 1812 / 2000 [ 90%]  (Sampling)\nChain 1: Iteration: 1813 / 2000 [ 90%]  (Sampling)\nChain 1: Iteration: 1814 / 2000 [ 90%]  (Sampling)\nChain 1: Iteration: 1815 / 2000 [ 90%]  (Sampling)\nChain 1: Iteration: 1816 / 2000 [ 90%]  (Sampling)\nChain 1: Iteration: 1817 / 2000 [ 90%]  (Sampling)\nChain 1: Iteration: 1818 / 2000 [ 90%]  (Sampling)\nChain 1: Iteration: 1819 / 2000 [ 90%]  (Sampling)\nChain 1: Iteration: 1820 / 2000 [ 91%]  (Sampling)\nChain 1: Iteration: 1821 / 2000 [ 91%]  (Sampling)\nChain 1: Iteration: 1822 / 2000 [ 91%]  (Sampling)\nChain 1: Iteration: 1823 / 2000 [ 91%]  (Sampling)\nChain 1: Iteration: 1824 / 2000 [ 91%]  (Sampling)\nChain 1: Iteration: 1825 / 2000 [ 91%]  (Sampling)\nChain 1: Iteration: 1826 / 2000 [ 91%]  (Sampling)\nChain 1: Iteration: 1827 / 2000 [ 91%]  (Sampling)\nChain 1: Iteration: 1828 / 2000 [ 91%]  (Sampling)\nChain 1: Iteration: 1829 / 2000 [ 91%]  (Sampling)\nChain 1: Iteration: 1830 / 2000 [ 91%]  (Sampling)\nChain 1: Iteration: 1831 / 2000 [ 91%]  (Sampling)\nChain 1: Iteration: 1832 / 2000 [ 91%]  (Sampling)\nChain 1: Iteration: 1833 / 2000 [ 91%]  (Sampling)\nChain 1: Iteration: 1834 / 2000 [ 91%]  (Sampling)\nChain 1: Iteration: 1835 / 2000 [ 91%]  (Sampling)\nChain 1: Iteration: 1836 / 2000 [ 91%]  (Sampling)\nChain 1: Iteration: 1837 / 2000 [ 91%]  (Sampling)\nChain 1: Iteration: 1838 / 2000 [ 91%]  (Sampling)\nChain 1: Iteration: 1839 / 2000 [ 91%]  (Sampling)\nChain 1: Iteration: 1840 / 2000 [ 92%]  (Sampling)\nChain 1: Iteration: 1841 / 2000 [ 92%]  (Sampling)\nChain 1: Iteration: 1842 / 2000 [ 92%]  (Sampling)\nChain 1: Iteration: 1843 / 2000 [ 92%]  (Sampling)\nChain 1: Iteration: 1844 / 2000 [ 92%]  (Sampling)\nChain 1: Iteration: 1845 / 2000 [ 92%]  (Sampling)\nChain 1: Iteration: 1846 / 2000 [ 92%]  (Sampling)\nChain 1: Iteration: 1847 / 2000 [ 92%]  (Sampling)\nChain 1: Iteration: 1848 / 2000 [ 92%]  (Sampling)\nChain 1: Iteration: 1849 / 2000 [ 92%]  (Sampling)\nChain 1: Iteration: 1850 / 2000 [ 92%]  (Sampling)\nChain 1: Iteration: 1851 / 2000 [ 92%]  (Sampling)\nChain 1: Iteration: 1852 / 2000 [ 92%]  (Sampling)\nChain 1: Iteration: 1853 / 2000 [ 92%]  (Sampling)\nChain 1: Iteration: 1854 / 2000 [ 92%]  (Sampling)\nChain 1: Iteration: 1855 / 2000 [ 92%]  (Sampling)\nChain 1: Iteration: 1856 / 2000 [ 92%]  (Sampling)\nChain 1: Iteration: 1857 / 2000 [ 92%]  (Sampling)\nChain 1: Iteration: 1858 / 2000 [ 92%]  (Sampling)\nChain 1: Iteration: 1859 / 2000 [ 92%]  (Sampling)\nChain 1: Iteration: 1860 / 2000 [ 93%]  (Sampling)\nChain 1: Iteration: 1861 / 2000 [ 93%]  (Sampling)\nChain 1: Iteration: 1862 / 2000 [ 93%]  (Sampling)\nChain 1: Iteration: 1863 / 2000 [ 93%]  (Sampling)\nChain 1: Iteration: 1864 / 2000 [ 93%]  (Sampling)\nChain 1: Iteration: 1865 / 2000 [ 93%]  (Sampling)\nChain 1: Iteration: 1866 / 2000 [ 93%]  (Sampling)\nChain 1: Iteration: 1867 / 2000 [ 93%]  (Sampling)\nChain 1: Iteration: 1868 / 2000 [ 93%]  (Sampling)\nChain 1: Iteration: 1869 / 2000 [ 93%]  (Sampling)\nChain 1: Iteration: 1870 / 2000 [ 93%]  (Sampling)\nChain 1: Iteration: 1871 / 2000 [ 93%]  (Sampling)\nChain 1: Iteration: 1872 / 2000 [ 93%]  (Sampling)\nChain 1: Iteration: 1873 / 2000 [ 93%]  (Sampling)\nChain 1: Iteration: 1874 / 2000 [ 93%]  (Sampling)\nChain 1: Iteration: 1875 / 2000 [ 93%]  (Sampling)\nChain 1: Iteration: 1876 / 2000 [ 93%]  (Sampling)\nChain 1: Iteration: 1877 / 2000 [ 93%]  (Sampling)\nChain 1: Iteration: 1878 / 2000 [ 93%]  (Sampling)\nChain 1: Iteration: 1879 / 2000 [ 93%]  (Sampling)\nChain 1: Iteration: 1880 / 2000 [ 94%]  (Sampling)\nChain 1: Iteration: 1881 / 2000 [ 94%]  (Sampling)\nChain 1: Iteration: 1882 / 2000 [ 94%]  (Sampling)\nChain 1: Iteration: 1883 / 2000 [ 94%]  (Sampling)\nChain 1: Iteration: 1884 / 2000 [ 94%]  (Sampling)\nChain 1: Iteration: 1885 / 2000 [ 94%]  (Sampling)\nChain 1: Iteration: 1886 / 2000 [ 94%]  (Sampling)\nChain 1: Iteration: 1887 / 2000 [ 94%]  (Sampling)\nChain 1: Iteration: 1888 / 2000 [ 94%]  (Sampling)\nChain 1: Iteration: 1889 / 2000 [ 94%]  (Sampling)\nChain 1: Iteration: 1890 / 2000 [ 94%]  (Sampling)\nChain 1: Iteration: 1891 / 2000 [ 94%]  (Sampling)\nChain 1: Iteration: 1892 / 2000 [ 94%]  (Sampling)\nChain 1: Iteration: 1893 / 2000 [ 94%]  (Sampling)\nChain 1: Iteration: 1894 / 2000 [ 94%]  (Sampling)\nChain 1: Iteration: 1895 / 2000 [ 94%]  (Sampling)\nChain 1: Iteration: 1896 / 2000 [ 94%]  (Sampling)\nChain 1: Iteration: 1897 / 2000 [ 94%]  (Sampling)\nChain 1: Iteration: 1898 / 2000 [ 94%]  (Sampling)\nChain 1: Iteration: 1899 / 2000 [ 94%]  (Sampling)\nChain 1: Iteration: 1900 / 2000 [ 95%]  (Sampling)\nChain 1: Iteration: 1901 / 2000 [ 95%]  (Sampling)\nChain 1: Iteration: 1902 / 2000 [ 95%]  (Sampling)\nChain 1: Iteration: 1903 / 2000 [ 95%]  (Sampling)\nChain 1: Iteration: 1904 / 2000 [ 95%]  (Sampling)\nChain 1: Iteration: 1905 / 2000 [ 95%]  (Sampling)\nChain 1: Iteration: 1906 / 2000 [ 95%]  (Sampling)\nChain 1: Iteration: 1907 / 2000 [ 95%]  (Sampling)\nChain 1: Iteration: 1908 / 2000 [ 95%]  (Sampling)\nChain 1: Iteration: 1909 / 2000 [ 95%]  (Sampling)\nChain 1: Iteration: 1910 / 2000 [ 95%]  (Sampling)\nChain 1: Iteration: 1911 / 2000 [ 95%]  (Sampling)\nChain 1: Iteration: 1912 / 2000 [ 95%]  (Sampling)\nChain 1: Iteration: 1913 / 2000 [ 95%]  (Sampling)\nChain 1: Iteration: 1914 / 2000 [ 95%]  (Sampling)\nChain 1: Iteration: 1915 / 2000 [ 95%]  (Sampling)\nChain 1: Iteration: 1916 / 2000 [ 95%]  (Sampling)\nChain 1: Iteration: 1917 / 2000 [ 95%]  (Sampling)\nChain 1: Iteration: 1918 / 2000 [ 95%]  (Sampling)\nChain 1: Iteration: 1919 / 2000 [ 95%]  (Sampling)\nChain 1: Iteration: 1920 / 2000 [ 96%]  (Sampling)\nChain 1: Iteration: 1921 / 2000 [ 96%]  (Sampling)\nChain 1: Iteration: 1922 / 2000 [ 96%]  (Sampling)\nChain 1: Iteration: 1923 / 2000 [ 96%]  (Sampling)\nChain 1: Iteration: 1924 / 2000 [ 96%]  (Sampling)\nChain 1: Iteration: 1925 / 2000 [ 96%]  (Sampling)\nChain 1: Iteration: 1926 / 2000 [ 96%]  (Sampling)\nChain 1: Iteration: 1927 / 2000 [ 96%]  (Sampling)\nChain 1: Iteration: 1928 / 2000 [ 96%]  (Sampling)\nChain 1: Iteration: 1929 / 2000 [ 96%]  (Sampling)\nChain 1: Iteration: 1930 / 2000 [ 96%]  (Sampling)\nChain 1: Iteration: 1931 / 2000 [ 96%]  (Sampling)\nChain 1: Iteration: 1932 / 2000 [ 96%]  (Sampling)\nChain 1: Iteration: 1933 / 2000 [ 96%]  (Sampling)\nChain 1: Iteration: 1934 / 2000 [ 96%]  (Sampling)\nChain 1: Iteration: 1935 / 2000 [ 96%]  (Sampling)\nChain 1: Iteration: 1936 / 2000 [ 96%]  (Sampling)\nChain 1: Iteration: 1937 / 2000 [ 96%]  (Sampling)\nChain 1: Iteration: 1938 / 2000 [ 96%]  (Sampling)\nChain 1: Iteration: 1939 / 2000 [ 96%]  (Sampling)\nChain 1: Iteration: 1940 / 2000 [ 97%]  (Sampling)\nChain 1: Iteration: 1941 / 2000 [ 97%]  (Sampling)\nChain 1: Iteration: 1942 / 2000 [ 97%]  (Sampling)\nChain 1: Iteration: 1943 / 2000 [ 97%]  (Sampling)\nChain 1: Iteration: 1944 / 2000 [ 97%]  (Sampling)\nChain 1: Iteration: 1945 / 2000 [ 97%]  (Sampling)\nChain 1: Iteration: 1946 / 2000 [ 97%]  (Sampling)\nChain 1: Iteration: 1947 / 2000 [ 97%]  (Sampling)\nChain 1: Iteration: 1948 / 2000 [ 97%]  (Sampling)\nChain 1: Iteration: 1949 / 2000 [ 97%]  (Sampling)\nChain 1: Iteration: 1950 / 2000 [ 97%]  (Sampling)\nChain 1: Iteration: 1951 / 2000 [ 97%]  (Sampling)\nChain 1: Iteration: 1952 / 2000 [ 97%]  (Sampling)\nChain 1: Iteration: 1953 / 2000 [ 97%]  (Sampling)\nChain 1: Iteration: 1954 / 2000 [ 97%]  (Sampling)\nChain 1: Iteration: 1955 / 2000 [ 97%]  (Sampling)\nChain 1: Iteration: 1956 / 2000 [ 97%]  (Sampling)\nChain 1: Iteration: 1957 / 2000 [ 97%]  (Sampling)\nChain 1: Iteration: 1958 / 2000 [ 97%]  (Sampling)\nChain 1: Iteration: 1959 / 2000 [ 97%]  (Sampling)\nChain 1: Iteration: 1960 / 2000 [ 98%]  (Sampling)\nChain 1: Iteration: 1961 / 2000 [ 98%]  (Sampling)\nChain 1: Iteration: 1962 / 2000 [ 98%]  (Sampling)\nChain 1: Iteration: 1963 / 2000 [ 98%]  (Sampling)\nChain 1: Iteration: 1964 / 2000 [ 98%]  (Sampling)\nChain 1: Iteration: 1965 / 2000 [ 98%]  (Sampling)\nChain 1: Iteration: 1966 / 2000 [ 98%]  (Sampling)\nChain 1: Iteration: 1967 / 2000 [ 98%]  (Sampling)\nChain 1: Iteration: 1968 / 2000 [ 98%]  (Sampling)\nChain 1: Iteration: 1969 / 2000 [ 98%]  (Sampling)\nChain 1: Iteration: 1970 / 2000 [ 98%]  (Sampling)\nChain 1: Iteration: 1971 / 2000 [ 98%]  (Sampling)\nChain 1: Iteration: 1972 / 2000 [ 98%]  (Sampling)\nChain 1: Iteration: 1973 / 2000 [ 98%]  (Sampling)\nChain 1: Iteration: 1974 / 2000 [ 98%]  (Sampling)\nChain 1: Iteration: 1975 / 2000 [ 98%]  (Sampling)\nChain 1: Iteration: 1976 / 2000 [ 98%]  (Sampling)\nChain 1: Iteration: 1977 / 2000 [ 98%]  (Sampling)\nChain 1: Iteration: 1978 / 2000 [ 98%]  (Sampling)\nChain 1: Iteration: 1979 / 2000 [ 98%]  (Sampling)\nChain 1: Iteration: 1980 / 2000 [ 99%]  (Sampling)\nChain 1: Iteration: 1981 / 2000 [ 99%]  (Sampling)\nChain 1: Iteration: 1982 / 2000 [ 99%]  (Sampling)\nChain 1: Iteration: 1983 / 2000 [ 99%]  (Sampling)\nChain 1: Iteration: 1984 / 2000 [ 99%]  (Sampling)\nChain 1: Iteration: 1985 / 2000 [ 99%]  (Sampling)\nChain 1: Iteration: 1986 / 2000 [ 99%]  (Sampling)\nChain 1: Iteration: 1987 / 2000 [ 99%]  (Sampling)\nChain 1: Iteration: 1988 / 2000 [ 99%]  (Sampling)\nChain 1: Iteration: 1989 / 2000 [ 99%]  (Sampling)\nChain 1: Iteration: 1990 / 2000 [ 99%]  (Sampling)\nChain 1: Iteration: 1991 / 2000 [ 99%]  (Sampling)\nChain 1: Iteration: 1992 / 2000 [ 99%]  (Sampling)\nChain 1: Iteration: 1993 / 2000 [ 99%]  (Sampling)\nChain 1: Iteration: 1994 / 2000 [ 99%]  (Sampling)\nChain 1: Iteration: 1995 / 2000 [ 99%]  (Sampling)\nChain 1: Iteration: 1996 / 2000 [ 99%]  (Sampling)\nChain 1: Iteration: 1997 / 2000 [ 99%]  (Sampling)\nChain 1: Iteration: 1998 / 2000 [ 99%]  (Sampling)\nChain 1: Iteration: 1999 / 2000 [ 99%]  (Sampling)\nChain 1: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 0.039 seconds (Warm-up)\nChain 1:                0.034 seconds (Sampling)\nChain 1:                0.073 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 5e-06 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.05 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 2: Iteration:    2 / 2000 [  0%]  (Warmup)\nChain 2: Iteration:    3 / 2000 [  0%]  (Warmup)\nChain 2: Iteration:    4 / 2000 [  0%]  (Warmup)\nChain 2: Iteration:    5 / 2000 [  0%]  (Warmup)\nChain 2: Iteration:    6 / 2000 [  0%]  (Warmup)\nChain 2: Iteration:    7 / 2000 [  0%]  (Warmup)\nChain 2: Iteration:    8 / 2000 [  0%]  (Warmup)\nChain 2: Iteration:    9 / 2000 [  0%]  (Warmup)\nChain 2: Iteration:   10 / 2000 [  0%]  (Warmup)\nChain 2: Iteration:   11 / 2000 [  0%]  (Warmup)\nChain 2: Iteration:   12 / 2000 [  0%]  (Warmup)\nChain 2: Iteration:   13 / 2000 [  0%]  (Warmup)\nChain 2: Iteration:   14 / 2000 [  0%]  (Warmup)\nChain 2: Iteration:   15 / 2000 [  0%]  (Warmup)\nChain 2: Iteration:   16 / 2000 [  0%]  (Warmup)\nChain 2: Iteration:   17 / 2000 [  0%]  (Warmup)\nChain 2: Iteration:   18 / 2000 [  0%]  (Warmup)\nChain 2: Iteration:   19 / 2000 [  0%]  (Warmup)\nChain 2: Iteration:   20 / 2000 [  1%]  (Warmup)\nChain 2: Iteration:   21 / 2000 [  1%]  (Warmup)\nChain 2: Iteration:   22 / 2000 [  1%]  (Warmup)\nChain 2: Iteration:   23 / 2000 [  1%]  (Warmup)\nChain 2: Iteration:   24 / 2000 [  1%]  (Warmup)\nChain 2: Iteration:   25 / 2000 [  1%]  (Warmup)\nChain 2: Iteration:   26 / 2000 [  1%]  (Warmup)\nChain 2: Iteration:   27 / 2000 [  1%]  (Warmup)\nChain 2: Iteration:   28 / 2000 [  1%]  (Warmup)\nChain 2: Iteration:   29 / 2000 [  1%]  (Warmup)\nChain 2: Iteration:   30 / 2000 [  1%]  (Warmup)\nChain 2: Iteration:   31 / 2000 [  1%]  (Warmup)\nChain 2: Iteration:   32 / 2000 [  1%]  (Warmup)\nChain 2: Iteration:   33 / 2000 [  1%]  (Warmup)\nChain 2: Iteration:   34 / 2000 [  1%]  (Warmup)\nChain 2: Iteration:   35 / 2000 [  1%]  (Warmup)\nChain 2: Iteration:   36 / 2000 [  1%]  (Warmup)\nChain 2: Iteration:   37 / 2000 [  1%]  (Warmup)\nChain 2: Iteration:   38 / 2000 [  1%]  (Warmup)\nChain 2: Iteration:   39 / 2000 [  1%]  (Warmup)\nChain 2: Iteration:   40 / 2000 [  2%]  (Warmup)\nChain 2: Iteration:   41 / 2000 [  2%]  (Warmup)\nChain 2: Iteration:   42 / 2000 [  2%]  (Warmup)\nChain 2: Iteration:   43 / 2000 [  2%]  (Warmup)\nChain 2: Iteration:   44 / 2000 [  2%]  (Warmup)\nChain 2: Iteration:   45 / 2000 [  2%]  (Warmup)\nChain 2: Iteration:   46 / 2000 [  2%]  (Warmup)\nChain 2: Iteration:   47 / 2000 [  2%]  (Warmup)\nChain 2: Iteration:   48 / 2000 [  2%]  (Warmup)\nChain 2: Iteration:   49 / 2000 [  2%]  (Warmup)\nChain 2: Iteration:   50 / 2000 [  2%]  (Warmup)\nChain 2: Iteration:   51 / 2000 [  2%]  (Warmup)\nChain 2: Iteration:   52 / 2000 [  2%]  (Warmup)\nChain 2: Iteration:   53 / 2000 [  2%]  (Warmup)\nChain 2: Iteration:   54 / 2000 [  2%]  (Warmup)\nChain 2: Iteration:   55 / 2000 [  2%]  (Warmup)\nChain 2: Iteration:   56 / 2000 [  2%]  (Warmup)\nChain 2: Iteration:   57 / 2000 [  2%]  (Warmup)\nChain 2: Iteration:   58 / 2000 [  2%]  (Warmup)\nChain 2: Iteration:   59 / 2000 [  2%]  (Warmup)\nChain 2: Iteration:   60 / 2000 [  3%]  (Warmup)\nChain 2: Iteration:   61 / 2000 [  3%]  (Warmup)\nChain 2: Iteration:   62 / 2000 [  3%]  (Warmup)\nChain 2: Iteration:   63 / 2000 [  3%]  (Warmup)\nChain 2: Iteration:   64 / 2000 [  3%]  (Warmup)\nChain 2: Iteration:   65 / 2000 [  3%]  (Warmup)\nChain 2: Iteration:   66 / 2000 [  3%]  (Warmup)\nChain 2: Iteration:   67 / 2000 [  3%]  (Warmup)\nChain 2: Iteration:   68 / 2000 [  3%]  (Warmup)\nChain 2: Iteration:   69 / 2000 [  3%]  (Warmup)\nChain 2: Iteration:   70 / 2000 [  3%]  (Warmup)\nChain 2: Iteration:   71 / 2000 [  3%]  (Warmup)\nChain 2: Iteration:   72 / 2000 [  3%]  (Warmup)\nChain 2: Iteration:   73 / 2000 [  3%]  (Warmup)\nChain 2: Iteration:   74 / 2000 [  3%]  (Warmup)\nChain 2: Iteration:   75 / 2000 [  3%]  (Warmup)\nChain 2: Iteration:   76 / 2000 [  3%]  (Warmup)\nChain 2: Iteration:   77 / 2000 [  3%]  (Warmup)\nChain 2: Iteration:   78 / 2000 [  3%]  (Warmup)\nChain 2: Iteration:   79 / 2000 [  3%]  (Warmup)\nChain 2: Iteration:   80 / 2000 [  4%]  (Warmup)\nChain 2: Iteration:   81 / 2000 [  4%]  (Warmup)\nChain 2: Iteration:   82 / 2000 [  4%]  (Warmup)\nChain 2: Iteration:   83 / 2000 [  4%]  (Warmup)\nChain 2: Iteration:   84 / 2000 [  4%]  (Warmup)\nChain 2: Iteration:   85 / 2000 [  4%]  (Warmup)\nChain 2: Iteration:   86 / 2000 [  4%]  (Warmup)\nChain 2: Iteration:   87 / 2000 [  4%]  (Warmup)\nChain 2: Iteration:   88 / 2000 [  4%]  (Warmup)\nChain 2: Iteration:   89 / 2000 [  4%]  (Warmup)\nChain 2: Iteration:   90 / 2000 [  4%]  (Warmup)\nChain 2: Iteration:   91 / 2000 [  4%]  (Warmup)\nChain 2: Iteration:   92 / 2000 [  4%]  (Warmup)\nChain 2: Iteration:   93 / 2000 [  4%]  (Warmup)\nChain 2: Iteration:   94 / 2000 [  4%]  (Warmup)\nChain 2: Iteration:   95 / 2000 [  4%]  (Warmup)\nChain 2: Iteration:   96 / 2000 [  4%]  (Warmup)\nChain 2: Iteration:   97 / 2000 [  4%]  (Warmup)\nChain 2: Iteration:   98 / 2000 [  4%]  (Warmup)\nChain 2: Iteration:   99 / 2000 [  4%]  (Warmup)\nChain 2: Iteration:  100 / 2000 [  5%]  (Warmup)\nChain 2: Iteration:  101 / 2000 [  5%]  (Warmup)\nChain 2: Iteration:  102 / 2000 [  5%]  (Warmup)\nChain 2: Iteration:  103 / 2000 [  5%]  (Warmup)\nChain 2: Iteration:  104 / 2000 [  5%]  (Warmup)\nChain 2: Iteration:  105 / 2000 [  5%]  (Warmup)\nChain 2: Iteration:  106 / 2000 [  5%]  (Warmup)\nChain 2: Iteration:  107 / 2000 [  5%]  (Warmup)\nChain 2: Iteration:  108 / 2000 [  5%]  (Warmup)\nChain 2: Iteration:  109 / 2000 [  5%]  (Warmup)\nChain 2: Iteration:  110 / 2000 [  5%]  (Warmup)\nChain 2: Iteration:  111 / 2000 [  5%]  (Warmup)\nChain 2: Iteration:  112 / 2000 [  5%]  (Warmup)\nChain 2: Iteration:  113 / 2000 [  5%]  (Warmup)\nChain 2: Iteration:  114 / 2000 [  5%]  (Warmup)\nChain 2: Iteration:  115 / 2000 [  5%]  (Warmup)\nChain 2: Iteration:  116 / 2000 [  5%]  (Warmup)\nChain 2: Iteration:  117 / 2000 [  5%]  (Warmup)\nChain 2: Iteration:  118 / 2000 [  5%]  (Warmup)\nChain 2: Iteration:  119 / 2000 [  5%]  (Warmup)\nChain 2: Iteration:  120 / 2000 [  6%]  (Warmup)\nChain 2: Iteration:  121 / 2000 [  6%]  (Warmup)\nChain 2: Iteration:  122 / 2000 [  6%]  (Warmup)\nChain 2: Iteration:  123 / 2000 [  6%]  (Warmup)\nChain 2: Iteration:  124 / 2000 [  6%]  (Warmup)\nChain 2: Iteration:  125 / 2000 [  6%]  (Warmup)\nChain 2: Iteration:  126 / 2000 [  6%]  (Warmup)\nChain 2: Iteration:  127 / 2000 [  6%]  (Warmup)\nChain 2: Iteration:  128 / 2000 [  6%]  (Warmup)\nChain 2: Iteration:  129 / 2000 [  6%]  (Warmup)\nChain 2: Iteration:  130 / 2000 [  6%]  (Warmup)\nChain 2: Iteration:  131 / 2000 [  6%]  (Warmup)\nChain 2: Iteration:  132 / 2000 [  6%]  (Warmup)\nChain 2: Iteration:  133 / 2000 [  6%]  (Warmup)\nChain 2: Iteration:  134 / 2000 [  6%]  (Warmup)\nChain 2: Iteration:  135 / 2000 [  6%]  (Warmup)\nChain 2: Iteration:  136 / 2000 [  6%]  (Warmup)\nChain 2: Iteration:  137 / 2000 [  6%]  (Warmup)\nChain 2: Iteration:  138 / 2000 [  6%]  (Warmup)\nChain 2: Iteration:  139 / 2000 [  6%]  (Warmup)\nChain 2: Iteration:  140 / 2000 [  7%]  (Warmup)\nChain 2: Iteration:  141 / 2000 [  7%]  (Warmup)\nChain 2: Iteration:  142 / 2000 [  7%]  (Warmup)\nChain 2: Iteration:  143 / 2000 [  7%]  (Warmup)\nChain 2: Iteration:  144 / 2000 [  7%]  (Warmup)\nChain 2: Iteration:  145 / 2000 [  7%]  (Warmup)\nChain 2: Iteration:  146 / 2000 [  7%]  (Warmup)\nChain 2: Iteration:  147 / 2000 [  7%]  (Warmup)\nChain 2: Iteration:  148 / 2000 [  7%]  (Warmup)\nChain 2: Iteration:  149 / 2000 [  7%]  (Warmup)\nChain 2: Iteration:  150 / 2000 [  7%]  (Warmup)\nChain 2: Iteration:  151 / 2000 [  7%]  (Warmup)\nChain 2: Iteration:  152 / 2000 [  7%]  (Warmup)\nChain 2: Iteration:  153 / 2000 [  7%]  (Warmup)\nChain 2: Iteration:  154 / 2000 [  7%]  (Warmup)\nChain 2: Iteration:  155 / 2000 [  7%]  (Warmup)\nChain 2: Iteration:  156 / 2000 [  7%]  (Warmup)\nChain 2: Iteration:  157 / 2000 [  7%]  (Warmup)\nChain 2: Iteration:  158 / 2000 [  7%]  (Warmup)\nChain 2: Iteration:  159 / 2000 [  7%]  (Warmup)\nChain 2: Iteration:  160 / 2000 [  8%]  (Warmup)\nChain 2: Iteration:  161 / 2000 [  8%]  (Warmup)\nChain 2: Iteration:  162 / 2000 [  8%]  (Warmup)\nChain 2: Iteration:  163 / 2000 [  8%]  (Warmup)\nChain 2: Iteration:  164 / 2000 [  8%]  (Warmup)\nChain 2: Iteration:  165 / 2000 [  8%]  (Warmup)\nChain 2: Iteration:  166 / 2000 [  8%]  (Warmup)\nChain 2: Iteration:  167 / 2000 [  8%]  (Warmup)\nChain 2: Iteration:  168 / 2000 [  8%]  (Warmup)\nChain 2: Iteration:  169 / 2000 [  8%]  (Warmup)\nChain 2: Iteration:  170 / 2000 [  8%]  (Warmup)\nChain 2: Iteration:  171 / 2000 [  8%]  (Warmup)\nChain 2: Iteration:  172 / 2000 [  8%]  (Warmup)\nChain 2: Iteration:  173 / 2000 [  8%]  (Warmup)\nChain 2: Iteration:  174 / 2000 [  8%]  (Warmup)\nChain 2: Iteration:  175 / 2000 [  8%]  (Warmup)\nChain 2: Iteration:  176 / 2000 [  8%]  (Warmup)\nChain 2: Iteration:  177 / 2000 [  8%]  (Warmup)\nChain 2: Iteration:  178 / 2000 [  8%]  (Warmup)\nChain 2: Iteration:  179 / 2000 [  8%]  (Warmup)\nChain 2: Iteration:  180 / 2000 [  9%]  (Warmup)\nChain 2: Iteration:  181 / 2000 [  9%]  (Warmup)\nChain 2: Iteration:  182 / 2000 [  9%]  (Warmup)\nChain 2: Iteration:  183 / 2000 [  9%]  (Warmup)\nChain 2: Iteration:  184 / 2000 [  9%]  (Warmup)\nChain 2: Iteration:  185 / 2000 [  9%]  (Warmup)\nChain 2: Iteration:  186 / 2000 [  9%]  (Warmup)\nChain 2: Iteration:  187 / 2000 [  9%]  (Warmup)\nChain 2: Iteration:  188 / 2000 [  9%]  (Warmup)\nChain 2: Iteration:  189 / 2000 [  9%]  (Warmup)\nChain 2: Iteration:  190 / 2000 [  9%]  (Warmup)\nChain 2: Iteration:  191 / 2000 [  9%]  (Warmup)\nChain 2: Iteration:  192 / 2000 [  9%]  (Warmup)\nChain 2: Iteration:  193 / 2000 [  9%]  (Warmup)\nChain 2: Iteration:  194 / 2000 [  9%]  (Warmup)\nChain 2: Iteration:  195 / 2000 [  9%]  (Warmup)\nChain 2: Iteration:  196 / 2000 [  9%]  (Warmup)\nChain 2: Iteration:  197 / 2000 [  9%]  (Warmup)\nChain 2: Iteration:  198 / 2000 [  9%]  (Warmup)\nChain 2: Iteration:  199 / 2000 [  9%]  (Warmup)\nChain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 2: Iteration:  201 / 2000 [ 10%]  (Warmup)\nChain 2: Iteration:  202 / 2000 [ 10%]  (Warmup)\nChain 2: Iteration:  203 / 2000 [ 10%]  (Warmup)\nChain 2: Iteration:  204 / 2000 [ 10%]  (Warmup)\nChain 2: Iteration:  205 / 2000 [ 10%]  (Warmup)\nChain 2: Iteration:  206 / 2000 [ 10%]  (Warmup)\nChain 2: Iteration:  207 / 2000 [ 10%]  (Warmup)\nChain 2: Iteration:  208 / 2000 [ 10%]  (Warmup)\nChain 2: Iteration:  209 / 2000 [ 10%]  (Warmup)\nChain 2: Iteration:  210 / 2000 [ 10%]  (Warmup)\nChain 2: Iteration:  211 / 2000 [ 10%]  (Warmup)\nChain 2: Iteration:  212 / 2000 [ 10%]  (Warmup)\nChain 2: Iteration:  213 / 2000 [ 10%]  (Warmup)\nChain 2: Iteration:  214 / 2000 [ 10%]  (Warmup)\nChain 2: Iteration:  215 / 2000 [ 10%]  (Warmup)\nChain 2: Iteration:  216 / 2000 [ 10%]  (Warmup)\nChain 2: Iteration:  217 / 2000 [ 10%]  (Warmup)\nChain 2: Iteration:  218 / 2000 [ 10%]  (Warmup)\nChain 2: Iteration:  219 / 2000 [ 10%]  (Warmup)\nChain 2: Iteration:  220 / 2000 [ 11%]  (Warmup)\nChain 2: Iteration:  221 / 2000 [ 11%]  (Warmup)\nChain 2: Iteration:  222 / 2000 [ 11%]  (Warmup)\nChain 2: Iteration:  223 / 2000 [ 11%]  (Warmup)\nChain 2: Iteration:  224 / 2000 [ 11%]  (Warmup)\nChain 2: Iteration:  225 / 2000 [ 11%]  (Warmup)\nChain 2: Iteration:  226 / 2000 [ 11%]  (Warmup)\nChain 2: Iteration:  227 / 2000 [ 11%]  (Warmup)\nChain 2: Iteration:  228 / 2000 [ 11%]  (Warmup)\nChain 2: Iteration:  229 / 2000 [ 11%]  (Warmup)\nChain 2: Iteration:  230 / 2000 [ 11%]  (Warmup)\nChain 2: Iteration:  231 / 2000 [ 11%]  (Warmup)\nChain 2: Iteration:  232 / 2000 [ 11%]  (Warmup)\nChain 2: Iteration:  233 / 2000 [ 11%]  (Warmup)\nChain 2: Iteration:  234 / 2000 [ 11%]  (Warmup)\nChain 2: Iteration:  235 / 2000 [ 11%]  (Warmup)\nChain 2: Iteration:  236 / 2000 [ 11%]  (Warmup)\nChain 2: Iteration:  237 / 2000 [ 11%]  (Warmup)\nChain 2: Iteration:  238 / 2000 [ 11%]  (Warmup)\nChain 2: Iteration:  239 / 2000 [ 11%]  (Warmup)\nChain 2: Iteration:  240 / 2000 [ 12%]  (Warmup)\nChain 2: Iteration:  241 / 2000 [ 12%]  (Warmup)\nChain 2: Iteration:  242 / 2000 [ 12%]  (Warmup)\nChain 2: Iteration:  243 / 2000 [ 12%]  (Warmup)\nChain 2: Iteration:  244 / 2000 [ 12%]  (Warmup)\nChain 2: Iteration:  245 / 2000 [ 12%]  (Warmup)\nChain 2: Iteration:  246 / 2000 [ 12%]  (Warmup)\nChain 2: Iteration:  247 / 2000 [ 12%]  (Warmup)\nChain 2: Iteration:  248 / 2000 [ 12%]  (Warmup)\nChain 2: Iteration:  249 / 2000 [ 12%]  (Warmup)\nChain 2: Iteration:  250 / 2000 [ 12%]  (Warmup)\nChain 2: Iteration:  251 / 2000 [ 12%]  (Warmup)\nChain 2: Iteration:  252 / 2000 [ 12%]  (Warmup)\nChain 2: Iteration:  253 / 2000 [ 12%]  (Warmup)\nChain 2: Iteration:  254 / 2000 [ 12%]  (Warmup)\nChain 2: Iteration:  255 / 2000 [ 12%]  (Warmup)\nChain 2: Iteration:  256 / 2000 [ 12%]  (Warmup)\nChain 2: Iteration:  257 / 2000 [ 12%]  (Warmup)\nChain 2: Iteration:  258 / 2000 [ 12%]  (Warmup)\nChain 2: Iteration:  259 / 2000 [ 12%]  (Warmup)\nChain 2: Iteration:  260 / 2000 [ 13%]  (Warmup)\nChain 2: Iteration:  261 / 2000 [ 13%]  (Warmup)\nChain 2: Iteration:  262 / 2000 [ 13%]  (Warmup)\nChain 2: Iteration:  263 / 2000 [ 13%]  (Warmup)\nChain 2: Iteration:  264 / 2000 [ 13%]  (Warmup)\nChain 2: Iteration:  265 / 2000 [ 13%]  (Warmup)\nChain 2: Iteration:  266 / 2000 [ 13%]  (Warmup)\nChain 2: Iteration:  267 / 2000 [ 13%]  (Warmup)\nChain 2: Iteration:  268 / 2000 [ 13%]  (Warmup)\nChain 2: Iteration:  269 / 2000 [ 13%]  (Warmup)\nChain 2: Iteration:  270 / 2000 [ 13%]  (Warmup)\nChain 2: Iteration:  271 / 2000 [ 13%]  (Warmup)\nChain 2: Iteration:  272 / 2000 [ 13%]  (Warmup)\nChain 2: Iteration:  273 / 2000 [ 13%]  (Warmup)\nChain 2: Iteration:  274 / 2000 [ 13%]  (Warmup)\nChain 2: Iteration:  275 / 2000 [ 13%]  (Warmup)\nChain 2: Iteration:  276 / 2000 [ 13%]  (Warmup)\nChain 2: Iteration:  277 / 2000 [ 13%]  (Warmup)\nChain 2: Iteration:  278 / 2000 [ 13%]  (Warmup)\nChain 2: Iteration:  279 / 2000 [ 13%]  (Warmup)\nChain 2: Iteration:  280 / 2000 [ 14%]  (Warmup)\nChain 2: Iteration:  281 / 2000 [ 14%]  (Warmup)\nChain 2: Iteration:  282 / 2000 [ 14%]  (Warmup)\nChain 2: Iteration:  283 / 2000 [ 14%]  (Warmup)\nChain 2: Iteration:  284 / 2000 [ 14%]  (Warmup)\nChain 2: Iteration:  285 / 2000 [ 14%]  (Warmup)\nChain 2: Iteration:  286 / 2000 [ 14%]  (Warmup)\nChain 2: Iteration:  287 / 2000 [ 14%]  (Warmup)\nChain 2: Iteration:  288 / 2000 [ 14%]  (Warmup)\nChain 2: Iteration:  289 / 2000 [ 14%]  (Warmup)\nChain 2: Iteration:  290 / 2000 [ 14%]  (Warmup)\nChain 2: Iteration:  291 / 2000 [ 14%]  (Warmup)\nChain 2: Iteration:  292 / 2000 [ 14%]  (Warmup)\nChain 2: Iteration:  293 / 2000 [ 14%]  (Warmup)\nChain 2: Iteration:  294 / 2000 [ 14%]  (Warmup)\nChain 2: Iteration:  295 / 2000 [ 14%]  (Warmup)\nChain 2: Iteration:  296 / 2000 [ 14%]  (Warmup)\nChain 2: Iteration:  297 / 2000 [ 14%]  (Warmup)\nChain 2: Iteration:  298 / 2000 [ 14%]  (Warmup)\nChain 2: Iteration:  299 / 2000 [ 14%]  (Warmup)\nChain 2: Iteration:  300 / 2000 [ 15%]  (Warmup)\nChain 2: Iteration:  301 / 2000 [ 15%]  (Warmup)\nChain 2: Iteration:  302 / 2000 [ 15%]  (Warmup)\nChain 2: Iteration:  303 / 2000 [ 15%]  (Warmup)\nChain 2: Iteration:  304 / 2000 [ 15%]  (Warmup)\nChain 2: Iteration:  305 / 2000 [ 15%]  (Warmup)\nChain 2: Iteration:  306 / 2000 [ 15%]  (Warmup)\nChain 2: Iteration:  307 / 2000 [ 15%]  (Warmup)\nChain 2: Iteration:  308 / 2000 [ 15%]  (Warmup)\nChain 2: Iteration:  309 / 2000 [ 15%]  (Warmup)\nChain 2: Iteration:  310 / 2000 [ 15%]  (Warmup)\nChain 2: Iteration:  311 / 2000 [ 15%]  (Warmup)\nChain 2: Iteration:  312 / 2000 [ 15%]  (Warmup)\nChain 2: Iteration:  313 / 2000 [ 15%]  (Warmup)\nChain 2: Iteration:  314 / 2000 [ 15%]  (Warmup)\nChain 2: Iteration:  315 / 2000 [ 15%]  (Warmup)\nChain 2: Iteration:  316 / 2000 [ 15%]  (Warmup)\nChain 2: Iteration:  317 / 2000 [ 15%]  (Warmup)\nChain 2: Iteration:  318 / 2000 [ 15%]  (Warmup)\nChain 2: Iteration:  319 / 2000 [ 15%]  (Warmup)\nChain 2: Iteration:  320 / 2000 [ 16%]  (Warmup)\nChain 2: Iteration:  321 / 2000 [ 16%]  (Warmup)\nChain 2: Iteration:  322 / 2000 [ 16%]  (Warmup)\nChain 2: Iteration:  323 / 2000 [ 16%]  (Warmup)\nChain 2: Iteration:  324 / 2000 [ 16%]  (Warmup)\nChain 2: Iteration:  325 / 2000 [ 16%]  (Warmup)\nChain 2: Iteration:  326 / 2000 [ 16%]  (Warmup)\nChain 2: Iteration:  327 / 2000 [ 16%]  (Warmup)\nChain 2: Iteration:  328 / 2000 [ 16%]  (Warmup)\nChain 2: Iteration:  329 / 2000 [ 16%]  (Warmup)\nChain 2: Iteration:  330 / 2000 [ 16%]  (Warmup)\nChain 2: Iteration:  331 / 2000 [ 16%]  (Warmup)\nChain 2: Iteration:  332 / 2000 [ 16%]  (Warmup)\nChain 2: Iteration:  333 / 2000 [ 16%]  (Warmup)\nChain 2: Iteration:  334 / 2000 [ 16%]  (Warmup)\nChain 2: Iteration:  335 / 2000 [ 16%]  (Warmup)\nChain 2: Iteration:  336 / 2000 [ 16%]  (Warmup)\nChain 2: Iteration:  337 / 2000 [ 16%]  (Warmup)\nChain 2: Iteration:  338 / 2000 [ 16%]  (Warmup)\nChain 2: Iteration:  339 / 2000 [ 16%]  (Warmup)\nChain 2: Iteration:  340 / 2000 [ 17%]  (Warmup)\nChain 2: Iteration:  341 / 2000 [ 17%]  (Warmup)\nChain 2: Iteration:  342 / 2000 [ 17%]  (Warmup)\nChain 2: Iteration:  343 / 2000 [ 17%]  (Warmup)\nChain 2: Iteration:  344 / 2000 [ 17%]  (Warmup)\nChain 2: Iteration:  345 / 2000 [ 17%]  (Warmup)\nChain 2: Iteration:  346 / 2000 [ 17%]  (Warmup)\nChain 2: Iteration:  347 / 2000 [ 17%]  (Warmup)\nChain 2: Iteration:  348 / 2000 [ 17%]  (Warmup)\nChain 2: Iteration:  349 / 2000 [ 17%]  (Warmup)\nChain 2: Iteration:  350 / 2000 [ 17%]  (Warmup)\nChain 2: Iteration:  351 / 2000 [ 17%]  (Warmup)\nChain 2: Iteration:  352 / 2000 [ 17%]  (Warmup)\nChain 2: Iteration:  353 / 2000 [ 17%]  (Warmup)\nChain 2: Iteration:  354 / 2000 [ 17%]  (Warmup)\nChain 2: Iteration:  355 / 2000 [ 17%]  (Warmup)\nChain 2: Iteration:  356 / 2000 [ 17%]  (Warmup)\nChain 2: Iteration:  357 / 2000 [ 17%]  (Warmup)\nChain 2: Iteration:  358 / 2000 [ 17%]  (Warmup)\nChain 2: Iteration:  359 / 2000 [ 17%]  (Warmup)\nChain 2: Iteration:  360 / 2000 [ 18%]  (Warmup)\nChain 2: Iteration:  361 / 2000 [ 18%]  (Warmup)\nChain 2: Iteration:  362 / 2000 [ 18%]  (Warmup)\nChain 2: Iteration:  363 / 2000 [ 18%]  (Warmup)\nChain 2: Iteration:  364 / 2000 [ 18%]  (Warmup)\nChain 2: Iteration:  365 / 2000 [ 18%]  (Warmup)\nChain 2: Iteration:  366 / 2000 [ 18%]  (Warmup)\nChain 2: Iteration:  367 / 2000 [ 18%]  (Warmup)\nChain 2: Iteration:  368 / 2000 [ 18%]  (Warmup)\nChain 2: Iteration:  369 / 2000 [ 18%]  (Warmup)\nChain 2: Iteration:  370 / 2000 [ 18%]  (Warmup)\nChain 2: Iteration:  371 / 2000 [ 18%]  (Warmup)\nChain 2: Iteration:  372 / 2000 [ 18%]  (Warmup)\nChain 2: Iteration:  373 / 2000 [ 18%]  (Warmup)\nChain 2: Iteration:  374 / 2000 [ 18%]  (Warmup)\nChain 2: Iteration:  375 / 2000 [ 18%]  (Warmup)\nChain 2: Iteration:  376 / 2000 [ 18%]  (Warmup)\nChain 2: Iteration:  377 / 2000 [ 18%]  (Warmup)\nChain 2: Iteration:  378 / 2000 [ 18%]  (Warmup)\nChain 2: Iteration:  379 / 2000 [ 18%]  (Warmup)\nChain 2: Iteration:  380 / 2000 [ 19%]  (Warmup)\nChain 2: Iteration:  381 / 2000 [ 19%]  (Warmup)\nChain 2: Iteration:  382 / 2000 [ 19%]  (Warmup)\nChain 2: Iteration:  383 / 2000 [ 19%]  (Warmup)\nChain 2: Iteration:  384 / 2000 [ 19%]  (Warmup)\nChain 2: Iteration:  385 / 2000 [ 19%]  (Warmup)\nChain 2: Iteration:  386 / 2000 [ 19%]  (Warmup)\nChain 2: Iteration:  387 / 2000 [ 19%]  (Warmup)\nChain 2: Iteration:  388 / 2000 [ 19%]  (Warmup)\nChain 2: Iteration:  389 / 2000 [ 19%]  (Warmup)\nChain 2: Iteration:  390 / 2000 [ 19%]  (Warmup)\nChain 2: Iteration:  391 / 2000 [ 19%]  (Warmup)\nChain 2: Iteration:  392 / 2000 [ 19%]  (Warmup)\nChain 2: Iteration:  393 / 2000 [ 19%]  (Warmup)\nChain 2: Iteration:  394 / 2000 [ 19%]  (Warmup)\nChain 2: Iteration:  395 / 2000 [ 19%]  (Warmup)\nChain 2: Iteration:  396 / 2000 [ 19%]  (Warmup)\nChain 2: Iteration:  397 / 2000 [ 19%]  (Warmup)\nChain 2: Iteration:  398 / 2000 [ 19%]  (Warmup)\nChain 2: Iteration:  399 / 2000 [ 19%]  (Warmup)\nChain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 2: Iteration:  401 / 2000 [ 20%]  (Warmup)\nChain 2: Iteration:  402 / 2000 [ 20%]  (Warmup)\nChain 2: Iteration:  403 / 2000 [ 20%]  (Warmup)\nChain 2: Iteration:  404 / 2000 [ 20%]  (Warmup)\nChain 2: Iteration:  405 / 2000 [ 20%]  (Warmup)\nChain 2: Iteration:  406 / 2000 [ 20%]  (Warmup)\nChain 2: Iteration:  407 / 2000 [ 20%]  (Warmup)\nChain 2: Iteration:  408 / 2000 [ 20%]  (Warmup)\nChain 2: Iteration:  409 / 2000 [ 20%]  (Warmup)\nChain 2: Iteration:  410 / 2000 [ 20%]  (Warmup)\nChain 2: Iteration:  411 / 2000 [ 20%]  (Warmup)\nChain 2: Iteration:  412 / 2000 [ 20%]  (Warmup)\nChain 2: Iteration:  413 / 2000 [ 20%]  (Warmup)\nChain 2: Iteration:  414 / 2000 [ 20%]  (Warmup)\nChain 2: Iteration:  415 / 2000 [ 20%]  (Warmup)\nChain 2: Iteration:  416 / 2000 [ 20%]  (Warmup)\nChain 2: Iteration:  417 / 2000 [ 20%]  (Warmup)\nChain 2: Iteration:  418 / 2000 [ 20%]  (Warmup)\nChain 2: Iteration:  419 / 2000 [ 20%]  (Warmup)\nChain 2: Iteration:  420 / 2000 [ 21%]  (Warmup)\nChain 2: Iteration:  421 / 2000 [ 21%]  (Warmup)\nChain 2: Iteration:  422 / 2000 [ 21%]  (Warmup)\nChain 2: Iteration:  423 / 2000 [ 21%]  (Warmup)\nChain 2: Iteration:  424 / 2000 [ 21%]  (Warmup)\nChain 2: Iteration:  425 / 2000 [ 21%]  (Warmup)\nChain 2: Iteration:  426 / 2000 [ 21%]  (Warmup)\nChain 2: Iteration:  427 / 2000 [ 21%]  (Warmup)\nChain 2: Iteration:  428 / 2000 [ 21%]  (Warmup)\nChain 2: Iteration:  429 / 2000 [ 21%]  (Warmup)\nChain 2: Iteration:  430 / 2000 [ 21%]  (Warmup)\nChain 2: Iteration:  431 / 2000 [ 21%]  (Warmup)\nChain 2: Iteration:  432 / 2000 [ 21%]  (Warmup)\nChain 2: Iteration:  433 / 2000 [ 21%]  (Warmup)\nChain 2: Iteration:  434 / 2000 [ 21%]  (Warmup)\nChain 2: Iteration:  435 / 2000 [ 21%]  (Warmup)\nChain 2: Iteration:  436 / 2000 [ 21%]  (Warmup)\nChain 2: Iteration:  437 / 2000 [ 21%]  (Warmup)\nChain 2: Iteration:  438 / 2000 [ 21%]  (Warmup)\nChain 2: Iteration:  439 / 2000 [ 21%]  (Warmup)\nChain 2: Iteration:  440 / 2000 [ 22%]  (Warmup)\nChain 2: Iteration:  441 / 2000 [ 22%]  (Warmup)\nChain 2: Iteration:  442 / 2000 [ 22%]  (Warmup)\nChain 2: Iteration:  443 / 2000 [ 22%]  (Warmup)\nChain 2: Iteration:  444 / 2000 [ 22%]  (Warmup)\nChain 2: Iteration:  445 / 2000 [ 22%]  (Warmup)\nChain 2: Iteration:  446 / 2000 [ 22%]  (Warmup)\nChain 2: Iteration:  447 / 2000 [ 22%]  (Warmup)\nChain 2: Iteration:  448 / 2000 [ 22%]  (Warmup)\nChain 2: Iteration:  449 / 2000 [ 22%]  (Warmup)\nChain 2: Iteration:  450 / 2000 [ 22%]  (Warmup)\nChain 2: Iteration:  451 / 2000 [ 22%]  (Warmup)\nChain 2: Iteration:  452 / 2000 [ 22%]  (Warmup)\nChain 2: Iteration:  453 / 2000 [ 22%]  (Warmup)\nChain 2: Iteration:  454 / 2000 [ 22%]  (Warmup)\nChain 2: Iteration:  455 / 2000 [ 22%]  (Warmup)\nChain 2: Iteration:  456 / 2000 [ 22%]  (Warmup)\nChain 2: Iteration:  457 / 2000 [ 22%]  (Warmup)\nChain 2: Iteration:  458 / 2000 [ 22%]  (Warmup)\nChain 2: Iteration:  459 / 2000 [ 22%]  (Warmup)\nChain 2: Iteration:  460 / 2000 [ 23%]  (Warmup)\nChain 2: Iteration:  461 / 2000 [ 23%]  (Warmup)\nChain 2: Iteration:  462 / 2000 [ 23%]  (Warmup)\nChain 2: Iteration:  463 / 2000 [ 23%]  (Warmup)\nChain 2: Iteration:  464 / 2000 [ 23%]  (Warmup)\nChain 2: Iteration:  465 / 2000 [ 23%]  (Warmup)\nChain 2: Iteration:  466 / 2000 [ 23%]  (Warmup)\nChain 2: Iteration:  467 / 2000 [ 23%]  (Warmup)\nChain 2: Iteration:  468 / 2000 [ 23%]  (Warmup)\nChain 2: Iteration:  469 / 2000 [ 23%]  (Warmup)\nChain 2: Iteration:  470 / 2000 [ 23%]  (Warmup)\nChain 2: Iteration:  471 / 2000 [ 23%]  (Warmup)\nChain 2: Iteration:  472 / 2000 [ 23%]  (Warmup)\nChain 2: Iteration:  473 / 2000 [ 23%]  (Warmup)\nChain 2: Iteration:  474 / 2000 [ 23%]  (Warmup)\nChain 2: Iteration:  475 / 2000 [ 23%]  (Warmup)\nChain 2: Iteration:  476 / 2000 [ 23%]  (Warmup)\nChain 2: Iteration:  477 / 2000 [ 23%]  (Warmup)\nChain 2: Iteration:  478 / 2000 [ 23%]  (Warmup)\nChain 2: Iteration:  479 / 2000 [ 23%]  (Warmup)\nChain 2: Iteration:  480 / 2000 [ 24%]  (Warmup)\nChain 2: Iteration:  481 / 2000 [ 24%]  (Warmup)\nChain 2: Iteration:  482 / 2000 [ 24%]  (Warmup)\nChain 2: Iteration:  483 / 2000 [ 24%]  (Warmup)\nChain 2: Iteration:  484 / 2000 [ 24%]  (Warmup)\nChain 2: Iteration:  485 / 2000 [ 24%]  (Warmup)\nChain 2: Iteration:  486 / 2000 [ 24%]  (Warmup)\nChain 2: Iteration:  487 / 2000 [ 24%]  (Warmup)\nChain 2: Iteration:  488 / 2000 [ 24%]  (Warmup)\nChain 2: Iteration:  489 / 2000 [ 24%]  (Warmup)\nChain 2: Iteration:  490 / 2000 [ 24%]  (Warmup)\nChain 2: Iteration:  491 / 2000 [ 24%]  (Warmup)\nChain 2: Iteration:  492 / 2000 [ 24%]  (Warmup)\nChain 2: Iteration:  493 / 2000 [ 24%]  (Warmup)\nChain 2: Iteration:  494 / 2000 [ 24%]  (Warmup)\nChain 2: Iteration:  495 / 2000 [ 24%]  (Warmup)\nChain 2: Iteration:  496 / 2000 [ 24%]  (Warmup)\nChain 2: Iteration:  497 / 2000 [ 24%]  (Warmup)\nChain 2: Iteration:  498 / 2000 [ 24%]  (Warmup)\nChain 2: Iteration:  499 / 2000 [ 24%]  (Warmup)\nChain 2: Iteration:  500 / 2000 [ 25%]  (Warmup)\nChain 2: Iteration:  501 / 2000 [ 25%]  (Warmup)\nChain 2: Iteration:  502 / 2000 [ 25%]  (Warmup)\nChain 2: Iteration:  503 / 2000 [ 25%]  (Warmup)\nChain 2: Iteration:  504 / 2000 [ 25%]  (Warmup)\nChain 2: Iteration:  505 / 2000 [ 25%]  (Warmup)\nChain 2: Iteration:  506 / 2000 [ 25%]  (Warmup)\nChain 2: Iteration:  507 / 2000 [ 25%]  (Warmup)\nChain 2: Iteration:  508 / 2000 [ 25%]  (Warmup)\nChain 2: Iteration:  509 / 2000 [ 25%]  (Warmup)\nChain 2: Iteration:  510 / 2000 [ 25%]  (Warmup)\nChain 2: Iteration:  511 / 2000 [ 25%]  (Warmup)\nChain 2: Iteration:  512 / 2000 [ 25%]  (Warmup)\nChain 2: Iteration:  513 / 2000 [ 25%]  (Warmup)\nChain 2: Iteration:  514 / 2000 [ 25%]  (Warmup)\nChain 2: Iteration:  515 / 2000 [ 25%]  (Warmup)\nChain 2: Iteration:  516 / 2000 [ 25%]  (Warmup)\nChain 2: Iteration:  517 / 2000 [ 25%]  (Warmup)\nChain 2: Iteration:  518 / 2000 [ 25%]  (Warmup)\nChain 2: Iteration:  519 / 2000 [ 25%]  (Warmup)\nChain 2: Iteration:  520 / 2000 [ 26%]  (Warmup)\nChain 2: Iteration:  521 / 2000 [ 26%]  (Warmup)\nChain 2: Iteration:  522 / 2000 [ 26%]  (Warmup)\nChain 2: Iteration:  523 / 2000 [ 26%]  (Warmup)\nChain 2: Iteration:  524 / 2000 [ 26%]  (Warmup)\nChain 2: Iteration:  525 / 2000 [ 26%]  (Warmup)\nChain 2: Iteration:  526 / 2000 [ 26%]  (Warmup)\nChain 2: Iteration:  527 / 2000 [ 26%]  (Warmup)\nChain 2: Iteration:  528 / 2000 [ 26%]  (Warmup)\nChain 2: Iteration:  529 / 2000 [ 26%]  (Warmup)\nChain 2: Iteration:  530 / 2000 [ 26%]  (Warmup)\nChain 2: Iteration:  531 / 2000 [ 26%]  (Warmup)\nChain 2: Iteration:  532 / 2000 [ 26%]  (Warmup)\nChain 2: Iteration:  533 / 2000 [ 26%]  (Warmup)\nChain 2: Iteration:  534 / 2000 [ 26%]  (Warmup)\nChain 2: Iteration:  535 / 2000 [ 26%]  (Warmup)\nChain 2: Iteration:  536 / 2000 [ 26%]  (Warmup)\nChain 2: Iteration:  537 / 2000 [ 26%]  (Warmup)\nChain 2: Iteration:  538 / 2000 [ 26%]  (Warmup)\nChain 2: Iteration:  539 / 2000 [ 26%]  (Warmup)\nChain 2: Iteration:  540 / 2000 [ 27%]  (Warmup)\nChain 2: Iteration:  541 / 2000 [ 27%]  (Warmup)\nChain 2: Iteration:  542 / 2000 [ 27%]  (Warmup)\nChain 2: Iteration:  543 / 2000 [ 27%]  (Warmup)\nChain 2: Iteration:  544 / 2000 [ 27%]  (Warmup)\nChain 2: Iteration:  545 / 2000 [ 27%]  (Warmup)\nChain 2: Iteration:  546 / 2000 [ 27%]  (Warmup)\nChain 2: Iteration:  547 / 2000 [ 27%]  (Warmup)\nChain 2: Iteration:  548 / 2000 [ 27%]  (Warmup)\nChain 2: Iteration:  549 / 2000 [ 27%]  (Warmup)\nChain 2: Iteration:  550 / 2000 [ 27%]  (Warmup)\nChain 2: Iteration:  551 / 2000 [ 27%]  (Warmup)\nChain 2: Iteration:  552 / 2000 [ 27%]  (Warmup)\nChain 2: Iteration:  553 / 2000 [ 27%]  (Warmup)\nChain 2: Iteration:  554 / 2000 [ 27%]  (Warmup)\nChain 2: Iteration:  555 / 2000 [ 27%]  (Warmup)\nChain 2: Iteration:  556 / 2000 [ 27%]  (Warmup)\nChain 2: Iteration:  557 / 2000 [ 27%]  (Warmup)\nChain 2: Iteration:  558 / 2000 [ 27%]  (Warmup)\nChain 2: Iteration:  559 / 2000 [ 27%]  (Warmup)\nChain 2: Iteration:  560 / 2000 [ 28%]  (Warmup)\nChain 2: Iteration:  561 / 2000 [ 28%]  (Warmup)\nChain 2: Iteration:  562 / 2000 [ 28%]  (Warmup)\nChain 2: Iteration:  563 / 2000 [ 28%]  (Warmup)\nChain 2: Iteration:  564 / 2000 [ 28%]  (Warmup)\nChain 2: Iteration:  565 / 2000 [ 28%]  (Warmup)\nChain 2: Iteration:  566 / 2000 [ 28%]  (Warmup)\nChain 2: Iteration:  567 / 2000 [ 28%]  (Warmup)\nChain 2: Iteration:  568 / 2000 [ 28%]  (Warmup)\nChain 2: Iteration:  569 / 2000 [ 28%]  (Warmup)\nChain 2: Iteration:  570 / 2000 [ 28%]  (Warmup)\nChain 2: Iteration:  571 / 2000 [ 28%]  (Warmup)\nChain 2: Iteration:  572 / 2000 [ 28%]  (Warmup)\nChain 2: Iteration:  573 / 2000 [ 28%]  (Warmup)\nChain 2: Iteration:  574 / 2000 [ 28%]  (Warmup)\nChain 2: Iteration:  575 / 2000 [ 28%]  (Warmup)\nChain 2: Iteration:  576 / 2000 [ 28%]  (Warmup)\nChain 2: Iteration:  577 / 2000 [ 28%]  (Warmup)\nChain 2: Iteration:  578 / 2000 [ 28%]  (Warmup)\nChain 2: Iteration:  579 / 2000 [ 28%]  (Warmup)\nChain 2: Iteration:  580 / 2000 [ 29%]  (Warmup)\nChain 2: Iteration:  581 / 2000 [ 29%]  (Warmup)\nChain 2: Iteration:  582 / 2000 [ 29%]  (Warmup)\nChain 2: Iteration:  583 / 2000 [ 29%]  (Warmup)\nChain 2: Iteration:  584 / 2000 [ 29%]  (Warmup)\nChain 2: Iteration:  585 / 2000 [ 29%]  (Warmup)\nChain 2: Iteration:  586 / 2000 [ 29%]  (Warmup)\nChain 2: Iteration:  587 / 2000 [ 29%]  (Warmup)\nChain 2: Iteration:  588 / 2000 [ 29%]  (Warmup)\nChain 2: Iteration:  589 / 2000 [ 29%]  (Warmup)\nChain 2: Iteration:  590 / 2000 [ 29%]  (Warmup)\nChain 2: Iteration:  591 / 2000 [ 29%]  (Warmup)\nChain 2: Iteration:  592 / 2000 [ 29%]  (Warmup)\nChain 2: Iteration:  593 / 2000 [ 29%]  (Warmup)\nChain 2: Iteration:  594 / 2000 [ 29%]  (Warmup)\nChain 2: Iteration:  595 / 2000 [ 29%]  (Warmup)\nChain 2: Iteration:  596 / 2000 [ 29%]  (Warmup)\nChain 2: Iteration:  597 / 2000 [ 29%]  (Warmup)\nChain 2: Iteration:  598 / 2000 [ 29%]  (Warmup)\nChain 2: Iteration:  599 / 2000 [ 29%]  (Warmup)\nChain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 2: Iteration:  601 / 2000 [ 30%]  (Warmup)\nChain 2: Iteration:  602 / 2000 [ 30%]  (Warmup)\nChain 2: Iteration:  603 / 2000 [ 30%]  (Warmup)\nChain 2: Iteration:  604 / 2000 [ 30%]  (Warmup)\nChain 2: Iteration:  605 / 2000 [ 30%]  (Warmup)\nChain 2: Iteration:  606 / 2000 [ 30%]  (Warmup)\nChain 2: Iteration:  607 / 2000 [ 30%]  (Warmup)\nChain 2: Iteration:  608 / 2000 [ 30%]  (Warmup)\nChain 2: Iteration:  609 / 2000 [ 30%]  (Warmup)\nChain 2: Iteration:  610 / 2000 [ 30%]  (Warmup)\nChain 2: Iteration:  611 / 2000 [ 30%]  (Warmup)\nChain 2: Iteration:  612 / 2000 [ 30%]  (Warmup)\nChain 2: Iteration:  613 / 2000 [ 30%]  (Warmup)\nChain 2: Iteration:  614 / 2000 [ 30%]  (Warmup)\nChain 2: Iteration:  615 / 2000 [ 30%]  (Warmup)\nChain 2: Iteration:  616 / 2000 [ 30%]  (Warmup)\nChain 2: Iteration:  617 / 2000 [ 30%]  (Warmup)\nChain 2: Iteration:  618 / 2000 [ 30%]  (Warmup)\nChain 2: Iteration:  619 / 2000 [ 30%]  (Warmup)\nChain 2: Iteration:  620 / 2000 [ 31%]  (Warmup)\nChain 2: Iteration:  621 / 2000 [ 31%]  (Warmup)\nChain 2: Iteration:  622 / 2000 [ 31%]  (Warmup)\nChain 2: Iteration:  623 / 2000 [ 31%]  (Warmup)\nChain 2: Iteration:  624 / 2000 [ 31%]  (Warmup)\nChain 2: Iteration:  625 / 2000 [ 31%]  (Warmup)\nChain 2: Iteration:  626 / 2000 [ 31%]  (Warmup)\nChain 2: Iteration:  627 / 2000 [ 31%]  (Warmup)\nChain 2: Iteration:  628 / 2000 [ 31%]  (Warmup)\nChain 2: Iteration:  629 / 2000 [ 31%]  (Warmup)\nChain 2: Iteration:  630 / 2000 [ 31%]  (Warmup)\nChain 2: Iteration:  631 / 2000 [ 31%]  (Warmup)\nChain 2: Iteration:  632 / 2000 [ 31%]  (Warmup)\nChain 2: Iteration:  633 / 2000 [ 31%]  (Warmup)\nChain 2: Iteration:  634 / 2000 [ 31%]  (Warmup)\nChain 2: Iteration:  635 / 2000 [ 31%]  (Warmup)\nChain 2: Iteration:  636 / 2000 [ 31%]  (Warmup)\nChain 2: Iteration:  637 / 2000 [ 31%]  (Warmup)\nChain 2: Iteration:  638 / 2000 [ 31%]  (Warmup)\nChain 2: Iteration:  639 / 2000 [ 31%]  (Warmup)\nChain 2: Iteration:  640 / 2000 [ 32%]  (Warmup)\nChain 2: Iteration:  641 / 2000 [ 32%]  (Warmup)\nChain 2: Iteration:  642 / 2000 [ 32%]  (Warmup)\nChain 2: Iteration:  643 / 2000 [ 32%]  (Warmup)\nChain 2: Iteration:  644 / 2000 [ 32%]  (Warmup)\nChain 2: Iteration:  645 / 2000 [ 32%]  (Warmup)\nChain 2: Iteration:  646 / 2000 [ 32%]  (Warmup)\nChain 2: Iteration:  647 / 2000 [ 32%]  (Warmup)\nChain 2: Iteration:  648 / 2000 [ 32%]  (Warmup)\nChain 2: Iteration:  649 / 2000 [ 32%]  (Warmup)\nChain 2: Iteration:  650 / 2000 [ 32%]  (Warmup)\nChain 2: Iteration:  651 / 2000 [ 32%]  (Warmup)\nChain 2: Iteration:  652 / 2000 [ 32%]  (Warmup)\nChain 2: Iteration:  653 / 2000 [ 32%]  (Warmup)\nChain 2: Iteration:  654 / 2000 [ 32%]  (Warmup)\nChain 2: Iteration:  655 / 2000 [ 32%]  (Warmup)\nChain 2: Iteration:  656 / 2000 [ 32%]  (Warmup)\nChain 2: Iteration:  657 / 2000 [ 32%]  (Warmup)\nChain 2: Iteration:  658 / 2000 [ 32%]  (Warmup)\nChain 2: Iteration:  659 / 2000 [ 32%]  (Warmup)\nChain 2: Iteration:  660 / 2000 [ 33%]  (Warmup)\nChain 2: Iteration:  661 / 2000 [ 33%]  (Warmup)\nChain 2: Iteration:  662 / 2000 [ 33%]  (Warmup)\nChain 2: Iteration:  663 / 2000 [ 33%]  (Warmup)\nChain 2: Iteration:  664 / 2000 [ 33%]  (Warmup)\nChain 2: Iteration:  665 / 2000 [ 33%]  (Warmup)\nChain 2: Iteration:  666 / 2000 [ 33%]  (Warmup)\nChain 2: Iteration:  667 / 2000 [ 33%]  (Warmup)\nChain 2: Iteration:  668 / 2000 [ 33%]  (Warmup)\nChain 2: Iteration:  669 / 2000 [ 33%]  (Warmup)\nChain 2: Iteration:  670 / 2000 [ 33%]  (Warmup)\nChain 2: Iteration:  671 / 2000 [ 33%]  (Warmup)\nChain 2: Iteration:  672 / 2000 [ 33%]  (Warmup)\nChain 2: Iteration:  673 / 2000 [ 33%]  (Warmup)\nChain 2: Iteration:  674 / 2000 [ 33%]  (Warmup)\nChain 2: Iteration:  675 / 2000 [ 33%]  (Warmup)\nChain 2: Iteration:  676 / 2000 [ 33%]  (Warmup)\nChain 2: Iteration:  677 / 2000 [ 33%]  (Warmup)\nChain 2: Iteration:  678 / 2000 [ 33%]  (Warmup)\nChain 2: Iteration:  679 / 2000 [ 33%]  (Warmup)\nChain 2: Iteration:  680 / 2000 [ 34%]  (Warmup)\nChain 2: Iteration:  681 / 2000 [ 34%]  (Warmup)\nChain 2: Iteration:  682 / 2000 [ 34%]  (Warmup)\nChain 2: Iteration:  683 / 2000 [ 34%]  (Warmup)\nChain 2: Iteration:  684 / 2000 [ 34%]  (Warmup)\nChain 2: Iteration:  685 / 2000 [ 34%]  (Warmup)\nChain 2: Iteration:  686 / 2000 [ 34%]  (Warmup)\nChain 2: Iteration:  687 / 2000 [ 34%]  (Warmup)\nChain 2: Iteration:  688 / 2000 [ 34%]  (Warmup)\nChain 2: Iteration:  689 / 2000 [ 34%]  (Warmup)\nChain 2: Iteration:  690 / 2000 [ 34%]  (Warmup)\nChain 2: Iteration:  691 / 2000 [ 34%]  (Warmup)\nChain 2: Iteration:  692 / 2000 [ 34%]  (Warmup)\nChain 2: Iteration:  693 / 2000 [ 34%]  (Warmup)\nChain 2: Iteration:  694 / 2000 [ 34%]  (Warmup)\nChain 2: Iteration:  695 / 2000 [ 34%]  (Warmup)\nChain 2: Iteration:  696 / 2000 [ 34%]  (Warmup)\nChain 2: Iteration:  697 / 2000 [ 34%]  (Warmup)\nChain 2: Iteration:  698 / 2000 [ 34%]  (Warmup)\nChain 2: Iteration:  699 / 2000 [ 34%]  (Warmup)\nChain 2: Iteration:  700 / 2000 [ 35%]  (Warmup)\nChain 2: Iteration:  701 / 2000 [ 35%]  (Warmup)\nChain 2: Iteration:  702 / 2000 [ 35%]  (Warmup)\nChain 2: Iteration:  703 / 2000 [ 35%]  (Warmup)\nChain 2: Iteration:  704 / 2000 [ 35%]  (Warmup)\nChain 2: Iteration:  705 / 2000 [ 35%]  (Warmup)\nChain 2: Iteration:  706 / 2000 [ 35%]  (Warmup)\nChain 2: Iteration:  707 / 2000 [ 35%]  (Warmup)\nChain 2: Iteration:  708 / 2000 [ 35%]  (Warmup)\nChain 2: Iteration:  709 / 2000 [ 35%]  (Warmup)\nChain 2: Iteration:  710 / 2000 [ 35%]  (Warmup)\nChain 2: Iteration:  711 / 2000 [ 35%]  (Warmup)\nChain 2: Iteration:  712 / 2000 [ 35%]  (Warmup)\nChain 2: Iteration:  713 / 2000 [ 35%]  (Warmup)\nChain 2: Iteration:  714 / 2000 [ 35%]  (Warmup)\nChain 2: Iteration:  715 / 2000 [ 35%]  (Warmup)\nChain 2: Iteration:  716 / 2000 [ 35%]  (Warmup)\nChain 2: Iteration:  717 / 2000 [ 35%]  (Warmup)\nChain 2: Iteration:  718 / 2000 [ 35%]  (Warmup)\nChain 2: Iteration:  719 / 2000 [ 35%]  (Warmup)\nChain 2: Iteration:  720 / 2000 [ 36%]  (Warmup)\nChain 2: Iteration:  721 / 2000 [ 36%]  (Warmup)\nChain 2: Iteration:  722 / 2000 [ 36%]  (Warmup)\nChain 2: Iteration:  723 / 2000 [ 36%]  (Warmup)\nChain 2: Iteration:  724 / 2000 [ 36%]  (Warmup)\nChain 2: Iteration:  725 / 2000 [ 36%]  (Warmup)\nChain 2: Iteration:  726 / 2000 [ 36%]  (Warmup)\nChain 2: Iteration:  727 / 2000 [ 36%]  (Warmup)\nChain 2: Iteration:  728 / 2000 [ 36%]  (Warmup)\nChain 2: Iteration:  729 / 2000 [ 36%]  (Warmup)\nChain 2: Iteration:  730 / 2000 [ 36%]  (Warmup)\nChain 2: Iteration:  731 / 2000 [ 36%]  (Warmup)\nChain 2: Iteration:  732 / 2000 [ 36%]  (Warmup)\nChain 2: Iteration:  733 / 2000 [ 36%]  (Warmup)\nChain 2: Iteration:  734 / 2000 [ 36%]  (Warmup)\nChain 2: Iteration:  735 / 2000 [ 36%]  (Warmup)\nChain 2: Iteration:  736 / 2000 [ 36%]  (Warmup)\nChain 2: Iteration:  737 / 2000 [ 36%]  (Warmup)\nChain 2: Iteration:  738 / 2000 [ 36%]  (Warmup)\nChain 2: Iteration:  739 / 2000 [ 36%]  (Warmup)\nChain 2: Iteration:  740 / 2000 [ 37%]  (Warmup)\nChain 2: Iteration:  741 / 2000 [ 37%]  (Warmup)\nChain 2: Iteration:  742 / 2000 [ 37%]  (Warmup)\nChain 2: Iteration:  743 / 2000 [ 37%]  (Warmup)\nChain 2: Iteration:  744 / 2000 [ 37%]  (Warmup)\nChain 2: Iteration:  745 / 2000 [ 37%]  (Warmup)\nChain 2: Iteration:  746 / 2000 [ 37%]  (Warmup)\nChain 2: Iteration:  747 / 2000 [ 37%]  (Warmup)\nChain 2: Iteration:  748 / 2000 [ 37%]  (Warmup)\nChain 2: Iteration:  749 / 2000 [ 37%]  (Warmup)\nChain 2: Iteration:  750 / 2000 [ 37%]  (Warmup)\nChain 2: Iteration:  751 / 2000 [ 37%]  (Warmup)\nChain 2: Iteration:  752 / 2000 [ 37%]  (Warmup)\nChain 2: Iteration:  753 / 2000 [ 37%]  (Warmup)\nChain 2: Iteration:  754 / 2000 [ 37%]  (Warmup)\nChain 2: Iteration:  755 / 2000 [ 37%]  (Warmup)\nChain 2: Iteration:  756 / 2000 [ 37%]  (Warmup)\nChain 2: Iteration:  757 / 2000 [ 37%]  (Warmup)\nChain 2: Iteration:  758 / 2000 [ 37%]  (Warmup)\nChain 2: Iteration:  759 / 2000 [ 37%]  (Warmup)\nChain 2: Iteration:  760 / 2000 [ 38%]  (Warmup)\nChain 2: Iteration:  761 / 2000 [ 38%]  (Warmup)\nChain 2: Iteration:  762 / 2000 [ 38%]  (Warmup)\nChain 2: Iteration:  763 / 2000 [ 38%]  (Warmup)\nChain 2: Iteration:  764 / 2000 [ 38%]  (Warmup)\nChain 2: Iteration:  765 / 2000 [ 38%]  (Warmup)\nChain 2: Iteration:  766 / 2000 [ 38%]  (Warmup)\nChain 2: Iteration:  767 / 2000 [ 38%]  (Warmup)\nChain 2: Iteration:  768 / 2000 [ 38%]  (Warmup)\nChain 2: Iteration:  769 / 2000 [ 38%]  (Warmup)\nChain 2: Iteration:  770 / 2000 [ 38%]  (Warmup)\nChain 2: Iteration:  771 / 2000 [ 38%]  (Warmup)\nChain 2: Iteration:  772 / 2000 [ 38%]  (Warmup)\nChain 2: Iteration:  773 / 2000 [ 38%]  (Warmup)\nChain 2: Iteration:  774 / 2000 [ 38%]  (Warmup)\nChain 2: Iteration:  775 / 2000 [ 38%]  (Warmup)\nChain 2: Iteration:  776 / 2000 [ 38%]  (Warmup)\nChain 2: Iteration:  777 / 2000 [ 38%]  (Warmup)\nChain 2: Iteration:  778 / 2000 [ 38%]  (Warmup)\nChain 2: Iteration:  779 / 2000 [ 38%]  (Warmup)\nChain 2: Iteration:  780 / 2000 [ 39%]  (Warmup)\nChain 2: Iteration:  781 / 2000 [ 39%]  (Warmup)\nChain 2: Iteration:  782 / 2000 [ 39%]  (Warmup)\nChain 2: Iteration:  783 / 2000 [ 39%]  (Warmup)\nChain 2: Iteration:  784 / 2000 [ 39%]  (Warmup)\nChain 2: Iteration:  785 / 2000 [ 39%]  (Warmup)\nChain 2: Iteration:  786 / 2000 [ 39%]  (Warmup)\nChain 2: Iteration:  787 / 2000 [ 39%]  (Warmup)\nChain 2: Iteration:  788 / 2000 [ 39%]  (Warmup)\nChain 2: Iteration:  789 / 2000 [ 39%]  (Warmup)\nChain 2: Iteration:  790 / 2000 [ 39%]  (Warmup)\nChain 2: Iteration:  791 / 2000 [ 39%]  (Warmup)\nChain 2: Iteration:  792 / 2000 [ 39%]  (Warmup)\nChain 2: Iteration:  793 / 2000 [ 39%]  (Warmup)\nChain 2: Iteration:  794 / 2000 [ 39%]  (Warmup)\nChain 2: Iteration:  795 / 2000 [ 39%]  (Warmup)\nChain 2: Iteration:  796 / 2000 [ 39%]  (Warmup)\nChain 2: Iteration:  797 / 2000 [ 39%]  (Warmup)\nChain 2: Iteration:  798 / 2000 [ 39%]  (Warmup)\nChain 2: Iteration:  799 / 2000 [ 39%]  (Warmup)\nChain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 2: Iteration:  801 / 2000 [ 40%]  (Warmup)\nChain 2: Iteration:  802 / 2000 [ 40%]  (Warmup)\nChain 2: Iteration:  803 / 2000 [ 40%]  (Warmup)\nChain 2: Iteration:  804 / 2000 [ 40%]  (Warmup)\nChain 2: Iteration:  805 / 2000 [ 40%]  (Warmup)\nChain 2: Iteration:  806 / 2000 [ 40%]  (Warmup)\nChain 2: Iteration:  807 / 2000 [ 40%]  (Warmup)\nChain 2: Iteration:  808 / 2000 [ 40%]  (Warmup)\nChain 2: Iteration:  809 / 2000 [ 40%]  (Warmup)\nChain 2: Iteration:  810 / 2000 [ 40%]  (Warmup)\nChain 2: Iteration:  811 / 2000 [ 40%]  (Warmup)\nChain 2: Iteration:  812 / 2000 [ 40%]  (Warmup)\nChain 2: Iteration:  813 / 2000 [ 40%]  (Warmup)\nChain 2: Iteration:  814 / 2000 [ 40%]  (Warmup)\nChain 2: Iteration:  815 / 2000 [ 40%]  (Warmup)\nChain 2: Iteration:  816 / 2000 [ 40%]  (Warmup)\nChain 2: Iteration:  817 / 2000 [ 40%]  (Warmup)\nChain 2: Iteration:  818 / 2000 [ 40%]  (Warmup)\nChain 2: Iteration:  819 / 2000 [ 40%]  (Warmup)\nChain 2: Iteration:  820 / 2000 [ 41%]  (Warmup)\nChain 2: Iteration:  821 / 2000 [ 41%]  (Warmup)\nChain 2: Iteration:  822 / 2000 [ 41%]  (Warmup)\nChain 2: Iteration:  823 / 2000 [ 41%]  (Warmup)\nChain 2: Iteration:  824 / 2000 [ 41%]  (Warmup)\nChain 2: Iteration:  825 / 2000 [ 41%]  (Warmup)\nChain 2: Iteration:  826 / 2000 [ 41%]  (Warmup)\nChain 2: Iteration:  827 / 2000 [ 41%]  (Warmup)\nChain 2: Iteration:  828 / 2000 [ 41%]  (Warmup)\nChain 2: Iteration:  829 / 2000 [ 41%]  (Warmup)\nChain 2: Iteration:  830 / 2000 [ 41%]  (Warmup)\nChain 2: Iteration:  831 / 2000 [ 41%]  (Warmup)\nChain 2: Iteration:  832 / 2000 [ 41%]  (Warmup)\nChain 2: Iteration:  833 / 2000 [ 41%]  (Warmup)\nChain 2: Iteration:  834 / 2000 [ 41%]  (Warmup)\nChain 2: Iteration:  835 / 2000 [ 41%]  (Warmup)\nChain 2: Iteration:  836 / 2000 [ 41%]  (Warmup)\nChain 2: Iteration:  837 / 2000 [ 41%]  (Warmup)\nChain 2: Iteration:  838 / 2000 [ 41%]  (Warmup)\nChain 2: Iteration:  839 / 2000 [ 41%]  (Warmup)\nChain 2: Iteration:  840 / 2000 [ 42%]  (Warmup)\nChain 2: Iteration:  841 / 2000 [ 42%]  (Warmup)\nChain 2: Iteration:  842 / 2000 [ 42%]  (Warmup)\nChain 2: Iteration:  843 / 2000 [ 42%]  (Warmup)\nChain 2: Iteration:  844 / 2000 [ 42%]  (Warmup)\nChain 2: Iteration:  845 / 2000 [ 42%]  (Warmup)\nChain 2: Iteration:  846 / 2000 [ 42%]  (Warmup)\nChain 2: Iteration:  847 / 2000 [ 42%]  (Warmup)\nChain 2: Iteration:  848 / 2000 [ 42%]  (Warmup)\nChain 2: Iteration:  849 / 2000 [ 42%]  (Warmup)\nChain 2: Iteration:  850 / 2000 [ 42%]  (Warmup)\nChain 2: Iteration:  851 / 2000 [ 42%]  (Warmup)\nChain 2: Iteration:  852 / 2000 [ 42%]  (Warmup)\nChain 2: Iteration:  853 / 2000 [ 42%]  (Warmup)\nChain 2: Iteration:  854 / 2000 [ 42%]  (Warmup)\nChain 2: Iteration:  855 / 2000 [ 42%]  (Warmup)\nChain 2: Iteration:  856 / 2000 [ 42%]  (Warmup)\nChain 2: Iteration:  857 / 2000 [ 42%]  (Warmup)\nChain 2: Iteration:  858 / 2000 [ 42%]  (Warmup)\nChain 2: Iteration:  859 / 2000 [ 42%]  (Warmup)\nChain 2: Iteration:  860 / 2000 [ 43%]  (Warmup)\nChain 2: Iteration:  861 / 2000 [ 43%]  (Warmup)\nChain 2: Iteration:  862 / 2000 [ 43%]  (Warmup)\nChain 2: Iteration:  863 / 2000 [ 43%]  (Warmup)\nChain 2: Iteration:  864 / 2000 [ 43%]  (Warmup)\nChain 2: Iteration:  865 / 2000 [ 43%]  (Warmup)\nChain 2: Iteration:  866 / 2000 [ 43%]  (Warmup)\nChain 2: Iteration:  867 / 2000 [ 43%]  (Warmup)\nChain 2: Iteration:  868 / 2000 [ 43%]  (Warmup)\nChain 2: Iteration:  869 / 2000 [ 43%]  (Warmup)\nChain 2: Iteration:  870 / 2000 [ 43%]  (Warmup)\nChain 2: Iteration:  871 / 2000 [ 43%]  (Warmup)\nChain 2: Iteration:  872 / 2000 [ 43%]  (Warmup)\nChain 2: Iteration:  873 / 2000 [ 43%]  (Warmup)\nChain 2: Iteration:  874 / 2000 [ 43%]  (Warmup)\nChain 2: Iteration:  875 / 2000 [ 43%]  (Warmup)\nChain 2: Iteration:  876 / 2000 [ 43%]  (Warmup)\nChain 2: Iteration:  877 / 2000 [ 43%]  (Warmup)\nChain 2: Iteration:  878 / 2000 [ 43%]  (Warmup)\nChain 2: Iteration:  879 / 2000 [ 43%]  (Warmup)\nChain 2: Iteration:  880 / 2000 [ 44%]  (Warmup)\nChain 2: Iteration:  881 / 2000 [ 44%]  (Warmup)\nChain 2: Iteration:  882 / 2000 [ 44%]  (Warmup)\nChain 2: Iteration:  883 / 2000 [ 44%]  (Warmup)\nChain 2: Iteration:  884 / 2000 [ 44%]  (Warmup)\nChain 2: Iteration:  885 / 2000 [ 44%]  (Warmup)\nChain 2: Iteration:  886 / 2000 [ 44%]  (Warmup)\nChain 2: Iteration:  887 / 2000 [ 44%]  (Warmup)\nChain 2: Iteration:  888 / 2000 [ 44%]  (Warmup)\nChain 2: Iteration:  889 / 2000 [ 44%]  (Warmup)\nChain 2: Iteration:  890 / 2000 [ 44%]  (Warmup)\nChain 2: Iteration:  891 / 2000 [ 44%]  (Warmup)\nChain 2: Iteration:  892 / 2000 [ 44%]  (Warmup)\nChain 2: Iteration:  893 / 2000 [ 44%]  (Warmup)\nChain 2: Iteration:  894 / 2000 [ 44%]  (Warmup)\nChain 2: Iteration:  895 / 2000 [ 44%]  (Warmup)\nChain 2: Iteration:  896 / 2000 [ 44%]  (Warmup)\nChain 2: Iteration:  897 / 2000 [ 44%]  (Warmup)\nChain 2: Iteration:  898 / 2000 [ 44%]  (Warmup)\nChain 2: Iteration:  899 / 2000 [ 44%]  (Warmup)\nChain 2: Iteration:  900 / 2000 [ 45%]  (Warmup)\nChain 2: Iteration:  901 / 2000 [ 45%]  (Warmup)\nChain 2: Iteration:  902 / 2000 [ 45%]  (Warmup)\nChain 2: Iteration:  903 / 2000 [ 45%]  (Warmup)\nChain 2: Iteration:  904 / 2000 [ 45%]  (Warmup)\nChain 2: Iteration:  905 / 2000 [ 45%]  (Warmup)\nChain 2: Iteration:  906 / 2000 [ 45%]  (Warmup)\nChain 2: Iteration:  907 / 2000 [ 45%]  (Warmup)\nChain 2: Iteration:  908 / 2000 [ 45%]  (Warmup)\nChain 2: Iteration:  909 / 2000 [ 45%]  (Warmup)\nChain 2: Iteration:  910 / 2000 [ 45%]  (Warmup)\nChain 2: Iteration:  911 / 2000 [ 45%]  (Warmup)\nChain 2: Iteration:  912 / 2000 [ 45%]  (Warmup)\nChain 2: Iteration:  913 / 2000 [ 45%]  (Warmup)\nChain 2: Iteration:  914 / 2000 [ 45%]  (Warmup)\nChain 2: Iteration:  915 / 2000 [ 45%]  (Warmup)\nChain 2: Iteration:  916 / 2000 [ 45%]  (Warmup)\nChain 2: Iteration:  917 / 2000 [ 45%]  (Warmup)\nChain 2: Iteration:  918 / 2000 [ 45%]  (Warmup)\nChain 2: Iteration:  919 / 2000 [ 45%]  (Warmup)\nChain 2: Iteration:  920 / 2000 [ 46%]  (Warmup)\nChain 2: Iteration:  921 / 2000 [ 46%]  (Warmup)\nChain 2: Iteration:  922 / 2000 [ 46%]  (Warmup)\nChain 2: Iteration:  923 / 2000 [ 46%]  (Warmup)\nChain 2: Iteration:  924 / 2000 [ 46%]  (Warmup)\nChain 2: Iteration:  925 / 2000 [ 46%]  (Warmup)\nChain 2: Iteration:  926 / 2000 [ 46%]  (Warmup)\nChain 2: Iteration:  927 / 2000 [ 46%]  (Warmup)\nChain 2: Iteration:  928 / 2000 [ 46%]  (Warmup)\nChain 2: Iteration:  929 / 2000 [ 46%]  (Warmup)\nChain 2: Iteration:  930 / 2000 [ 46%]  (Warmup)\nChain 2: Iteration:  931 / 2000 [ 46%]  (Warmup)\nChain 2: Iteration:  932 / 2000 [ 46%]  (Warmup)\nChain 2: Iteration:  933 / 2000 [ 46%]  (Warmup)\nChain 2: Iteration:  934 / 2000 [ 46%]  (Warmup)\nChain 2: Iteration:  935 / 2000 [ 46%]  (Warmup)\nChain 2: Iteration:  936 / 2000 [ 46%]  (Warmup)\nChain 2: Iteration:  937 / 2000 [ 46%]  (Warmup)\nChain 2: Iteration:  938 / 2000 [ 46%]  (Warmup)\nChain 2: Iteration:  939 / 2000 [ 46%]  (Warmup)\nChain 2: Iteration:  940 / 2000 [ 47%]  (Warmup)\nChain 2: Iteration:  941 / 2000 [ 47%]  (Warmup)\nChain 2: Iteration:  942 / 2000 [ 47%]  (Warmup)\nChain 2: Iteration:  943 / 2000 [ 47%]  (Warmup)\nChain 2: Iteration:  944 / 2000 [ 47%]  (Warmup)\nChain 2: Iteration:  945 / 2000 [ 47%]  (Warmup)\nChain 2: Iteration:  946 / 2000 [ 47%]  (Warmup)\nChain 2: Iteration:  947 / 2000 [ 47%]  (Warmup)\nChain 2: Iteration:  948 / 2000 [ 47%]  (Warmup)\nChain 2: Iteration:  949 / 2000 [ 47%]  (Warmup)\nChain 2: Iteration:  950 / 2000 [ 47%]  (Warmup)\nChain 2: Iteration:  951 / 2000 [ 47%]  (Warmup)\nChain 2: Iteration:  952 / 2000 [ 47%]  (Warmup)\nChain 2: Iteration:  953 / 2000 [ 47%]  (Warmup)\nChain 2: Iteration:  954 / 2000 [ 47%]  (Warmup)\nChain 2: Iteration:  955 / 2000 [ 47%]  (Warmup)\nChain 2: Iteration:  956 / 2000 [ 47%]  (Warmup)\nChain 2: Iteration:  957 / 2000 [ 47%]  (Warmup)\nChain 2: Iteration:  958 / 2000 [ 47%]  (Warmup)\nChain 2: Iteration:  959 / 2000 [ 47%]  (Warmup)\nChain 2: Iteration:  960 / 2000 [ 48%]  (Warmup)\nChain 2: Iteration:  961 / 2000 [ 48%]  (Warmup)\nChain 2: Iteration:  962 / 2000 [ 48%]  (Warmup)\nChain 2: Iteration:  963 / 2000 [ 48%]  (Warmup)\nChain 2: Iteration:  964 / 2000 [ 48%]  (Warmup)\nChain 2: Iteration:  965 / 2000 [ 48%]  (Warmup)\nChain 2: Iteration:  966 / 2000 [ 48%]  (Warmup)\nChain 2: Iteration:  967 / 2000 [ 48%]  (Warmup)\nChain 2: Iteration:  968 / 2000 [ 48%]  (Warmup)\nChain 2: Iteration:  969 / 2000 [ 48%]  (Warmup)\nChain 2: Iteration:  970 / 2000 [ 48%]  (Warmup)\nChain 2: Iteration:  971 / 2000 [ 48%]  (Warmup)\nChain 2: Iteration:  972 / 2000 [ 48%]  (Warmup)\nChain 2: Iteration:  973 / 2000 [ 48%]  (Warmup)\nChain 2: Iteration:  974 / 2000 [ 48%]  (Warmup)\nChain 2: Iteration:  975 / 2000 [ 48%]  (Warmup)\nChain 2: Iteration:  976 / 2000 [ 48%]  (Warmup)\nChain 2: Iteration:  977 / 2000 [ 48%]  (Warmup)\nChain 2: Iteration:  978 / 2000 [ 48%]  (Warmup)\nChain 2: Iteration:  979 / 2000 [ 48%]  (Warmup)\nChain 2: Iteration:  980 / 2000 [ 49%]  (Warmup)\nChain 2: Iteration:  981 / 2000 [ 49%]  (Warmup)\nChain 2: Iteration:  982 / 2000 [ 49%]  (Warmup)\nChain 2: Iteration:  983 / 2000 [ 49%]  (Warmup)\nChain 2: Iteration:  984 / 2000 [ 49%]  (Warmup)\nChain 2: Iteration:  985 / 2000 [ 49%]  (Warmup)\nChain 2: Iteration:  986 / 2000 [ 49%]  (Warmup)\nChain 2: Iteration:  987 / 2000 [ 49%]  (Warmup)\nChain 2: Iteration:  988 / 2000 [ 49%]  (Warmup)\nChain 2: Iteration:  989 / 2000 [ 49%]  (Warmup)\nChain 2: Iteration:  990 / 2000 [ 49%]  (Warmup)\nChain 2: Iteration:  991 / 2000 [ 49%]  (Warmup)\nChain 2: Iteration:  992 / 2000 [ 49%]  (Warmup)\nChain 2: Iteration:  993 / 2000 [ 49%]  (Warmup)\nChain 2: Iteration:  994 / 2000 [ 49%]  (Warmup)\nChain 2: Iteration:  995 / 2000 [ 49%]  (Warmup)\nChain 2: Iteration:  996 / 2000 [ 49%]  (Warmup)\nChain 2: Iteration:  997 / 2000 [ 49%]  (Warmup)\nChain 2: Iteration:  998 / 2000 [ 49%]  (Warmup)\nChain 2: Iteration:  999 / 2000 [ 49%]  (Warmup)\nChain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 2: Iteration: 1002 / 2000 [ 50%]  (Sampling)\nChain 2: Iteration: 1003 / 2000 [ 50%]  (Sampling)\nChain 2: Iteration: 1004 / 2000 [ 50%]  (Sampling)\nChain 2: Iteration: 1005 / 2000 [ 50%]  (Sampling)\nChain 2: Iteration: 1006 / 2000 [ 50%]  (Sampling)\nChain 2: Iteration: 1007 / 2000 [ 50%]  (Sampling)\nChain 2: Iteration: 1008 / 2000 [ 50%]  (Sampling)\nChain 2: Iteration: 1009 / 2000 [ 50%]  (Sampling)\nChain 2: Iteration: 1010 / 2000 [ 50%]  (Sampling)\nChain 2: Iteration: 1011 / 2000 [ 50%]  (Sampling)\nChain 2: Iteration: 1012 / 2000 [ 50%]  (Sampling)\nChain 2: Iteration: 1013 / 2000 [ 50%]  (Sampling)\nChain 2: Iteration: 1014 / 2000 [ 50%]  (Sampling)\nChain 2: Iteration: 1015 / 2000 [ 50%]  (Sampling)\nChain 2: Iteration: 1016 / 2000 [ 50%]  (Sampling)\nChain 2: Iteration: 1017 / 2000 [ 50%]  (Sampling)\nChain 2: Iteration: 1018 / 2000 [ 50%]  (Sampling)\nChain 2: Iteration: 1019 / 2000 [ 50%]  (Sampling)\nChain 2: Iteration: 1020 / 2000 [ 51%]  (Sampling)\nChain 2: Iteration: 1021 / 2000 [ 51%]  (Sampling)\nChain 2: Iteration: 1022 / 2000 [ 51%]  (Sampling)\nChain 2: Iteration: 1023 / 2000 [ 51%]  (Sampling)\nChain 2: Iteration: 1024 / 2000 [ 51%]  (Sampling)\nChain 2: Iteration: 1025 / 2000 [ 51%]  (Sampling)\nChain 2: Iteration: 1026 / 2000 [ 51%]  (Sampling)\nChain 2: Iteration: 1027 / 2000 [ 51%]  (Sampling)\nChain 2: Iteration: 1028 / 2000 [ 51%]  (Sampling)\nChain 2: Iteration: 1029 / 2000 [ 51%]  (Sampling)\nChain 2: Iteration: 1030 / 2000 [ 51%]  (Sampling)\nChain 2: Iteration: 1031 / 2000 [ 51%]  (Sampling)\nChain 2: Iteration: 1032 / 2000 [ 51%]  (Sampling)\nChain 2: Iteration: 1033 / 2000 [ 51%]  (Sampling)\nChain 2: Iteration: 1034 / 2000 [ 51%]  (Sampling)\nChain 2: Iteration: 1035 / 2000 [ 51%]  (Sampling)\nChain 2: Iteration: 1036 / 2000 [ 51%]  (Sampling)\nChain 2: Iteration: 1037 / 2000 [ 51%]  (Sampling)\nChain 2: Iteration: 1038 / 2000 [ 51%]  (Sampling)\nChain 2: Iteration: 1039 / 2000 [ 51%]  (Sampling)\nChain 2: Iteration: 1040 / 2000 [ 52%]  (Sampling)\nChain 2: Iteration: 1041 / 2000 [ 52%]  (Sampling)\nChain 2: Iteration: 1042 / 2000 [ 52%]  (Sampling)\nChain 2: Iteration: 1043 / 2000 [ 52%]  (Sampling)\nChain 2: Iteration: 1044 / 2000 [ 52%]  (Sampling)\nChain 2: Iteration: 1045 / 2000 [ 52%]  (Sampling)\nChain 2: Iteration: 1046 / 2000 [ 52%]  (Sampling)\nChain 2: Iteration: 1047 / 2000 [ 52%]  (Sampling)\nChain 2: Iteration: 1048 / 2000 [ 52%]  (Sampling)\nChain 2: Iteration: 1049 / 2000 [ 52%]  (Sampling)\nChain 2: Iteration: 1050 / 2000 [ 52%]  (Sampling)\nChain 2: Iteration: 1051 / 2000 [ 52%]  (Sampling)\nChain 2: Iteration: 1052 / 2000 [ 52%]  (Sampling)\nChain 2: Iteration: 1053 / 2000 [ 52%]  (Sampling)\nChain 2: Iteration: 1054 / 2000 [ 52%]  (Sampling)\nChain 2: Iteration: 1055 / 2000 [ 52%]  (Sampling)\nChain 2: Iteration: 1056 / 2000 [ 52%]  (Sampling)\nChain 2: Iteration: 1057 / 2000 [ 52%]  (Sampling)\nChain 2: Iteration: 1058 / 2000 [ 52%]  (Sampling)\nChain 2: Iteration: 1059 / 2000 [ 52%]  (Sampling)\nChain 2: Iteration: 1060 / 2000 [ 53%]  (Sampling)\nChain 2: Iteration: 1061 / 2000 [ 53%]  (Sampling)\nChain 2: Iteration: 1062 / 2000 [ 53%]  (Sampling)\nChain 2: Iteration: 1063 / 2000 [ 53%]  (Sampling)\nChain 2: Iteration: 1064 / 2000 [ 53%]  (Sampling)\nChain 2: Iteration: 1065 / 2000 [ 53%]  (Sampling)\nChain 2: Iteration: 1066 / 2000 [ 53%]  (Sampling)\nChain 2: Iteration: 1067 / 2000 [ 53%]  (Sampling)\nChain 2: Iteration: 1068 / 2000 [ 53%]  (Sampling)\nChain 2: Iteration: 1069 / 2000 [ 53%]  (Sampling)\nChain 2: Iteration: 1070 / 2000 [ 53%]  (Sampling)\nChain 2: Iteration: 1071 / 2000 [ 53%]  (Sampling)\nChain 2: Iteration: 1072 / 2000 [ 53%]  (Sampling)\nChain 2: Iteration: 1073 / 2000 [ 53%]  (Sampling)\nChain 2: Iteration: 1074 / 2000 [ 53%]  (Sampling)\nChain 2: Iteration: 1075 / 2000 [ 53%]  (Sampling)\nChain 2: Iteration: 1076 / 2000 [ 53%]  (Sampling)\nChain 2: Iteration: 1077 / 2000 [ 53%]  (Sampling)\nChain 2: Iteration: 1078 / 2000 [ 53%]  (Sampling)\nChain 2: Iteration: 1079 / 2000 [ 53%]  (Sampling)\nChain 2: Iteration: 1080 / 2000 [ 54%]  (Sampling)\nChain 2: Iteration: 1081 / 2000 [ 54%]  (Sampling)\nChain 2: Iteration: 1082 / 2000 [ 54%]  (Sampling)\nChain 2: Iteration: 1083 / 2000 [ 54%]  (Sampling)\nChain 2: Iteration: 1084 / 2000 [ 54%]  (Sampling)\nChain 2: Iteration: 1085 / 2000 [ 54%]  (Sampling)\nChain 2: Iteration: 1086 / 2000 [ 54%]  (Sampling)\nChain 2: Iteration: 1087 / 2000 [ 54%]  (Sampling)\nChain 2: Iteration: 1088 / 2000 [ 54%]  (Sampling)\nChain 2: Iteration: 1089 / 2000 [ 54%]  (Sampling)\nChain 2: Iteration: 1090 / 2000 [ 54%]  (Sampling)\nChain 2: Iteration: 1091 / 2000 [ 54%]  (Sampling)\nChain 2: Iteration: 1092 / 2000 [ 54%]  (Sampling)\nChain 2: Iteration: 1093 / 2000 [ 54%]  (Sampling)\nChain 2: Iteration: 1094 / 2000 [ 54%]  (Sampling)\nChain 2: Iteration: 1095 / 2000 [ 54%]  (Sampling)\nChain 2: Iteration: 1096 / 2000 [ 54%]  (Sampling)\nChain 2: Iteration: 1097 / 2000 [ 54%]  (Sampling)\nChain 2: Iteration: 1098 / 2000 [ 54%]  (Sampling)\nChain 2: Iteration: 1099 / 2000 [ 54%]  (Sampling)\nChain 2: Iteration: 1100 / 2000 [ 55%]  (Sampling)\nChain 2: Iteration: 1101 / 2000 [ 55%]  (Sampling)\nChain 2: Iteration: 1102 / 2000 [ 55%]  (Sampling)\nChain 2: Iteration: 1103 / 2000 [ 55%]  (Sampling)\nChain 2: Iteration: 1104 / 2000 [ 55%]  (Sampling)\nChain 2: Iteration: 1105 / 2000 [ 55%]  (Sampling)\nChain 2: Iteration: 1106 / 2000 [ 55%]  (Sampling)\nChain 2: Iteration: 1107 / 2000 [ 55%]  (Sampling)\nChain 2: Iteration: 1108 / 2000 [ 55%]  (Sampling)\nChain 2: Iteration: 1109 / 2000 [ 55%]  (Sampling)\nChain 2: Iteration: 1110 / 2000 [ 55%]  (Sampling)\nChain 2: Iteration: 1111 / 2000 [ 55%]  (Sampling)\nChain 2: Iteration: 1112 / 2000 [ 55%]  (Sampling)\nChain 2: Iteration: 1113 / 2000 [ 55%]  (Sampling)\nChain 2: Iteration: 1114 / 2000 [ 55%]  (Sampling)\nChain 2: Iteration: 1115 / 2000 [ 55%]  (Sampling)\nChain 2: Iteration: 1116 / 2000 [ 55%]  (Sampling)\nChain 2: Iteration: 1117 / 2000 [ 55%]  (Sampling)\nChain 2: Iteration: 1118 / 2000 [ 55%]  (Sampling)\nChain 2: Iteration: 1119 / 2000 [ 55%]  (Sampling)\nChain 2: Iteration: 1120 / 2000 [ 56%]  (Sampling)\nChain 2: Iteration: 1121 / 2000 [ 56%]  (Sampling)\nChain 2: Iteration: 1122 / 2000 [ 56%]  (Sampling)\nChain 2: Iteration: 1123 / 2000 [ 56%]  (Sampling)\nChain 2: Iteration: 1124 / 2000 [ 56%]  (Sampling)\nChain 2: Iteration: 1125 / 2000 [ 56%]  (Sampling)\nChain 2: Iteration: 1126 / 2000 [ 56%]  (Sampling)\nChain 2: Iteration: 1127 / 2000 [ 56%]  (Sampling)\nChain 2: Iteration: 1128 / 2000 [ 56%]  (Sampling)\nChain 2: Iteration: 1129 / 2000 [ 56%]  (Sampling)\nChain 2: Iteration: 1130 / 2000 [ 56%]  (Sampling)\nChain 2: Iteration: 1131 / 2000 [ 56%]  (Sampling)\nChain 2: Iteration: 1132 / 2000 [ 56%]  (Sampling)\nChain 2: Iteration: 1133 / 2000 [ 56%]  (Sampling)\nChain 2: Iteration: 1134 / 2000 [ 56%]  (Sampling)\nChain 2: Iteration: 1135 / 2000 [ 56%]  (Sampling)\nChain 2: Iteration: 1136 / 2000 [ 56%]  (Sampling)\nChain 2: Iteration: 1137 / 2000 [ 56%]  (Sampling)\nChain 2: Iteration: 1138 / 2000 [ 56%]  (Sampling)\nChain 2: Iteration: 1139 / 2000 [ 56%]  (Sampling)\nChain 2: Iteration: 1140 / 2000 [ 57%]  (Sampling)\nChain 2: Iteration: 1141 / 2000 [ 57%]  (Sampling)\nChain 2: Iteration: 1142 / 2000 [ 57%]  (Sampling)\nChain 2: Iteration: 1143 / 2000 [ 57%]  (Sampling)\nChain 2: Iteration: 1144 / 2000 [ 57%]  (Sampling)\nChain 2: Iteration: 1145 / 2000 [ 57%]  (Sampling)\nChain 2: Iteration: 1146 / 2000 [ 57%]  (Sampling)\nChain 2: Iteration: 1147 / 2000 [ 57%]  (Sampling)\nChain 2: Iteration: 1148 / 2000 [ 57%]  (Sampling)\nChain 2: Iteration: 1149 / 2000 [ 57%]  (Sampling)\nChain 2: Iteration: 1150 / 2000 [ 57%]  (Sampling)\nChain 2: Iteration: 1151 / 2000 [ 57%]  (Sampling)\nChain 2: Iteration: 1152 / 2000 [ 57%]  (Sampling)\nChain 2: Iteration: 1153 / 2000 [ 57%]  (Sampling)\nChain 2: Iteration: 1154 / 2000 [ 57%]  (Sampling)\nChain 2: Iteration: 1155 / 2000 [ 57%]  (Sampling)\nChain 2: Iteration: 1156 / 2000 [ 57%]  (Sampling)\nChain 2: Iteration: 1157 / 2000 [ 57%]  (Sampling)\nChain 2: Iteration: 1158 / 2000 [ 57%]  (Sampling)\nChain 2: Iteration: 1159 / 2000 [ 57%]  (Sampling)\nChain 2: Iteration: 1160 / 2000 [ 58%]  (Sampling)\nChain 2: Iteration: 1161 / 2000 [ 58%]  (Sampling)\nChain 2: Iteration: 1162 / 2000 [ 58%]  (Sampling)\nChain 2: Iteration: 1163 / 2000 [ 58%]  (Sampling)\nChain 2: Iteration: 1164 / 2000 [ 58%]  (Sampling)\nChain 2: Iteration: 1165 / 2000 [ 58%]  (Sampling)\nChain 2: Iteration: 1166 / 2000 [ 58%]  (Sampling)\nChain 2: Iteration: 1167 / 2000 [ 58%]  (Sampling)\nChain 2: Iteration: 1168 / 2000 [ 58%]  (Sampling)\nChain 2: Iteration: 1169 / 2000 [ 58%]  (Sampling)\nChain 2: Iteration: 1170 / 2000 [ 58%]  (Sampling)\nChain 2: Iteration: 1171 / 2000 [ 58%]  (Sampling)\nChain 2: Iteration: 1172 / 2000 [ 58%]  (Sampling)\nChain 2: Iteration: 1173 / 2000 [ 58%]  (Sampling)\nChain 2: Iteration: 1174 / 2000 [ 58%]  (Sampling)\nChain 2: Iteration: 1175 / 2000 [ 58%]  (Sampling)\nChain 2: Iteration: 1176 / 2000 [ 58%]  (Sampling)\nChain 2: Iteration: 1177 / 2000 [ 58%]  (Sampling)\nChain 2: Iteration: 1178 / 2000 [ 58%]  (Sampling)\nChain 2: Iteration: 1179 / 2000 [ 58%]  (Sampling)\nChain 2: Iteration: 1180 / 2000 [ 59%]  (Sampling)\nChain 2: Iteration: 1181 / 2000 [ 59%]  (Sampling)\nChain 2: Iteration: 1182 / 2000 [ 59%]  (Sampling)\nChain 2: Iteration: 1183 / 2000 [ 59%]  (Sampling)\nChain 2: Iteration: 1184 / 2000 [ 59%]  (Sampling)\nChain 2: Iteration: 1185 / 2000 [ 59%]  (Sampling)\nChain 2: Iteration: 1186 / 2000 [ 59%]  (Sampling)\nChain 2: Iteration: 1187 / 2000 [ 59%]  (Sampling)\nChain 2: Iteration: 1188 / 2000 [ 59%]  (Sampling)\nChain 2: Iteration: 1189 / 2000 [ 59%]  (Sampling)\nChain 2: Iteration: 1190 / 2000 [ 59%]  (Sampling)\nChain 2: Iteration: 1191 / 2000 [ 59%]  (Sampling)\nChain 2: Iteration: 1192 / 2000 [ 59%]  (Sampling)\nChain 2: Iteration: 1193 / 2000 [ 59%]  (Sampling)\nChain 2: Iteration: 1194 / 2000 [ 59%]  (Sampling)\nChain 2: Iteration: 1195 / 2000 [ 59%]  (Sampling)\nChain 2: Iteration: 1196 / 2000 [ 59%]  (Sampling)\nChain 2: Iteration: 1197 / 2000 [ 59%]  (Sampling)\nChain 2: Iteration: 1198 / 2000 [ 59%]  (Sampling)\nChain 2: Iteration: 1199 / 2000 [ 59%]  (Sampling)\nChain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 2: Iteration: 1201 / 2000 [ 60%]  (Sampling)\nChain 2: Iteration: 1202 / 2000 [ 60%]  (Sampling)\nChain 2: Iteration: 1203 / 2000 [ 60%]  (Sampling)\nChain 2: Iteration: 1204 / 2000 [ 60%]  (Sampling)\nChain 2: Iteration: 1205 / 2000 [ 60%]  (Sampling)\nChain 2: Iteration: 1206 / 2000 [ 60%]  (Sampling)\nChain 2: Iteration: 1207 / 2000 [ 60%]  (Sampling)\nChain 2: Iteration: 1208 / 2000 [ 60%]  (Sampling)\nChain 2: Iteration: 1209 / 2000 [ 60%]  (Sampling)\nChain 2: Iteration: 1210 / 2000 [ 60%]  (Sampling)\nChain 2: Iteration: 1211 / 2000 [ 60%]  (Sampling)\nChain 2: Iteration: 1212 / 2000 [ 60%]  (Sampling)\nChain 2: Iteration: 1213 / 2000 [ 60%]  (Sampling)\nChain 2: Iteration: 1214 / 2000 [ 60%]  (Sampling)\nChain 2: Iteration: 1215 / 2000 [ 60%]  (Sampling)\nChain 2: Iteration: 1216 / 2000 [ 60%]  (Sampling)\nChain 2: Iteration: 1217 / 2000 [ 60%]  (Sampling)\nChain 2: Iteration: 1218 / 2000 [ 60%]  (Sampling)\nChain 2: Iteration: 1219 / 2000 [ 60%]  (Sampling)\nChain 2: Iteration: 1220 / 2000 [ 61%]  (Sampling)\nChain 2: Iteration: 1221 / 2000 [ 61%]  (Sampling)\nChain 2: Iteration: 1222 / 2000 [ 61%]  (Sampling)\nChain 2: Iteration: 1223 / 2000 [ 61%]  (Sampling)\nChain 2: Iteration: 1224 / 2000 [ 61%]  (Sampling)\nChain 2: Iteration: 1225 / 2000 [ 61%]  (Sampling)\nChain 2: Iteration: 1226 / 2000 [ 61%]  (Sampling)\nChain 2: Iteration: 1227 / 2000 [ 61%]  (Sampling)\nChain 2: Iteration: 1228 / 2000 [ 61%]  (Sampling)\nChain 2: Iteration: 1229 / 2000 [ 61%]  (Sampling)\nChain 2: Iteration: 1230 / 2000 [ 61%]  (Sampling)\nChain 2: Iteration: 1231 / 2000 [ 61%]  (Sampling)\nChain 2: Iteration: 1232 / 2000 [ 61%]  (Sampling)\nChain 2: Iteration: 1233 / 2000 [ 61%]  (Sampling)\nChain 2: Iteration: 1234 / 2000 [ 61%]  (Sampling)\nChain 2: Iteration: 1235 / 2000 [ 61%]  (Sampling)\nChain 2: Iteration: 1236 / 2000 [ 61%]  (Sampling)\nChain 2: Iteration: 1237 / 2000 [ 61%]  (Sampling)\nChain 2: Iteration: 1238 / 2000 [ 61%]  (Sampling)\nChain 2: Iteration: 1239 / 2000 [ 61%]  (Sampling)\nChain 2: Iteration: 1240 / 2000 [ 62%]  (Sampling)\nChain 2: Iteration: 1241 / 2000 [ 62%]  (Sampling)\nChain 2: Iteration: 1242 / 2000 [ 62%]  (Sampling)\nChain 2: Iteration: 1243 / 2000 [ 62%]  (Sampling)\nChain 2: Iteration: 1244 / 2000 [ 62%]  (Sampling)\nChain 2: Iteration: 1245 / 2000 [ 62%]  (Sampling)\nChain 2: Iteration: 1246 / 2000 [ 62%]  (Sampling)\nChain 2: Iteration: 1247 / 2000 [ 62%]  (Sampling)\nChain 2: Iteration: 1248 / 2000 [ 62%]  (Sampling)\nChain 2: Iteration: 1249 / 2000 [ 62%]  (Sampling)\nChain 2: Iteration: 1250 / 2000 [ 62%]  (Sampling)\nChain 2: Iteration: 1251 / 2000 [ 62%]  (Sampling)\nChain 2: Iteration: 1252 / 2000 [ 62%]  (Sampling)\nChain 2: Iteration: 1253 / 2000 [ 62%]  (Sampling)\nChain 2: Iteration: 1254 / 2000 [ 62%]  (Sampling)\nChain 2: Iteration: 1255 / 2000 [ 62%]  (Sampling)\nChain 2: Iteration: 1256 / 2000 [ 62%]  (Sampling)\nChain 2: Iteration: 1257 / 2000 [ 62%]  (Sampling)\nChain 2: Iteration: 1258 / 2000 [ 62%]  (Sampling)\nChain 2: Iteration: 1259 / 2000 [ 62%]  (Sampling)\nChain 2: Iteration: 1260 / 2000 [ 63%]  (Sampling)\nChain 2: Iteration: 1261 / 2000 [ 63%]  (Sampling)\nChain 2: Iteration: 1262 / 2000 [ 63%]  (Sampling)\nChain 2: Iteration: 1263 / 2000 [ 63%]  (Sampling)\nChain 2: Iteration: 1264 / 2000 [ 63%]  (Sampling)\nChain 2: Iteration: 1265 / 2000 [ 63%]  (Sampling)\nChain 2: Iteration: 1266 / 2000 [ 63%]  (Sampling)\nChain 2: Iteration: 1267 / 2000 [ 63%]  (Sampling)\nChain 2: Iteration: 1268 / 2000 [ 63%]  (Sampling)\nChain 2: Iteration: 1269 / 2000 [ 63%]  (Sampling)\nChain 2: Iteration: 1270 / 2000 [ 63%]  (Sampling)\nChain 2: Iteration: 1271 / 2000 [ 63%]  (Sampling)\nChain 2: Iteration: 1272 / 2000 [ 63%]  (Sampling)\nChain 2: Iteration: 1273 / 2000 [ 63%]  (Sampling)\nChain 2: Iteration: 1274 / 2000 [ 63%]  (Sampling)\nChain 2: Iteration: 1275 / 2000 [ 63%]  (Sampling)\nChain 2: Iteration: 1276 / 2000 [ 63%]  (Sampling)\nChain 2: Iteration: 1277 / 2000 [ 63%]  (Sampling)\nChain 2: Iteration: 1278 / 2000 [ 63%]  (Sampling)\nChain 2: Iteration: 1279 / 2000 [ 63%]  (Sampling)\nChain 2: Iteration: 1280 / 2000 [ 64%]  (Sampling)\nChain 2: Iteration: 1281 / 2000 [ 64%]  (Sampling)\nChain 2: Iteration: 1282 / 2000 [ 64%]  (Sampling)\nChain 2: Iteration: 1283 / 2000 [ 64%]  (Sampling)\nChain 2: Iteration: 1284 / 2000 [ 64%]  (Sampling)\nChain 2: Iteration: 1285 / 2000 [ 64%]  (Sampling)\nChain 2: Iteration: 1286 / 2000 [ 64%]  (Sampling)\nChain 2: Iteration: 1287 / 2000 [ 64%]  (Sampling)\nChain 2: Iteration: 1288 / 2000 [ 64%]  (Sampling)\nChain 2: Iteration: 1289 / 2000 [ 64%]  (Sampling)\nChain 2: Iteration: 1290 / 2000 [ 64%]  (Sampling)\nChain 2: Iteration: 1291 / 2000 [ 64%]  (Sampling)\nChain 2: Iteration: 1292 / 2000 [ 64%]  (Sampling)\nChain 2: Iteration: 1293 / 2000 [ 64%]  (Sampling)\nChain 2: Iteration: 1294 / 2000 [ 64%]  (Sampling)\nChain 2: Iteration: 1295 / 2000 [ 64%]  (Sampling)\nChain 2: Iteration: 1296 / 2000 [ 64%]  (Sampling)\nChain 2: Iteration: 1297 / 2000 [ 64%]  (Sampling)\nChain 2: Iteration: 1298 / 2000 [ 64%]  (Sampling)\nChain 2: Iteration: 1299 / 2000 [ 64%]  (Sampling)\nChain 2: Iteration: 1300 / 2000 [ 65%]  (Sampling)\nChain 2: Iteration: 1301 / 2000 [ 65%]  (Sampling)\nChain 2: Iteration: 1302 / 2000 [ 65%]  (Sampling)\nChain 2: Iteration: 1303 / 2000 [ 65%]  (Sampling)\nChain 2: Iteration: 1304 / 2000 [ 65%]  (Sampling)\nChain 2: Iteration: 1305 / 2000 [ 65%]  (Sampling)\nChain 2: Iteration: 1306 / 2000 [ 65%]  (Sampling)\nChain 2: Iteration: 1307 / 2000 [ 65%]  (Sampling)\nChain 2: Iteration: 1308 / 2000 [ 65%]  (Sampling)\nChain 2: Iteration: 1309 / 2000 [ 65%]  (Sampling)\nChain 2: Iteration: 1310 / 2000 [ 65%]  (Sampling)\nChain 2: Iteration: 1311 / 2000 [ 65%]  (Sampling)\nChain 2: Iteration: 1312 / 2000 [ 65%]  (Sampling)\nChain 2: Iteration: 1313 / 2000 [ 65%]  (Sampling)\nChain 2: Iteration: 1314 / 2000 [ 65%]  (Sampling)\nChain 2: Iteration: 1315 / 2000 [ 65%]  (Sampling)\nChain 2: Iteration: 1316 / 2000 [ 65%]  (Sampling)\nChain 2: Iteration: 1317 / 2000 [ 65%]  (Sampling)\nChain 2: Iteration: 1318 / 2000 [ 65%]  (Sampling)\nChain 2: Iteration: 1319 / 2000 [ 65%]  (Sampling)\nChain 2: Iteration: 1320 / 2000 [ 66%]  (Sampling)\nChain 2: Iteration: 1321 / 2000 [ 66%]  (Sampling)\nChain 2: Iteration: 1322 / 2000 [ 66%]  (Sampling)\nChain 2: Iteration: 1323 / 2000 [ 66%]  (Sampling)\nChain 2: Iteration: 1324 / 2000 [ 66%]  (Sampling)\nChain 2: Iteration: 1325 / 2000 [ 66%]  (Sampling)\nChain 2: Iteration: 1326 / 2000 [ 66%]  (Sampling)\nChain 2: Iteration: 1327 / 2000 [ 66%]  (Sampling)\nChain 2: Iteration: 1328 / 2000 [ 66%]  (Sampling)\nChain 2: Iteration: 1329 / 2000 [ 66%]  (Sampling)\nChain 2: Iteration: 1330 / 2000 [ 66%]  (Sampling)\nChain 2: Iteration: 1331 / 2000 [ 66%]  (Sampling)\nChain 2: Iteration: 1332 / 2000 [ 66%]  (Sampling)\nChain 2: Iteration: 1333 / 2000 [ 66%]  (Sampling)\nChain 2: Iteration: 1334 / 2000 [ 66%]  (Sampling)\nChain 2: Iteration: 1335 / 2000 [ 66%]  (Sampling)\nChain 2: Iteration: 1336 / 2000 [ 66%]  (Sampling)\nChain 2: Iteration: 1337 / 2000 [ 66%]  (Sampling)\nChain 2: Iteration: 1338 / 2000 [ 66%]  (Sampling)\nChain 2: Iteration: 1339 / 2000 [ 66%]  (Sampling)\nChain 2: Iteration: 1340 / 2000 [ 67%]  (Sampling)\nChain 2: Iteration: 1341 / 2000 [ 67%]  (Sampling)\nChain 2: Iteration: 1342 / 2000 [ 67%]  (Sampling)\nChain 2: Iteration: 1343 / 2000 [ 67%]  (Sampling)\nChain 2: Iteration: 1344 / 2000 [ 67%]  (Sampling)\nChain 2: Iteration: 1345 / 2000 [ 67%]  (Sampling)\nChain 2: Iteration: 1346 / 2000 [ 67%]  (Sampling)\nChain 2: Iteration: 1347 / 2000 [ 67%]  (Sampling)\nChain 2: Iteration: 1348 / 2000 [ 67%]  (Sampling)\nChain 2: Iteration: 1349 / 2000 [ 67%]  (Sampling)\nChain 2: Iteration: 1350 / 2000 [ 67%]  (Sampling)\nChain 2: Iteration: 1351 / 2000 [ 67%]  (Sampling)\nChain 2: Iteration: 1352 / 2000 [ 67%]  (Sampling)\nChain 2: Iteration: 1353 / 2000 [ 67%]  (Sampling)\nChain 2: Iteration: 1354 / 2000 [ 67%]  (Sampling)\nChain 2: Iteration: 1355 / 2000 [ 67%]  (Sampling)\nChain 2: Iteration: 1356 / 2000 [ 67%]  (Sampling)\nChain 2: Iteration: 1357 / 2000 [ 67%]  (Sampling)\nChain 2: Iteration: 1358 / 2000 [ 67%]  (Sampling)\nChain 2: Iteration: 1359 / 2000 [ 67%]  (Sampling)\nChain 2: Iteration: 1360 / 2000 [ 68%]  (Sampling)\nChain 2: Iteration: 1361 / 2000 [ 68%]  (Sampling)\nChain 2: Iteration: 1362 / 2000 [ 68%]  (Sampling)\nChain 2: Iteration: 1363 / 2000 [ 68%]  (Sampling)\nChain 2: Iteration: 1364 / 2000 [ 68%]  (Sampling)\nChain 2: Iteration: 1365 / 2000 [ 68%]  (Sampling)\nChain 2: Iteration: 1366 / 2000 [ 68%]  (Sampling)\nChain 2: Iteration: 1367 / 2000 [ 68%]  (Sampling)\nChain 2: Iteration: 1368 / 2000 [ 68%]  (Sampling)\nChain 2: Iteration: 1369 / 2000 [ 68%]  (Sampling)\nChain 2: Iteration: 1370 / 2000 [ 68%]  (Sampling)\nChain 2: Iteration: 1371 / 2000 [ 68%]  (Sampling)\nChain 2: Iteration: 1372 / 2000 [ 68%]  (Sampling)\nChain 2: Iteration: 1373 / 2000 [ 68%]  (Sampling)\nChain 2: Iteration: 1374 / 2000 [ 68%]  (Sampling)\nChain 2: Iteration: 1375 / 2000 [ 68%]  (Sampling)\nChain 2: Iteration: 1376 / 2000 [ 68%]  (Sampling)\nChain 2: Iteration: 1377 / 2000 [ 68%]  (Sampling)\nChain 2: Iteration: 1378 / 2000 [ 68%]  (Sampling)\nChain 2: Iteration: 1379 / 2000 [ 68%]  (Sampling)\nChain 2: Iteration: 1380 / 2000 [ 69%]  (Sampling)\nChain 2: Iteration: 1381 / 2000 [ 69%]  (Sampling)\nChain 2: Iteration: 1382 / 2000 [ 69%]  (Sampling)\nChain 2: Iteration: 1383 / 2000 [ 69%]  (Sampling)\nChain 2: Iteration: 1384 / 2000 [ 69%]  (Sampling)\nChain 2: Iteration: 1385 / 2000 [ 69%]  (Sampling)\nChain 2: Iteration: 1386 / 2000 [ 69%]  (Sampling)\nChain 2: Iteration: 1387 / 2000 [ 69%]  (Sampling)\nChain 2: Iteration: 1388 / 2000 [ 69%]  (Sampling)\nChain 2: Iteration: 1389 / 2000 [ 69%]  (Sampling)\nChain 2: Iteration: 1390 / 2000 [ 69%]  (Sampling)\nChain 2: Iteration: 1391 / 2000 [ 69%]  (Sampling)\nChain 2: Iteration: 1392 / 2000 [ 69%]  (Sampling)\nChain 2: Iteration: 1393 / 2000 [ 69%]  (Sampling)\nChain 2: Iteration: 1394 / 2000 [ 69%]  (Sampling)\nChain 2: Iteration: 1395 / 2000 [ 69%]  (Sampling)\nChain 2: Iteration: 1396 / 2000 [ 69%]  (Sampling)\nChain 2: Iteration: 1397 / 2000 [ 69%]  (Sampling)\nChain 2: Iteration: 1398 / 2000 [ 69%]  (Sampling)\nChain 2: Iteration: 1399 / 2000 [ 69%]  (Sampling)\nChain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 2: Iteration: 1401 / 2000 [ 70%]  (Sampling)\nChain 2: Iteration: 1402 / 2000 [ 70%]  (Sampling)\nChain 2: Iteration: 1403 / 2000 [ 70%]  (Sampling)\nChain 2: Iteration: 1404 / 2000 [ 70%]  (Sampling)\nChain 2: Iteration: 1405 / 2000 [ 70%]  (Sampling)\nChain 2: Iteration: 1406 / 2000 [ 70%]  (Sampling)\nChain 2: Iteration: 1407 / 2000 [ 70%]  (Sampling)\nChain 2: Iteration: 1408 / 2000 [ 70%]  (Sampling)\nChain 2: Iteration: 1409 / 2000 [ 70%]  (Sampling)\nChain 2: Iteration: 1410 / 2000 [ 70%]  (Sampling)\nChain 2: Iteration: 1411 / 2000 [ 70%]  (Sampling)\nChain 2: Iteration: 1412 / 2000 [ 70%]  (Sampling)\nChain 2: Iteration: 1413 / 2000 [ 70%]  (Sampling)\nChain 2: Iteration: 1414 / 2000 [ 70%]  (Sampling)\nChain 2: Iteration: 1415 / 2000 [ 70%]  (Sampling)\nChain 2: Iteration: 1416 / 2000 [ 70%]  (Sampling)\nChain 2: Iteration: 1417 / 2000 [ 70%]  (Sampling)\nChain 2: Iteration: 1418 / 2000 [ 70%]  (Sampling)\nChain 2: Iteration: 1419 / 2000 [ 70%]  (Sampling)\nChain 2: Iteration: 1420 / 2000 [ 71%]  (Sampling)\nChain 2: Iteration: 1421 / 2000 [ 71%]  (Sampling)\nChain 2: Iteration: 1422 / 2000 [ 71%]  (Sampling)\nChain 2: Iteration: 1423 / 2000 [ 71%]  (Sampling)\nChain 2: Iteration: 1424 / 2000 [ 71%]  (Sampling)\nChain 2: Iteration: 1425 / 2000 [ 71%]  (Sampling)\nChain 2: Iteration: 1426 / 2000 [ 71%]  (Sampling)\nChain 2: Iteration: 1427 / 2000 [ 71%]  (Sampling)\nChain 2: Iteration: 1428 / 2000 [ 71%]  (Sampling)\nChain 2: Iteration: 1429 / 2000 [ 71%]  (Sampling)\nChain 2: Iteration: 1430 / 2000 [ 71%]  (Sampling)\nChain 2: Iteration: 1431 / 2000 [ 71%]  (Sampling)\nChain 2: Iteration: 1432 / 2000 [ 71%]  (Sampling)\nChain 2: Iteration: 1433 / 2000 [ 71%]  (Sampling)\nChain 2: Iteration: 1434 / 2000 [ 71%]  (Sampling)\nChain 2: Iteration: 1435 / 2000 [ 71%]  (Sampling)\nChain 2: Iteration: 1436 / 2000 [ 71%]  (Sampling)\nChain 2: Iteration: 1437 / 2000 [ 71%]  (Sampling)\nChain 2: Iteration: 1438 / 2000 [ 71%]  (Sampling)\nChain 2: Iteration: 1439 / 2000 [ 71%]  (Sampling)\nChain 2: Iteration: 1440 / 2000 [ 72%]  (Sampling)\nChain 2: Iteration: 1441 / 2000 [ 72%]  (Sampling)\nChain 2: Iteration: 1442 / 2000 [ 72%]  (Sampling)\nChain 2: Iteration: 1443 / 2000 [ 72%]  (Sampling)\nChain 2: Iteration: 1444 / 2000 [ 72%]  (Sampling)\nChain 2: Iteration: 1445 / 2000 [ 72%]  (Sampling)\nChain 2: Iteration: 1446 / 2000 [ 72%]  (Sampling)\nChain 2: Iteration: 1447 / 2000 [ 72%]  (Sampling)\nChain 2: Iteration: 1448 / 2000 [ 72%]  (Sampling)\nChain 2: Iteration: 1449 / 2000 [ 72%]  (Sampling)\nChain 2: Iteration: 1450 / 2000 [ 72%]  (Sampling)\nChain 2: Iteration: 1451 / 2000 [ 72%]  (Sampling)\nChain 2: Iteration: 1452 / 2000 [ 72%]  (Sampling)\nChain 2: Iteration: 1453 / 2000 [ 72%]  (Sampling)\nChain 2: Iteration: 1454 / 2000 [ 72%]  (Sampling)\nChain 2: Iteration: 1455 / 2000 [ 72%]  (Sampling)\nChain 2: Iteration: 1456 / 2000 [ 72%]  (Sampling)\nChain 2: Iteration: 1457 / 2000 [ 72%]  (Sampling)\nChain 2: Iteration: 1458 / 2000 [ 72%]  (Sampling)\nChain 2: Iteration: 1459 / 2000 [ 72%]  (Sampling)\nChain 2: Iteration: 1460 / 2000 [ 73%]  (Sampling)\nChain 2: Iteration: 1461 / 2000 [ 73%]  (Sampling)\nChain 2: Iteration: 1462 / 2000 [ 73%]  (Sampling)\nChain 2: Iteration: 1463 / 2000 [ 73%]  (Sampling)\nChain 2: Iteration: 1464 / 2000 [ 73%]  (Sampling)\nChain 2: Iteration: 1465 / 2000 [ 73%]  (Sampling)\nChain 2: Iteration: 1466 / 2000 [ 73%]  (Sampling)\nChain 2: Iteration: 1467 / 2000 [ 73%]  (Sampling)\nChain 2: Iteration: 1468 / 2000 [ 73%]  (Sampling)\nChain 2: Iteration: 1469 / 2000 [ 73%]  (Sampling)\nChain 2: Iteration: 1470 / 2000 [ 73%]  (Sampling)\nChain 2: Iteration: 1471 / 2000 [ 73%]  (Sampling)\nChain 2: Iteration: 1472 / 2000 [ 73%]  (Sampling)\nChain 2: Iteration: 1473 / 2000 [ 73%]  (Sampling)\nChain 2: Iteration: 1474 / 2000 [ 73%]  (Sampling)\nChain 2: Iteration: 1475 / 2000 [ 73%]  (Sampling)\nChain 2: Iteration: 1476 / 2000 [ 73%]  (Sampling)\nChain 2: Iteration: 1477 / 2000 [ 73%]  (Sampling)\nChain 2: Iteration: 1478 / 2000 [ 73%]  (Sampling)\nChain 2: Iteration: 1479 / 2000 [ 73%]  (Sampling)\nChain 2: Iteration: 1480 / 2000 [ 74%]  (Sampling)\nChain 2: Iteration: 1481 / 2000 [ 74%]  (Sampling)\nChain 2: Iteration: 1482 / 2000 [ 74%]  (Sampling)\nChain 2: Iteration: 1483 / 2000 [ 74%]  (Sampling)\nChain 2: Iteration: 1484 / 2000 [ 74%]  (Sampling)\nChain 2: Iteration: 1485 / 2000 [ 74%]  (Sampling)\nChain 2: Iteration: 1486 / 2000 [ 74%]  (Sampling)\nChain 2: Iteration: 1487 / 2000 [ 74%]  (Sampling)\nChain 2: Iteration: 1488 / 2000 [ 74%]  (Sampling)\nChain 2: Iteration: 1489 / 2000 [ 74%]  (Sampling)\nChain 2: Iteration: 1490 / 2000 [ 74%]  (Sampling)\nChain 2: Iteration: 1491 / 2000 [ 74%]  (Sampling)\nChain 2: Iteration: 1492 / 2000 [ 74%]  (Sampling)\nChain 2: Iteration: 1493 / 2000 [ 74%]  (Sampling)\nChain 2: Iteration: 1494 / 2000 [ 74%]  (Sampling)\nChain 2: Iteration: 1495 / 2000 [ 74%]  (Sampling)\nChain 2: Iteration: 1496 / 2000 [ 74%]  (Sampling)\nChain 2: Iteration: 1497 / 2000 [ 74%]  (Sampling)\nChain 2: Iteration: 1498 / 2000 [ 74%]  (Sampling)\nChain 2: Iteration: 1499 / 2000 [ 74%]  (Sampling)\nChain 2: Iteration: 1500 / 2000 [ 75%]  (Sampling)\nChain 2: Iteration: 1501 / 2000 [ 75%]  (Sampling)\nChain 2: Iteration: 1502 / 2000 [ 75%]  (Sampling)\nChain 2: Iteration: 1503 / 2000 [ 75%]  (Sampling)\nChain 2: Iteration: 1504 / 2000 [ 75%]  (Sampling)\nChain 2: Iteration: 1505 / 2000 [ 75%]  (Sampling)\nChain 2: Iteration: 1506 / 2000 [ 75%]  (Sampling)\nChain 2: Iteration: 1507 / 2000 [ 75%]  (Sampling)\nChain 2: Iteration: 1508 / 2000 [ 75%]  (Sampling)\nChain 2: Iteration: 1509 / 2000 [ 75%]  (Sampling)\nChain 2: Iteration: 1510 / 2000 [ 75%]  (Sampling)\nChain 2: Iteration: 1511 / 2000 [ 75%]  (Sampling)\nChain 2: Iteration: 1512 / 2000 [ 75%]  (Sampling)\nChain 2: Iteration: 1513 / 2000 [ 75%]  (Sampling)\nChain 2: Iteration: 1514 / 2000 [ 75%]  (Sampling)\nChain 2: Iteration: 1515 / 2000 [ 75%]  (Sampling)\nChain 2: Iteration: 1516 / 2000 [ 75%]  (Sampling)\nChain 2: Iteration: 1517 / 2000 [ 75%]  (Sampling)\nChain 2: Iteration: 1518 / 2000 [ 75%]  (Sampling)\nChain 2: Iteration: 1519 / 2000 [ 75%]  (Sampling)\nChain 2: Iteration: 1520 / 2000 [ 76%]  (Sampling)\nChain 2: Iteration: 1521 / 2000 [ 76%]  (Sampling)\nChain 2: Iteration: 1522 / 2000 [ 76%]  (Sampling)\nChain 2: Iteration: 1523 / 2000 [ 76%]  (Sampling)\nChain 2: Iteration: 1524 / 2000 [ 76%]  (Sampling)\nChain 2: Iteration: 1525 / 2000 [ 76%]  (Sampling)\nChain 2: Iteration: 1526 / 2000 [ 76%]  (Sampling)\nChain 2: Iteration: 1527 / 2000 [ 76%]  (Sampling)\nChain 2: Iteration: 1528 / 2000 [ 76%]  (Sampling)\nChain 2: Iteration: 1529 / 2000 [ 76%]  (Sampling)\nChain 2: Iteration: 1530 / 2000 [ 76%]  (Sampling)\nChain 2: Iteration: 1531 / 2000 [ 76%]  (Sampling)\nChain 2: Iteration: 1532 / 2000 [ 76%]  (Sampling)\nChain 2: Iteration: 1533 / 2000 [ 76%]  (Sampling)\nChain 2: Iteration: 1534 / 2000 [ 76%]  (Sampling)\nChain 2: Iteration: 1535 / 2000 [ 76%]  (Sampling)\nChain 2: Iteration: 1536 / 2000 [ 76%]  (Sampling)\nChain 2: Iteration: 1537 / 2000 [ 76%]  (Sampling)\nChain 2: Iteration: 1538 / 2000 [ 76%]  (Sampling)\nChain 2: Iteration: 1539 / 2000 [ 76%]  (Sampling)\nChain 2: Iteration: 1540 / 2000 [ 77%]  (Sampling)\nChain 2: Iteration: 1541 / 2000 [ 77%]  (Sampling)\nChain 2: Iteration: 1542 / 2000 [ 77%]  (Sampling)\nChain 2: Iteration: 1543 / 2000 [ 77%]  (Sampling)\nChain 2: Iteration: 1544 / 2000 [ 77%]  (Sampling)\nChain 2: Iteration: 1545 / 2000 [ 77%]  (Sampling)\nChain 2: Iteration: 1546 / 2000 [ 77%]  (Sampling)\nChain 2: Iteration: 1547 / 2000 [ 77%]  (Sampling)\nChain 2: Iteration: 1548 / 2000 [ 77%]  (Sampling)\nChain 2: Iteration: 1549 / 2000 [ 77%]  (Sampling)\nChain 2: Iteration: 1550 / 2000 [ 77%]  (Sampling)\nChain 2: Iteration: 1551 / 2000 [ 77%]  (Sampling)\nChain 2: Iteration: 1552 / 2000 [ 77%]  (Sampling)\nChain 2: Iteration: 1553 / 2000 [ 77%]  (Sampling)\nChain 2: Iteration: 1554 / 2000 [ 77%]  (Sampling)\nChain 2: Iteration: 1555 / 2000 [ 77%]  (Sampling)\nChain 2: Iteration: 1556 / 2000 [ 77%]  (Sampling)\nChain 2: Iteration: 1557 / 2000 [ 77%]  (Sampling)\nChain 2: Iteration: 1558 / 2000 [ 77%]  (Sampling)\nChain 2: Iteration: 1559 / 2000 [ 77%]  (Sampling)\nChain 2: Iteration: 1560 / 2000 [ 78%]  (Sampling)\nChain 2: Iteration: 1561 / 2000 [ 78%]  (Sampling)\nChain 2: Iteration: 1562 / 2000 [ 78%]  (Sampling)\nChain 2: Iteration: 1563 / 2000 [ 78%]  (Sampling)\nChain 2: Iteration: 1564 / 2000 [ 78%]  (Sampling)\nChain 2: Iteration: 1565 / 2000 [ 78%]  (Sampling)\nChain 2: Iteration: 1566 / 2000 [ 78%]  (Sampling)\nChain 2: Iteration: 1567 / 2000 [ 78%]  (Sampling)\nChain 2: Iteration: 1568 / 2000 [ 78%]  (Sampling)\nChain 2: Iteration: 1569 / 2000 [ 78%]  (Sampling)\nChain 2: Iteration: 1570 / 2000 [ 78%]  (Sampling)\nChain 2: Iteration: 1571 / 2000 [ 78%]  (Sampling)\nChain 2: Iteration: 1572 / 2000 [ 78%]  (Sampling)\nChain 2: Iteration: 1573 / 2000 [ 78%]  (Sampling)\nChain 2: Iteration: 1574 / 2000 [ 78%]  (Sampling)\nChain 2: Iteration: 1575 / 2000 [ 78%]  (Sampling)\nChain 2: Iteration: 1576 / 2000 [ 78%]  (Sampling)\nChain 2: Iteration: 1577 / 2000 [ 78%]  (Sampling)\nChain 2: Iteration: 1578 / 2000 [ 78%]  (Sampling)\nChain 2: Iteration: 1579 / 2000 [ 78%]  (Sampling)\nChain 2: Iteration: 1580 / 2000 [ 79%]  (Sampling)\nChain 2: Iteration: 1581 / 2000 [ 79%]  (Sampling)\nChain 2: Iteration: 1582 / 2000 [ 79%]  (Sampling)\nChain 2: Iteration: 1583 / 2000 [ 79%]  (Sampling)\nChain 2: Iteration: 1584 / 2000 [ 79%]  (Sampling)\nChain 2: Iteration: 1585 / 2000 [ 79%]  (Sampling)\nChain 2: Iteration: 1586 / 2000 [ 79%]  (Sampling)\nChain 2: Iteration: 1587 / 2000 [ 79%]  (Sampling)\nChain 2: Iteration: 1588 / 2000 [ 79%]  (Sampling)\nChain 2: Iteration: 1589 / 2000 [ 79%]  (Sampling)\nChain 2: Iteration: 1590 / 2000 [ 79%]  (Sampling)\nChain 2: Iteration: 1591 / 2000 [ 79%]  (Sampling)\nChain 2: Iteration: 1592 / 2000 [ 79%]  (Sampling)\nChain 2: Iteration: 1593 / 2000 [ 79%]  (Sampling)\nChain 2: Iteration: 1594 / 2000 [ 79%]  (Sampling)\nChain 2: Iteration: 1595 / 2000 [ 79%]  (Sampling)\nChain 2: Iteration: 1596 / 2000 [ 79%]  (Sampling)\nChain 2: Iteration: 1597 / 2000 [ 79%]  (Sampling)\nChain 2: Iteration: 1598 / 2000 [ 79%]  (Sampling)\nChain 2: Iteration: 1599 / 2000 [ 79%]  (Sampling)\nChain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 2: Iteration: 1601 / 2000 [ 80%]  (Sampling)\nChain 2: Iteration: 1602 / 2000 [ 80%]  (Sampling)\nChain 2: Iteration: 1603 / 2000 [ 80%]  (Sampling)\nChain 2: Iteration: 1604 / 2000 [ 80%]  (Sampling)\nChain 2: Iteration: 1605 / 2000 [ 80%]  (Sampling)\nChain 2: Iteration: 1606 / 2000 [ 80%]  (Sampling)\nChain 2: Iteration: 1607 / 2000 [ 80%]  (Sampling)\nChain 2: Iteration: 1608 / 2000 [ 80%]  (Sampling)\nChain 2: Iteration: 1609 / 2000 [ 80%]  (Sampling)\nChain 2: Iteration: 1610 / 2000 [ 80%]  (Sampling)\nChain 2: Iteration: 1611 / 2000 [ 80%]  (Sampling)\nChain 2: Iteration: 1612 / 2000 [ 80%]  (Sampling)\nChain 2: Iteration: 1613 / 2000 [ 80%]  (Sampling)\nChain 2: Iteration: 1614 / 2000 [ 80%]  (Sampling)\nChain 2: Iteration: 1615 / 2000 [ 80%]  (Sampling)\nChain 2: Iteration: 1616 / 2000 [ 80%]  (Sampling)\nChain 2: Iteration: 1617 / 2000 [ 80%]  (Sampling)\nChain 2: Iteration: 1618 / 2000 [ 80%]  (Sampling)\nChain 2: Iteration: 1619 / 2000 [ 80%]  (Sampling)\nChain 2: Iteration: 1620 / 2000 [ 81%]  (Sampling)\nChain 2: Iteration: 1621 / 2000 [ 81%]  (Sampling)\nChain 2: Iteration: 1622 / 2000 [ 81%]  (Sampling)\nChain 2: Iteration: 1623 / 2000 [ 81%]  (Sampling)\nChain 2: Iteration: 1624 / 2000 [ 81%]  (Sampling)\nChain 2: Iteration: 1625 / 2000 [ 81%]  (Sampling)\nChain 2: Iteration: 1626 / 2000 [ 81%]  (Sampling)\nChain 2: Iteration: 1627 / 2000 [ 81%]  (Sampling)\nChain 2: Iteration: 1628 / 2000 [ 81%]  (Sampling)\nChain 2: Iteration: 1629 / 2000 [ 81%]  (Sampling)\nChain 2: Iteration: 1630 / 2000 [ 81%]  (Sampling)\nChain 2: Iteration: 1631 / 2000 [ 81%]  (Sampling)\nChain 2: Iteration: 1632 / 2000 [ 81%]  (Sampling)\nChain 2: Iteration: 1633 / 2000 [ 81%]  (Sampling)\nChain 2: Iteration: 1634 / 2000 [ 81%]  (Sampling)\nChain 2: Iteration: 1635 / 2000 [ 81%]  (Sampling)\nChain 2: Iteration: 1636 / 2000 [ 81%]  (Sampling)\nChain 2: Iteration: 1637 / 2000 [ 81%]  (Sampling)\nChain 2: Iteration: 1638 / 2000 [ 81%]  (Sampling)\nChain 2: Iteration: 1639 / 2000 [ 81%]  (Sampling)\nChain 2: Iteration: 1640 / 2000 [ 82%]  (Sampling)\nChain 2: Iteration: 1641 / 2000 [ 82%]  (Sampling)\nChain 2: Iteration: 1642 / 2000 [ 82%]  (Sampling)\nChain 2: Iteration: 1643 / 2000 [ 82%]  (Sampling)\nChain 2: Iteration: 1644 / 2000 [ 82%]  (Sampling)\nChain 2: Iteration: 1645 / 2000 [ 82%]  (Sampling)\nChain 2: Iteration: 1646 / 2000 [ 82%]  (Sampling)\nChain 2: Iteration: 1647 / 2000 [ 82%]  (Sampling)\nChain 2: Iteration: 1648 / 2000 [ 82%]  (Sampling)\nChain 2: Iteration: 1649 / 2000 [ 82%]  (Sampling)\nChain 2: Iteration: 1650 / 2000 [ 82%]  (Sampling)\nChain 2: Iteration: 1651 / 2000 [ 82%]  (Sampling)\nChain 2: Iteration: 1652 / 2000 [ 82%]  (Sampling)\nChain 2: Iteration: 1653 / 2000 [ 82%]  (Sampling)\nChain 2: Iteration: 1654 / 2000 [ 82%]  (Sampling)\nChain 2: Iteration: 1655 / 2000 [ 82%]  (Sampling)\nChain 2: Iteration: 1656 / 2000 [ 82%]  (Sampling)\nChain 2: Iteration: 1657 / 2000 [ 82%]  (Sampling)\nChain 2: Iteration: 1658 / 2000 [ 82%]  (Sampling)\nChain 2: Iteration: 1659 / 2000 [ 82%]  (Sampling)\nChain 2: Iteration: 1660 / 2000 [ 83%]  (Sampling)\nChain 2: Iteration: 1661 / 2000 [ 83%]  (Sampling)\nChain 2: Iteration: 1662 / 2000 [ 83%]  (Sampling)\nChain 2: Iteration: 1663 / 2000 [ 83%]  (Sampling)\nChain 2: Iteration: 1664 / 2000 [ 83%]  (Sampling)\nChain 2: Iteration: 1665 / 2000 [ 83%]  (Sampling)\nChain 2: Iteration: 1666 / 2000 [ 83%]  (Sampling)\nChain 2: Iteration: 1667 / 2000 [ 83%]  (Sampling)\nChain 2: Iteration: 1668 / 2000 [ 83%]  (Sampling)\nChain 2: Iteration: 1669 / 2000 [ 83%]  (Sampling)\nChain 2: Iteration: 1670 / 2000 [ 83%]  (Sampling)\nChain 2: Iteration: 1671 / 2000 [ 83%]  (Sampling)\nChain 2: Iteration: 1672 / 2000 [ 83%]  (Sampling)\nChain 2: Iteration: 1673 / 2000 [ 83%]  (Sampling)\nChain 2: Iteration: 1674 / 2000 [ 83%]  (Sampling)\nChain 2: Iteration: 1675 / 2000 [ 83%]  (Sampling)\nChain 2: Iteration: 1676 / 2000 [ 83%]  (Sampling)\nChain 2: Iteration: 1677 / 2000 [ 83%]  (Sampling)\nChain 2: Iteration: 1678 / 2000 [ 83%]  (Sampling)\nChain 2: Iteration: 1679 / 2000 [ 83%]  (Sampling)\nChain 2: Iteration: 1680 / 2000 [ 84%]  (Sampling)\nChain 2: Iteration: 1681 / 2000 [ 84%]  (Sampling)\nChain 2: Iteration: 1682 / 2000 [ 84%]  (Sampling)\nChain 2: Iteration: 1683 / 2000 [ 84%]  (Sampling)\nChain 2: Iteration: 1684 / 2000 [ 84%]  (Sampling)\nChain 2: Iteration: 1685 / 2000 [ 84%]  (Sampling)\nChain 2: Iteration: 1686 / 2000 [ 84%]  (Sampling)\nChain 2: Iteration: 1687 / 2000 [ 84%]  (Sampling)\nChain 2: Iteration: 1688 / 2000 [ 84%]  (Sampling)\nChain 2: Iteration: 1689 / 2000 [ 84%]  (Sampling)\nChain 2: Iteration: 1690 / 2000 [ 84%]  (Sampling)\nChain 2: Iteration: 1691 / 2000 [ 84%]  (Sampling)\nChain 2: Iteration: 1692 / 2000 [ 84%]  (Sampling)\nChain 2: Iteration: 1693 / 2000 [ 84%]  (Sampling)\nChain 2: Iteration: 1694 / 2000 [ 84%]  (Sampling)\nChain 2: Iteration: 1695 / 2000 [ 84%]  (Sampling)\nChain 2: Iteration: 1696 / 2000 [ 84%]  (Sampling)\nChain 2: Iteration: 1697 / 2000 [ 84%]  (Sampling)\nChain 2: Iteration: 1698 / 2000 [ 84%]  (Sampling)\nChain 2: Iteration: 1699 / 2000 [ 84%]  (Sampling)\nChain 2: Iteration: 1700 / 2000 [ 85%]  (Sampling)\nChain 2: Iteration: 1701 / 2000 [ 85%]  (Sampling)\nChain 2: Iteration: 1702 / 2000 [ 85%]  (Sampling)\nChain 2: Iteration: 1703 / 2000 [ 85%]  (Sampling)\nChain 2: Iteration: 1704 / 2000 [ 85%]  (Sampling)\nChain 2: Iteration: 1705 / 2000 [ 85%]  (Sampling)\nChain 2: Iteration: 1706 / 2000 [ 85%]  (Sampling)\nChain 2: Iteration: 1707 / 2000 [ 85%]  (Sampling)\nChain 2: Iteration: 1708 / 2000 [ 85%]  (Sampling)\nChain 2: Iteration: 1709 / 2000 [ 85%]  (Sampling)\nChain 2: Iteration: 1710 / 2000 [ 85%]  (Sampling)\nChain 2: Iteration: 1711 / 2000 [ 85%]  (Sampling)\nChain 2: Iteration: 1712 / 2000 [ 85%]  (Sampling)\nChain 2: Iteration: 1713 / 2000 [ 85%]  (Sampling)\nChain 2: Iteration: 1714 / 2000 [ 85%]  (Sampling)\nChain 2: Iteration: 1715 / 2000 [ 85%]  (Sampling)\nChain 2: Iteration: 1716 / 2000 [ 85%]  (Sampling)\nChain 2: Iteration: 1717 / 2000 [ 85%]  (Sampling)\nChain 2: Iteration: 1718 / 2000 [ 85%]  (Sampling)\nChain 2: Iteration: 1719 / 2000 [ 85%]  (Sampling)\nChain 2: Iteration: 1720 / 2000 [ 86%]  (Sampling)\nChain 2: Iteration: 1721 / 2000 [ 86%]  (Sampling)\nChain 2: Iteration: 1722 / 2000 [ 86%]  (Sampling)\nChain 2: Iteration: 1723 / 2000 [ 86%]  (Sampling)\nChain 2: Iteration: 1724 / 2000 [ 86%]  (Sampling)\nChain 2: Iteration: 1725 / 2000 [ 86%]  (Sampling)\nChain 2: Iteration: 1726 / 2000 [ 86%]  (Sampling)\nChain 2: Iteration: 1727 / 2000 [ 86%]  (Sampling)\nChain 2: Iteration: 1728 / 2000 [ 86%]  (Sampling)\nChain 2: Iteration: 1729 / 2000 [ 86%]  (Sampling)\nChain 2: Iteration: 1730 / 2000 [ 86%]  (Sampling)\nChain 2: Iteration: 1731 / 2000 [ 86%]  (Sampling)\nChain 2: Iteration: 1732 / 2000 [ 86%]  (Sampling)\nChain 2: Iteration: 1733 / 2000 [ 86%]  (Sampling)\nChain 2: Iteration: 1734 / 2000 [ 86%]  (Sampling)\nChain 2: Iteration: 1735 / 2000 [ 86%]  (Sampling)\nChain 2: Iteration: 1736 / 2000 [ 86%]  (Sampling)\nChain 2: Iteration: 1737 / 2000 [ 86%]  (Sampling)\nChain 2: Iteration: 1738 / 2000 [ 86%]  (Sampling)\nChain 2: Iteration: 1739 / 2000 [ 86%]  (Sampling)\nChain 2: Iteration: 1740 / 2000 [ 87%]  (Sampling)\nChain 2: Iteration: 1741 / 2000 [ 87%]  (Sampling)\nChain 2: Iteration: 1742 / 2000 [ 87%]  (Sampling)\nChain 2: Iteration: 1743 / 2000 [ 87%]  (Sampling)\nChain 2: Iteration: 1744 / 2000 [ 87%]  (Sampling)\nChain 2: Iteration: 1745 / 2000 [ 87%]  (Sampling)\nChain 2: Iteration: 1746 / 2000 [ 87%]  (Sampling)\nChain 2: Iteration: 1747 / 2000 [ 87%]  (Sampling)\nChain 2: Iteration: 1748 / 2000 [ 87%]  (Sampling)\nChain 2: Iteration: 1749 / 2000 [ 87%]  (Sampling)\nChain 2: Iteration: 1750 / 2000 [ 87%]  (Sampling)\nChain 2: Iteration: 1751 / 2000 [ 87%]  (Sampling)\nChain 2: Iteration: 1752 / 2000 [ 87%]  (Sampling)\nChain 2: Iteration: 1753 / 2000 [ 87%]  (Sampling)\nChain 2: Iteration: 1754 / 2000 [ 87%]  (Sampling)\nChain 2: Iteration: 1755 / 2000 [ 87%]  (Sampling)\nChain 2: Iteration: 1756 / 2000 [ 87%]  (Sampling)\nChain 2: Iteration: 1757 / 2000 [ 87%]  (Sampling)\nChain 2: Iteration: 1758 / 2000 [ 87%]  (Sampling)\nChain 2: Iteration: 1759 / 2000 [ 87%]  (Sampling)\nChain 2: Iteration: 1760 / 2000 [ 88%]  (Sampling)\nChain 2: Iteration: 1761 / 2000 [ 88%]  (Sampling)\nChain 2: Iteration: 1762 / 2000 [ 88%]  (Sampling)\nChain 2: Iteration: 1763 / 2000 [ 88%]  (Sampling)\nChain 2: Iteration: 1764 / 2000 [ 88%]  (Sampling)\nChain 2: Iteration: 1765 / 2000 [ 88%]  (Sampling)\nChain 2: Iteration: 1766 / 2000 [ 88%]  (Sampling)\nChain 2: Iteration: 1767 / 2000 [ 88%]  (Sampling)\nChain 2: Iteration: 1768 / 2000 [ 88%]  (Sampling)\nChain 2: Iteration: 1769 / 2000 [ 88%]  (Sampling)\nChain 2: Iteration: 1770 / 2000 [ 88%]  (Sampling)\nChain 2: Iteration: 1771 / 2000 [ 88%]  (Sampling)\nChain 2: Iteration: 1772 / 2000 [ 88%]  (Sampling)\nChain 2: Iteration: 1773 / 2000 [ 88%]  (Sampling)\nChain 2: Iteration: 1774 / 2000 [ 88%]  (Sampling)\nChain 2: Iteration: 1775 / 2000 [ 88%]  (Sampling)\nChain 2: Iteration: 1776 / 2000 [ 88%]  (Sampling)\nChain 2: Iteration: 1777 / 2000 [ 88%]  (Sampling)\nChain 2: Iteration: 1778 / 2000 [ 88%]  (Sampling)\nChain 2: Iteration: 1779 / 2000 [ 88%]  (Sampling)\nChain 2: Iteration: 1780 / 2000 [ 89%]  (Sampling)\nChain 2: Iteration: 1781 / 2000 [ 89%]  (Sampling)\nChain 2: Iteration: 1782 / 2000 [ 89%]  (Sampling)\nChain 2: Iteration: 1783 / 2000 [ 89%]  (Sampling)\nChain 2: Iteration: 1784 / 2000 [ 89%]  (Sampling)\nChain 2: Iteration: 1785 / 2000 [ 89%]  (Sampling)\nChain 2: Iteration: 1786 / 2000 [ 89%]  (Sampling)\nChain 2: Iteration: 1787 / 2000 [ 89%]  (Sampling)\nChain 2: Iteration: 1788 / 2000 [ 89%]  (Sampling)\nChain 2: Iteration: 1789 / 2000 [ 89%]  (Sampling)\nChain 2: Iteration: 1790 / 2000 [ 89%]  (Sampling)\nChain 2: Iteration: 1791 / 2000 [ 89%]  (Sampling)\nChain 2: Iteration: 1792 / 2000 [ 89%]  (Sampling)\nChain 2: Iteration: 1793 / 2000 [ 89%]  (Sampling)\nChain 2: Iteration: 1794 / 2000 [ 89%]  (Sampling)\nChain 2: Iteration: 1795 / 2000 [ 89%]  (Sampling)\nChain 2: Iteration: 1796 / 2000 [ 89%]  (Sampling)\nChain 2: Iteration: 1797 / 2000 [ 89%]  (Sampling)\nChain 2: Iteration: 1798 / 2000 [ 89%]  (Sampling)\nChain 2: Iteration: 1799 / 2000 [ 89%]  (Sampling)\nChain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 2: Iteration: 1801 / 2000 [ 90%]  (Sampling)\nChain 2: Iteration: 1802 / 2000 [ 90%]  (Sampling)\nChain 2: Iteration: 1803 / 2000 [ 90%]  (Sampling)\nChain 2: Iteration: 1804 / 2000 [ 90%]  (Sampling)\nChain 2: Iteration: 1805 / 2000 [ 90%]  (Sampling)\nChain 2: Iteration: 1806 / 2000 [ 90%]  (Sampling)\nChain 2: Iteration: 1807 / 2000 [ 90%]  (Sampling)\nChain 2: Iteration: 1808 / 2000 [ 90%]  (Sampling)\nChain 2: Iteration: 1809 / 2000 [ 90%]  (Sampling)\nChain 2: Iteration: 1810 / 2000 [ 90%]  (Sampling)\nChain 2: Iteration: 1811 / 2000 [ 90%]  (Sampling)\nChain 2: Iteration: 1812 / 2000 [ 90%]  (Sampling)\nChain 2: Iteration: 1813 / 2000 [ 90%]  (Sampling)\nChain 2: Iteration: 1814 / 2000 [ 90%]  (Sampling)\nChain 2: Iteration: 1815 / 2000 [ 90%]  (Sampling)\nChain 2: Iteration: 1816 / 2000 [ 90%]  (Sampling)\nChain 2: Iteration: 1817 / 2000 [ 90%]  (Sampling)\nChain 2: Iteration: 1818 / 2000 [ 90%]  (Sampling)\nChain 2: Iteration: 1819 / 2000 [ 90%]  (Sampling)\nChain 2: Iteration: 1820 / 2000 [ 91%]  (Sampling)\nChain 2: Iteration: 1821 / 2000 [ 91%]  (Sampling)\nChain 2: Iteration: 1822 / 2000 [ 91%]  (Sampling)\nChain 2: Iteration: 1823 / 2000 [ 91%]  (Sampling)\nChain 2: Iteration: 1824 / 2000 [ 91%]  (Sampling)\nChain 2: Iteration: 1825 / 2000 [ 91%]  (Sampling)\nChain 2: Iteration: 1826 / 2000 [ 91%]  (Sampling)\nChain 2: Iteration: 1827 / 2000 [ 91%]  (Sampling)\nChain 2: Iteration: 1828 / 2000 [ 91%]  (Sampling)\nChain 2: Iteration: 1829 / 2000 [ 91%]  (Sampling)\nChain 2: Iteration: 1830 / 2000 [ 91%]  (Sampling)\nChain 2: Iteration: 1831 / 2000 [ 91%]  (Sampling)\nChain 2: Iteration: 1832 / 2000 [ 91%]  (Sampling)\nChain 2: Iteration: 1833 / 2000 [ 91%]  (Sampling)\nChain 2: Iteration: 1834 / 2000 [ 91%]  (Sampling)\nChain 2: Iteration: 1835 / 2000 [ 91%]  (Sampling)\nChain 2: Iteration: 1836 / 2000 [ 91%]  (Sampling)\nChain 2: Iteration: 1837 / 2000 [ 91%]  (Sampling)\nChain 2: Iteration: 1838 / 2000 [ 91%]  (Sampling)\nChain 2: Iteration: 1839 / 2000 [ 91%]  (Sampling)\nChain 2: Iteration: 1840 / 2000 [ 92%]  (Sampling)\nChain 2: Iteration: 1841 / 2000 [ 92%]  (Sampling)\nChain 2: Iteration: 1842 / 2000 [ 92%]  (Sampling)\nChain 2: Iteration: 1843 / 2000 [ 92%]  (Sampling)\nChain 2: Iteration: 1844 / 2000 [ 92%]  (Sampling)\nChain 2: Iteration: 1845 / 2000 [ 92%]  (Sampling)\nChain 2: Iteration: 1846 / 2000 [ 92%]  (Sampling)\nChain 2: Iteration: 1847 / 2000 [ 92%]  (Sampling)\nChain 2: Iteration: 1848 / 2000 [ 92%]  (Sampling)\nChain 2: Iteration: 1849 / 2000 [ 92%]  (Sampling)\nChain 2: Iteration: 1850 / 2000 [ 92%]  (Sampling)\nChain 2: Iteration: 1851 / 2000 [ 92%]  (Sampling)\nChain 2: Iteration: 1852 / 2000 [ 92%]  (Sampling)\nChain 2: Iteration: 1853 / 2000 [ 92%]  (Sampling)\nChain 2: Iteration: 1854 / 2000 [ 92%]  (Sampling)\nChain 2: Iteration: 1855 / 2000 [ 92%]  (Sampling)\nChain 2: Iteration: 1856 / 2000 [ 92%]  (Sampling)\nChain 2: Iteration: 1857 / 2000 [ 92%]  (Sampling)\nChain 2: Iteration: 1858 / 2000 [ 92%]  (Sampling)\nChain 2: Iteration: 1859 / 2000 [ 92%]  (Sampling)\nChain 2: Iteration: 1860 / 2000 [ 93%]  (Sampling)\nChain 2: Iteration: 1861 / 2000 [ 93%]  (Sampling)\nChain 2: Iteration: 1862 / 2000 [ 93%]  (Sampling)\nChain 2: Iteration: 1863 / 2000 [ 93%]  (Sampling)\nChain 2: Iteration: 1864 / 2000 [ 93%]  (Sampling)\nChain 2: Iteration: 1865 / 2000 [ 93%]  (Sampling)\nChain 2: Iteration: 1866 / 2000 [ 93%]  (Sampling)\nChain 2: Iteration: 1867 / 2000 [ 93%]  (Sampling)\nChain 2: Iteration: 1868 / 2000 [ 93%]  (Sampling)\nChain 2: Iteration: 1869 / 2000 [ 93%]  (Sampling)\nChain 2: Iteration: 1870 / 2000 [ 93%]  (Sampling)\nChain 2: Iteration: 1871 / 2000 [ 93%]  (Sampling)\nChain 2: Iteration: 1872 / 2000 [ 93%]  (Sampling)\nChain 2: Iteration: 1873 / 2000 [ 93%]  (Sampling)\nChain 2: Iteration: 1874 / 2000 [ 93%]  (Sampling)\nChain 2: Iteration: 1875 / 2000 [ 93%]  (Sampling)\nChain 2: Iteration: 1876 / 2000 [ 93%]  (Sampling)\nChain 2: Iteration: 1877 / 2000 [ 93%]  (Sampling)\nChain 2: Iteration: 1878 / 2000 [ 93%]  (Sampling)\nChain 2: Iteration: 1879 / 2000 [ 93%]  (Sampling)\nChain 2: Iteration: 1880 / 2000 [ 94%]  (Sampling)\nChain 2: Iteration: 1881 / 2000 [ 94%]  (Sampling)\nChain 2: Iteration: 1882 / 2000 [ 94%]  (Sampling)\nChain 2: Iteration: 1883 / 2000 [ 94%]  (Sampling)\nChain 2: Iteration: 1884 / 2000 [ 94%]  (Sampling)\nChain 2: Iteration: 1885 / 2000 [ 94%]  (Sampling)\nChain 2: Iteration: 1886 / 2000 [ 94%]  (Sampling)\nChain 2: Iteration: 1887 / 2000 [ 94%]  (Sampling)\nChain 2: Iteration: 1888 / 2000 [ 94%]  (Sampling)\nChain 2: Iteration: 1889 / 2000 [ 94%]  (Sampling)\nChain 2: Iteration: 1890 / 2000 [ 94%]  (Sampling)\nChain 2: Iteration: 1891 / 2000 [ 94%]  (Sampling)\nChain 2: Iteration: 1892 / 2000 [ 94%]  (Sampling)\nChain 2: Iteration: 1893 / 2000 [ 94%]  (Sampling)\nChain 2: Iteration: 1894 / 2000 [ 94%]  (Sampling)\nChain 2: Iteration: 1895 / 2000 [ 94%]  (Sampling)\nChain 2: Iteration: 1896 / 2000 [ 94%]  (Sampling)\nChain 2: Iteration: 1897 / 2000 [ 94%]  (Sampling)\nChain 2: Iteration: 1898 / 2000 [ 94%]  (Sampling)\nChain 2: Iteration: 1899 / 2000 [ 94%]  (Sampling)\nChain 2: Iteration: 1900 / 2000 [ 95%]  (Sampling)\nChain 2: Iteration: 1901 / 2000 [ 95%]  (Sampling)\nChain 2: Iteration: 1902 / 2000 [ 95%]  (Sampling)\nChain 2: Iteration: 1903 / 2000 [ 95%]  (Sampling)\nChain 2: Iteration: 1904 / 2000 [ 95%]  (Sampling)\nChain 2: Iteration: 1905 / 2000 [ 95%]  (Sampling)\nChain 2: Iteration: 1906 / 2000 [ 95%]  (Sampling)\nChain 2: Iteration: 1907 / 2000 [ 95%]  (Sampling)\nChain 2: Iteration: 1908 / 2000 [ 95%]  (Sampling)\nChain 2: Iteration: 1909 / 2000 [ 95%]  (Sampling)\nChain 2: Iteration: 1910 / 2000 [ 95%]  (Sampling)\nChain 2: Iteration: 1911 / 2000 [ 95%]  (Sampling)\nChain 2: Iteration: 1912 / 2000 [ 95%]  (Sampling)\nChain 2: Iteration: 1913 / 2000 [ 95%]  (Sampling)\nChain 2: Iteration: 1914 / 2000 [ 95%]  (Sampling)\nChain 2: Iteration: 1915 / 2000 [ 95%]  (Sampling)\nChain 2: Iteration: 1916 / 2000 [ 95%]  (Sampling)\nChain 2: Iteration: 1917 / 2000 [ 95%]  (Sampling)\nChain 2: Iteration: 1918 / 2000 [ 95%]  (Sampling)\nChain 2: Iteration: 1919 / 2000 [ 95%]  (Sampling)\nChain 2: Iteration: 1920 / 2000 [ 96%]  (Sampling)\nChain 2: Iteration: 1921 / 2000 [ 96%]  (Sampling)\nChain 2: Iteration: 1922 / 2000 [ 96%]  (Sampling)\nChain 2: Iteration: 1923 / 2000 [ 96%]  (Sampling)\nChain 2: Iteration: 1924 / 2000 [ 96%]  (Sampling)\nChain 2: Iteration: 1925 / 2000 [ 96%]  (Sampling)\nChain 2: Iteration: 1926 / 2000 [ 96%]  (Sampling)\nChain 2: Iteration: 1927 / 2000 [ 96%]  (Sampling)\nChain 2: Iteration: 1928 / 2000 [ 96%]  (Sampling)\nChain 2: Iteration: 1929 / 2000 [ 96%]  (Sampling)\nChain 2: Iteration: 1930 / 2000 [ 96%]  (Sampling)\nChain 2: Iteration: 1931 / 2000 [ 96%]  (Sampling)\nChain 2: Iteration: 1932 / 2000 [ 96%]  (Sampling)\nChain 2: Iteration: 1933 / 2000 [ 96%]  (Sampling)\nChain 2: Iteration: 1934 / 2000 [ 96%]  (Sampling)\nChain 2: Iteration: 1935 / 2000 [ 96%]  (Sampling)\nChain 2: Iteration: 1936 / 2000 [ 96%]  (Sampling)\nChain 2: Iteration: 1937 / 2000 [ 96%]  (Sampling)\nChain 2: Iteration: 1938 / 2000 [ 96%]  (Sampling)\nChain 2: Iteration: 1939 / 2000 [ 96%]  (Sampling)\nChain 2: Iteration: 1940 / 2000 [ 97%]  (Sampling)\nChain 2: Iteration: 1941 / 2000 [ 97%]  (Sampling)\nChain 2: Iteration: 1942 / 2000 [ 97%]  (Sampling)\nChain 2: Iteration: 1943 / 2000 [ 97%]  (Sampling)\nChain 2: Iteration: 1944 / 2000 [ 97%]  (Sampling)\nChain 2: Iteration: 1945 / 2000 [ 97%]  (Sampling)\nChain 2: Iteration: 1946 / 2000 [ 97%]  (Sampling)\nChain 2: Iteration: 1947 / 2000 [ 97%]  (Sampling)\nChain 2: Iteration: 1948 / 2000 [ 97%]  (Sampling)\nChain 2: Iteration: 1949 / 2000 [ 97%]  (Sampling)\nChain 2: Iteration: 1950 / 2000 [ 97%]  (Sampling)\nChain 2: Iteration: 1951 / 2000 [ 97%]  (Sampling)\nChain 2: Iteration: 1952 / 2000 [ 97%]  (Sampling)\nChain 2: Iteration: 1953 / 2000 [ 97%]  (Sampling)\nChain 2: Iteration: 1954 / 2000 [ 97%]  (Sampling)\nChain 2: Iteration: 1955 / 2000 [ 97%]  (Sampling)\nChain 2: Iteration: 1956 / 2000 [ 97%]  (Sampling)\nChain 2: Iteration: 1957 / 2000 [ 97%]  (Sampling)\nChain 2: Iteration: 1958 / 2000 [ 97%]  (Sampling)\nChain 2: Iteration: 1959 / 2000 [ 97%]  (Sampling)\nChain 2: Iteration: 1960 / 2000 [ 98%]  (Sampling)\nChain 2: Iteration: 1961 / 2000 [ 98%]  (Sampling)\nChain 2: Iteration: 1962 / 2000 [ 98%]  (Sampling)\nChain 2: Iteration: 1963 / 2000 [ 98%]  (Sampling)\nChain 2: Iteration: 1964 / 2000 [ 98%]  (Sampling)\nChain 2: Iteration: 1965 / 2000 [ 98%]  (Sampling)\nChain 2: Iteration: 1966 / 2000 [ 98%]  (Sampling)\nChain 2: Iteration: 1967 / 2000 [ 98%]  (Sampling)\nChain 2: Iteration: 1968 / 2000 [ 98%]  (Sampling)\nChain 2: Iteration: 1969 / 2000 [ 98%]  (Sampling)\nChain 2: Iteration: 1970 / 2000 [ 98%]  (Sampling)\nChain 2: Iteration: 1971 / 2000 [ 98%]  (Sampling)\nChain 2: Iteration: 1972 / 2000 [ 98%]  (Sampling)\nChain 2: Iteration: 1973 / 2000 [ 98%]  (Sampling)\nChain 2: Iteration: 1974 / 2000 [ 98%]  (Sampling)\nChain 2: Iteration: 1975 / 2000 [ 98%]  (Sampling)\nChain 2: Iteration: 1976 / 2000 [ 98%]  (Sampling)\nChain 2: Iteration: 1977 / 2000 [ 98%]  (Sampling)\nChain 2: Iteration: 1978 / 2000 [ 98%]  (Sampling)\nChain 2: Iteration: 1979 / 2000 [ 98%]  (Sampling)\nChain 2: Iteration: 1980 / 2000 [ 99%]  (Sampling)\nChain 2: Iteration: 1981 / 2000 [ 99%]  (Sampling)\nChain 2: Iteration: 1982 / 2000 [ 99%]  (Sampling)\nChain 2: Iteration: 1983 / 2000 [ 99%]  (Sampling)\nChain 2: Iteration: 1984 / 2000 [ 99%]  (Sampling)\nChain 2: Iteration: 1985 / 2000 [ 99%]  (Sampling)\nChain 2: Iteration: 1986 / 2000 [ 99%]  (Sampling)\nChain 2: Iteration: 1987 / 2000 [ 99%]  (Sampling)\nChain 2: Iteration: 1988 / 2000 [ 99%]  (Sampling)\nChain 2: Iteration: 1989 / 2000 [ 99%]  (Sampling)\nChain 2: Iteration: 1990 / 2000 [ 99%]  (Sampling)\nChain 2: Iteration: 1991 / 2000 [ 99%]  (Sampling)\nChain 2: Iteration: 1992 / 2000 [ 99%]  (Sampling)\nChain 2: Iteration: 1993 / 2000 [ 99%]  (Sampling)\nChain 2: Iteration: 1994 / 2000 [ 99%]  (Sampling)\nChain 2: Iteration: 1995 / 2000 [ 99%]  (Sampling)\nChain 2: Iteration: 1996 / 2000 [ 99%]  (Sampling)\nChain 2: Iteration: 1997 / 2000 [ 99%]  (Sampling)\nChain 2: Iteration: 1998 / 2000 [ 99%]  (Sampling)\nChain 2: Iteration: 1999 / 2000 [ 99%]  (Sampling)\nChain 2: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 0.039 seconds (Warm-up)\nChain 2:                0.039 seconds (Sampling)\nChain 2:                0.078 seconds (Total)\nChain 2: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 3).\nChain 3: \nChain 3: Gradient evaluation took 3e-06 seconds\nChain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.03 seconds.\nChain 3: Adjust your expectations accordingly!\nChain 3: \nChain 3: \nChain 3: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 3: Iteration:    2 / 2000 [  0%]  (Warmup)\nChain 3: Iteration:    3 / 2000 [  0%]  (Warmup)\nChain 3: Iteration:    4 / 2000 [  0%]  (Warmup)\nChain 3: Iteration:    5 / 2000 [  0%]  (Warmup)\nChain 3: Iteration:    6 / 2000 [  0%]  (Warmup)\nChain 3: Iteration:    7 / 2000 [  0%]  (Warmup)\nChain 3: Iteration:    8 / 2000 [  0%]  (Warmup)\nChain 3: Iteration:    9 / 2000 [  0%]  (Warmup)\nChain 3: Iteration:   10 / 2000 [  0%]  (Warmup)\nChain 3: Iteration:   11 / 2000 [  0%]  (Warmup)\nChain 3: Iteration:   12 / 2000 [  0%]  (Warmup)\nChain 3: Iteration:   13 / 2000 [  0%]  (Warmup)\nChain 3: Iteration:   14 / 2000 [  0%]  (Warmup)\nChain 3: Iteration:   15 / 2000 [  0%]  (Warmup)\nChain 3: Iteration:   16 / 2000 [  0%]  (Warmup)\nChain 3: Iteration:   17 / 2000 [  0%]  (Warmup)\nChain 3: Iteration:   18 / 2000 [  0%]  (Warmup)\nChain 3: Iteration:   19 / 2000 [  0%]  (Warmup)\nChain 3: Iteration:   20 / 2000 [  1%]  (Warmup)\nChain 3: Iteration:   21 / 2000 [  1%]  (Warmup)\nChain 3: Iteration:   22 / 2000 [  1%]  (Warmup)\nChain 3: Iteration:   23 / 2000 [  1%]  (Warmup)\nChain 3: Iteration:   24 / 2000 [  1%]  (Warmup)\nChain 3: Iteration:   25 / 2000 [  1%]  (Warmup)\nChain 3: Iteration:   26 / 2000 [  1%]  (Warmup)\nChain 3: Iteration:   27 / 2000 [  1%]  (Warmup)\nChain 3: Iteration:   28 / 2000 [  1%]  (Warmup)\nChain 3: Iteration:   29 / 2000 [  1%]  (Warmup)\nChain 3: Iteration:   30 / 2000 [  1%]  (Warmup)\nChain 3: Iteration:   31 / 2000 [  1%]  (Warmup)\nChain 3: Iteration:   32 / 2000 [  1%]  (Warmup)\nChain 3: Iteration:   33 / 2000 [  1%]  (Warmup)\nChain 3: Iteration:   34 / 2000 [  1%]  (Warmup)\nChain 3: Iteration:   35 / 2000 [  1%]  (Warmup)\nChain 3: Iteration:   36 / 2000 [  1%]  (Warmup)\nChain 3: Iteration:   37 / 2000 [  1%]  (Warmup)\nChain 3: Iteration:   38 / 2000 [  1%]  (Warmup)\nChain 3: Iteration:   39 / 2000 [  1%]  (Warmup)\nChain 3: Iteration:   40 / 2000 [  2%]  (Warmup)\nChain 3: Iteration:   41 / 2000 [  2%]  (Warmup)\nChain 3: Iteration:   42 / 2000 [  2%]  (Warmup)\nChain 3: Iteration:   43 / 2000 [  2%]  (Warmup)\nChain 3: Iteration:   44 / 2000 [  2%]  (Warmup)\nChain 3: Iteration:   45 / 2000 [  2%]  (Warmup)\nChain 3: Iteration:   46 / 2000 [  2%]  (Warmup)\nChain 3: Iteration:   47 / 2000 [  2%]  (Warmup)\nChain 3: Iteration:   48 / 2000 [  2%]  (Warmup)\nChain 3: Iteration:   49 / 2000 [  2%]  (Warmup)\nChain 3: Iteration:   50 / 2000 [  2%]  (Warmup)\nChain 3: Iteration:   51 / 2000 [  2%]  (Warmup)\nChain 3: Iteration:   52 / 2000 [  2%]  (Warmup)\nChain 3: Iteration:   53 / 2000 [  2%]  (Warmup)\nChain 3: Iteration:   54 / 2000 [  2%]  (Warmup)\nChain 3: Iteration:   55 / 2000 [  2%]  (Warmup)\nChain 3: Iteration:   56 / 2000 [  2%]  (Warmup)\nChain 3: Iteration:   57 / 2000 [  2%]  (Warmup)\nChain 3: Iteration:   58 / 2000 [  2%]  (Warmup)\nChain 3: Iteration:   59 / 2000 [  2%]  (Warmup)\nChain 3: Iteration:   60 / 2000 [  3%]  (Warmup)\nChain 3: Iteration:   61 / 2000 [  3%]  (Warmup)\nChain 3: Iteration:   62 / 2000 [  3%]  (Warmup)\nChain 3: Iteration:   63 / 2000 [  3%]  (Warmup)\nChain 3: Iteration:   64 / 2000 [  3%]  (Warmup)\nChain 3: Iteration:   65 / 2000 [  3%]  (Warmup)\nChain 3: Iteration:   66 / 2000 [  3%]  (Warmup)\nChain 3: Iteration:   67 / 2000 [  3%]  (Warmup)\nChain 3: Iteration:   68 / 2000 [  3%]  (Warmup)\nChain 3: Iteration:   69 / 2000 [  3%]  (Warmup)\nChain 3: Iteration:   70 / 2000 [  3%]  (Warmup)\nChain 3: Iteration:   71 / 2000 [  3%]  (Warmup)\nChain 3: Iteration:   72 / 2000 [  3%]  (Warmup)\nChain 3: Iteration:   73 / 2000 [  3%]  (Warmup)\nChain 3: Iteration:   74 / 2000 [  3%]  (Warmup)\nChain 3: Iteration:   75 / 2000 [  3%]  (Warmup)\nChain 3: Iteration:   76 / 2000 [  3%]  (Warmup)\nChain 3: Iteration:   77 / 2000 [  3%]  (Warmup)\nChain 3: Iteration:   78 / 2000 [  3%]  (Warmup)\nChain 3: Iteration:   79 / 2000 [  3%]  (Warmup)\nChain 3: Iteration:   80 / 2000 [  4%]  (Warmup)\nChain 3: Iteration:   81 / 2000 [  4%]  (Warmup)\nChain 3: Iteration:   82 / 2000 [  4%]  (Warmup)\nChain 3: Iteration:   83 / 2000 [  4%]  (Warmup)\nChain 3: Iteration:   84 / 2000 [  4%]  (Warmup)\nChain 3: Iteration:   85 / 2000 [  4%]  (Warmup)\nChain 3: Iteration:   86 / 2000 [  4%]  (Warmup)\nChain 3: Iteration:   87 / 2000 [  4%]  (Warmup)\nChain 3: Iteration:   88 / 2000 [  4%]  (Warmup)\nChain 3: Iteration:   89 / 2000 [  4%]  (Warmup)\nChain 3: Iteration:   90 / 2000 [  4%]  (Warmup)\nChain 3: Iteration:   91 / 2000 [  4%]  (Warmup)\nChain 3: Iteration:   92 / 2000 [  4%]  (Warmup)\nChain 3: Iteration:   93 / 2000 [  4%]  (Warmup)\nChain 3: Iteration:   94 / 2000 [  4%]  (Warmup)\nChain 3: Iteration:   95 / 2000 [  4%]  (Warmup)\nChain 3: Iteration:   96 / 2000 [  4%]  (Warmup)\nChain 3: Iteration:   97 / 2000 [  4%]  (Warmup)\nChain 3: Iteration:   98 / 2000 [  4%]  (Warmup)\nChain 3: Iteration:   99 / 2000 [  4%]  (Warmup)\nChain 3: Iteration:  100 / 2000 [  5%]  (Warmup)\nChain 3: Iteration:  101 / 2000 [  5%]  (Warmup)\nChain 3: Iteration:  102 / 2000 [  5%]  (Warmup)\nChain 3: Iteration:  103 / 2000 [  5%]  (Warmup)\nChain 3: Iteration:  104 / 2000 [  5%]  (Warmup)\nChain 3: Iteration:  105 / 2000 [  5%]  (Warmup)\nChain 3: Iteration:  106 / 2000 [  5%]  (Warmup)\nChain 3: Iteration:  107 / 2000 [  5%]  (Warmup)\nChain 3: Iteration:  108 / 2000 [  5%]  (Warmup)\nChain 3: Iteration:  109 / 2000 [  5%]  (Warmup)\nChain 3: Iteration:  110 / 2000 [  5%]  (Warmup)\nChain 3: Iteration:  111 / 2000 [  5%]  (Warmup)\nChain 3: Iteration:  112 / 2000 [  5%]  (Warmup)\nChain 3: Iteration:  113 / 2000 [  5%]  (Warmup)\nChain 3: Iteration:  114 / 2000 [  5%]  (Warmup)\nChain 3: Iteration:  115 / 2000 [  5%]  (Warmup)\nChain 3: Iteration:  116 / 2000 [  5%]  (Warmup)\nChain 3: Iteration:  117 / 2000 [  5%]  (Warmup)\nChain 3: Iteration:  118 / 2000 [  5%]  (Warmup)\nChain 3: Iteration:  119 / 2000 [  5%]  (Warmup)\nChain 3: Iteration:  120 / 2000 [  6%]  (Warmup)\nChain 3: Iteration:  121 / 2000 [  6%]  (Warmup)\nChain 3: Iteration:  122 / 2000 [  6%]  (Warmup)\nChain 3: Iteration:  123 / 2000 [  6%]  (Warmup)\nChain 3: Iteration:  124 / 2000 [  6%]  (Warmup)\nChain 3: Iteration:  125 / 2000 [  6%]  (Warmup)\nChain 3: Iteration:  126 / 2000 [  6%]  (Warmup)\nChain 3: Iteration:  127 / 2000 [  6%]  (Warmup)\nChain 3: Iteration:  128 / 2000 [  6%]  (Warmup)\nChain 3: Iteration:  129 / 2000 [  6%]  (Warmup)\nChain 3: Iteration:  130 / 2000 [  6%]  (Warmup)\nChain 3: Iteration:  131 / 2000 [  6%]  (Warmup)\nChain 3: Iteration:  132 / 2000 [  6%]  (Warmup)\nChain 3: Iteration:  133 / 2000 [  6%]  (Warmup)\nChain 3: Iteration:  134 / 2000 [  6%]  (Warmup)\nChain 3: Iteration:  135 / 2000 [  6%]  (Warmup)\nChain 3: Iteration:  136 / 2000 [  6%]  (Warmup)\nChain 3: Iteration:  137 / 2000 [  6%]  (Warmup)\nChain 3: Iteration:  138 / 2000 [  6%]  (Warmup)\nChain 3: Iteration:  139 / 2000 [  6%]  (Warmup)\nChain 3: Iteration:  140 / 2000 [  7%]  (Warmup)\nChain 3: Iteration:  141 / 2000 [  7%]  (Warmup)\nChain 3: Iteration:  142 / 2000 [  7%]  (Warmup)\nChain 3: Iteration:  143 / 2000 [  7%]  (Warmup)\nChain 3: Iteration:  144 / 2000 [  7%]  (Warmup)\nChain 3: Iteration:  145 / 2000 [  7%]  (Warmup)\nChain 3: Iteration:  146 / 2000 [  7%]  (Warmup)\nChain 3: Iteration:  147 / 2000 [  7%]  (Warmup)\nChain 3: Iteration:  148 / 2000 [  7%]  (Warmup)\nChain 3: Iteration:  149 / 2000 [  7%]  (Warmup)\nChain 3: Iteration:  150 / 2000 [  7%]  (Warmup)\nChain 3: Iteration:  151 / 2000 [  7%]  (Warmup)\nChain 3: Iteration:  152 / 2000 [  7%]  (Warmup)\nChain 3: Iteration:  153 / 2000 [  7%]  (Warmup)\nChain 3: Iteration:  154 / 2000 [  7%]  (Warmup)\nChain 3: Iteration:  155 / 2000 [  7%]  (Warmup)\nChain 3: Iteration:  156 / 2000 [  7%]  (Warmup)\nChain 3: Iteration:  157 / 2000 [  7%]  (Warmup)\nChain 3: Iteration:  158 / 2000 [  7%]  (Warmup)\nChain 3: Iteration:  159 / 2000 [  7%]  (Warmup)\nChain 3: Iteration:  160 / 2000 [  8%]  (Warmup)\nChain 3: Iteration:  161 / 2000 [  8%]  (Warmup)\nChain 3: Iteration:  162 / 2000 [  8%]  (Warmup)\nChain 3: Iteration:  163 / 2000 [  8%]  (Warmup)\nChain 3: Iteration:  164 / 2000 [  8%]  (Warmup)\nChain 3: Iteration:  165 / 2000 [  8%]  (Warmup)\nChain 3: Iteration:  166 / 2000 [  8%]  (Warmup)\nChain 3: Iteration:  167 / 2000 [  8%]  (Warmup)\nChain 3: Iteration:  168 / 2000 [  8%]  (Warmup)\nChain 3: Iteration:  169 / 2000 [  8%]  (Warmup)\nChain 3: Iteration:  170 / 2000 [  8%]  (Warmup)\nChain 3: Iteration:  171 / 2000 [  8%]  (Warmup)\nChain 3: Iteration:  172 / 2000 [  8%]  (Warmup)\nChain 3: Iteration:  173 / 2000 [  8%]  (Warmup)\nChain 3: Iteration:  174 / 2000 [  8%]  (Warmup)\nChain 3: Iteration:  175 / 2000 [  8%]  (Warmup)\nChain 3: Iteration:  176 / 2000 [  8%]  (Warmup)\nChain 3: Iteration:  177 / 2000 [  8%]  (Warmup)\nChain 3: Iteration:  178 / 2000 [  8%]  (Warmup)\nChain 3: Iteration:  179 / 2000 [  8%]  (Warmup)\nChain 3: Iteration:  180 / 2000 [  9%]  (Warmup)\nChain 3: Iteration:  181 / 2000 [  9%]  (Warmup)\nChain 3: Iteration:  182 / 2000 [  9%]  (Warmup)\nChain 3: Iteration:  183 / 2000 [  9%]  (Warmup)\nChain 3: Iteration:  184 / 2000 [  9%]  (Warmup)\nChain 3: Iteration:  185 / 2000 [  9%]  (Warmup)\nChain 3: Iteration:  186 / 2000 [  9%]  (Warmup)\nChain 3: Iteration:  187 / 2000 [  9%]  (Warmup)\nChain 3: Iteration:  188 / 2000 [  9%]  (Warmup)\nChain 3: Iteration:  189 / 2000 [  9%]  (Warmup)\nChain 3: Iteration:  190 / 2000 [  9%]  (Warmup)\nChain 3: Iteration:  191 / 2000 [  9%]  (Warmup)\nChain 3: Iteration:  192 / 2000 [  9%]  (Warmup)\nChain 3: Iteration:  193 / 2000 [  9%]  (Warmup)\nChain 3: Iteration:  194 / 2000 [  9%]  (Warmup)\nChain 3: Iteration:  195 / 2000 [  9%]  (Warmup)\nChain 3: Iteration:  196 / 2000 [  9%]  (Warmup)\nChain 3: Iteration:  197 / 2000 [  9%]  (Warmup)\nChain 3: Iteration:  198 / 2000 [  9%]  (Warmup)\nChain 3: Iteration:  199 / 2000 [  9%]  (Warmup)\nChain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 3: Iteration:  201 / 2000 [ 10%]  (Warmup)\nChain 3: Iteration:  202 / 2000 [ 10%]  (Warmup)\nChain 3: Iteration:  203 / 2000 [ 10%]  (Warmup)\nChain 3: Iteration:  204 / 2000 [ 10%]  (Warmup)\nChain 3: Iteration:  205 / 2000 [ 10%]  (Warmup)\nChain 3: Iteration:  206 / 2000 [ 10%]  (Warmup)\nChain 3: Iteration:  207 / 2000 [ 10%]  (Warmup)\nChain 3: Iteration:  208 / 2000 [ 10%]  (Warmup)\nChain 3: Iteration:  209 / 2000 [ 10%]  (Warmup)\nChain 3: Iteration:  210 / 2000 [ 10%]  (Warmup)\nChain 3: Iteration:  211 / 2000 [ 10%]  (Warmup)\nChain 3: Iteration:  212 / 2000 [ 10%]  (Warmup)\nChain 3: Iteration:  213 / 2000 [ 10%]  (Warmup)\nChain 3: Iteration:  214 / 2000 [ 10%]  (Warmup)\nChain 3: Iteration:  215 / 2000 [ 10%]  (Warmup)\nChain 3: Iteration:  216 / 2000 [ 10%]  (Warmup)\nChain 3: Iteration:  217 / 2000 [ 10%]  (Warmup)\nChain 3: Iteration:  218 / 2000 [ 10%]  (Warmup)\nChain 3: Iteration:  219 / 2000 [ 10%]  (Warmup)\nChain 3: Iteration:  220 / 2000 [ 11%]  (Warmup)\nChain 3: Iteration:  221 / 2000 [ 11%]  (Warmup)\nChain 3: Iteration:  222 / 2000 [ 11%]  (Warmup)\nChain 3: Iteration:  223 / 2000 [ 11%]  (Warmup)\nChain 3: Iteration:  224 / 2000 [ 11%]  (Warmup)\nChain 3: Iteration:  225 / 2000 [ 11%]  (Warmup)\nChain 3: Iteration:  226 / 2000 [ 11%]  (Warmup)\nChain 3: Iteration:  227 / 2000 [ 11%]  (Warmup)\nChain 3: Iteration:  228 / 2000 [ 11%]  (Warmup)\nChain 3: Iteration:  229 / 2000 [ 11%]  (Warmup)\nChain 3: Iteration:  230 / 2000 [ 11%]  (Warmup)\nChain 3: Iteration:  231 / 2000 [ 11%]  (Warmup)\nChain 3: Iteration:  232 / 2000 [ 11%]  (Warmup)\nChain 3: Iteration:  233 / 2000 [ 11%]  (Warmup)\nChain 3: Iteration:  234 / 2000 [ 11%]  (Warmup)\nChain 3: Iteration:  235 / 2000 [ 11%]  (Warmup)\nChain 3: Iteration:  236 / 2000 [ 11%]  (Warmup)\nChain 3: Iteration:  237 / 2000 [ 11%]  (Warmup)\nChain 3: Iteration:  238 / 2000 [ 11%]  (Warmup)\nChain 3: Iteration:  239 / 2000 [ 11%]  (Warmup)\nChain 3: Iteration:  240 / 2000 [ 12%]  (Warmup)\nChain 3: Iteration:  241 / 2000 [ 12%]  (Warmup)\nChain 3: Iteration:  242 / 2000 [ 12%]  (Warmup)\nChain 3: Iteration:  243 / 2000 [ 12%]  (Warmup)\nChain 3: Iteration:  244 / 2000 [ 12%]  (Warmup)\nChain 3: Iteration:  245 / 2000 [ 12%]  (Warmup)\nChain 3: Iteration:  246 / 2000 [ 12%]  (Warmup)\nChain 3: Iteration:  247 / 2000 [ 12%]  (Warmup)\nChain 3: Iteration:  248 / 2000 [ 12%]  (Warmup)\nChain 3: Iteration:  249 / 2000 [ 12%]  (Warmup)\nChain 3: Iteration:  250 / 2000 [ 12%]  (Warmup)\nChain 3: Iteration:  251 / 2000 [ 12%]  (Warmup)\nChain 3: Iteration:  252 / 2000 [ 12%]  (Warmup)\nChain 3: Iteration:  253 / 2000 [ 12%]  (Warmup)\nChain 3: Iteration:  254 / 2000 [ 12%]  (Warmup)\nChain 3: Iteration:  255 / 2000 [ 12%]  (Warmup)\nChain 3: Iteration:  256 / 2000 [ 12%]  (Warmup)\nChain 3: Iteration:  257 / 2000 [ 12%]  (Warmup)\nChain 3: Iteration:  258 / 2000 [ 12%]  (Warmup)\nChain 3: Iteration:  259 / 2000 [ 12%]  (Warmup)\nChain 3: Iteration:  260 / 2000 [ 13%]  (Warmup)\nChain 3: Iteration:  261 / 2000 [ 13%]  (Warmup)\nChain 3: Iteration:  262 / 2000 [ 13%]  (Warmup)\nChain 3: Iteration:  263 / 2000 [ 13%]  (Warmup)\nChain 3: Iteration:  264 / 2000 [ 13%]  (Warmup)\nChain 3: Iteration:  265 / 2000 [ 13%]  (Warmup)\nChain 3: Iteration:  266 / 2000 [ 13%]  (Warmup)\nChain 3: Iteration:  267 / 2000 [ 13%]  (Warmup)\nChain 3: Iteration:  268 / 2000 [ 13%]  (Warmup)\nChain 3: Iteration:  269 / 2000 [ 13%]  (Warmup)\nChain 3: Iteration:  270 / 2000 [ 13%]  (Warmup)\nChain 3: Iteration:  271 / 2000 [ 13%]  (Warmup)\nChain 3: Iteration:  272 / 2000 [ 13%]  (Warmup)\nChain 3: Iteration:  273 / 2000 [ 13%]  (Warmup)\nChain 3: Iteration:  274 / 2000 [ 13%]  (Warmup)\nChain 3: Iteration:  275 / 2000 [ 13%]  (Warmup)\nChain 3: Iteration:  276 / 2000 [ 13%]  (Warmup)\nChain 3: Iteration:  277 / 2000 [ 13%]  (Warmup)\nChain 3: Iteration:  278 / 2000 [ 13%]  (Warmup)\nChain 3: Iteration:  279 / 2000 [ 13%]  (Warmup)\nChain 3: Iteration:  280 / 2000 [ 14%]  (Warmup)\nChain 3: Iteration:  281 / 2000 [ 14%]  (Warmup)\nChain 3: Iteration:  282 / 2000 [ 14%]  (Warmup)\nChain 3: Iteration:  283 / 2000 [ 14%]  (Warmup)\nChain 3: Iteration:  284 / 2000 [ 14%]  (Warmup)\nChain 3: Iteration:  285 / 2000 [ 14%]  (Warmup)\nChain 3: Iteration:  286 / 2000 [ 14%]  (Warmup)\nChain 3: Iteration:  287 / 2000 [ 14%]  (Warmup)\nChain 3: Iteration:  288 / 2000 [ 14%]  (Warmup)\nChain 3: Iteration:  289 / 2000 [ 14%]  (Warmup)\nChain 3: Iteration:  290 / 2000 [ 14%]  (Warmup)\nChain 3: Iteration:  291 / 2000 [ 14%]  (Warmup)\nChain 3: Iteration:  292 / 2000 [ 14%]  (Warmup)\nChain 3: Iteration:  293 / 2000 [ 14%]  (Warmup)\nChain 3: Iteration:  294 / 2000 [ 14%]  (Warmup)\nChain 3: Iteration:  295 / 2000 [ 14%]  (Warmup)\nChain 3: Iteration:  296 / 2000 [ 14%]  (Warmup)\nChain 3: Iteration:  297 / 2000 [ 14%]  (Warmup)\nChain 3: Iteration:  298 / 2000 [ 14%]  (Warmup)\nChain 3: Iteration:  299 / 2000 [ 14%]  (Warmup)\nChain 3: Iteration:  300 / 2000 [ 15%]  (Warmup)\nChain 3: Iteration:  301 / 2000 [ 15%]  (Warmup)\nChain 3: Iteration:  302 / 2000 [ 15%]  (Warmup)\nChain 3: Iteration:  303 / 2000 [ 15%]  (Warmup)\nChain 3: Iteration:  304 / 2000 [ 15%]  (Warmup)\nChain 3: Iteration:  305 / 2000 [ 15%]  (Warmup)\nChain 3: Iteration:  306 / 2000 [ 15%]  (Warmup)\nChain 3: Iteration:  307 / 2000 [ 15%]  (Warmup)\nChain 3: Iteration:  308 / 2000 [ 15%]  (Warmup)\nChain 3: Iteration:  309 / 2000 [ 15%]  (Warmup)\nChain 3: Iteration:  310 / 2000 [ 15%]  (Warmup)\nChain 3: Iteration:  311 / 2000 [ 15%]  (Warmup)\nChain 3: Iteration:  312 / 2000 [ 15%]  (Warmup)\nChain 3: Iteration:  313 / 2000 [ 15%]  (Warmup)\nChain 3: Iteration:  314 / 2000 [ 15%]  (Warmup)\nChain 3: Iteration:  315 / 2000 [ 15%]  (Warmup)\nChain 3: Iteration:  316 / 2000 [ 15%]  (Warmup)\nChain 3: Iteration:  317 / 2000 [ 15%]  (Warmup)\nChain 3: Iteration:  318 / 2000 [ 15%]  (Warmup)\nChain 3: Iteration:  319 / 2000 [ 15%]  (Warmup)\nChain 3: Iteration:  320 / 2000 [ 16%]  (Warmup)\nChain 3: Iteration:  321 / 2000 [ 16%]  (Warmup)\nChain 3: Iteration:  322 / 2000 [ 16%]  (Warmup)\nChain 3: Iteration:  323 / 2000 [ 16%]  (Warmup)\nChain 3: Iteration:  324 / 2000 [ 16%]  (Warmup)\nChain 3: Iteration:  325 / 2000 [ 16%]  (Warmup)\nChain 3: Iteration:  326 / 2000 [ 16%]  (Warmup)\nChain 3: Iteration:  327 / 2000 [ 16%]  (Warmup)\nChain 3: Iteration:  328 / 2000 [ 16%]  (Warmup)\nChain 3: Iteration:  329 / 2000 [ 16%]  (Warmup)\nChain 3: Iteration:  330 / 2000 [ 16%]  (Warmup)\nChain 3: Iteration:  331 / 2000 [ 16%]  (Warmup)\nChain 3: Iteration:  332 / 2000 [ 16%]  (Warmup)\nChain 3: Iteration:  333 / 2000 [ 16%]  (Warmup)\nChain 3: Iteration:  334 / 2000 [ 16%]  (Warmup)\nChain 3: Iteration:  335 / 2000 [ 16%]  (Warmup)\nChain 3: Iteration:  336 / 2000 [ 16%]  (Warmup)\nChain 3: Iteration:  337 / 2000 [ 16%]  (Warmup)\nChain 3: Iteration:  338 / 2000 [ 16%]  (Warmup)\nChain 3: Iteration:  339 / 2000 [ 16%]  (Warmup)\nChain 3: Iteration:  340 / 2000 [ 17%]  (Warmup)\nChain 3: Iteration:  341 / 2000 [ 17%]  (Warmup)\nChain 3: Iteration:  342 / 2000 [ 17%]  (Warmup)\nChain 3: Iteration:  343 / 2000 [ 17%]  (Warmup)\nChain 3: Iteration:  344 / 2000 [ 17%]  (Warmup)\nChain 3: Iteration:  345 / 2000 [ 17%]  (Warmup)\nChain 3: Iteration:  346 / 2000 [ 17%]  (Warmup)\nChain 3: Iteration:  347 / 2000 [ 17%]  (Warmup)\nChain 3: Iteration:  348 / 2000 [ 17%]  (Warmup)\nChain 3: Iteration:  349 / 2000 [ 17%]  (Warmup)\nChain 3: Iteration:  350 / 2000 [ 17%]  (Warmup)\nChain 3: Iteration:  351 / 2000 [ 17%]  (Warmup)\nChain 3: Iteration:  352 / 2000 [ 17%]  (Warmup)\nChain 3: Iteration:  353 / 2000 [ 17%]  (Warmup)\nChain 3: Iteration:  354 / 2000 [ 17%]  (Warmup)\nChain 3: Iteration:  355 / 2000 [ 17%]  (Warmup)\nChain 3: Iteration:  356 / 2000 [ 17%]  (Warmup)\nChain 3: Iteration:  357 / 2000 [ 17%]  (Warmup)\nChain 3: Iteration:  358 / 2000 [ 17%]  (Warmup)\nChain 3: Iteration:  359 / 2000 [ 17%]  (Warmup)\nChain 3: Iteration:  360 / 2000 [ 18%]  (Warmup)\nChain 3: Iteration:  361 / 2000 [ 18%]  (Warmup)\nChain 3: Iteration:  362 / 2000 [ 18%]  (Warmup)\nChain 3: Iteration:  363 / 2000 [ 18%]  (Warmup)\nChain 3: Iteration:  364 / 2000 [ 18%]  (Warmup)\nChain 3: Iteration:  365 / 2000 [ 18%]  (Warmup)\nChain 3: Iteration:  366 / 2000 [ 18%]  (Warmup)\nChain 3: Iteration:  367 / 2000 [ 18%]  (Warmup)\nChain 3: Iteration:  368 / 2000 [ 18%]  (Warmup)\nChain 3: Iteration:  369 / 2000 [ 18%]  (Warmup)\nChain 3: Iteration:  370 / 2000 [ 18%]  (Warmup)\nChain 3: Iteration:  371 / 2000 [ 18%]  (Warmup)\nChain 3: Iteration:  372 / 2000 [ 18%]  (Warmup)\nChain 3: Iteration:  373 / 2000 [ 18%]  (Warmup)\nChain 3: Iteration:  374 / 2000 [ 18%]  (Warmup)\nChain 3: Iteration:  375 / 2000 [ 18%]  (Warmup)\nChain 3: Iteration:  376 / 2000 [ 18%]  (Warmup)\nChain 3: Iteration:  377 / 2000 [ 18%]  (Warmup)\nChain 3: Iteration:  378 / 2000 [ 18%]  (Warmup)\nChain 3: Iteration:  379 / 2000 [ 18%]  (Warmup)\nChain 3: Iteration:  380 / 2000 [ 19%]  (Warmup)\nChain 3: Iteration:  381 / 2000 [ 19%]  (Warmup)\nChain 3: Iteration:  382 / 2000 [ 19%]  (Warmup)\nChain 3: Iteration:  383 / 2000 [ 19%]  (Warmup)\nChain 3: Iteration:  384 / 2000 [ 19%]  (Warmup)\nChain 3: Iteration:  385 / 2000 [ 19%]  (Warmup)\nChain 3: Iteration:  386 / 2000 [ 19%]  (Warmup)\nChain 3: Iteration:  387 / 2000 [ 19%]  (Warmup)\nChain 3: Iteration:  388 / 2000 [ 19%]  (Warmup)\nChain 3: Iteration:  389 / 2000 [ 19%]  (Warmup)\nChain 3: Iteration:  390 / 2000 [ 19%]  (Warmup)\nChain 3: Iteration:  391 / 2000 [ 19%]  (Warmup)\nChain 3: Iteration:  392 / 2000 [ 19%]  (Warmup)\nChain 3: Iteration:  393 / 2000 [ 19%]  (Warmup)\nChain 3: Iteration:  394 / 2000 [ 19%]  (Warmup)\nChain 3: Iteration:  395 / 2000 [ 19%]  (Warmup)\nChain 3: Iteration:  396 / 2000 [ 19%]  (Warmup)\nChain 3: Iteration:  397 / 2000 [ 19%]  (Warmup)\nChain 3: Iteration:  398 / 2000 [ 19%]  (Warmup)\nChain 3: Iteration:  399 / 2000 [ 19%]  (Warmup)\nChain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 3: Iteration:  401 / 2000 [ 20%]  (Warmup)\nChain 3: Iteration:  402 / 2000 [ 20%]  (Warmup)\nChain 3: Iteration:  403 / 2000 [ 20%]  (Warmup)\nChain 3: Iteration:  404 / 2000 [ 20%]  (Warmup)\nChain 3: Iteration:  405 / 2000 [ 20%]  (Warmup)\nChain 3: Iteration:  406 / 2000 [ 20%]  (Warmup)\nChain 3: Iteration:  407 / 2000 [ 20%]  (Warmup)\nChain 3: Iteration:  408 / 2000 [ 20%]  (Warmup)\nChain 3: Iteration:  409 / 2000 [ 20%]  (Warmup)\nChain 3: Iteration:  410 / 2000 [ 20%]  (Warmup)\nChain 3: Iteration:  411 / 2000 [ 20%]  (Warmup)\nChain 3: Iteration:  412 / 2000 [ 20%]  (Warmup)\nChain 3: Iteration:  413 / 2000 [ 20%]  (Warmup)\nChain 3: Iteration:  414 / 2000 [ 20%]  (Warmup)\nChain 3: Iteration:  415 / 2000 [ 20%]  (Warmup)\nChain 3: Iteration:  416 / 2000 [ 20%]  (Warmup)\nChain 3: Iteration:  417 / 2000 [ 20%]  (Warmup)\nChain 3: Iteration:  418 / 2000 [ 20%]  (Warmup)\nChain 3: Iteration:  419 / 2000 [ 20%]  (Warmup)\nChain 3: Iteration:  420 / 2000 [ 21%]  (Warmup)\nChain 3: Iteration:  421 / 2000 [ 21%]  (Warmup)\nChain 3: Iteration:  422 / 2000 [ 21%]  (Warmup)\nChain 3: Iteration:  423 / 2000 [ 21%]  (Warmup)\nChain 3: Iteration:  424 / 2000 [ 21%]  (Warmup)\nChain 3: Iteration:  425 / 2000 [ 21%]  (Warmup)\nChain 3: Iteration:  426 / 2000 [ 21%]  (Warmup)\nChain 3: Iteration:  427 / 2000 [ 21%]  (Warmup)\nChain 3: Iteration:  428 / 2000 [ 21%]  (Warmup)\nChain 3: Iteration:  429 / 2000 [ 21%]  (Warmup)\nChain 3: Iteration:  430 / 2000 [ 21%]  (Warmup)\nChain 3: Iteration:  431 / 2000 [ 21%]  (Warmup)\nChain 3: Iteration:  432 / 2000 [ 21%]  (Warmup)\nChain 3: Iteration:  433 / 2000 [ 21%]  (Warmup)\nChain 3: Iteration:  434 / 2000 [ 21%]  (Warmup)\nChain 3: Iteration:  435 / 2000 [ 21%]  (Warmup)\nChain 3: Iteration:  436 / 2000 [ 21%]  (Warmup)\nChain 3: Iteration:  437 / 2000 [ 21%]  (Warmup)\nChain 3: Iteration:  438 / 2000 [ 21%]  (Warmup)\nChain 3: Iteration:  439 / 2000 [ 21%]  (Warmup)\nChain 3: Iteration:  440 / 2000 [ 22%]  (Warmup)\nChain 3: Iteration:  441 / 2000 [ 22%]  (Warmup)\nChain 3: Iteration:  442 / 2000 [ 22%]  (Warmup)\nChain 3: Iteration:  443 / 2000 [ 22%]  (Warmup)\nChain 3: Iteration:  444 / 2000 [ 22%]  (Warmup)\nChain 3: Iteration:  445 / 2000 [ 22%]  (Warmup)\nChain 3: Iteration:  446 / 2000 [ 22%]  (Warmup)\nChain 3: Iteration:  447 / 2000 [ 22%]  (Warmup)\nChain 3: Iteration:  448 / 2000 [ 22%]  (Warmup)\nChain 3: Iteration:  449 / 2000 [ 22%]  (Warmup)\nChain 3: Iteration:  450 / 2000 [ 22%]  (Warmup)\nChain 3: Iteration:  451 / 2000 [ 22%]  (Warmup)\nChain 3: Iteration:  452 / 2000 [ 22%]  (Warmup)\nChain 3: Iteration:  453 / 2000 [ 22%]  (Warmup)\nChain 3: Iteration:  454 / 2000 [ 22%]  (Warmup)\nChain 3: Iteration:  455 / 2000 [ 22%]  (Warmup)\nChain 3: Iteration:  456 / 2000 [ 22%]  (Warmup)\nChain 3: Iteration:  457 / 2000 [ 22%]  (Warmup)\nChain 3: Iteration:  458 / 2000 [ 22%]  (Warmup)\nChain 3: Iteration:  459 / 2000 [ 22%]  (Warmup)\nChain 3: Iteration:  460 / 2000 [ 23%]  (Warmup)\nChain 3: Iteration:  461 / 2000 [ 23%]  (Warmup)\nChain 3: Iteration:  462 / 2000 [ 23%]  (Warmup)\nChain 3: Iteration:  463 / 2000 [ 23%]  (Warmup)\nChain 3: Iteration:  464 / 2000 [ 23%]  (Warmup)\nChain 3: Iteration:  465 / 2000 [ 23%]  (Warmup)\nChain 3: Iteration:  466 / 2000 [ 23%]  (Warmup)\nChain 3: Iteration:  467 / 2000 [ 23%]  (Warmup)\nChain 3: Iteration:  468 / 2000 [ 23%]  (Warmup)\nChain 3: Iteration:  469 / 2000 [ 23%]  (Warmup)\nChain 3: Iteration:  470 / 2000 [ 23%]  (Warmup)\nChain 3: Iteration:  471 / 2000 [ 23%]  (Warmup)\nChain 3: Iteration:  472 / 2000 [ 23%]  (Warmup)\nChain 3: Iteration:  473 / 2000 [ 23%]  (Warmup)\nChain 3: Iteration:  474 / 2000 [ 23%]  (Warmup)\nChain 3: Iteration:  475 / 2000 [ 23%]  (Warmup)\nChain 3: Iteration:  476 / 2000 [ 23%]  (Warmup)\nChain 3: Iteration:  477 / 2000 [ 23%]  (Warmup)\nChain 3: Iteration:  478 / 2000 [ 23%]  (Warmup)\nChain 3: Iteration:  479 / 2000 [ 23%]  (Warmup)\nChain 3: Iteration:  480 / 2000 [ 24%]  (Warmup)\nChain 3: Iteration:  481 / 2000 [ 24%]  (Warmup)\nChain 3: Iteration:  482 / 2000 [ 24%]  (Warmup)\nChain 3: Iteration:  483 / 2000 [ 24%]  (Warmup)\nChain 3: Iteration:  484 / 2000 [ 24%]  (Warmup)\nChain 3: Iteration:  485 / 2000 [ 24%]  (Warmup)\nChain 3: Iteration:  486 / 2000 [ 24%]  (Warmup)\nChain 3: Iteration:  487 / 2000 [ 24%]  (Warmup)\nChain 3: Iteration:  488 / 2000 [ 24%]  (Warmup)\nChain 3: Iteration:  489 / 2000 [ 24%]  (Warmup)\nChain 3: Iteration:  490 / 2000 [ 24%]  (Warmup)\nChain 3: Iteration:  491 / 2000 [ 24%]  (Warmup)\nChain 3: Iteration:  492 / 2000 [ 24%]  (Warmup)\nChain 3: Iteration:  493 / 2000 [ 24%]  (Warmup)\nChain 3: Iteration:  494 / 2000 [ 24%]  (Warmup)\nChain 3: Iteration:  495 / 2000 [ 24%]  (Warmup)\nChain 3: Iteration:  496 / 2000 [ 24%]  (Warmup)\nChain 3: Iteration:  497 / 2000 [ 24%]  (Warmup)\nChain 3: Iteration:  498 / 2000 [ 24%]  (Warmup)\nChain 3: Iteration:  499 / 2000 [ 24%]  (Warmup)\nChain 3: Iteration:  500 / 2000 [ 25%]  (Warmup)\nChain 3: Iteration:  501 / 2000 [ 25%]  (Warmup)\nChain 3: Iteration:  502 / 2000 [ 25%]  (Warmup)\nChain 3: Iteration:  503 / 2000 [ 25%]  (Warmup)\nChain 3: Iteration:  504 / 2000 [ 25%]  (Warmup)\nChain 3: Iteration:  505 / 2000 [ 25%]  (Warmup)\nChain 3: Iteration:  506 / 2000 [ 25%]  (Warmup)\nChain 3: Iteration:  507 / 2000 [ 25%]  (Warmup)\nChain 3: Iteration:  508 / 2000 [ 25%]  (Warmup)\nChain 3: Iteration:  509 / 2000 [ 25%]  (Warmup)\nChain 3: Iteration:  510 / 2000 [ 25%]  (Warmup)\nChain 3: Iteration:  511 / 2000 [ 25%]  (Warmup)\nChain 3: Iteration:  512 / 2000 [ 25%]  (Warmup)\nChain 3: Iteration:  513 / 2000 [ 25%]  (Warmup)\nChain 3: Iteration:  514 / 2000 [ 25%]  (Warmup)\nChain 3: Iteration:  515 / 2000 [ 25%]  (Warmup)\nChain 3: Iteration:  516 / 2000 [ 25%]  (Warmup)\nChain 3: Iteration:  517 / 2000 [ 25%]  (Warmup)\nChain 3: Iteration:  518 / 2000 [ 25%]  (Warmup)\nChain 3: Iteration:  519 / 2000 [ 25%]  (Warmup)\nChain 3: Iteration:  520 / 2000 [ 26%]  (Warmup)\nChain 3: Iteration:  521 / 2000 [ 26%]  (Warmup)\nChain 3: Iteration:  522 / 2000 [ 26%]  (Warmup)\nChain 3: Iteration:  523 / 2000 [ 26%]  (Warmup)\nChain 3: Iteration:  524 / 2000 [ 26%]  (Warmup)\nChain 3: Iteration:  525 / 2000 [ 26%]  (Warmup)\nChain 3: Iteration:  526 / 2000 [ 26%]  (Warmup)\nChain 3: Iteration:  527 / 2000 [ 26%]  (Warmup)\nChain 3: Iteration:  528 / 2000 [ 26%]  (Warmup)\nChain 3: Iteration:  529 / 2000 [ 26%]  (Warmup)\nChain 3: Iteration:  530 / 2000 [ 26%]  (Warmup)\nChain 3: Iteration:  531 / 2000 [ 26%]  (Warmup)\nChain 3: Iteration:  532 / 2000 [ 26%]  (Warmup)\nChain 3: Iteration:  533 / 2000 [ 26%]  (Warmup)\nChain 3: Iteration:  534 / 2000 [ 26%]  (Warmup)\nChain 3: Iteration:  535 / 2000 [ 26%]  (Warmup)\nChain 3: Iteration:  536 / 2000 [ 26%]  (Warmup)\nChain 3: Iteration:  537 / 2000 [ 26%]  (Warmup)\nChain 3: Iteration:  538 / 2000 [ 26%]  (Warmup)\nChain 3: Iteration:  539 / 2000 [ 26%]  (Warmup)\nChain 3: Iteration:  540 / 2000 [ 27%]  (Warmup)\nChain 3: Iteration:  541 / 2000 [ 27%]  (Warmup)\nChain 3: Iteration:  542 / 2000 [ 27%]  (Warmup)\nChain 3: Iteration:  543 / 2000 [ 27%]  (Warmup)\nChain 3: Iteration:  544 / 2000 [ 27%]  (Warmup)\nChain 3: Iteration:  545 / 2000 [ 27%]  (Warmup)\nChain 3: Iteration:  546 / 2000 [ 27%]  (Warmup)\nChain 3: Iteration:  547 / 2000 [ 27%]  (Warmup)\nChain 3: Iteration:  548 / 2000 [ 27%]  (Warmup)\nChain 3: Iteration:  549 / 2000 [ 27%]  (Warmup)\nChain 3: Iteration:  550 / 2000 [ 27%]  (Warmup)\nChain 3: Iteration:  551 / 2000 [ 27%]  (Warmup)\nChain 3: Iteration:  552 / 2000 [ 27%]  (Warmup)\nChain 3: Iteration:  553 / 2000 [ 27%]  (Warmup)\nChain 3: Iteration:  554 / 2000 [ 27%]  (Warmup)\nChain 3: Iteration:  555 / 2000 [ 27%]  (Warmup)\nChain 3: Iteration:  556 / 2000 [ 27%]  (Warmup)\nChain 3: Iteration:  557 / 2000 [ 27%]  (Warmup)\nChain 3: Iteration:  558 / 2000 [ 27%]  (Warmup)\nChain 3: Iteration:  559 / 2000 [ 27%]  (Warmup)\nChain 3: Iteration:  560 / 2000 [ 28%]  (Warmup)\nChain 3: Iteration:  561 / 2000 [ 28%]  (Warmup)\nChain 3: Iteration:  562 / 2000 [ 28%]  (Warmup)\nChain 3: Iteration:  563 / 2000 [ 28%]  (Warmup)\nChain 3: Iteration:  564 / 2000 [ 28%]  (Warmup)\nChain 3: Iteration:  565 / 2000 [ 28%]  (Warmup)\nChain 3: Iteration:  566 / 2000 [ 28%]  (Warmup)\nChain 3: Iteration:  567 / 2000 [ 28%]  (Warmup)\nChain 3: Iteration:  568 / 2000 [ 28%]  (Warmup)\nChain 3: Iteration:  569 / 2000 [ 28%]  (Warmup)\nChain 3: Iteration:  570 / 2000 [ 28%]  (Warmup)\nChain 3: Iteration:  571 / 2000 [ 28%]  (Warmup)\nChain 3: Iteration:  572 / 2000 [ 28%]  (Warmup)\nChain 3: Iteration:  573 / 2000 [ 28%]  (Warmup)\nChain 3: Iteration:  574 / 2000 [ 28%]  (Warmup)\nChain 3: Iteration:  575 / 2000 [ 28%]  (Warmup)\nChain 3: Iteration:  576 / 2000 [ 28%]  (Warmup)\nChain 3: Iteration:  577 / 2000 [ 28%]  (Warmup)\nChain 3: Iteration:  578 / 2000 [ 28%]  (Warmup)\nChain 3: Iteration:  579 / 2000 [ 28%]  (Warmup)\nChain 3: Iteration:  580 / 2000 [ 29%]  (Warmup)\nChain 3: Iteration:  581 / 2000 [ 29%]  (Warmup)\nChain 3: Iteration:  582 / 2000 [ 29%]  (Warmup)\nChain 3: Iteration:  583 / 2000 [ 29%]  (Warmup)\nChain 3: Iteration:  584 / 2000 [ 29%]  (Warmup)\nChain 3: Iteration:  585 / 2000 [ 29%]  (Warmup)\nChain 3: Iteration:  586 / 2000 [ 29%]  (Warmup)\nChain 3: Iteration:  587 / 2000 [ 29%]  (Warmup)\nChain 3: Iteration:  588 / 2000 [ 29%]  (Warmup)\nChain 3: Iteration:  589 / 2000 [ 29%]  (Warmup)\nChain 3: Iteration:  590 / 2000 [ 29%]  (Warmup)\nChain 3: Iteration:  591 / 2000 [ 29%]  (Warmup)\nChain 3: Iteration:  592 / 2000 [ 29%]  (Warmup)\nChain 3: Iteration:  593 / 2000 [ 29%]  (Warmup)\nChain 3: Iteration:  594 / 2000 [ 29%]  (Warmup)\nChain 3: Iteration:  595 / 2000 [ 29%]  (Warmup)\nChain 3: Iteration:  596 / 2000 [ 29%]  (Warmup)\nChain 3: Iteration:  597 / 2000 [ 29%]  (Warmup)\nChain 3: Iteration:  598 / 2000 [ 29%]  (Warmup)\nChain 3: Iteration:  599 / 2000 [ 29%]  (Warmup)\nChain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 3: Iteration:  601 / 2000 [ 30%]  (Warmup)\nChain 3: Iteration:  602 / 2000 [ 30%]  (Warmup)\nChain 3: Iteration:  603 / 2000 [ 30%]  (Warmup)\nChain 3: Iteration:  604 / 2000 [ 30%]  (Warmup)\nChain 3: Iteration:  605 / 2000 [ 30%]  (Warmup)\nChain 3: Iteration:  606 / 2000 [ 30%]  (Warmup)\nChain 3: Iteration:  607 / 2000 [ 30%]  (Warmup)\nChain 3: Iteration:  608 / 2000 [ 30%]  (Warmup)\nChain 3: Iteration:  609 / 2000 [ 30%]  (Warmup)\nChain 3: Iteration:  610 / 2000 [ 30%]  (Warmup)\nChain 3: Iteration:  611 / 2000 [ 30%]  (Warmup)\nChain 3: Iteration:  612 / 2000 [ 30%]  (Warmup)\nChain 3: Iteration:  613 / 2000 [ 30%]  (Warmup)\nChain 3: Iteration:  614 / 2000 [ 30%]  (Warmup)\nChain 3: Iteration:  615 / 2000 [ 30%]  (Warmup)\nChain 3: Iteration:  616 / 2000 [ 30%]  (Warmup)\nChain 3: Iteration:  617 / 2000 [ 30%]  (Warmup)\nChain 3: Iteration:  618 / 2000 [ 30%]  (Warmup)\nChain 3: Iteration:  619 / 2000 [ 30%]  (Warmup)\nChain 3: Iteration:  620 / 2000 [ 31%]  (Warmup)\nChain 3: Iteration:  621 / 2000 [ 31%]  (Warmup)\nChain 3: Iteration:  622 / 2000 [ 31%]  (Warmup)\nChain 3: Iteration:  623 / 2000 [ 31%]  (Warmup)\nChain 3: Iteration:  624 / 2000 [ 31%]  (Warmup)\nChain 3: Iteration:  625 / 2000 [ 31%]  (Warmup)\nChain 3: Iteration:  626 / 2000 [ 31%]  (Warmup)\nChain 3: Iteration:  627 / 2000 [ 31%]  (Warmup)\nChain 3: Iteration:  628 / 2000 [ 31%]  (Warmup)\nChain 3: Iteration:  629 / 2000 [ 31%]  (Warmup)\nChain 3: Iteration:  630 / 2000 [ 31%]  (Warmup)\nChain 3: Iteration:  631 / 2000 [ 31%]  (Warmup)\nChain 3: Iteration:  632 / 2000 [ 31%]  (Warmup)\nChain 3: Iteration:  633 / 2000 [ 31%]  (Warmup)\nChain 3: Iteration:  634 / 2000 [ 31%]  (Warmup)\nChain 3: Iteration:  635 / 2000 [ 31%]  (Warmup)\nChain 3: Iteration:  636 / 2000 [ 31%]  (Warmup)\nChain 3: Iteration:  637 / 2000 [ 31%]  (Warmup)\nChain 3: Iteration:  638 / 2000 [ 31%]  (Warmup)\nChain 3: Iteration:  639 / 2000 [ 31%]  (Warmup)\nChain 3: Iteration:  640 / 2000 [ 32%]  (Warmup)\nChain 3: Iteration:  641 / 2000 [ 32%]  (Warmup)\nChain 3: Iteration:  642 / 2000 [ 32%]  (Warmup)\nChain 3: Iteration:  643 / 2000 [ 32%]  (Warmup)\nChain 3: Iteration:  644 / 2000 [ 32%]  (Warmup)\nChain 3: Iteration:  645 / 2000 [ 32%]  (Warmup)\nChain 3: Iteration:  646 / 2000 [ 32%]  (Warmup)\nChain 3: Iteration:  647 / 2000 [ 32%]  (Warmup)\nChain 3: Iteration:  648 / 2000 [ 32%]  (Warmup)\nChain 3: Iteration:  649 / 2000 [ 32%]  (Warmup)\nChain 3: Iteration:  650 / 2000 [ 32%]  (Warmup)\nChain 3: Iteration:  651 / 2000 [ 32%]  (Warmup)\nChain 3: Iteration:  652 / 2000 [ 32%]  (Warmup)\nChain 3: Iteration:  653 / 2000 [ 32%]  (Warmup)\nChain 3: Iteration:  654 / 2000 [ 32%]  (Warmup)\nChain 3: Iteration:  655 / 2000 [ 32%]  (Warmup)\nChain 3: Iteration:  656 / 2000 [ 32%]  (Warmup)\nChain 3: Iteration:  657 / 2000 [ 32%]  (Warmup)\nChain 3: Iteration:  658 / 2000 [ 32%]  (Warmup)\nChain 3: Iteration:  659 / 2000 [ 32%]  (Warmup)\nChain 3: Iteration:  660 / 2000 [ 33%]  (Warmup)\nChain 3: Iteration:  661 / 2000 [ 33%]  (Warmup)\nChain 3: Iteration:  662 / 2000 [ 33%]  (Warmup)\nChain 3: Iteration:  663 / 2000 [ 33%]  (Warmup)\nChain 3: Iteration:  664 / 2000 [ 33%]  (Warmup)\nChain 3: Iteration:  665 / 2000 [ 33%]  (Warmup)\nChain 3: Iteration:  666 / 2000 [ 33%]  (Warmup)\nChain 3: Iteration:  667 / 2000 [ 33%]  (Warmup)\nChain 3: Iteration:  668 / 2000 [ 33%]  (Warmup)\nChain 3: Iteration:  669 / 2000 [ 33%]  (Warmup)\nChain 3: Iteration:  670 / 2000 [ 33%]  (Warmup)\nChain 3: Iteration:  671 / 2000 [ 33%]  (Warmup)\nChain 3: Iteration:  672 / 2000 [ 33%]  (Warmup)\nChain 3: Iteration:  673 / 2000 [ 33%]  (Warmup)\nChain 3: Iteration:  674 / 2000 [ 33%]  (Warmup)\nChain 3: Iteration:  675 / 2000 [ 33%]  (Warmup)\nChain 3: Iteration:  676 / 2000 [ 33%]  (Warmup)\nChain 3: Iteration:  677 / 2000 [ 33%]  (Warmup)\nChain 3: Iteration:  678 / 2000 [ 33%]  (Warmup)\nChain 3: Iteration:  679 / 2000 [ 33%]  (Warmup)\nChain 3: Iteration:  680 / 2000 [ 34%]  (Warmup)\nChain 3: Iteration:  681 / 2000 [ 34%]  (Warmup)\nChain 3: Iteration:  682 / 2000 [ 34%]  (Warmup)\nChain 3: Iteration:  683 / 2000 [ 34%]  (Warmup)\nChain 3: Iteration:  684 / 2000 [ 34%]  (Warmup)\nChain 3: Iteration:  685 / 2000 [ 34%]  (Warmup)\nChain 3: Iteration:  686 / 2000 [ 34%]  (Warmup)\nChain 3: Iteration:  687 / 2000 [ 34%]  (Warmup)\nChain 3: Iteration:  688 / 2000 [ 34%]  (Warmup)\nChain 3: Iteration:  689 / 2000 [ 34%]  (Warmup)\nChain 3: Iteration:  690 / 2000 [ 34%]  (Warmup)\nChain 3: Iteration:  691 / 2000 [ 34%]  (Warmup)\nChain 3: Iteration:  692 / 2000 [ 34%]  (Warmup)\nChain 3: Iteration:  693 / 2000 [ 34%]  (Warmup)\nChain 3: Iteration:  694 / 2000 [ 34%]  (Warmup)\nChain 3: Iteration:  695 / 2000 [ 34%]  (Warmup)\nChain 3: Iteration:  696 / 2000 [ 34%]  (Warmup)\nChain 3: Iteration:  697 / 2000 [ 34%]  (Warmup)\nChain 3: Iteration:  698 / 2000 [ 34%]  (Warmup)\nChain 3: Iteration:  699 / 2000 [ 34%]  (Warmup)\nChain 3: Iteration:  700 / 2000 [ 35%]  (Warmup)\nChain 3: Iteration:  701 / 2000 [ 35%]  (Warmup)\nChain 3: Iteration:  702 / 2000 [ 35%]  (Warmup)\nChain 3: Iteration:  703 / 2000 [ 35%]  (Warmup)\nChain 3: Iteration:  704 / 2000 [ 35%]  (Warmup)\nChain 3: Iteration:  705 / 2000 [ 35%]  (Warmup)\nChain 3: Iteration:  706 / 2000 [ 35%]  (Warmup)\nChain 3: Iteration:  707 / 2000 [ 35%]  (Warmup)\nChain 3: Iteration:  708 / 2000 [ 35%]  (Warmup)\nChain 3: Iteration:  709 / 2000 [ 35%]  (Warmup)\nChain 3: Iteration:  710 / 2000 [ 35%]  (Warmup)\nChain 3: Iteration:  711 / 2000 [ 35%]  (Warmup)\nChain 3: Iteration:  712 / 2000 [ 35%]  (Warmup)\nChain 3: Iteration:  713 / 2000 [ 35%]  (Warmup)\nChain 3: Iteration:  714 / 2000 [ 35%]  (Warmup)\nChain 3: Iteration:  715 / 2000 [ 35%]  (Warmup)\nChain 3: Iteration:  716 / 2000 [ 35%]  (Warmup)\nChain 3: Iteration:  717 / 2000 [ 35%]  (Warmup)\nChain 3: Iteration:  718 / 2000 [ 35%]  (Warmup)\nChain 3: Iteration:  719 / 2000 [ 35%]  (Warmup)\nChain 3: Iteration:  720 / 2000 [ 36%]  (Warmup)\nChain 3: Iteration:  721 / 2000 [ 36%]  (Warmup)\nChain 3: Iteration:  722 / 2000 [ 36%]  (Warmup)\nChain 3: Iteration:  723 / 2000 [ 36%]  (Warmup)\nChain 3: Iteration:  724 / 2000 [ 36%]  (Warmup)\nChain 3: Iteration:  725 / 2000 [ 36%]  (Warmup)\nChain 3: Iteration:  726 / 2000 [ 36%]  (Warmup)\nChain 3: Iteration:  727 / 2000 [ 36%]  (Warmup)\nChain 3: Iteration:  728 / 2000 [ 36%]  (Warmup)\nChain 3: Iteration:  729 / 2000 [ 36%]  (Warmup)\nChain 3: Iteration:  730 / 2000 [ 36%]  (Warmup)\nChain 3: Iteration:  731 / 2000 [ 36%]  (Warmup)\nChain 3: Iteration:  732 / 2000 [ 36%]  (Warmup)\nChain 3: Iteration:  733 / 2000 [ 36%]  (Warmup)\nChain 3: Iteration:  734 / 2000 [ 36%]  (Warmup)\nChain 3: Iteration:  735 / 2000 [ 36%]  (Warmup)\nChain 3: Iteration:  736 / 2000 [ 36%]  (Warmup)\nChain 3: Iteration:  737 / 2000 [ 36%]  (Warmup)\nChain 3: Iteration:  738 / 2000 [ 36%]  (Warmup)\nChain 3: Iteration:  739 / 2000 [ 36%]  (Warmup)\nChain 3: Iteration:  740 / 2000 [ 37%]  (Warmup)\nChain 3: Iteration:  741 / 2000 [ 37%]  (Warmup)\nChain 3: Iteration:  742 / 2000 [ 37%]  (Warmup)\nChain 3: Iteration:  743 / 2000 [ 37%]  (Warmup)\nChain 3: Iteration:  744 / 2000 [ 37%]  (Warmup)\nChain 3: Iteration:  745 / 2000 [ 37%]  (Warmup)\nChain 3: Iteration:  746 / 2000 [ 37%]  (Warmup)\nChain 3: Iteration:  747 / 2000 [ 37%]  (Warmup)\nChain 3: Iteration:  748 / 2000 [ 37%]  (Warmup)\nChain 3: Iteration:  749 / 2000 [ 37%]  (Warmup)\nChain 3: Iteration:  750 / 2000 [ 37%]  (Warmup)\nChain 3: Iteration:  751 / 2000 [ 37%]  (Warmup)\nChain 3: Iteration:  752 / 2000 [ 37%]  (Warmup)\nChain 3: Iteration:  753 / 2000 [ 37%]  (Warmup)\nChain 3: Iteration:  754 / 2000 [ 37%]  (Warmup)\nChain 3: Iteration:  755 / 2000 [ 37%]  (Warmup)\nChain 3: Iteration:  756 / 2000 [ 37%]  (Warmup)\nChain 3: Iteration:  757 / 2000 [ 37%]  (Warmup)\nChain 3: Iteration:  758 / 2000 [ 37%]  (Warmup)\nChain 3: Iteration:  759 / 2000 [ 37%]  (Warmup)\nChain 3: Iteration:  760 / 2000 [ 38%]  (Warmup)\nChain 3: Iteration:  761 / 2000 [ 38%]  (Warmup)\nChain 3: Iteration:  762 / 2000 [ 38%]  (Warmup)\nChain 3: Iteration:  763 / 2000 [ 38%]  (Warmup)\nChain 3: Iteration:  764 / 2000 [ 38%]  (Warmup)\nChain 3: Iteration:  765 / 2000 [ 38%]  (Warmup)\nChain 3: Iteration:  766 / 2000 [ 38%]  (Warmup)\nChain 3: Iteration:  767 / 2000 [ 38%]  (Warmup)\nChain 3: Iteration:  768 / 2000 [ 38%]  (Warmup)\nChain 3: Iteration:  769 / 2000 [ 38%]  (Warmup)\nChain 3: Iteration:  770 / 2000 [ 38%]  (Warmup)\nChain 3: Iteration:  771 / 2000 [ 38%]  (Warmup)\nChain 3: Iteration:  772 / 2000 [ 38%]  (Warmup)\nChain 3: Iteration:  773 / 2000 [ 38%]  (Warmup)\nChain 3: Iteration:  774 / 2000 [ 38%]  (Warmup)\nChain 3: Iteration:  775 / 2000 [ 38%]  (Warmup)\nChain 3: Iteration:  776 / 2000 [ 38%]  (Warmup)\nChain 3: Iteration:  777 / 2000 [ 38%]  (Warmup)\nChain 3: Iteration:  778 / 2000 [ 38%]  (Warmup)\nChain 3: Iteration:  779 / 2000 [ 38%]  (Warmup)\nChain 3: Iteration:  780 / 2000 [ 39%]  (Warmup)\nChain 3: Iteration:  781 / 2000 [ 39%]  (Warmup)\nChain 3: Iteration:  782 / 2000 [ 39%]  (Warmup)\nChain 3: Iteration:  783 / 2000 [ 39%]  (Warmup)\nChain 3: Iteration:  784 / 2000 [ 39%]  (Warmup)\nChain 3: Iteration:  785 / 2000 [ 39%]  (Warmup)\nChain 3: Iteration:  786 / 2000 [ 39%]  (Warmup)\nChain 3: Iteration:  787 / 2000 [ 39%]  (Warmup)\nChain 3: Iteration:  788 / 2000 [ 39%]  (Warmup)\nChain 3: Iteration:  789 / 2000 [ 39%]  (Warmup)\nChain 3: Iteration:  790 / 2000 [ 39%]  (Warmup)\nChain 3: Iteration:  791 / 2000 [ 39%]  (Warmup)\nChain 3: Iteration:  792 / 2000 [ 39%]  (Warmup)\nChain 3: Iteration:  793 / 2000 [ 39%]  (Warmup)\nChain 3: Iteration:  794 / 2000 [ 39%]  (Warmup)\nChain 3: Iteration:  795 / 2000 [ 39%]  (Warmup)\nChain 3: Iteration:  796 / 2000 [ 39%]  (Warmup)\nChain 3: Iteration:  797 / 2000 [ 39%]  (Warmup)\nChain 3: Iteration:  798 / 2000 [ 39%]  (Warmup)\nChain 3: Iteration:  799 / 2000 [ 39%]  (Warmup)\nChain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 3: Iteration:  801 / 2000 [ 40%]  (Warmup)\nChain 3: Iteration:  802 / 2000 [ 40%]  (Warmup)\nChain 3: Iteration:  803 / 2000 [ 40%]  (Warmup)\nChain 3: Iteration:  804 / 2000 [ 40%]  (Warmup)\nChain 3: Iteration:  805 / 2000 [ 40%]  (Warmup)\nChain 3: Iteration:  806 / 2000 [ 40%]  (Warmup)\nChain 3: Iteration:  807 / 2000 [ 40%]  (Warmup)\nChain 3: Iteration:  808 / 2000 [ 40%]  (Warmup)\nChain 3: Iteration:  809 / 2000 [ 40%]  (Warmup)\nChain 3: Iteration:  810 / 2000 [ 40%]  (Warmup)\nChain 3: Iteration:  811 / 2000 [ 40%]  (Warmup)\nChain 3: Iteration:  812 / 2000 [ 40%]  (Warmup)\nChain 3: Iteration:  813 / 2000 [ 40%]  (Warmup)\nChain 3: Iteration:  814 / 2000 [ 40%]  (Warmup)\nChain 3: Iteration:  815 / 2000 [ 40%]  (Warmup)\nChain 3: Iteration:  816 / 2000 [ 40%]  (Warmup)\nChain 3: Iteration:  817 / 2000 [ 40%]  (Warmup)\nChain 3: Iteration:  818 / 2000 [ 40%]  (Warmup)\nChain 3: Iteration:  819 / 2000 [ 40%]  (Warmup)\nChain 3: Iteration:  820 / 2000 [ 41%]  (Warmup)\nChain 3: Iteration:  821 / 2000 [ 41%]  (Warmup)\nChain 3: Iteration:  822 / 2000 [ 41%]  (Warmup)\nChain 3: Iteration:  823 / 2000 [ 41%]  (Warmup)\nChain 3: Iteration:  824 / 2000 [ 41%]  (Warmup)\nChain 3: Iteration:  825 / 2000 [ 41%]  (Warmup)\nChain 3: Iteration:  826 / 2000 [ 41%]  (Warmup)\nChain 3: Iteration:  827 / 2000 [ 41%]  (Warmup)\nChain 3: Iteration:  828 / 2000 [ 41%]  (Warmup)\nChain 3: Iteration:  829 / 2000 [ 41%]  (Warmup)\nChain 3: Iteration:  830 / 2000 [ 41%]  (Warmup)\nChain 3: Iteration:  831 / 2000 [ 41%]  (Warmup)\nChain 3: Iteration:  832 / 2000 [ 41%]  (Warmup)\nChain 3: Iteration:  833 / 2000 [ 41%]  (Warmup)\nChain 3: Iteration:  834 / 2000 [ 41%]  (Warmup)\nChain 3: Iteration:  835 / 2000 [ 41%]  (Warmup)\nChain 3: Iteration:  836 / 2000 [ 41%]  (Warmup)\nChain 3: Iteration:  837 / 2000 [ 41%]  (Warmup)\nChain 3: Iteration:  838 / 2000 [ 41%]  (Warmup)\nChain 3: Iteration:  839 / 2000 [ 41%]  (Warmup)\nChain 3: Iteration:  840 / 2000 [ 42%]  (Warmup)\nChain 3: Iteration:  841 / 2000 [ 42%]  (Warmup)\nChain 3: Iteration:  842 / 2000 [ 42%]  (Warmup)\nChain 3: Iteration:  843 / 2000 [ 42%]  (Warmup)\nChain 3: Iteration:  844 / 2000 [ 42%]  (Warmup)\nChain 3: Iteration:  845 / 2000 [ 42%]  (Warmup)\nChain 3: Iteration:  846 / 2000 [ 42%]  (Warmup)\nChain 3: Iteration:  847 / 2000 [ 42%]  (Warmup)\nChain 3: Iteration:  848 / 2000 [ 42%]  (Warmup)\nChain 3: Iteration:  849 / 2000 [ 42%]  (Warmup)\nChain 3: Iteration:  850 / 2000 [ 42%]  (Warmup)\nChain 3: Iteration:  851 / 2000 [ 42%]  (Warmup)\nChain 3: Iteration:  852 / 2000 [ 42%]  (Warmup)\nChain 3: Iteration:  853 / 2000 [ 42%]  (Warmup)\nChain 3: Iteration:  854 / 2000 [ 42%]  (Warmup)\nChain 3: Iteration:  855 / 2000 [ 42%]  (Warmup)\nChain 3: Iteration:  856 / 2000 [ 42%]  (Warmup)\nChain 3: Iteration:  857 / 2000 [ 42%]  (Warmup)\nChain 3: Iteration:  858 / 2000 [ 42%]  (Warmup)\nChain 3: Iteration:  859 / 2000 [ 42%]  (Warmup)\nChain 3: Iteration:  860 / 2000 [ 43%]  (Warmup)\nChain 3: Iteration:  861 / 2000 [ 43%]  (Warmup)\nChain 3: Iteration:  862 / 2000 [ 43%]  (Warmup)\nChain 3: Iteration:  863 / 2000 [ 43%]  (Warmup)\nChain 3: Iteration:  864 / 2000 [ 43%]  (Warmup)\nChain 3: Iteration:  865 / 2000 [ 43%]  (Warmup)\nChain 3: Iteration:  866 / 2000 [ 43%]  (Warmup)\nChain 3: Iteration:  867 / 2000 [ 43%]  (Warmup)\nChain 3: Iteration:  868 / 2000 [ 43%]  (Warmup)\nChain 3: Iteration:  869 / 2000 [ 43%]  (Warmup)\nChain 3: Iteration:  870 / 2000 [ 43%]  (Warmup)\nChain 3: Iteration:  871 / 2000 [ 43%]  (Warmup)\nChain 3: Iteration:  872 / 2000 [ 43%]  (Warmup)\nChain 3: Iteration:  873 / 2000 [ 43%]  (Warmup)\nChain 3: Iteration:  874 / 2000 [ 43%]  (Warmup)\nChain 3: Iteration:  875 / 2000 [ 43%]  (Warmup)\nChain 3: Iteration:  876 / 2000 [ 43%]  (Warmup)\nChain 3: Iteration:  877 / 2000 [ 43%]  (Warmup)\nChain 3: Iteration:  878 / 2000 [ 43%]  (Warmup)\nChain 3: Iteration:  879 / 2000 [ 43%]  (Warmup)\nChain 3: Iteration:  880 / 2000 [ 44%]  (Warmup)\nChain 3: Iteration:  881 / 2000 [ 44%]  (Warmup)\nChain 3: Iteration:  882 / 2000 [ 44%]  (Warmup)\nChain 3: Iteration:  883 / 2000 [ 44%]  (Warmup)\nChain 3: Iteration:  884 / 2000 [ 44%]  (Warmup)\nChain 3: Iteration:  885 / 2000 [ 44%]  (Warmup)\nChain 3: Iteration:  886 / 2000 [ 44%]  (Warmup)\nChain 3: Iteration:  887 / 2000 [ 44%]  (Warmup)\nChain 3: Iteration:  888 / 2000 [ 44%]  (Warmup)\nChain 3: Iteration:  889 / 2000 [ 44%]  (Warmup)\nChain 3: Iteration:  890 / 2000 [ 44%]  (Warmup)\nChain 3: Iteration:  891 / 2000 [ 44%]  (Warmup)\nChain 3: Iteration:  892 / 2000 [ 44%]  (Warmup)\nChain 3: Iteration:  893 / 2000 [ 44%]  (Warmup)\nChain 3: Iteration:  894 / 2000 [ 44%]  (Warmup)\nChain 3: Iteration:  895 / 2000 [ 44%]  (Warmup)\nChain 3: Iteration:  896 / 2000 [ 44%]  (Warmup)\nChain 3: Iteration:  897 / 2000 [ 44%]  (Warmup)\nChain 3: Iteration:  898 / 2000 [ 44%]  (Warmup)\nChain 3: Iteration:  899 / 2000 [ 44%]  (Warmup)\nChain 3: Iteration:  900 / 2000 [ 45%]  (Warmup)\nChain 3: Iteration:  901 / 2000 [ 45%]  (Warmup)\nChain 3: Iteration:  902 / 2000 [ 45%]  (Warmup)\nChain 3: Iteration:  903 / 2000 [ 45%]  (Warmup)\nChain 3: Iteration:  904 / 2000 [ 45%]  (Warmup)\nChain 3: Iteration:  905 / 2000 [ 45%]  (Warmup)\nChain 3: Iteration:  906 / 2000 [ 45%]  (Warmup)\nChain 3: Iteration:  907 / 2000 [ 45%]  (Warmup)\nChain 3: Iteration:  908 / 2000 [ 45%]  (Warmup)\nChain 3: Iteration:  909 / 2000 [ 45%]  (Warmup)\nChain 3: Iteration:  910 / 2000 [ 45%]  (Warmup)\nChain 3: Iteration:  911 / 2000 [ 45%]  (Warmup)\nChain 3: Iteration:  912 / 2000 [ 45%]  (Warmup)\nChain 3: Iteration:  913 / 2000 [ 45%]  (Warmup)\nChain 3: Iteration:  914 / 2000 [ 45%]  (Warmup)\nChain 3: Iteration:  915 / 2000 [ 45%]  (Warmup)\nChain 3: Iteration:  916 / 2000 [ 45%]  (Warmup)\nChain 3: Iteration:  917 / 2000 [ 45%]  (Warmup)\nChain 3: Iteration:  918 / 2000 [ 45%]  (Warmup)\nChain 3: Iteration:  919 / 2000 [ 45%]  (Warmup)\nChain 3: Iteration:  920 / 2000 [ 46%]  (Warmup)\nChain 3: Iteration:  921 / 2000 [ 46%]  (Warmup)\nChain 3: Iteration:  922 / 2000 [ 46%]  (Warmup)\nChain 3: Iteration:  923 / 2000 [ 46%]  (Warmup)\nChain 3: Iteration:  924 / 2000 [ 46%]  (Warmup)\nChain 3: Iteration:  925 / 2000 [ 46%]  (Warmup)\nChain 3: Iteration:  926 / 2000 [ 46%]  (Warmup)\nChain 3: Iteration:  927 / 2000 [ 46%]  (Warmup)\nChain 3: Iteration:  928 / 2000 [ 46%]  (Warmup)\nChain 3: Iteration:  929 / 2000 [ 46%]  (Warmup)\nChain 3: Iteration:  930 / 2000 [ 46%]  (Warmup)\nChain 3: Iteration:  931 / 2000 [ 46%]  (Warmup)\nChain 3: Iteration:  932 / 2000 [ 46%]  (Warmup)\nChain 3: Iteration:  933 / 2000 [ 46%]  (Warmup)\nChain 3: Iteration:  934 / 2000 [ 46%]  (Warmup)\nChain 3: Iteration:  935 / 2000 [ 46%]  (Warmup)\nChain 3: Iteration:  936 / 2000 [ 46%]  (Warmup)\nChain 3: Iteration:  937 / 2000 [ 46%]  (Warmup)\nChain 3: Iteration:  938 / 2000 [ 46%]  (Warmup)\nChain 3: Iteration:  939 / 2000 [ 46%]  (Warmup)\nChain 3: Iteration:  940 / 2000 [ 47%]  (Warmup)\nChain 3: Iteration:  941 / 2000 [ 47%]  (Warmup)\nChain 3: Iteration:  942 / 2000 [ 47%]  (Warmup)\nChain 3: Iteration:  943 / 2000 [ 47%]  (Warmup)\nChain 3: Iteration:  944 / 2000 [ 47%]  (Warmup)\nChain 3: Iteration:  945 / 2000 [ 47%]  (Warmup)\nChain 3: Iteration:  946 / 2000 [ 47%]  (Warmup)\nChain 3: Iteration:  947 / 2000 [ 47%]  (Warmup)\nChain 3: Iteration:  948 / 2000 [ 47%]  (Warmup)\nChain 3: Iteration:  949 / 2000 [ 47%]  (Warmup)\nChain 3: Iteration:  950 / 2000 [ 47%]  (Warmup)\nChain 3: Iteration:  951 / 2000 [ 47%]  (Warmup)\nChain 3: Iteration:  952 / 2000 [ 47%]  (Warmup)\nChain 3: Iteration:  953 / 2000 [ 47%]  (Warmup)\nChain 3: Iteration:  954 / 2000 [ 47%]  (Warmup)\nChain 3: Iteration:  955 / 2000 [ 47%]  (Warmup)\nChain 3: Iteration:  956 / 2000 [ 47%]  (Warmup)\nChain 3: Iteration:  957 / 2000 [ 47%]  (Warmup)\nChain 3: Iteration:  958 / 2000 [ 47%]  (Warmup)\nChain 3: Iteration:  959 / 2000 [ 47%]  (Warmup)\nChain 3: Iteration:  960 / 2000 [ 48%]  (Warmup)\nChain 3: Iteration:  961 / 2000 [ 48%]  (Warmup)\nChain 3: Iteration:  962 / 2000 [ 48%]  (Warmup)\nChain 3: Iteration:  963 / 2000 [ 48%]  (Warmup)\nChain 3: Iteration:  964 / 2000 [ 48%]  (Warmup)\nChain 3: Iteration:  965 / 2000 [ 48%]  (Warmup)\nChain 3: Iteration:  966 / 2000 [ 48%]  (Warmup)\nChain 3: Iteration:  967 / 2000 [ 48%]  (Warmup)\nChain 3: Iteration:  968 / 2000 [ 48%]  (Warmup)\nChain 3: Iteration:  969 / 2000 [ 48%]  (Warmup)\nChain 3: Iteration:  970 / 2000 [ 48%]  (Warmup)\nChain 3: Iteration:  971 / 2000 [ 48%]  (Warmup)\nChain 3: Iteration:  972 / 2000 [ 48%]  (Warmup)\nChain 3: Iteration:  973 / 2000 [ 48%]  (Warmup)\nChain 3: Iteration:  974 / 2000 [ 48%]  (Warmup)\nChain 3: Iteration:  975 / 2000 [ 48%]  (Warmup)\nChain 3: Iteration:  976 / 2000 [ 48%]  (Warmup)\nChain 3: Iteration:  977 / 2000 [ 48%]  (Warmup)\nChain 3: Iteration:  978 / 2000 [ 48%]  (Warmup)\nChain 3: Iteration:  979 / 2000 [ 48%]  (Warmup)\nChain 3: Iteration:  980 / 2000 [ 49%]  (Warmup)\nChain 3: Iteration:  981 / 2000 [ 49%]  (Warmup)\nChain 3: Iteration:  982 / 2000 [ 49%]  (Warmup)\nChain 3: Iteration:  983 / 2000 [ 49%]  (Warmup)\nChain 3: Iteration:  984 / 2000 [ 49%]  (Warmup)\nChain 3: Iteration:  985 / 2000 [ 49%]  (Warmup)\nChain 3: Iteration:  986 / 2000 [ 49%]  (Warmup)\nChain 3: Iteration:  987 / 2000 [ 49%]  (Warmup)\nChain 3: Iteration:  988 / 2000 [ 49%]  (Warmup)\nChain 3: Iteration:  989 / 2000 [ 49%]  (Warmup)\nChain 3: Iteration:  990 / 2000 [ 49%]  (Warmup)\nChain 3: Iteration:  991 / 2000 [ 49%]  (Warmup)\nChain 3: Iteration:  992 / 2000 [ 49%]  (Warmup)\nChain 3: Iteration:  993 / 2000 [ 49%]  (Warmup)\nChain 3: Iteration:  994 / 2000 [ 49%]  (Warmup)\nChain 3: Iteration:  995 / 2000 [ 49%]  (Warmup)\nChain 3: Iteration:  996 / 2000 [ 49%]  (Warmup)\nChain 3: Iteration:  997 / 2000 [ 49%]  (Warmup)\nChain 3: Iteration:  998 / 2000 [ 49%]  (Warmup)\nChain 3: Iteration:  999 / 2000 [ 49%]  (Warmup)\nChain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 3: Iteration: 1002 / 2000 [ 50%]  (Sampling)\nChain 3: Iteration: 1003 / 2000 [ 50%]  (Sampling)\nChain 3: Iteration: 1004 / 2000 [ 50%]  (Sampling)\nChain 3: Iteration: 1005 / 2000 [ 50%]  (Sampling)\nChain 3: Iteration: 1006 / 2000 [ 50%]  (Sampling)\nChain 3: Iteration: 1007 / 2000 [ 50%]  (Sampling)\nChain 3: Iteration: 1008 / 2000 [ 50%]  (Sampling)\nChain 3: Iteration: 1009 / 2000 [ 50%]  (Sampling)\nChain 3: Iteration: 1010 / 2000 [ 50%]  (Sampling)\nChain 3: Iteration: 1011 / 2000 [ 50%]  (Sampling)\nChain 3: Iteration: 1012 / 2000 [ 50%]  (Sampling)\nChain 3: Iteration: 1013 / 2000 [ 50%]  (Sampling)\nChain 3: Iteration: 1014 / 2000 [ 50%]  (Sampling)\nChain 3: Iteration: 1015 / 2000 [ 50%]  (Sampling)\nChain 3: Iteration: 1016 / 2000 [ 50%]  (Sampling)\nChain 3: Iteration: 1017 / 2000 [ 50%]  (Sampling)\nChain 3: Iteration: 1018 / 2000 [ 50%]  (Sampling)\nChain 3: Iteration: 1019 / 2000 [ 50%]  (Sampling)\nChain 3: Iteration: 1020 / 2000 [ 51%]  (Sampling)\nChain 3: Iteration: 1021 / 2000 [ 51%]  (Sampling)\nChain 3: Iteration: 1022 / 2000 [ 51%]  (Sampling)\nChain 3: Iteration: 1023 / 2000 [ 51%]  (Sampling)\nChain 3: Iteration: 1024 / 2000 [ 51%]  (Sampling)\nChain 3: Iteration: 1025 / 2000 [ 51%]  (Sampling)\nChain 3: Iteration: 1026 / 2000 [ 51%]  (Sampling)\nChain 3: Iteration: 1027 / 2000 [ 51%]  (Sampling)\nChain 3: Iteration: 1028 / 2000 [ 51%]  (Sampling)\nChain 3: Iteration: 1029 / 2000 [ 51%]  (Sampling)\nChain 3: Iteration: 1030 / 2000 [ 51%]  (Sampling)\nChain 3: Iteration: 1031 / 2000 [ 51%]  (Sampling)\nChain 3: Iteration: 1032 / 2000 [ 51%]  (Sampling)\nChain 3: Iteration: 1033 / 2000 [ 51%]  (Sampling)\nChain 3: Iteration: 1034 / 2000 [ 51%]  (Sampling)\nChain 3: Iteration: 1035 / 2000 [ 51%]  (Sampling)\nChain 3: Iteration: 1036 / 2000 [ 51%]  (Sampling)\nChain 3: Iteration: 1037 / 2000 [ 51%]  (Sampling)\nChain 3: Iteration: 1038 / 2000 [ 51%]  (Sampling)\nChain 3: Iteration: 1039 / 2000 [ 51%]  (Sampling)\nChain 3: Iteration: 1040 / 2000 [ 52%]  (Sampling)\nChain 3: Iteration: 1041 / 2000 [ 52%]  (Sampling)\nChain 3: Iteration: 1042 / 2000 [ 52%]  (Sampling)\nChain 3: Iteration: 1043 / 2000 [ 52%]  (Sampling)\nChain 3: Iteration: 1044 / 2000 [ 52%]  (Sampling)\nChain 3: Iteration: 1045 / 2000 [ 52%]  (Sampling)\nChain 3: Iteration: 1046 / 2000 [ 52%]  (Sampling)\nChain 3: Iteration: 1047 / 2000 [ 52%]  (Sampling)\nChain 3: Iteration: 1048 / 2000 [ 52%]  (Sampling)\nChain 3: Iteration: 1049 / 2000 [ 52%]  (Sampling)\nChain 3: Iteration: 1050 / 2000 [ 52%]  (Sampling)\nChain 3: Iteration: 1051 / 2000 [ 52%]  (Sampling)\nChain 3: Iteration: 1052 / 2000 [ 52%]  (Sampling)\nChain 3: Iteration: 1053 / 2000 [ 52%]  (Sampling)\nChain 3: Iteration: 1054 / 2000 [ 52%]  (Sampling)\nChain 3: Iteration: 1055 / 2000 [ 52%]  (Sampling)\nChain 3: Iteration: 1056 / 2000 [ 52%]  (Sampling)\nChain 3: Iteration: 1057 / 2000 [ 52%]  (Sampling)\nChain 3: Iteration: 1058 / 2000 [ 52%]  (Sampling)\nChain 3: Iteration: 1059 / 2000 [ 52%]  (Sampling)\nChain 3: Iteration: 1060 / 2000 [ 53%]  (Sampling)\nChain 3: Iteration: 1061 / 2000 [ 53%]  (Sampling)\nChain 3: Iteration: 1062 / 2000 [ 53%]  (Sampling)\nChain 3: Iteration: 1063 / 2000 [ 53%]  (Sampling)\nChain 3: Iteration: 1064 / 2000 [ 53%]  (Sampling)\nChain 3: Iteration: 1065 / 2000 [ 53%]  (Sampling)\nChain 3: Iteration: 1066 / 2000 [ 53%]  (Sampling)\nChain 3: Iteration: 1067 / 2000 [ 53%]  (Sampling)\nChain 3: Iteration: 1068 / 2000 [ 53%]  (Sampling)\nChain 3: Iteration: 1069 / 2000 [ 53%]  (Sampling)\nChain 3: Iteration: 1070 / 2000 [ 53%]  (Sampling)\nChain 3: Iteration: 1071 / 2000 [ 53%]  (Sampling)\nChain 3: Iteration: 1072 / 2000 [ 53%]  (Sampling)\nChain 3: Iteration: 1073 / 2000 [ 53%]  (Sampling)\nChain 3: Iteration: 1074 / 2000 [ 53%]  (Sampling)\nChain 3: Iteration: 1075 / 2000 [ 53%]  (Sampling)\nChain 3: Iteration: 1076 / 2000 [ 53%]  (Sampling)\nChain 3: Iteration: 1077 / 2000 [ 53%]  (Sampling)\nChain 3: Iteration: 1078 / 2000 [ 53%]  (Sampling)\nChain 3: Iteration: 1079 / 2000 [ 53%]  (Sampling)\nChain 3: Iteration: 1080 / 2000 [ 54%]  (Sampling)\nChain 3: Iteration: 1081 / 2000 [ 54%]  (Sampling)\nChain 3: Iteration: 1082 / 2000 [ 54%]  (Sampling)\nChain 3: Iteration: 1083 / 2000 [ 54%]  (Sampling)\nChain 3: Iteration: 1084 / 2000 [ 54%]  (Sampling)\nChain 3: Iteration: 1085 / 2000 [ 54%]  (Sampling)\nChain 3: Iteration: 1086 / 2000 [ 54%]  (Sampling)\nChain 3: Iteration: 1087 / 2000 [ 54%]  (Sampling)\nChain 3: Iteration: 1088 / 2000 [ 54%]  (Sampling)\nChain 3: Iteration: 1089 / 2000 [ 54%]  (Sampling)\nChain 3: Iteration: 1090 / 2000 [ 54%]  (Sampling)\nChain 3: Iteration: 1091 / 2000 [ 54%]  (Sampling)\nChain 3: Iteration: 1092 / 2000 [ 54%]  (Sampling)\nChain 3: Iteration: 1093 / 2000 [ 54%]  (Sampling)\nChain 3: Iteration: 1094 / 2000 [ 54%]  (Sampling)\nChain 3: Iteration: 1095 / 2000 [ 54%]  (Sampling)\nChain 3: Iteration: 1096 / 2000 [ 54%]  (Sampling)\nChain 3: Iteration: 1097 / 2000 [ 54%]  (Sampling)\nChain 3: Iteration: 1098 / 2000 [ 54%]  (Sampling)\nChain 3: Iteration: 1099 / 2000 [ 54%]  (Sampling)\nChain 3: Iteration: 1100 / 2000 [ 55%]  (Sampling)\nChain 3: Iteration: 1101 / 2000 [ 55%]  (Sampling)\nChain 3: Iteration: 1102 / 2000 [ 55%]  (Sampling)\nChain 3: Iteration: 1103 / 2000 [ 55%]  (Sampling)\nChain 3: Iteration: 1104 / 2000 [ 55%]  (Sampling)\nChain 3: Iteration: 1105 / 2000 [ 55%]  (Sampling)\nChain 3: Iteration: 1106 / 2000 [ 55%]  (Sampling)\nChain 3: Iteration: 1107 / 2000 [ 55%]  (Sampling)\nChain 3: Iteration: 1108 / 2000 [ 55%]  (Sampling)\nChain 3: Iteration: 1109 / 2000 [ 55%]  (Sampling)\nChain 3: Iteration: 1110 / 2000 [ 55%]  (Sampling)\nChain 3: Iteration: 1111 / 2000 [ 55%]  (Sampling)\nChain 3: Iteration: 1112 / 2000 [ 55%]  (Sampling)\nChain 3: Iteration: 1113 / 2000 [ 55%]  (Sampling)\nChain 3: Iteration: 1114 / 2000 [ 55%]  (Sampling)\nChain 3: Iteration: 1115 / 2000 [ 55%]  (Sampling)\nChain 3: Iteration: 1116 / 2000 [ 55%]  (Sampling)\nChain 3: Iteration: 1117 / 2000 [ 55%]  (Sampling)\nChain 3: Iteration: 1118 / 2000 [ 55%]  (Sampling)\nChain 3: Iteration: 1119 / 2000 [ 55%]  (Sampling)\nChain 3: Iteration: 1120 / 2000 [ 56%]  (Sampling)\nChain 3: Iteration: 1121 / 2000 [ 56%]  (Sampling)\nChain 3: Iteration: 1122 / 2000 [ 56%]  (Sampling)\nChain 3: Iteration: 1123 / 2000 [ 56%]  (Sampling)\nChain 3: Iteration: 1124 / 2000 [ 56%]  (Sampling)\nChain 3: Iteration: 1125 / 2000 [ 56%]  (Sampling)\nChain 3: Iteration: 1126 / 2000 [ 56%]  (Sampling)\nChain 3: Iteration: 1127 / 2000 [ 56%]  (Sampling)\nChain 3: Iteration: 1128 / 2000 [ 56%]  (Sampling)\nChain 3: Iteration: 1129 / 2000 [ 56%]  (Sampling)\nChain 3: Iteration: 1130 / 2000 [ 56%]  (Sampling)\nChain 3: Iteration: 1131 / 2000 [ 56%]  (Sampling)\nChain 3: Iteration: 1132 / 2000 [ 56%]  (Sampling)\nChain 3: Iteration: 1133 / 2000 [ 56%]  (Sampling)\nChain 3: Iteration: 1134 / 2000 [ 56%]  (Sampling)\nChain 3: Iteration: 1135 / 2000 [ 56%]  (Sampling)\nChain 3: Iteration: 1136 / 2000 [ 56%]  (Sampling)\nChain 3: Iteration: 1137 / 2000 [ 56%]  (Sampling)\nChain 3: Iteration: 1138 / 2000 [ 56%]  (Sampling)\nChain 3: Iteration: 1139 / 2000 [ 56%]  (Sampling)\nChain 3: Iteration: 1140 / 2000 [ 57%]  (Sampling)\nChain 3: Iteration: 1141 / 2000 [ 57%]  (Sampling)\nChain 3: Iteration: 1142 / 2000 [ 57%]  (Sampling)\nChain 3: Iteration: 1143 / 2000 [ 57%]  (Sampling)\nChain 3: Iteration: 1144 / 2000 [ 57%]  (Sampling)\nChain 3: Iteration: 1145 / 2000 [ 57%]  (Sampling)\nChain 3: Iteration: 1146 / 2000 [ 57%]  (Sampling)\nChain 3: Iteration: 1147 / 2000 [ 57%]  (Sampling)\nChain 3: Iteration: 1148 / 2000 [ 57%]  (Sampling)\nChain 3: Iteration: 1149 / 2000 [ 57%]  (Sampling)\nChain 3: Iteration: 1150 / 2000 [ 57%]  (Sampling)\nChain 3: Iteration: 1151 / 2000 [ 57%]  (Sampling)\nChain 3: Iteration: 1152 / 2000 [ 57%]  (Sampling)\nChain 3: Iteration: 1153 / 2000 [ 57%]  (Sampling)\nChain 3: Iteration: 1154 / 2000 [ 57%]  (Sampling)\nChain 3: Iteration: 1155 / 2000 [ 57%]  (Sampling)\nChain 3: Iteration: 1156 / 2000 [ 57%]  (Sampling)\nChain 3: Iteration: 1157 / 2000 [ 57%]  (Sampling)\nChain 3: Iteration: 1158 / 2000 [ 57%]  (Sampling)\nChain 3: Iteration: 1159 / 2000 [ 57%]  (Sampling)\nChain 3: Iteration: 1160 / 2000 [ 58%]  (Sampling)\nChain 3: Iteration: 1161 / 2000 [ 58%]  (Sampling)\nChain 3: Iteration: 1162 / 2000 [ 58%]  (Sampling)\nChain 3: Iteration: 1163 / 2000 [ 58%]  (Sampling)\nChain 3: Iteration: 1164 / 2000 [ 58%]  (Sampling)\nChain 3: Iteration: 1165 / 2000 [ 58%]  (Sampling)\nChain 3: Iteration: 1166 / 2000 [ 58%]  (Sampling)\nChain 3: Iteration: 1167 / 2000 [ 58%]  (Sampling)\nChain 3: Iteration: 1168 / 2000 [ 58%]  (Sampling)\nChain 3: Iteration: 1169 / 2000 [ 58%]  (Sampling)\nChain 3: Iteration: 1170 / 2000 [ 58%]  (Sampling)\nChain 3: Iteration: 1171 / 2000 [ 58%]  (Sampling)\nChain 3: Iteration: 1172 / 2000 [ 58%]  (Sampling)\nChain 3: Iteration: 1173 / 2000 [ 58%]  (Sampling)\nChain 3: Iteration: 1174 / 2000 [ 58%]  (Sampling)\nChain 3: Iteration: 1175 / 2000 [ 58%]  (Sampling)\nChain 3: Iteration: 1176 / 2000 [ 58%]  (Sampling)\nChain 3: Iteration: 1177 / 2000 [ 58%]  (Sampling)\nChain 3: Iteration: 1178 / 2000 [ 58%]  (Sampling)\nChain 3: Iteration: 1179 / 2000 [ 58%]  (Sampling)\nChain 3: Iteration: 1180 / 2000 [ 59%]  (Sampling)\nChain 3: Iteration: 1181 / 2000 [ 59%]  (Sampling)\nChain 3: Iteration: 1182 / 2000 [ 59%]  (Sampling)\nChain 3: Iteration: 1183 / 2000 [ 59%]  (Sampling)\nChain 3: Iteration: 1184 / 2000 [ 59%]  (Sampling)\nChain 3: Iteration: 1185 / 2000 [ 59%]  (Sampling)\nChain 3: Iteration: 1186 / 2000 [ 59%]  (Sampling)\nChain 3: Iteration: 1187 / 2000 [ 59%]  (Sampling)\nChain 3: Iteration: 1188 / 2000 [ 59%]  (Sampling)\nChain 3: Iteration: 1189 / 2000 [ 59%]  (Sampling)\nChain 3: Iteration: 1190 / 2000 [ 59%]  (Sampling)\nChain 3: Iteration: 1191 / 2000 [ 59%]  (Sampling)\nChain 3: Iteration: 1192 / 2000 [ 59%]  (Sampling)\nChain 3: Iteration: 1193 / 2000 [ 59%]  (Sampling)\nChain 3: Iteration: 1194 / 2000 [ 59%]  (Sampling)\nChain 3: Iteration: 1195 / 2000 [ 59%]  (Sampling)\nChain 3: Iteration: 1196 / 2000 [ 59%]  (Sampling)\nChain 3: Iteration: 1197 / 2000 [ 59%]  (Sampling)\nChain 3: Iteration: 1198 / 2000 [ 59%]  (Sampling)\nChain 3: Iteration: 1199 / 2000 [ 59%]  (Sampling)\nChain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 3: Iteration: 1201 / 2000 [ 60%]  (Sampling)\nChain 3: Iteration: 1202 / 2000 [ 60%]  (Sampling)\nChain 3: Iteration: 1203 / 2000 [ 60%]  (Sampling)\nChain 3: Iteration: 1204 / 2000 [ 60%]  (Sampling)\nChain 3: Iteration: 1205 / 2000 [ 60%]  (Sampling)\nChain 3: Iteration: 1206 / 2000 [ 60%]  (Sampling)\nChain 3: Iteration: 1207 / 2000 [ 60%]  (Sampling)\nChain 3: Iteration: 1208 / 2000 [ 60%]  (Sampling)\nChain 3: Iteration: 1209 / 2000 [ 60%]  (Sampling)\nChain 3: Iteration: 1210 / 2000 [ 60%]  (Sampling)\nChain 3: Iteration: 1211 / 2000 [ 60%]  (Sampling)\nChain 3: Iteration: 1212 / 2000 [ 60%]  (Sampling)\nChain 3: Iteration: 1213 / 2000 [ 60%]  (Sampling)\nChain 3: Iteration: 1214 / 2000 [ 60%]  (Sampling)\nChain 3: Iteration: 1215 / 2000 [ 60%]  (Sampling)\nChain 3: Iteration: 1216 / 2000 [ 60%]  (Sampling)\nChain 3: Iteration: 1217 / 2000 [ 60%]  (Sampling)\nChain 3: Iteration: 1218 / 2000 [ 60%]  (Sampling)\nChain 3: Iteration: 1219 / 2000 [ 60%]  (Sampling)\nChain 3: Iteration: 1220 / 2000 [ 61%]  (Sampling)\nChain 3: Iteration: 1221 / 2000 [ 61%]  (Sampling)\nChain 3: Iteration: 1222 / 2000 [ 61%]  (Sampling)\nChain 3: Iteration: 1223 / 2000 [ 61%]  (Sampling)\nChain 3: Iteration: 1224 / 2000 [ 61%]  (Sampling)\nChain 3: Iteration: 1225 / 2000 [ 61%]  (Sampling)\nChain 3: Iteration: 1226 / 2000 [ 61%]  (Sampling)\nChain 3: Iteration: 1227 / 2000 [ 61%]  (Sampling)\nChain 3: Iteration: 1228 / 2000 [ 61%]  (Sampling)\nChain 3: Iteration: 1229 / 2000 [ 61%]  (Sampling)\nChain 3: Iteration: 1230 / 2000 [ 61%]  (Sampling)\nChain 3: Iteration: 1231 / 2000 [ 61%]  (Sampling)\nChain 3: Iteration: 1232 / 2000 [ 61%]  (Sampling)\nChain 3: Iteration: 1233 / 2000 [ 61%]  (Sampling)\nChain 3: Iteration: 1234 / 2000 [ 61%]  (Sampling)\nChain 3: Iteration: 1235 / 2000 [ 61%]  (Sampling)\nChain 3: Iteration: 1236 / 2000 [ 61%]  (Sampling)\nChain 3: Iteration: 1237 / 2000 [ 61%]  (Sampling)\nChain 3: Iteration: 1238 / 2000 [ 61%]  (Sampling)\nChain 3: Iteration: 1239 / 2000 [ 61%]  (Sampling)\nChain 3: Iteration: 1240 / 2000 [ 62%]  (Sampling)\nChain 3: Iteration: 1241 / 2000 [ 62%]  (Sampling)\nChain 3: Iteration: 1242 / 2000 [ 62%]  (Sampling)\nChain 3: Iteration: 1243 / 2000 [ 62%]  (Sampling)\nChain 3: Iteration: 1244 / 2000 [ 62%]  (Sampling)\nChain 3: Iteration: 1245 / 2000 [ 62%]  (Sampling)\nChain 3: Iteration: 1246 / 2000 [ 62%]  (Sampling)\nChain 3: Iteration: 1247 / 2000 [ 62%]  (Sampling)\nChain 3: Iteration: 1248 / 2000 [ 62%]  (Sampling)\nChain 3: Iteration: 1249 / 2000 [ 62%]  (Sampling)\nChain 3: Iteration: 1250 / 2000 [ 62%]  (Sampling)\nChain 3: Iteration: 1251 / 2000 [ 62%]  (Sampling)\nChain 3: Iteration: 1252 / 2000 [ 62%]  (Sampling)\nChain 3: Iteration: 1253 / 2000 [ 62%]  (Sampling)\nChain 3: Iteration: 1254 / 2000 [ 62%]  (Sampling)\nChain 3: Iteration: 1255 / 2000 [ 62%]  (Sampling)\nChain 3: Iteration: 1256 / 2000 [ 62%]  (Sampling)\nChain 3: Iteration: 1257 / 2000 [ 62%]  (Sampling)\nChain 3: Iteration: 1258 / 2000 [ 62%]  (Sampling)\nChain 3: Iteration: 1259 / 2000 [ 62%]  (Sampling)\nChain 3: Iteration: 1260 / 2000 [ 63%]  (Sampling)\nChain 3: Iteration: 1261 / 2000 [ 63%]  (Sampling)\nChain 3: Iteration: 1262 / 2000 [ 63%]  (Sampling)\nChain 3: Iteration: 1263 / 2000 [ 63%]  (Sampling)\nChain 3: Iteration: 1264 / 2000 [ 63%]  (Sampling)\nChain 3: Iteration: 1265 / 2000 [ 63%]  (Sampling)\nChain 3: Iteration: 1266 / 2000 [ 63%]  (Sampling)\nChain 3: Iteration: 1267 / 2000 [ 63%]  (Sampling)\nChain 3: Iteration: 1268 / 2000 [ 63%]  (Sampling)\nChain 3: Iteration: 1269 / 2000 [ 63%]  (Sampling)\nChain 3: Iteration: 1270 / 2000 [ 63%]  (Sampling)\nChain 3: Iteration: 1271 / 2000 [ 63%]  (Sampling)\nChain 3: Iteration: 1272 / 2000 [ 63%]  (Sampling)\nChain 3: Iteration: 1273 / 2000 [ 63%]  (Sampling)\nChain 3: Iteration: 1274 / 2000 [ 63%]  (Sampling)\nChain 3: Iteration: 1275 / 2000 [ 63%]  (Sampling)\nChain 3: Iteration: 1276 / 2000 [ 63%]  (Sampling)\nChain 3: Iteration: 1277 / 2000 [ 63%]  (Sampling)\nChain 3: Iteration: 1278 / 2000 [ 63%]  (Sampling)\nChain 3: Iteration: 1279 / 2000 [ 63%]  (Sampling)\nChain 3: Iteration: 1280 / 2000 [ 64%]  (Sampling)\nChain 3: Iteration: 1281 / 2000 [ 64%]  (Sampling)\nChain 3: Iteration: 1282 / 2000 [ 64%]  (Sampling)\nChain 3: Iteration: 1283 / 2000 [ 64%]  (Sampling)\nChain 3: Iteration: 1284 / 2000 [ 64%]  (Sampling)\nChain 3: Iteration: 1285 / 2000 [ 64%]  (Sampling)\nChain 3: Iteration: 1286 / 2000 [ 64%]  (Sampling)\nChain 3: Iteration: 1287 / 2000 [ 64%]  (Sampling)\nChain 3: Iteration: 1288 / 2000 [ 64%]  (Sampling)\nChain 3: Iteration: 1289 / 2000 [ 64%]  (Sampling)\nChain 3: Iteration: 1290 / 2000 [ 64%]  (Sampling)\nChain 3: Iteration: 1291 / 2000 [ 64%]  (Sampling)\nChain 3: Iteration: 1292 / 2000 [ 64%]  (Sampling)\nChain 3: Iteration: 1293 / 2000 [ 64%]  (Sampling)\nChain 3: Iteration: 1294 / 2000 [ 64%]  (Sampling)\nChain 3: Iteration: 1295 / 2000 [ 64%]  (Sampling)\nChain 3: Iteration: 1296 / 2000 [ 64%]  (Sampling)\nChain 3: Iteration: 1297 / 2000 [ 64%]  (Sampling)\nChain 3: Iteration: 1298 / 2000 [ 64%]  (Sampling)\nChain 3: Iteration: 1299 / 2000 [ 64%]  (Sampling)\nChain 3: Iteration: 1300 / 2000 [ 65%]  (Sampling)\nChain 3: Iteration: 1301 / 2000 [ 65%]  (Sampling)\nChain 3: Iteration: 1302 / 2000 [ 65%]  (Sampling)\nChain 3: Iteration: 1303 / 2000 [ 65%]  (Sampling)\nChain 3: Iteration: 1304 / 2000 [ 65%]  (Sampling)\nChain 3: Iteration: 1305 / 2000 [ 65%]  (Sampling)\nChain 3: Iteration: 1306 / 2000 [ 65%]  (Sampling)\nChain 3: Iteration: 1307 / 2000 [ 65%]  (Sampling)\nChain 3: Iteration: 1308 / 2000 [ 65%]  (Sampling)\nChain 3: Iteration: 1309 / 2000 [ 65%]  (Sampling)\nChain 3: Iteration: 1310 / 2000 [ 65%]  (Sampling)\nChain 3: Iteration: 1311 / 2000 [ 65%]  (Sampling)\nChain 3: Iteration: 1312 / 2000 [ 65%]  (Sampling)\nChain 3: Iteration: 1313 / 2000 [ 65%]  (Sampling)\nChain 3: Iteration: 1314 / 2000 [ 65%]  (Sampling)\nChain 3: Iteration: 1315 / 2000 [ 65%]  (Sampling)\nChain 3: Iteration: 1316 / 2000 [ 65%]  (Sampling)\nChain 3: Iteration: 1317 / 2000 [ 65%]  (Sampling)\nChain 3: Iteration: 1318 / 2000 [ 65%]  (Sampling)\nChain 3: Iteration: 1319 / 2000 [ 65%]  (Sampling)\nChain 3: Iteration: 1320 / 2000 [ 66%]  (Sampling)\nChain 3: Iteration: 1321 / 2000 [ 66%]  (Sampling)\nChain 3: Iteration: 1322 / 2000 [ 66%]  (Sampling)\nChain 3: Iteration: 1323 / 2000 [ 66%]  (Sampling)\nChain 3: Iteration: 1324 / 2000 [ 66%]  (Sampling)\nChain 3: Iteration: 1325 / 2000 [ 66%]  (Sampling)\nChain 3: Iteration: 1326 / 2000 [ 66%]  (Sampling)\nChain 3: Iteration: 1327 / 2000 [ 66%]  (Sampling)\nChain 3: Iteration: 1328 / 2000 [ 66%]  (Sampling)\nChain 3: Iteration: 1329 / 2000 [ 66%]  (Sampling)\nChain 3: Iteration: 1330 / 2000 [ 66%]  (Sampling)\nChain 3: Iteration: 1331 / 2000 [ 66%]  (Sampling)\nChain 3: Iteration: 1332 / 2000 [ 66%]  (Sampling)\nChain 3: Iteration: 1333 / 2000 [ 66%]  (Sampling)\nChain 3: Iteration: 1334 / 2000 [ 66%]  (Sampling)\nChain 3: Iteration: 1335 / 2000 [ 66%]  (Sampling)\nChain 3: Iteration: 1336 / 2000 [ 66%]  (Sampling)\nChain 3: Iteration: 1337 / 2000 [ 66%]  (Sampling)\nChain 3: Iteration: 1338 / 2000 [ 66%]  (Sampling)\nChain 3: Iteration: 1339 / 2000 [ 66%]  (Sampling)\nChain 3: Iteration: 1340 / 2000 [ 67%]  (Sampling)\nChain 3: Iteration: 1341 / 2000 [ 67%]  (Sampling)\nChain 3: Iteration: 1342 / 2000 [ 67%]  (Sampling)\nChain 3: Iteration: 1343 / 2000 [ 67%]  (Sampling)\nChain 3: Iteration: 1344 / 2000 [ 67%]  (Sampling)\nChain 3: Iteration: 1345 / 2000 [ 67%]  (Sampling)\nChain 3: Iteration: 1346 / 2000 [ 67%]  (Sampling)\nChain 3: Iteration: 1347 / 2000 [ 67%]  (Sampling)\nChain 3: Iteration: 1348 / 2000 [ 67%]  (Sampling)\nChain 3: Iteration: 1349 / 2000 [ 67%]  (Sampling)\nChain 3: Iteration: 1350 / 2000 [ 67%]  (Sampling)\nChain 3: Iteration: 1351 / 2000 [ 67%]  (Sampling)\nChain 3: Iteration: 1352 / 2000 [ 67%]  (Sampling)\nChain 3: Iteration: 1353 / 2000 [ 67%]  (Sampling)\nChain 3: Iteration: 1354 / 2000 [ 67%]  (Sampling)\nChain 3: Iteration: 1355 / 2000 [ 67%]  (Sampling)\nChain 3: Iteration: 1356 / 2000 [ 67%]  (Sampling)\nChain 3: Iteration: 1357 / 2000 [ 67%]  (Sampling)\nChain 3: Iteration: 1358 / 2000 [ 67%]  (Sampling)\nChain 3: Iteration: 1359 / 2000 [ 67%]  (Sampling)\nChain 3: Iteration: 1360 / 2000 [ 68%]  (Sampling)\nChain 3: Iteration: 1361 / 2000 [ 68%]  (Sampling)\nChain 3: Iteration: 1362 / 2000 [ 68%]  (Sampling)\nChain 3: Iteration: 1363 / 2000 [ 68%]  (Sampling)\nChain 3: Iteration: 1364 / 2000 [ 68%]  (Sampling)\nChain 3: Iteration: 1365 / 2000 [ 68%]  (Sampling)\nChain 3: Iteration: 1366 / 2000 [ 68%]  (Sampling)\nChain 3: Iteration: 1367 / 2000 [ 68%]  (Sampling)\nChain 3: Iteration: 1368 / 2000 [ 68%]  (Sampling)\nChain 3: Iteration: 1369 / 2000 [ 68%]  (Sampling)\nChain 3: Iteration: 1370 / 2000 [ 68%]  (Sampling)\nChain 3: Iteration: 1371 / 2000 [ 68%]  (Sampling)\nChain 3: Iteration: 1372 / 2000 [ 68%]  (Sampling)\nChain 3: Iteration: 1373 / 2000 [ 68%]  (Sampling)\nChain 3: Iteration: 1374 / 2000 [ 68%]  (Sampling)\nChain 3: Iteration: 1375 / 2000 [ 68%]  (Sampling)\nChain 3: Iteration: 1376 / 2000 [ 68%]  (Sampling)\nChain 3: Iteration: 1377 / 2000 [ 68%]  (Sampling)\nChain 3: Iteration: 1378 / 2000 [ 68%]  (Sampling)\nChain 3: Iteration: 1379 / 2000 [ 68%]  (Sampling)\nChain 3: Iteration: 1380 / 2000 [ 69%]  (Sampling)\nChain 3: Iteration: 1381 / 2000 [ 69%]  (Sampling)\nChain 3: Iteration: 1382 / 2000 [ 69%]  (Sampling)\nChain 3: Iteration: 1383 / 2000 [ 69%]  (Sampling)\nChain 3: Iteration: 1384 / 2000 [ 69%]  (Sampling)\nChain 3: Iteration: 1385 / 2000 [ 69%]  (Sampling)\nChain 3: Iteration: 1386 / 2000 [ 69%]  (Sampling)\nChain 3: Iteration: 1387 / 2000 [ 69%]  (Sampling)\nChain 3: Iteration: 1388 / 2000 [ 69%]  (Sampling)\nChain 3: Iteration: 1389 / 2000 [ 69%]  (Sampling)\nChain 3: Iteration: 1390 / 2000 [ 69%]  (Sampling)\nChain 3: Iteration: 1391 / 2000 [ 69%]  (Sampling)\nChain 3: Iteration: 1392 / 2000 [ 69%]  (Sampling)\nChain 3: Iteration: 1393 / 2000 [ 69%]  (Sampling)\nChain 3: Iteration: 1394 / 2000 [ 69%]  (Sampling)\nChain 3: Iteration: 1395 / 2000 [ 69%]  (Sampling)\nChain 3: Iteration: 1396 / 2000 [ 69%]  (Sampling)\nChain 3: Iteration: 1397 / 2000 [ 69%]  (Sampling)\nChain 3: Iteration: 1398 / 2000 [ 69%]  (Sampling)\nChain 3: Iteration: 1399 / 2000 [ 69%]  (Sampling)\nChain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 3: Iteration: 1401 / 2000 [ 70%]  (Sampling)\nChain 3: Iteration: 1402 / 2000 [ 70%]  (Sampling)\nChain 3: Iteration: 1403 / 2000 [ 70%]  (Sampling)\nChain 3: Iteration: 1404 / 2000 [ 70%]  (Sampling)\nChain 3: Iteration: 1405 / 2000 [ 70%]  (Sampling)\nChain 3: Iteration: 1406 / 2000 [ 70%]  (Sampling)\nChain 3: Iteration: 1407 / 2000 [ 70%]  (Sampling)\nChain 3: Iteration: 1408 / 2000 [ 70%]  (Sampling)\nChain 3: Iteration: 1409 / 2000 [ 70%]  (Sampling)\nChain 3: Iteration: 1410 / 2000 [ 70%]  (Sampling)\nChain 3: Iteration: 1411 / 2000 [ 70%]  (Sampling)\nChain 3: Iteration: 1412 / 2000 [ 70%]  (Sampling)\nChain 3: Iteration: 1413 / 2000 [ 70%]  (Sampling)\nChain 3: Iteration: 1414 / 2000 [ 70%]  (Sampling)\nChain 3: Iteration: 1415 / 2000 [ 70%]  (Sampling)\nChain 3: Iteration: 1416 / 2000 [ 70%]  (Sampling)\nChain 3: Iteration: 1417 / 2000 [ 70%]  (Sampling)\nChain 3: Iteration: 1418 / 2000 [ 70%]  (Sampling)\nChain 3: Iteration: 1419 / 2000 [ 70%]  (Sampling)\nChain 3: Iteration: 1420 / 2000 [ 71%]  (Sampling)\nChain 3: Iteration: 1421 / 2000 [ 71%]  (Sampling)\nChain 3: Iteration: 1422 / 2000 [ 71%]  (Sampling)\nChain 3: Iteration: 1423 / 2000 [ 71%]  (Sampling)\nChain 3: Iteration: 1424 / 2000 [ 71%]  (Sampling)\nChain 3: Iteration: 1425 / 2000 [ 71%]  (Sampling)\nChain 3: Iteration: 1426 / 2000 [ 71%]  (Sampling)\nChain 3: Iteration: 1427 / 2000 [ 71%]  (Sampling)\nChain 3: Iteration: 1428 / 2000 [ 71%]  (Sampling)\nChain 3: Iteration: 1429 / 2000 [ 71%]  (Sampling)\nChain 3: Iteration: 1430 / 2000 [ 71%]  (Sampling)\nChain 3: Iteration: 1431 / 2000 [ 71%]  (Sampling)\nChain 3: Iteration: 1432 / 2000 [ 71%]  (Sampling)\nChain 3: Iteration: 1433 / 2000 [ 71%]  (Sampling)\nChain 3: Iteration: 1434 / 2000 [ 71%]  (Sampling)\nChain 3: Iteration: 1435 / 2000 [ 71%]  (Sampling)\nChain 3: Iteration: 1436 / 2000 [ 71%]  (Sampling)\nChain 3: Iteration: 1437 / 2000 [ 71%]  (Sampling)\nChain 3: Iteration: 1438 / 2000 [ 71%]  (Sampling)\nChain 3: Iteration: 1439 / 2000 [ 71%]  (Sampling)\nChain 3: Iteration: 1440 / 2000 [ 72%]  (Sampling)\nChain 3: Iteration: 1441 / 2000 [ 72%]  (Sampling)\nChain 3: Iteration: 1442 / 2000 [ 72%]  (Sampling)\nChain 3: Iteration: 1443 / 2000 [ 72%]  (Sampling)\nChain 3: Iteration: 1444 / 2000 [ 72%]  (Sampling)\nChain 3: Iteration: 1445 / 2000 [ 72%]  (Sampling)\nChain 3: Iteration: 1446 / 2000 [ 72%]  (Sampling)\nChain 3: Iteration: 1447 / 2000 [ 72%]  (Sampling)\nChain 3: Iteration: 1448 / 2000 [ 72%]  (Sampling)\nChain 3: Iteration: 1449 / 2000 [ 72%]  (Sampling)\nChain 3: Iteration: 1450 / 2000 [ 72%]  (Sampling)\nChain 3: Iteration: 1451 / 2000 [ 72%]  (Sampling)\nChain 3: Iteration: 1452 / 2000 [ 72%]  (Sampling)\nChain 3: Iteration: 1453 / 2000 [ 72%]  (Sampling)\nChain 3: Iteration: 1454 / 2000 [ 72%]  (Sampling)\nChain 3: Iteration: 1455 / 2000 [ 72%]  (Sampling)\nChain 3: Iteration: 1456 / 2000 [ 72%]  (Sampling)\nChain 3: Iteration: 1457 / 2000 [ 72%]  (Sampling)\nChain 3: Iteration: 1458 / 2000 [ 72%]  (Sampling)\nChain 3: Iteration: 1459 / 2000 [ 72%]  (Sampling)\nChain 3: Iteration: 1460 / 2000 [ 73%]  (Sampling)\nChain 3: Iteration: 1461 / 2000 [ 73%]  (Sampling)\nChain 3: Iteration: 1462 / 2000 [ 73%]  (Sampling)\nChain 3: Iteration: 1463 / 2000 [ 73%]  (Sampling)\nChain 3: Iteration: 1464 / 2000 [ 73%]  (Sampling)\nChain 3: Iteration: 1465 / 2000 [ 73%]  (Sampling)\nChain 3: Iteration: 1466 / 2000 [ 73%]  (Sampling)\nChain 3: Iteration: 1467 / 2000 [ 73%]  (Sampling)\nChain 3: Iteration: 1468 / 2000 [ 73%]  (Sampling)\nChain 3: Iteration: 1469 / 2000 [ 73%]  (Sampling)\nChain 3: Iteration: 1470 / 2000 [ 73%]  (Sampling)\nChain 3: Iteration: 1471 / 2000 [ 73%]  (Sampling)\nChain 3: Iteration: 1472 / 2000 [ 73%]  (Sampling)\nChain 3: Iteration: 1473 / 2000 [ 73%]  (Sampling)\nChain 3: Iteration: 1474 / 2000 [ 73%]  (Sampling)\nChain 3: Iteration: 1475 / 2000 [ 73%]  (Sampling)\nChain 3: Iteration: 1476 / 2000 [ 73%]  (Sampling)\nChain 3: Iteration: 1477 / 2000 [ 73%]  (Sampling)\nChain 3: Iteration: 1478 / 2000 [ 73%]  (Sampling)\nChain 3: Iteration: 1479 / 2000 [ 73%]  (Sampling)\nChain 3: Iteration: 1480 / 2000 [ 74%]  (Sampling)\nChain 3: Iteration: 1481 / 2000 [ 74%]  (Sampling)\nChain 3: Iteration: 1482 / 2000 [ 74%]  (Sampling)\nChain 3: Iteration: 1483 / 2000 [ 74%]  (Sampling)\nChain 3: Iteration: 1484 / 2000 [ 74%]  (Sampling)\nChain 3: Iteration: 1485 / 2000 [ 74%]  (Sampling)\nChain 3: Iteration: 1486 / 2000 [ 74%]  (Sampling)\nChain 3: Iteration: 1487 / 2000 [ 74%]  (Sampling)\nChain 3: Iteration: 1488 / 2000 [ 74%]  (Sampling)\nChain 3: Iteration: 1489 / 2000 [ 74%]  (Sampling)\nChain 3: Iteration: 1490 / 2000 [ 74%]  (Sampling)\nChain 3: Iteration: 1491 / 2000 [ 74%]  (Sampling)\nChain 3: Iteration: 1492 / 2000 [ 74%]  (Sampling)\nChain 3: Iteration: 1493 / 2000 [ 74%]  (Sampling)\nChain 3: Iteration: 1494 / 2000 [ 74%]  (Sampling)\nChain 3: Iteration: 1495 / 2000 [ 74%]  (Sampling)\nChain 3: Iteration: 1496 / 2000 [ 74%]  (Sampling)\nChain 3: Iteration: 1497 / 2000 [ 74%]  (Sampling)\nChain 3: Iteration: 1498 / 2000 [ 74%]  (Sampling)\nChain 3: Iteration: 1499 / 2000 [ 74%]  (Sampling)\nChain 3: Iteration: 1500 / 2000 [ 75%]  (Sampling)\nChain 3: Iteration: 1501 / 2000 [ 75%]  (Sampling)\nChain 3: Iteration: 1502 / 2000 [ 75%]  (Sampling)\nChain 3: Iteration: 1503 / 2000 [ 75%]  (Sampling)\nChain 3: Iteration: 1504 / 2000 [ 75%]  (Sampling)\nChain 3: Iteration: 1505 / 2000 [ 75%]  (Sampling)\nChain 3: Iteration: 1506 / 2000 [ 75%]  (Sampling)\nChain 3: Iteration: 1507 / 2000 [ 75%]  (Sampling)\nChain 3: Iteration: 1508 / 2000 [ 75%]  (Sampling)\nChain 3: Iteration: 1509 / 2000 [ 75%]  (Sampling)\nChain 3: Iteration: 1510 / 2000 [ 75%]  (Sampling)\nChain 3: Iteration: 1511 / 2000 [ 75%]  (Sampling)\nChain 3: Iteration: 1512 / 2000 [ 75%]  (Sampling)\nChain 3: Iteration: 1513 / 2000 [ 75%]  (Sampling)\nChain 3: Iteration: 1514 / 2000 [ 75%]  (Sampling)\nChain 3: Iteration: 1515 / 2000 [ 75%]  (Sampling)\nChain 3: Iteration: 1516 / 2000 [ 75%]  (Sampling)\nChain 3: Iteration: 1517 / 2000 [ 75%]  (Sampling)\nChain 3: Iteration: 1518 / 2000 [ 75%]  (Sampling)\nChain 3: Iteration: 1519 / 2000 [ 75%]  (Sampling)\nChain 3: Iteration: 1520 / 2000 [ 76%]  (Sampling)\nChain 3: Iteration: 1521 / 2000 [ 76%]  (Sampling)\nChain 3: Iteration: 1522 / 2000 [ 76%]  (Sampling)\nChain 3: Iteration: 1523 / 2000 [ 76%]  (Sampling)\nChain 3: Iteration: 1524 / 2000 [ 76%]  (Sampling)\nChain 3: Iteration: 1525 / 2000 [ 76%]  (Sampling)\nChain 3: Iteration: 1526 / 2000 [ 76%]  (Sampling)\nChain 3: Iteration: 1527 / 2000 [ 76%]  (Sampling)\nChain 3: Iteration: 1528 / 2000 [ 76%]  (Sampling)\nChain 3: Iteration: 1529 / 2000 [ 76%]  (Sampling)\nChain 3: Iteration: 1530 / 2000 [ 76%]  (Sampling)\nChain 3: Iteration: 1531 / 2000 [ 76%]  (Sampling)\nChain 3: Iteration: 1532 / 2000 [ 76%]  (Sampling)\nChain 3: Iteration: 1533 / 2000 [ 76%]  (Sampling)\nChain 3: Iteration: 1534 / 2000 [ 76%]  (Sampling)\nChain 3: Iteration: 1535 / 2000 [ 76%]  (Sampling)\nChain 3: Iteration: 1536 / 2000 [ 76%]  (Sampling)\nChain 3: Iteration: 1537 / 2000 [ 76%]  (Sampling)\nChain 3: Iteration: 1538 / 2000 [ 76%]  (Sampling)\nChain 3: Iteration: 1539 / 2000 [ 76%]  (Sampling)\nChain 3: Iteration: 1540 / 2000 [ 77%]  (Sampling)\nChain 3: Iteration: 1541 / 2000 [ 77%]  (Sampling)\nChain 3: Iteration: 1542 / 2000 [ 77%]  (Sampling)\nChain 3: Iteration: 1543 / 2000 [ 77%]  (Sampling)\nChain 3: Iteration: 1544 / 2000 [ 77%]  (Sampling)\nChain 3: Iteration: 1545 / 2000 [ 77%]  (Sampling)\nChain 3: Iteration: 1546 / 2000 [ 77%]  (Sampling)\nChain 3: Iteration: 1547 / 2000 [ 77%]  (Sampling)\nChain 3: Iteration: 1548 / 2000 [ 77%]  (Sampling)\nChain 3: Iteration: 1549 / 2000 [ 77%]  (Sampling)\nChain 3: Iteration: 1550 / 2000 [ 77%]  (Sampling)\nChain 3: Iteration: 1551 / 2000 [ 77%]  (Sampling)\nChain 3: Iteration: 1552 / 2000 [ 77%]  (Sampling)\nChain 3: Iteration: 1553 / 2000 [ 77%]  (Sampling)\nChain 3: Iteration: 1554 / 2000 [ 77%]  (Sampling)\nChain 3: Iteration: 1555 / 2000 [ 77%]  (Sampling)\nChain 3: Iteration: 1556 / 2000 [ 77%]  (Sampling)\nChain 3: Iteration: 1557 / 2000 [ 77%]  (Sampling)\nChain 3: Iteration: 1558 / 2000 [ 77%]  (Sampling)\nChain 3: Iteration: 1559 / 2000 [ 77%]  (Sampling)\nChain 3: Iteration: 1560 / 2000 [ 78%]  (Sampling)\nChain 3: Iteration: 1561 / 2000 [ 78%]  (Sampling)\nChain 3: Iteration: 1562 / 2000 [ 78%]  (Sampling)\nChain 3: Iteration: 1563 / 2000 [ 78%]  (Sampling)\nChain 3: Iteration: 1564 / 2000 [ 78%]  (Sampling)\nChain 3: Iteration: 1565 / 2000 [ 78%]  (Sampling)\nChain 3: Iteration: 1566 / 2000 [ 78%]  (Sampling)\nChain 3: Iteration: 1567 / 2000 [ 78%]  (Sampling)\nChain 3: Iteration: 1568 / 2000 [ 78%]  (Sampling)\nChain 3: Iteration: 1569 / 2000 [ 78%]  (Sampling)\nChain 3: Iteration: 1570 / 2000 [ 78%]  (Sampling)\nChain 3: Iteration: 1571 / 2000 [ 78%]  (Sampling)\nChain 3: Iteration: 1572 / 2000 [ 78%]  (Sampling)\nChain 3: Iteration: 1573 / 2000 [ 78%]  (Sampling)\nChain 3: Iteration: 1574 / 2000 [ 78%]  (Sampling)\nChain 3: Iteration: 1575 / 2000 [ 78%]  (Sampling)\nChain 3: Iteration: 1576 / 2000 [ 78%]  (Sampling)\nChain 3: Iteration: 1577 / 2000 [ 78%]  (Sampling)\nChain 3: Iteration: 1578 / 2000 [ 78%]  (Sampling)\nChain 3: Iteration: 1579 / 2000 [ 78%]  (Sampling)\nChain 3: Iteration: 1580 / 2000 [ 79%]  (Sampling)\nChain 3: Iteration: 1581 / 2000 [ 79%]  (Sampling)\nChain 3: Iteration: 1582 / 2000 [ 79%]  (Sampling)\nChain 3: Iteration: 1583 / 2000 [ 79%]  (Sampling)\nChain 3: Iteration: 1584 / 2000 [ 79%]  (Sampling)\nChain 3: Iteration: 1585 / 2000 [ 79%]  (Sampling)\nChain 3: Iteration: 1586 / 2000 [ 79%]  (Sampling)\nChain 3: Iteration: 1587 / 2000 [ 79%]  (Sampling)\nChain 3: Iteration: 1588 / 2000 [ 79%]  (Sampling)\nChain 3: Iteration: 1589 / 2000 [ 79%]  (Sampling)\nChain 3: Iteration: 1590 / 2000 [ 79%]  (Sampling)\nChain 3: Iteration: 1591 / 2000 [ 79%]  (Sampling)\nChain 3: Iteration: 1592 / 2000 [ 79%]  (Sampling)\nChain 3: Iteration: 1593 / 2000 [ 79%]  (Sampling)\nChain 3: Iteration: 1594 / 2000 [ 79%]  (Sampling)\nChain 3: Iteration: 1595 / 2000 [ 79%]  (Sampling)\nChain 3: Iteration: 1596 / 2000 [ 79%]  (Sampling)\nChain 3: Iteration: 1597 / 2000 [ 79%]  (Sampling)\nChain 3: Iteration: 1598 / 2000 [ 79%]  (Sampling)\nChain 3: Iteration: 1599 / 2000 [ 79%]  (Sampling)\nChain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 3: Iteration: 1601 / 2000 [ 80%]  (Sampling)\nChain 3: Iteration: 1602 / 2000 [ 80%]  (Sampling)\nChain 3: Iteration: 1603 / 2000 [ 80%]  (Sampling)\nChain 3: Iteration: 1604 / 2000 [ 80%]  (Sampling)\nChain 3: Iteration: 1605 / 2000 [ 80%]  (Sampling)\nChain 3: Iteration: 1606 / 2000 [ 80%]  (Sampling)\nChain 3: Iteration: 1607 / 2000 [ 80%]  (Sampling)\nChain 3: Iteration: 1608 / 2000 [ 80%]  (Sampling)\nChain 3: Iteration: 1609 / 2000 [ 80%]  (Sampling)\nChain 3: Iteration: 1610 / 2000 [ 80%]  (Sampling)\nChain 3: Iteration: 1611 / 2000 [ 80%]  (Sampling)\nChain 3: Iteration: 1612 / 2000 [ 80%]  (Sampling)\nChain 3: Iteration: 1613 / 2000 [ 80%]  (Sampling)\nChain 3: Iteration: 1614 / 2000 [ 80%]  (Sampling)\nChain 3: Iteration: 1615 / 2000 [ 80%]  (Sampling)\nChain 3: Iteration: 1616 / 2000 [ 80%]  (Sampling)\nChain 3: Iteration: 1617 / 2000 [ 80%]  (Sampling)\nChain 3: Iteration: 1618 / 2000 [ 80%]  (Sampling)\nChain 3: Iteration: 1619 / 2000 [ 80%]  (Sampling)\nChain 3: Iteration: 1620 / 2000 [ 81%]  (Sampling)\nChain 3: Iteration: 1621 / 2000 [ 81%]  (Sampling)\nChain 3: Iteration: 1622 / 2000 [ 81%]  (Sampling)\nChain 3: Iteration: 1623 / 2000 [ 81%]  (Sampling)\nChain 3: Iteration: 1624 / 2000 [ 81%]  (Sampling)\nChain 3: Iteration: 1625 / 2000 [ 81%]  (Sampling)\nChain 3: Iteration: 1626 / 2000 [ 81%]  (Sampling)\nChain 3: Iteration: 1627 / 2000 [ 81%]  (Sampling)\nChain 3: Iteration: 1628 / 2000 [ 81%]  (Sampling)\nChain 3: Iteration: 1629 / 2000 [ 81%]  (Sampling)\nChain 3: Iteration: 1630 / 2000 [ 81%]  (Sampling)\nChain 3: Iteration: 1631 / 2000 [ 81%]  (Sampling)\nChain 3: Iteration: 1632 / 2000 [ 81%]  (Sampling)\nChain 3: Iteration: 1633 / 2000 [ 81%]  (Sampling)\nChain 3: Iteration: 1634 / 2000 [ 81%]  (Sampling)\nChain 3: Iteration: 1635 / 2000 [ 81%]  (Sampling)\nChain 3: Iteration: 1636 / 2000 [ 81%]  (Sampling)\nChain 3: Iteration: 1637 / 2000 [ 81%]  (Sampling)\nChain 3: Iteration: 1638 / 2000 [ 81%]  (Sampling)\nChain 3: Iteration: 1639 / 2000 [ 81%]  (Sampling)\nChain 3: Iteration: 1640 / 2000 [ 82%]  (Sampling)\nChain 3: Iteration: 1641 / 2000 [ 82%]  (Sampling)\nChain 3: Iteration: 1642 / 2000 [ 82%]  (Sampling)\nChain 3: Iteration: 1643 / 2000 [ 82%]  (Sampling)\nChain 3: Iteration: 1644 / 2000 [ 82%]  (Sampling)\nChain 3: Iteration: 1645 / 2000 [ 82%]  (Sampling)\nChain 3: Iteration: 1646 / 2000 [ 82%]  (Sampling)\nChain 3: Iteration: 1647 / 2000 [ 82%]  (Sampling)\nChain 3: Iteration: 1648 / 2000 [ 82%]  (Sampling)\nChain 3: Iteration: 1649 / 2000 [ 82%]  (Sampling)\nChain 3: Iteration: 1650 / 2000 [ 82%]  (Sampling)\nChain 3: Iteration: 1651 / 2000 [ 82%]  (Sampling)\nChain 3: Iteration: 1652 / 2000 [ 82%]  (Sampling)\nChain 3: Iteration: 1653 / 2000 [ 82%]  (Sampling)\nChain 3: Iteration: 1654 / 2000 [ 82%]  (Sampling)\nChain 3: Iteration: 1655 / 2000 [ 82%]  (Sampling)\nChain 3: Iteration: 1656 / 2000 [ 82%]  (Sampling)\nChain 3: Iteration: 1657 / 2000 [ 82%]  (Sampling)\nChain 3: Iteration: 1658 / 2000 [ 82%]  (Sampling)\nChain 3: Iteration: 1659 / 2000 [ 82%]  (Sampling)\nChain 3: Iteration: 1660 / 2000 [ 83%]  (Sampling)\nChain 3: Iteration: 1661 / 2000 [ 83%]  (Sampling)\nChain 3: Iteration: 1662 / 2000 [ 83%]  (Sampling)\nChain 3: Iteration: 1663 / 2000 [ 83%]  (Sampling)\nChain 3: Iteration: 1664 / 2000 [ 83%]  (Sampling)\nChain 3: Iteration: 1665 / 2000 [ 83%]  (Sampling)\nChain 3: Iteration: 1666 / 2000 [ 83%]  (Sampling)\nChain 3: Iteration: 1667 / 2000 [ 83%]  (Sampling)\nChain 3: Iteration: 1668 / 2000 [ 83%]  (Sampling)\nChain 3: Iteration: 1669 / 2000 [ 83%]  (Sampling)\nChain 3: Iteration: 1670 / 2000 [ 83%]  (Sampling)\nChain 3: Iteration: 1671 / 2000 [ 83%]  (Sampling)\nChain 3: Iteration: 1672 / 2000 [ 83%]  (Sampling)\nChain 3: Iteration: 1673 / 2000 [ 83%]  (Sampling)\nChain 3: Iteration: 1674 / 2000 [ 83%]  (Sampling)\nChain 3: Iteration: 1675 / 2000 [ 83%]  (Sampling)\nChain 3: Iteration: 1676 / 2000 [ 83%]  (Sampling)\nChain 3: Iteration: 1677 / 2000 [ 83%]  (Sampling)\nChain 3: Iteration: 1678 / 2000 [ 83%]  (Sampling)\nChain 3: Iteration: 1679 / 2000 [ 83%]  (Sampling)\nChain 3: Iteration: 1680 / 2000 [ 84%]  (Sampling)\nChain 3: Iteration: 1681 / 2000 [ 84%]  (Sampling)\nChain 3: Iteration: 1682 / 2000 [ 84%]  (Sampling)\nChain 3: Iteration: 1683 / 2000 [ 84%]  (Sampling)\nChain 3: Iteration: 1684 / 2000 [ 84%]  (Sampling)\nChain 3: Iteration: 1685 / 2000 [ 84%]  (Sampling)\nChain 3: Iteration: 1686 / 2000 [ 84%]  (Sampling)\nChain 3: Iteration: 1687 / 2000 [ 84%]  (Sampling)\nChain 3: Iteration: 1688 / 2000 [ 84%]  (Sampling)\nChain 3: Iteration: 1689 / 2000 [ 84%]  (Sampling)\nChain 3: Iteration: 1690 / 2000 [ 84%]  (Sampling)\nChain 3: Iteration: 1691 / 2000 [ 84%]  (Sampling)\nChain 3: Iteration: 1692 / 2000 [ 84%]  (Sampling)\nChain 3: Iteration: 1693 / 2000 [ 84%]  (Sampling)\nChain 3: Iteration: 1694 / 2000 [ 84%]  (Sampling)\nChain 3: Iteration: 1695 / 2000 [ 84%]  (Sampling)\nChain 3: Iteration: 1696 / 2000 [ 84%]  (Sampling)\nChain 3: Iteration: 1697 / 2000 [ 84%]  (Sampling)\nChain 3: Iteration: 1698 / 2000 [ 84%]  (Sampling)\nChain 3: Iteration: 1699 / 2000 [ 84%]  (Sampling)\nChain 3: Iteration: 1700 / 2000 [ 85%]  (Sampling)\nChain 3: Iteration: 1701 / 2000 [ 85%]  (Sampling)\nChain 3: Iteration: 1702 / 2000 [ 85%]  (Sampling)\nChain 3: Iteration: 1703 / 2000 [ 85%]  (Sampling)\nChain 3: Iteration: 1704 / 2000 [ 85%]  (Sampling)\nChain 3: Iteration: 1705 / 2000 [ 85%]  (Sampling)\nChain 3: Iteration: 1706 / 2000 [ 85%]  (Sampling)\nChain 3: Iteration: 1707 / 2000 [ 85%]  (Sampling)\nChain 3: Iteration: 1708 / 2000 [ 85%]  (Sampling)\nChain 3: Iteration: 1709 / 2000 [ 85%]  (Sampling)\nChain 3: Iteration: 1710 / 2000 [ 85%]  (Sampling)\nChain 3: Iteration: 1711 / 2000 [ 85%]  (Sampling)\nChain 3: Iteration: 1712 / 2000 [ 85%]  (Sampling)\nChain 3: Iteration: 1713 / 2000 [ 85%]  (Sampling)\nChain 3: Iteration: 1714 / 2000 [ 85%]  (Sampling)\nChain 3: Iteration: 1715 / 2000 [ 85%]  (Sampling)\nChain 3: Iteration: 1716 / 2000 [ 85%]  (Sampling)\nChain 3: Iteration: 1717 / 2000 [ 85%]  (Sampling)\nChain 3: Iteration: 1718 / 2000 [ 85%]  (Sampling)\nChain 3: Iteration: 1719 / 2000 [ 85%]  (Sampling)\nChain 3: Iteration: 1720 / 2000 [ 86%]  (Sampling)\nChain 3: Iteration: 1721 / 2000 [ 86%]  (Sampling)\nChain 3: Iteration: 1722 / 2000 [ 86%]  (Sampling)\nChain 3: Iteration: 1723 / 2000 [ 86%]  (Sampling)\nChain 3: Iteration: 1724 / 2000 [ 86%]  (Sampling)\nChain 3: Iteration: 1725 / 2000 [ 86%]  (Sampling)\nChain 3: Iteration: 1726 / 2000 [ 86%]  (Sampling)\nChain 3: Iteration: 1727 / 2000 [ 86%]  (Sampling)\nChain 3: Iteration: 1728 / 2000 [ 86%]  (Sampling)\nChain 3: Iteration: 1729 / 2000 [ 86%]  (Sampling)\nChain 3: Iteration: 1730 / 2000 [ 86%]  (Sampling)\nChain 3: Iteration: 1731 / 2000 [ 86%]  (Sampling)\nChain 3: Iteration: 1732 / 2000 [ 86%]  (Sampling)\nChain 3: Iteration: 1733 / 2000 [ 86%]  (Sampling)\nChain 3: Iteration: 1734 / 2000 [ 86%]  (Sampling)\nChain 3: Iteration: 1735 / 2000 [ 86%]  (Sampling)\nChain 3: Iteration: 1736 / 2000 [ 86%]  (Sampling)\nChain 3: Iteration: 1737 / 2000 [ 86%]  (Sampling)\nChain 3: Iteration: 1738 / 2000 [ 86%]  (Sampling)\nChain 3: Iteration: 1739 / 2000 [ 86%]  (Sampling)\nChain 3: Iteration: 1740 / 2000 [ 87%]  (Sampling)\nChain 3: Iteration: 1741 / 2000 [ 87%]  (Sampling)\nChain 3: Iteration: 1742 / 2000 [ 87%]  (Sampling)\nChain 3: Iteration: 1743 / 2000 [ 87%]  (Sampling)\nChain 3: Iteration: 1744 / 2000 [ 87%]  (Sampling)\nChain 3: Iteration: 1745 / 2000 [ 87%]  (Sampling)\nChain 3: Iteration: 1746 / 2000 [ 87%]  (Sampling)\nChain 3: Iteration: 1747 / 2000 [ 87%]  (Sampling)\nChain 3: Iteration: 1748 / 2000 [ 87%]  (Sampling)\nChain 3: Iteration: 1749 / 2000 [ 87%]  (Sampling)\nChain 3: Iteration: 1750 / 2000 [ 87%]  (Sampling)\nChain 3: Iteration: 1751 / 2000 [ 87%]  (Sampling)\nChain 3: Iteration: 1752 / 2000 [ 87%]  (Sampling)\nChain 3: Iteration: 1753 / 2000 [ 87%]  (Sampling)\nChain 3: Iteration: 1754 / 2000 [ 87%]  (Sampling)\nChain 3: Iteration: 1755 / 2000 [ 87%]  (Sampling)\nChain 3: Iteration: 1756 / 2000 [ 87%]  (Sampling)\nChain 3: Iteration: 1757 / 2000 [ 87%]  (Sampling)\nChain 3: Iteration: 1758 / 2000 [ 87%]  (Sampling)\nChain 3: Iteration: 1759 / 2000 [ 87%]  (Sampling)\nChain 3: Iteration: 1760 / 2000 [ 88%]  (Sampling)\nChain 3: Iteration: 1761 / 2000 [ 88%]  (Sampling)\nChain 3: Iteration: 1762 / 2000 [ 88%]  (Sampling)\nChain 3: Iteration: 1763 / 2000 [ 88%]  (Sampling)\nChain 3: Iteration: 1764 / 2000 [ 88%]  (Sampling)\nChain 3: Iteration: 1765 / 2000 [ 88%]  (Sampling)\nChain 3: Iteration: 1766 / 2000 [ 88%]  (Sampling)\nChain 3: Iteration: 1767 / 2000 [ 88%]  (Sampling)\nChain 3: Iteration: 1768 / 2000 [ 88%]  (Sampling)\nChain 3: Iteration: 1769 / 2000 [ 88%]  (Sampling)\nChain 3: Iteration: 1770 / 2000 [ 88%]  (Sampling)\nChain 3: Iteration: 1771 / 2000 [ 88%]  (Sampling)\nChain 3: Iteration: 1772 / 2000 [ 88%]  (Sampling)\nChain 3: Iteration: 1773 / 2000 [ 88%]  (Sampling)\nChain 3: Iteration: 1774 / 2000 [ 88%]  (Sampling)\nChain 3: Iteration: 1775 / 2000 [ 88%]  (Sampling)\nChain 3: Iteration: 1776 / 2000 [ 88%]  (Sampling)\nChain 3: Iteration: 1777 / 2000 [ 88%]  (Sampling)\nChain 3: Iteration: 1778 / 2000 [ 88%]  (Sampling)\nChain 3: Iteration: 1779 / 2000 [ 88%]  (Sampling)\nChain 3: Iteration: 1780 / 2000 [ 89%]  (Sampling)\nChain 3: Iteration: 1781 / 2000 [ 89%]  (Sampling)\nChain 3: Iteration: 1782 / 2000 [ 89%]  (Sampling)\nChain 3: Iteration: 1783 / 2000 [ 89%]  (Sampling)\nChain 3: Iteration: 1784 / 2000 [ 89%]  (Sampling)\nChain 3: Iteration: 1785 / 2000 [ 89%]  (Sampling)\nChain 3: Iteration: 1786 / 2000 [ 89%]  (Sampling)\nChain 3: Iteration: 1787 / 2000 [ 89%]  (Sampling)\nChain 3: Iteration: 1788 / 2000 [ 89%]  (Sampling)\nChain 3: Iteration: 1789 / 2000 [ 89%]  (Sampling)\nChain 3: Iteration: 1790 / 2000 [ 89%]  (Sampling)\nChain 3: Iteration: 1791 / 2000 [ 89%]  (Sampling)\nChain 3: Iteration: 1792 / 2000 [ 89%]  (Sampling)\nChain 3: Iteration: 1793 / 2000 [ 89%]  (Sampling)\nChain 3: Iteration: 1794 / 2000 [ 89%]  (Sampling)\nChain 3: Iteration: 1795 / 2000 [ 89%]  (Sampling)\nChain 3: Iteration: 1796 / 2000 [ 89%]  (Sampling)\nChain 3: Iteration: 1797 / 2000 [ 89%]  (Sampling)\nChain 3: Iteration: 1798 / 2000 [ 89%]  (Sampling)\nChain 3: Iteration: 1799 / 2000 [ 89%]  (Sampling)\nChain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 3: Iteration: 1801 / 2000 [ 90%]  (Sampling)\nChain 3: Iteration: 1802 / 2000 [ 90%]  (Sampling)\nChain 3: Iteration: 1803 / 2000 [ 90%]  (Sampling)\nChain 3: Iteration: 1804 / 2000 [ 90%]  (Sampling)\nChain 3: Iteration: 1805 / 2000 [ 90%]  (Sampling)\nChain 3: Iteration: 1806 / 2000 [ 90%]  (Sampling)\nChain 3: Iteration: 1807 / 2000 [ 90%]  (Sampling)\nChain 3: Iteration: 1808 / 2000 [ 90%]  (Sampling)\nChain 3: Iteration: 1809 / 2000 [ 90%]  (Sampling)\nChain 3: Iteration: 1810 / 2000 [ 90%]  (Sampling)\nChain 3: Iteration: 1811 / 2000 [ 90%]  (Sampling)\nChain 3: Iteration: 1812 / 2000 [ 90%]  (Sampling)\nChain 3: Iteration: 1813 / 2000 [ 90%]  (Sampling)\nChain 3: Iteration: 1814 / 2000 [ 90%]  (Sampling)\nChain 3: Iteration: 1815 / 2000 [ 90%]  (Sampling)\nChain 3: Iteration: 1816 / 2000 [ 90%]  (Sampling)\nChain 3: Iteration: 1817 / 2000 [ 90%]  (Sampling)\nChain 3: Iteration: 1818 / 2000 [ 90%]  (Sampling)\nChain 3: Iteration: 1819 / 2000 [ 90%]  (Sampling)\nChain 3: Iteration: 1820 / 2000 [ 91%]  (Sampling)\nChain 3: Iteration: 1821 / 2000 [ 91%]  (Sampling)\nChain 3: Iteration: 1822 / 2000 [ 91%]  (Sampling)\nChain 3: Iteration: 1823 / 2000 [ 91%]  (Sampling)\nChain 3: Iteration: 1824 / 2000 [ 91%]  (Sampling)\nChain 3: Iteration: 1825 / 2000 [ 91%]  (Sampling)\nChain 3: Iteration: 1826 / 2000 [ 91%]  (Sampling)\nChain 3: Iteration: 1827 / 2000 [ 91%]  (Sampling)\nChain 3: Iteration: 1828 / 2000 [ 91%]  (Sampling)\nChain 3: Iteration: 1829 / 2000 [ 91%]  (Sampling)\nChain 3: Iteration: 1830 / 2000 [ 91%]  (Sampling)\nChain 3: Iteration: 1831 / 2000 [ 91%]  (Sampling)\nChain 3: Iteration: 1832 / 2000 [ 91%]  (Sampling)\nChain 3: Iteration: 1833 / 2000 [ 91%]  (Sampling)\nChain 3: Iteration: 1834 / 2000 [ 91%]  (Sampling)\nChain 3: Iteration: 1835 / 2000 [ 91%]  (Sampling)\nChain 3: Iteration: 1836 / 2000 [ 91%]  (Sampling)\nChain 3: Iteration: 1837 / 2000 [ 91%]  (Sampling)\nChain 3: Iteration: 1838 / 2000 [ 91%]  (Sampling)\nChain 3: Iteration: 1839 / 2000 [ 91%]  (Sampling)\nChain 3: Iteration: 1840 / 2000 [ 92%]  (Sampling)\nChain 3: Iteration: 1841 / 2000 [ 92%]  (Sampling)\nChain 3: Iteration: 1842 / 2000 [ 92%]  (Sampling)\nChain 3: Iteration: 1843 / 2000 [ 92%]  (Sampling)\nChain 3: Iteration: 1844 / 2000 [ 92%]  (Sampling)\nChain 3: Iteration: 1845 / 2000 [ 92%]  (Sampling)\nChain 3: Iteration: 1846 / 2000 [ 92%]  (Sampling)\nChain 3: Iteration: 1847 / 2000 [ 92%]  (Sampling)\nChain 3: Iteration: 1848 / 2000 [ 92%]  (Sampling)\nChain 3: Iteration: 1849 / 2000 [ 92%]  (Sampling)\nChain 3: Iteration: 1850 / 2000 [ 92%]  (Sampling)\nChain 3: Iteration: 1851 / 2000 [ 92%]  (Sampling)\nChain 3: Iteration: 1852 / 2000 [ 92%]  (Sampling)\nChain 3: Iteration: 1853 / 2000 [ 92%]  (Sampling)\nChain 3: Iteration: 1854 / 2000 [ 92%]  (Sampling)\nChain 3: Iteration: 1855 / 2000 [ 92%]  (Sampling)\nChain 3: Iteration: 1856 / 2000 [ 92%]  (Sampling)\nChain 3: Iteration: 1857 / 2000 [ 92%]  (Sampling)\nChain 3: Iteration: 1858 / 2000 [ 92%]  (Sampling)\nChain 3: Iteration: 1859 / 2000 [ 92%]  (Sampling)\nChain 3: Iteration: 1860 / 2000 [ 93%]  (Sampling)\nChain 3: Iteration: 1861 / 2000 [ 93%]  (Sampling)\nChain 3: Iteration: 1862 / 2000 [ 93%]  (Sampling)\nChain 3: Iteration: 1863 / 2000 [ 93%]  (Sampling)\nChain 3: Iteration: 1864 / 2000 [ 93%]  (Sampling)\nChain 3: Iteration: 1865 / 2000 [ 93%]  (Sampling)\nChain 3: Iteration: 1866 / 2000 [ 93%]  (Sampling)\nChain 3: Iteration: 1867 / 2000 [ 93%]  (Sampling)\nChain 3: Iteration: 1868 / 2000 [ 93%]  (Sampling)\nChain 3: Iteration: 1869 / 2000 [ 93%]  (Sampling)\nChain 3: Iteration: 1870 / 2000 [ 93%]  (Sampling)\nChain 3: Iteration: 1871 / 2000 [ 93%]  (Sampling)\nChain 3: Iteration: 1872 / 2000 [ 93%]  (Sampling)\nChain 3: Iteration: 1873 / 2000 [ 93%]  (Sampling)\nChain 3: Iteration: 1874 / 2000 [ 93%]  (Sampling)\nChain 3: Iteration: 1875 / 2000 [ 93%]  (Sampling)\nChain 3: Iteration: 1876 / 2000 [ 93%]  (Sampling)\nChain 3: Iteration: 1877 / 2000 [ 93%]  (Sampling)\nChain 3: Iteration: 1878 / 2000 [ 93%]  (Sampling)\nChain 3: Iteration: 1879 / 2000 [ 93%]  (Sampling)\nChain 3: Iteration: 1880 / 2000 [ 94%]  (Sampling)\nChain 3: Iteration: 1881 / 2000 [ 94%]  (Sampling)\nChain 3: Iteration: 1882 / 2000 [ 94%]  (Sampling)\nChain 3: Iteration: 1883 / 2000 [ 94%]  (Sampling)\nChain 3: Iteration: 1884 / 2000 [ 94%]  (Sampling)\nChain 3: Iteration: 1885 / 2000 [ 94%]  (Sampling)\nChain 3: Iteration: 1886 / 2000 [ 94%]  (Sampling)\nChain 3: Iteration: 1887 / 2000 [ 94%]  (Sampling)\nChain 3: Iteration: 1888 / 2000 [ 94%]  (Sampling)\nChain 3: Iteration: 1889 / 2000 [ 94%]  (Sampling)\nChain 3: Iteration: 1890 / 2000 [ 94%]  (Sampling)\nChain 3: Iteration: 1891 / 2000 [ 94%]  (Sampling)\nChain 3: Iteration: 1892 / 2000 [ 94%]  (Sampling)\nChain 3: Iteration: 1893 / 2000 [ 94%]  (Sampling)\nChain 3: Iteration: 1894 / 2000 [ 94%]  (Sampling)\nChain 3: Iteration: 1895 / 2000 [ 94%]  (Sampling)\nChain 3: Iteration: 1896 / 2000 [ 94%]  (Sampling)\nChain 3: Iteration: 1897 / 2000 [ 94%]  (Sampling)\nChain 3: Iteration: 1898 / 2000 [ 94%]  (Sampling)\nChain 3: Iteration: 1899 / 2000 [ 94%]  (Sampling)\nChain 3: Iteration: 1900 / 2000 [ 95%]  (Sampling)\nChain 3: Iteration: 1901 / 2000 [ 95%]  (Sampling)\nChain 3: Iteration: 1902 / 2000 [ 95%]  (Sampling)\nChain 3: Iteration: 1903 / 2000 [ 95%]  (Sampling)\nChain 3: Iteration: 1904 / 2000 [ 95%]  (Sampling)\nChain 3: Iteration: 1905 / 2000 [ 95%]  (Sampling)\nChain 3: Iteration: 1906 / 2000 [ 95%]  (Sampling)\nChain 3: Iteration: 1907 / 2000 [ 95%]  (Sampling)\nChain 3: Iteration: 1908 / 2000 [ 95%]  (Sampling)\nChain 3: Iteration: 1909 / 2000 [ 95%]  (Sampling)\nChain 3: Iteration: 1910 / 2000 [ 95%]  (Sampling)\nChain 3: Iteration: 1911 / 2000 [ 95%]  (Sampling)\nChain 3: Iteration: 1912 / 2000 [ 95%]  (Sampling)\nChain 3: Iteration: 1913 / 2000 [ 95%]  (Sampling)\nChain 3: Iteration: 1914 / 2000 [ 95%]  (Sampling)\nChain 3: Iteration: 1915 / 2000 [ 95%]  (Sampling)\nChain 3: Iteration: 1916 / 2000 [ 95%]  (Sampling)\nChain 3: Iteration: 1917 / 2000 [ 95%]  (Sampling)\nChain 3: Iteration: 1918 / 2000 [ 95%]  (Sampling)\nChain 3: Iteration: 1919 / 2000 [ 95%]  (Sampling)\nChain 3: Iteration: 1920 / 2000 [ 96%]  (Sampling)\nChain 3: Iteration: 1921 / 2000 [ 96%]  (Sampling)\nChain 3: Iteration: 1922 / 2000 [ 96%]  (Sampling)\nChain 3: Iteration: 1923 / 2000 [ 96%]  (Sampling)\nChain 3: Iteration: 1924 / 2000 [ 96%]  (Sampling)\nChain 3: Iteration: 1925 / 2000 [ 96%]  (Sampling)\nChain 3: Iteration: 1926 / 2000 [ 96%]  (Sampling)\nChain 3: Iteration: 1927 / 2000 [ 96%]  (Sampling)\nChain 3: Iteration: 1928 / 2000 [ 96%]  (Sampling)\nChain 3: Iteration: 1929 / 2000 [ 96%]  (Sampling)\nChain 3: Iteration: 1930 / 2000 [ 96%]  (Sampling)\nChain 3: Iteration: 1931 / 2000 [ 96%]  (Sampling)\nChain 3: Iteration: 1932 / 2000 [ 96%]  (Sampling)\nChain 3: Iteration: 1933 / 2000 [ 96%]  (Sampling)\nChain 3: Iteration: 1934 / 2000 [ 96%]  (Sampling)\nChain 3: Iteration: 1935 / 2000 [ 96%]  (Sampling)\nChain 3: Iteration: 1936 / 2000 [ 96%]  (Sampling)\nChain 3: Iteration: 1937 / 2000 [ 96%]  (Sampling)\nChain 3: Iteration: 1938 / 2000 [ 96%]  (Sampling)\nChain 3: Iteration: 1939 / 2000 [ 96%]  (Sampling)\nChain 3: Iteration: 1940 / 2000 [ 97%]  (Sampling)\nChain 3: Iteration: 1941 / 2000 [ 97%]  (Sampling)\nChain 3: Iteration: 1942 / 2000 [ 97%]  (Sampling)\nChain 3: Iteration: 1943 / 2000 [ 97%]  (Sampling)\nChain 3: Iteration: 1944 / 2000 [ 97%]  (Sampling)\nChain 3: Iteration: 1945 / 2000 [ 97%]  (Sampling)\nChain 3: Iteration: 1946 / 2000 [ 97%]  (Sampling)\nChain 3: Iteration: 1947 / 2000 [ 97%]  (Sampling)\nChain 3: Iteration: 1948 / 2000 [ 97%]  (Sampling)\nChain 3: Iteration: 1949 / 2000 [ 97%]  (Sampling)\nChain 3: Iteration: 1950 / 2000 [ 97%]  (Sampling)\nChain 3: Iteration: 1951 / 2000 [ 97%]  (Sampling)\nChain 3: Iteration: 1952 / 2000 [ 97%]  (Sampling)\nChain 3: Iteration: 1953 / 2000 [ 97%]  (Sampling)\nChain 3: Iteration: 1954 / 2000 [ 97%]  (Sampling)\nChain 3: Iteration: 1955 / 2000 [ 97%]  (Sampling)\nChain 3: Iteration: 1956 / 2000 [ 97%]  (Sampling)\nChain 3: Iteration: 1957 / 2000 [ 97%]  (Sampling)\nChain 3: Iteration: 1958 / 2000 [ 97%]  (Sampling)\nChain 3: Iteration: 1959 / 2000 [ 97%]  (Sampling)\nChain 3: Iteration: 1960 / 2000 [ 98%]  (Sampling)\nChain 3: Iteration: 1961 / 2000 [ 98%]  (Sampling)\nChain 3: Iteration: 1962 / 2000 [ 98%]  (Sampling)\nChain 3: Iteration: 1963 / 2000 [ 98%]  (Sampling)\nChain 3: Iteration: 1964 / 2000 [ 98%]  (Sampling)\nChain 3: Iteration: 1965 / 2000 [ 98%]  (Sampling)\nChain 3: Iteration: 1966 / 2000 [ 98%]  (Sampling)\nChain 3: Iteration: 1967 / 2000 [ 98%]  (Sampling)\nChain 3: Iteration: 1968 / 2000 [ 98%]  (Sampling)\nChain 3: Iteration: 1969 / 2000 [ 98%]  (Sampling)\nChain 3: Iteration: 1970 / 2000 [ 98%]  (Sampling)\nChain 3: Iteration: 1971 / 2000 [ 98%]  (Sampling)\nChain 3: Iteration: 1972 / 2000 [ 98%]  (Sampling)\nChain 3: Iteration: 1973 / 2000 [ 98%]  (Sampling)\nChain 3: Iteration: 1974 / 2000 [ 98%]  (Sampling)\nChain 3: Iteration: 1975 / 2000 [ 98%]  (Sampling)\nChain 3: Iteration: 1976 / 2000 [ 98%]  (Sampling)\nChain 3: Iteration: 1977 / 2000 [ 98%]  (Sampling)\nChain 3: Iteration: 1978 / 2000 [ 98%]  (Sampling)\nChain 3: Iteration: 1979 / 2000 [ 98%]  (Sampling)\nChain 3: Iteration: 1980 / 2000 [ 99%]  (Sampling)\nChain 3: Iteration: 1981 / 2000 [ 99%]  (Sampling)\nChain 3: Iteration: 1982 / 2000 [ 99%]  (Sampling)\nChain 3: Iteration: 1983 / 2000 [ 99%]  (Sampling)\nChain 3: Iteration: 1984 / 2000 [ 99%]  (Sampling)\nChain 3: Iteration: 1985 / 2000 [ 99%]  (Sampling)\nChain 3: Iteration: 1986 / 2000 [ 99%]  (Sampling)\nChain 3: Iteration: 1987 / 2000 [ 99%]  (Sampling)\nChain 3: Iteration: 1988 / 2000 [ 99%]  (Sampling)\nChain 3: Iteration: 1989 / 2000 [ 99%]  (Sampling)\nChain 3: Iteration: 1990 / 2000 [ 99%]  (Sampling)\nChain 3: Iteration: 1991 / 2000 [ 99%]  (Sampling)\nChain 3: Iteration: 1992 / 2000 [ 99%]  (Sampling)\nChain 3: Iteration: 1993 / 2000 [ 99%]  (Sampling)\nChain 3: Iteration: 1994 / 2000 [ 99%]  (Sampling)\nChain 3: Iteration: 1995 / 2000 [ 99%]  (Sampling)\nChain 3: Iteration: 1996 / 2000 [ 99%]  (Sampling)\nChain 3: Iteration: 1997 / 2000 [ 99%]  (Sampling)\nChain 3: Iteration: 1998 / 2000 [ 99%]  (Sampling)\nChain 3: Iteration: 1999 / 2000 [ 99%]  (Sampling)\nChain 3: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 3: \nChain 3:  Elapsed Time: 0.051 seconds (Warm-up)\nChain 3:                0.035 seconds (Sampling)\nChain 3:                0.086 seconds (Total)\nChain 3: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 4).\nChain 4: \nChain 4: Gradient evaluation took 4e-06 seconds\nChain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.04 seconds.\nChain 4: Adjust your expectations accordingly!\nChain 4: \nChain 4: \nChain 4: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 4: Iteration:    2 / 2000 [  0%]  (Warmup)\nChain 4: Iteration:    3 / 2000 [  0%]  (Warmup)\nChain 4: Iteration:    4 / 2000 [  0%]  (Warmup)\nChain 4: Iteration:    5 / 2000 [  0%]  (Warmup)\nChain 4: Iteration:    6 / 2000 [  0%]  (Warmup)\nChain 4: Iteration:    7 / 2000 [  0%]  (Warmup)\nChain 4: Iteration:    8 / 2000 [  0%]  (Warmup)\nChain 4: Iteration:    9 / 2000 [  0%]  (Warmup)\nChain 4: Iteration:   10 / 2000 [  0%]  (Warmup)\nChain 4: Iteration:   11 / 2000 [  0%]  (Warmup)\nChain 4: Iteration:   12 / 2000 [  0%]  (Warmup)\nChain 4: Iteration:   13 / 2000 [  0%]  (Warmup)\nChain 4: Iteration:   14 / 2000 [  0%]  (Warmup)\nChain 4: Iteration:   15 / 2000 [  0%]  (Warmup)\nChain 4: Iteration:   16 / 2000 [  0%]  (Warmup)\nChain 4: Iteration:   17 / 2000 [  0%]  (Warmup)\nChain 4: Iteration:   18 / 2000 [  0%]  (Warmup)\nChain 4: Iteration:   19 / 2000 [  0%]  (Warmup)\nChain 4: Iteration:   20 / 2000 [  1%]  (Warmup)\nChain 4: Iteration:   21 / 2000 [  1%]  (Warmup)\nChain 4: Iteration:   22 / 2000 [  1%]  (Warmup)\nChain 4: Iteration:   23 / 2000 [  1%]  (Warmup)\nChain 4: Iteration:   24 / 2000 [  1%]  (Warmup)\nChain 4: Iteration:   25 / 2000 [  1%]  (Warmup)\nChain 4: Iteration:   26 / 2000 [  1%]  (Warmup)\nChain 4: Iteration:   27 / 2000 [  1%]  (Warmup)\nChain 4: Iteration:   28 / 2000 [  1%]  (Warmup)\nChain 4: Iteration:   29 / 2000 [  1%]  (Warmup)\nChain 4: Iteration:   30 / 2000 [  1%]  (Warmup)\nChain 4: Iteration:   31 / 2000 [  1%]  (Warmup)\nChain 4: Iteration:   32 / 2000 [  1%]  (Warmup)\nChain 4: Iteration:   33 / 2000 [  1%]  (Warmup)\nChain 4: Iteration:   34 / 2000 [  1%]  (Warmup)\nChain 4: Iteration:   35 / 2000 [  1%]  (Warmup)\nChain 4: Iteration:   36 / 2000 [  1%]  (Warmup)\nChain 4: Iteration:   37 / 2000 [  1%]  (Warmup)\nChain 4: Iteration:   38 / 2000 [  1%]  (Warmup)\nChain 4: Iteration:   39 / 2000 [  1%]  (Warmup)\nChain 4: Iteration:   40 / 2000 [  2%]  (Warmup)\nChain 4: Iteration:   41 / 2000 [  2%]  (Warmup)\nChain 4: Iteration:   42 / 2000 [  2%]  (Warmup)\nChain 4: Iteration:   43 / 2000 [  2%]  (Warmup)\nChain 4: Iteration:   44 / 2000 [  2%]  (Warmup)\nChain 4: Iteration:   45 / 2000 [  2%]  (Warmup)\nChain 4: Iteration:   46 / 2000 [  2%]  (Warmup)\nChain 4: Iteration:   47 / 2000 [  2%]  (Warmup)\nChain 4: Iteration:   48 / 2000 [  2%]  (Warmup)\nChain 4: Iteration:   49 / 2000 [  2%]  (Warmup)\nChain 4: Iteration:   50 / 2000 [  2%]  (Warmup)\nChain 4: Iteration:   51 / 2000 [  2%]  (Warmup)\nChain 4: Iteration:   52 / 2000 [  2%]  (Warmup)\nChain 4: Iteration:   53 / 2000 [  2%]  (Warmup)\nChain 4: Iteration:   54 / 2000 [  2%]  (Warmup)\nChain 4: Iteration:   55 / 2000 [  2%]  (Warmup)\nChain 4: Iteration:   56 / 2000 [  2%]  (Warmup)\nChain 4: Iteration:   57 / 2000 [  2%]  (Warmup)\nChain 4: Iteration:   58 / 2000 [  2%]  (Warmup)\nChain 4: Iteration:   59 / 2000 [  2%]  (Warmup)\nChain 4: Iteration:   60 / 2000 [  3%]  (Warmup)\nChain 4: Iteration:   61 / 2000 [  3%]  (Warmup)\nChain 4: Iteration:   62 / 2000 [  3%]  (Warmup)\nChain 4: Iteration:   63 / 2000 [  3%]  (Warmup)\nChain 4: Iteration:   64 / 2000 [  3%]  (Warmup)\nChain 4: Iteration:   65 / 2000 [  3%]  (Warmup)\nChain 4: Iteration:   66 / 2000 [  3%]  (Warmup)\nChain 4: Iteration:   67 / 2000 [  3%]  (Warmup)\nChain 4: Iteration:   68 / 2000 [  3%]  (Warmup)\nChain 4: Iteration:   69 / 2000 [  3%]  (Warmup)\nChain 4: Iteration:   70 / 2000 [  3%]  (Warmup)\nChain 4: Iteration:   71 / 2000 [  3%]  (Warmup)\nChain 4: Iteration:   72 / 2000 [  3%]  (Warmup)\nChain 4: Iteration:   73 / 2000 [  3%]  (Warmup)\nChain 4: Iteration:   74 / 2000 [  3%]  (Warmup)\nChain 4: Iteration:   75 / 2000 [  3%]  (Warmup)\nChain 4: Iteration:   76 / 2000 [  3%]  (Warmup)\nChain 4: Iteration:   77 / 2000 [  3%]  (Warmup)\nChain 4: Iteration:   78 / 2000 [  3%]  (Warmup)\nChain 4: Iteration:   79 / 2000 [  3%]  (Warmup)\nChain 4: Iteration:   80 / 2000 [  4%]  (Warmup)\nChain 4: Iteration:   81 / 2000 [  4%]  (Warmup)\nChain 4: Iteration:   82 / 2000 [  4%]  (Warmup)\nChain 4: Iteration:   83 / 2000 [  4%]  (Warmup)\nChain 4: Iteration:   84 / 2000 [  4%]  (Warmup)\nChain 4: Iteration:   85 / 2000 [  4%]  (Warmup)\nChain 4: Iteration:   86 / 2000 [  4%]  (Warmup)\nChain 4: Iteration:   87 / 2000 [  4%]  (Warmup)\nChain 4: Iteration:   88 / 2000 [  4%]  (Warmup)\nChain 4: Iteration:   89 / 2000 [  4%]  (Warmup)\nChain 4: Iteration:   90 / 2000 [  4%]  (Warmup)\nChain 4: Iteration:   91 / 2000 [  4%]  (Warmup)\nChain 4: Iteration:   92 / 2000 [  4%]  (Warmup)\nChain 4: Iteration:   93 / 2000 [  4%]  (Warmup)\nChain 4: Iteration:   94 / 2000 [  4%]  (Warmup)\nChain 4: Iteration:   95 / 2000 [  4%]  (Warmup)\nChain 4: Iteration:   96 / 2000 [  4%]  (Warmup)\nChain 4: Iteration:   97 / 2000 [  4%]  (Warmup)\nChain 4: Iteration:   98 / 2000 [  4%]  (Warmup)\nChain 4: Iteration:   99 / 2000 [  4%]  (Warmup)\nChain 4: Iteration:  100 / 2000 [  5%]  (Warmup)\nChain 4: Iteration:  101 / 2000 [  5%]  (Warmup)\nChain 4: Iteration:  102 / 2000 [  5%]  (Warmup)\nChain 4: Iteration:  103 / 2000 [  5%]  (Warmup)\nChain 4: Iteration:  104 / 2000 [  5%]  (Warmup)\nChain 4: Iteration:  105 / 2000 [  5%]  (Warmup)\nChain 4: Iteration:  106 / 2000 [  5%]  (Warmup)\nChain 4: Iteration:  107 / 2000 [  5%]  (Warmup)\nChain 4: Iteration:  108 / 2000 [  5%]  (Warmup)\nChain 4: Iteration:  109 / 2000 [  5%]  (Warmup)\nChain 4: Iteration:  110 / 2000 [  5%]  (Warmup)\nChain 4: Iteration:  111 / 2000 [  5%]  (Warmup)\nChain 4: Iteration:  112 / 2000 [  5%]  (Warmup)\nChain 4: Iteration:  113 / 2000 [  5%]  (Warmup)\nChain 4: Iteration:  114 / 2000 [  5%]  (Warmup)\nChain 4: Iteration:  115 / 2000 [  5%]  (Warmup)\nChain 4: Iteration:  116 / 2000 [  5%]  (Warmup)\nChain 4: Iteration:  117 / 2000 [  5%]  (Warmup)\nChain 4: Iteration:  118 / 2000 [  5%]  (Warmup)\nChain 4: Iteration:  119 / 2000 [  5%]  (Warmup)\nChain 4: Iteration:  120 / 2000 [  6%]  (Warmup)\nChain 4: Iteration:  121 / 2000 [  6%]  (Warmup)\nChain 4: Iteration:  122 / 2000 [  6%]  (Warmup)\nChain 4: Iteration:  123 / 2000 [  6%]  (Warmup)\nChain 4: Iteration:  124 / 2000 [  6%]  (Warmup)\nChain 4: Iteration:  125 / 2000 [  6%]  (Warmup)\nChain 4: Iteration:  126 / 2000 [  6%]  (Warmup)\nChain 4: Iteration:  127 / 2000 [  6%]  (Warmup)\nChain 4: Iteration:  128 / 2000 [  6%]  (Warmup)\nChain 4: Iteration:  129 / 2000 [  6%]  (Warmup)\nChain 4: Iteration:  130 / 2000 [  6%]  (Warmup)\nChain 4: Iteration:  131 / 2000 [  6%]  (Warmup)\nChain 4: Iteration:  132 / 2000 [  6%]  (Warmup)\nChain 4: Iteration:  133 / 2000 [  6%]  (Warmup)\nChain 4: Iteration:  134 / 2000 [  6%]  (Warmup)\nChain 4: Iteration:  135 / 2000 [  6%]  (Warmup)\nChain 4: Iteration:  136 / 2000 [  6%]  (Warmup)\nChain 4: Iteration:  137 / 2000 [  6%]  (Warmup)\nChain 4: Iteration:  138 / 2000 [  6%]  (Warmup)\nChain 4: Iteration:  139 / 2000 [  6%]  (Warmup)\nChain 4: Iteration:  140 / 2000 [  7%]  (Warmup)\nChain 4: Iteration:  141 / 2000 [  7%]  (Warmup)\nChain 4: Iteration:  142 / 2000 [  7%]  (Warmup)\nChain 4: Iteration:  143 / 2000 [  7%]  (Warmup)\nChain 4: Iteration:  144 / 2000 [  7%]  (Warmup)\nChain 4: Iteration:  145 / 2000 [  7%]  (Warmup)\nChain 4: Iteration:  146 / 2000 [  7%]  (Warmup)\nChain 4: Iteration:  147 / 2000 [  7%]  (Warmup)\nChain 4: Iteration:  148 / 2000 [  7%]  (Warmup)\nChain 4: Iteration:  149 / 2000 [  7%]  (Warmup)\nChain 4: Iteration:  150 / 2000 [  7%]  (Warmup)\nChain 4: Iteration:  151 / 2000 [  7%]  (Warmup)\nChain 4: Iteration:  152 / 2000 [  7%]  (Warmup)\nChain 4: Iteration:  153 / 2000 [  7%]  (Warmup)\nChain 4: Iteration:  154 / 2000 [  7%]  (Warmup)\nChain 4: Iteration:  155 / 2000 [  7%]  (Warmup)\nChain 4: Iteration:  156 / 2000 [  7%]  (Warmup)\nChain 4: Iteration:  157 / 2000 [  7%]  (Warmup)\nChain 4: Iteration:  158 / 2000 [  7%]  (Warmup)\nChain 4: Iteration:  159 / 2000 [  7%]  (Warmup)\nChain 4: Iteration:  160 / 2000 [  8%]  (Warmup)\nChain 4: Iteration:  161 / 2000 [  8%]  (Warmup)\nChain 4: Iteration:  162 / 2000 [  8%]  (Warmup)\nChain 4: Iteration:  163 / 2000 [  8%]  (Warmup)\nChain 4: Iteration:  164 / 2000 [  8%]  (Warmup)\nChain 4: Iteration:  165 / 2000 [  8%]  (Warmup)\nChain 4: Iteration:  166 / 2000 [  8%]  (Warmup)\nChain 4: Iteration:  167 / 2000 [  8%]  (Warmup)\nChain 4: Iteration:  168 / 2000 [  8%]  (Warmup)\nChain 4: Iteration:  169 / 2000 [  8%]  (Warmup)\nChain 4: Iteration:  170 / 2000 [  8%]  (Warmup)\nChain 4: Iteration:  171 / 2000 [  8%]  (Warmup)\nChain 4: Iteration:  172 / 2000 [  8%]  (Warmup)\nChain 4: Iteration:  173 / 2000 [  8%]  (Warmup)\nChain 4: Iteration:  174 / 2000 [  8%]  (Warmup)\nChain 4: Iteration:  175 / 2000 [  8%]  (Warmup)\nChain 4: Iteration:  176 / 2000 [  8%]  (Warmup)\nChain 4: Iteration:  177 / 2000 [  8%]  (Warmup)\nChain 4: Iteration:  178 / 2000 [  8%]  (Warmup)\nChain 4: Iteration:  179 / 2000 [  8%]  (Warmup)\nChain 4: Iteration:  180 / 2000 [  9%]  (Warmup)\nChain 4: Iteration:  181 / 2000 [  9%]  (Warmup)\nChain 4: Iteration:  182 / 2000 [  9%]  (Warmup)\nChain 4: Iteration:  183 / 2000 [  9%]  (Warmup)\nChain 4: Iteration:  184 / 2000 [  9%]  (Warmup)\nChain 4: Iteration:  185 / 2000 [  9%]  (Warmup)\nChain 4: Iteration:  186 / 2000 [  9%]  (Warmup)\nChain 4: Iteration:  187 / 2000 [  9%]  (Warmup)\nChain 4: Iteration:  188 / 2000 [  9%]  (Warmup)\nChain 4: Iteration:  189 / 2000 [  9%]  (Warmup)\nChain 4: Iteration:  190 / 2000 [  9%]  (Warmup)\nChain 4: Iteration:  191 / 2000 [  9%]  (Warmup)\nChain 4: Iteration:  192 / 2000 [  9%]  (Warmup)\nChain 4: Iteration:  193 / 2000 [  9%]  (Warmup)\nChain 4: Iteration:  194 / 2000 [  9%]  (Warmup)\nChain 4: Iteration:  195 / 2000 [  9%]  (Warmup)\nChain 4: Iteration:  196 / 2000 [  9%]  (Warmup)\nChain 4: Iteration:  197 / 2000 [  9%]  (Warmup)\nChain 4: Iteration:  198 / 2000 [  9%]  (Warmup)\nChain 4: Iteration:  199 / 2000 [  9%]  (Warmup)\nChain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 4: Iteration:  201 / 2000 [ 10%]  (Warmup)\nChain 4: Iteration:  202 / 2000 [ 10%]  (Warmup)\nChain 4: Iteration:  203 / 2000 [ 10%]  (Warmup)\nChain 4: Iteration:  204 / 2000 [ 10%]  (Warmup)\nChain 4: Iteration:  205 / 2000 [ 10%]  (Warmup)\nChain 4: Iteration:  206 / 2000 [ 10%]  (Warmup)\nChain 4: Iteration:  207 / 2000 [ 10%]  (Warmup)\nChain 4: Iteration:  208 / 2000 [ 10%]  (Warmup)\nChain 4: Iteration:  209 / 2000 [ 10%]  (Warmup)\nChain 4: Iteration:  210 / 2000 [ 10%]  (Warmup)\nChain 4: Iteration:  211 / 2000 [ 10%]  (Warmup)\nChain 4: Iteration:  212 / 2000 [ 10%]  (Warmup)\nChain 4: Iteration:  213 / 2000 [ 10%]  (Warmup)\nChain 4: Iteration:  214 / 2000 [ 10%]  (Warmup)\nChain 4: Iteration:  215 / 2000 [ 10%]  (Warmup)\nChain 4: Iteration:  216 / 2000 [ 10%]  (Warmup)\nChain 4: Iteration:  217 / 2000 [ 10%]  (Warmup)\nChain 4: Iteration:  218 / 2000 [ 10%]  (Warmup)\nChain 4: Iteration:  219 / 2000 [ 10%]  (Warmup)\nChain 4: Iteration:  220 / 2000 [ 11%]  (Warmup)\nChain 4: Iteration:  221 / 2000 [ 11%]  (Warmup)\nChain 4: Iteration:  222 / 2000 [ 11%]  (Warmup)\nChain 4: Iteration:  223 / 2000 [ 11%]  (Warmup)\nChain 4: Iteration:  224 / 2000 [ 11%]  (Warmup)\nChain 4: Iteration:  225 / 2000 [ 11%]  (Warmup)\nChain 4: Iteration:  226 / 2000 [ 11%]  (Warmup)\nChain 4: Iteration:  227 / 2000 [ 11%]  (Warmup)\nChain 4: Iteration:  228 / 2000 [ 11%]  (Warmup)\nChain 4: Iteration:  229 / 2000 [ 11%]  (Warmup)\nChain 4: Iteration:  230 / 2000 [ 11%]  (Warmup)\nChain 4: Iteration:  231 / 2000 [ 11%]  (Warmup)\nChain 4: Iteration:  232 / 2000 [ 11%]  (Warmup)\nChain 4: Iteration:  233 / 2000 [ 11%]  (Warmup)\nChain 4: Iteration:  234 / 2000 [ 11%]  (Warmup)\nChain 4: Iteration:  235 / 2000 [ 11%]  (Warmup)\nChain 4: Iteration:  236 / 2000 [ 11%]  (Warmup)\nChain 4: Iteration:  237 / 2000 [ 11%]  (Warmup)\nChain 4: Iteration:  238 / 2000 [ 11%]  (Warmup)\nChain 4: Iteration:  239 / 2000 [ 11%]  (Warmup)\nChain 4: Iteration:  240 / 2000 [ 12%]  (Warmup)\nChain 4: Iteration:  241 / 2000 [ 12%]  (Warmup)\nChain 4: Iteration:  242 / 2000 [ 12%]  (Warmup)\nChain 4: Iteration:  243 / 2000 [ 12%]  (Warmup)\nChain 4: Iteration:  244 / 2000 [ 12%]  (Warmup)\nChain 4: Iteration:  245 / 2000 [ 12%]  (Warmup)\nChain 4: Iteration:  246 / 2000 [ 12%]  (Warmup)\nChain 4: Iteration:  247 / 2000 [ 12%]  (Warmup)\nChain 4: Iteration:  248 / 2000 [ 12%]  (Warmup)\nChain 4: Iteration:  249 / 2000 [ 12%]  (Warmup)\nChain 4: Iteration:  250 / 2000 [ 12%]  (Warmup)\nChain 4: Iteration:  251 / 2000 [ 12%]  (Warmup)\nChain 4: Iteration:  252 / 2000 [ 12%]  (Warmup)\nChain 4: Iteration:  253 / 2000 [ 12%]  (Warmup)\nChain 4: Iteration:  254 / 2000 [ 12%]  (Warmup)\nChain 4: Iteration:  255 / 2000 [ 12%]  (Warmup)\nChain 4: Iteration:  256 / 2000 [ 12%]  (Warmup)\nChain 4: Iteration:  257 / 2000 [ 12%]  (Warmup)\nChain 4: Iteration:  258 / 2000 [ 12%]  (Warmup)\nChain 4: Iteration:  259 / 2000 [ 12%]  (Warmup)\nChain 4: Iteration:  260 / 2000 [ 13%]  (Warmup)\nChain 4: Iteration:  261 / 2000 [ 13%]  (Warmup)\nChain 4: Iteration:  262 / 2000 [ 13%]  (Warmup)\nChain 4: Iteration:  263 / 2000 [ 13%]  (Warmup)\nChain 4: Iteration:  264 / 2000 [ 13%]  (Warmup)\nChain 4: Iteration:  265 / 2000 [ 13%]  (Warmup)\nChain 4: Iteration:  266 / 2000 [ 13%]  (Warmup)\nChain 4: Iteration:  267 / 2000 [ 13%]  (Warmup)\nChain 4: Iteration:  268 / 2000 [ 13%]  (Warmup)\nChain 4: Iteration:  269 / 2000 [ 13%]  (Warmup)\nChain 4: Iteration:  270 / 2000 [ 13%]  (Warmup)\nChain 4: Iteration:  271 / 2000 [ 13%]  (Warmup)\nChain 4: Iteration:  272 / 2000 [ 13%]  (Warmup)\nChain 4: Iteration:  273 / 2000 [ 13%]  (Warmup)\nChain 4: Iteration:  274 / 2000 [ 13%]  (Warmup)\nChain 4: Iteration:  275 / 2000 [ 13%]  (Warmup)\nChain 4: Iteration:  276 / 2000 [ 13%]  (Warmup)\nChain 4: Iteration:  277 / 2000 [ 13%]  (Warmup)\nChain 4: Iteration:  278 / 2000 [ 13%]  (Warmup)\nChain 4: Iteration:  279 / 2000 [ 13%]  (Warmup)\nChain 4: Iteration:  280 / 2000 [ 14%]  (Warmup)\nChain 4: Iteration:  281 / 2000 [ 14%]  (Warmup)\nChain 4: Iteration:  282 / 2000 [ 14%]  (Warmup)\nChain 4: Iteration:  283 / 2000 [ 14%]  (Warmup)\nChain 4: Iteration:  284 / 2000 [ 14%]  (Warmup)\nChain 4: Iteration:  285 / 2000 [ 14%]  (Warmup)\nChain 4: Iteration:  286 / 2000 [ 14%]  (Warmup)\nChain 4: Iteration:  287 / 2000 [ 14%]  (Warmup)\nChain 4: Iteration:  288 / 2000 [ 14%]  (Warmup)\nChain 4: Iteration:  289 / 2000 [ 14%]  (Warmup)\nChain 4: Iteration:  290 / 2000 [ 14%]  (Warmup)\nChain 4: Iteration:  291 / 2000 [ 14%]  (Warmup)\nChain 4: Iteration:  292 / 2000 [ 14%]  (Warmup)\nChain 4: Iteration:  293 / 2000 [ 14%]  (Warmup)\nChain 4: Iteration:  294 / 2000 [ 14%]  (Warmup)\nChain 4: Iteration:  295 / 2000 [ 14%]  (Warmup)\nChain 4: Iteration:  296 / 2000 [ 14%]  (Warmup)\nChain 4: Iteration:  297 / 2000 [ 14%]  (Warmup)\nChain 4: Iteration:  298 / 2000 [ 14%]  (Warmup)\nChain 4: Iteration:  299 / 2000 [ 14%]  (Warmup)\nChain 4: Iteration:  300 / 2000 [ 15%]  (Warmup)\nChain 4: Iteration:  301 / 2000 [ 15%]  (Warmup)\nChain 4: Iteration:  302 / 2000 [ 15%]  (Warmup)\nChain 4: Iteration:  303 / 2000 [ 15%]  (Warmup)\nChain 4: Iteration:  304 / 2000 [ 15%]  (Warmup)\nChain 4: Iteration:  305 / 2000 [ 15%]  (Warmup)\nChain 4: Iteration:  306 / 2000 [ 15%]  (Warmup)\nChain 4: Iteration:  307 / 2000 [ 15%]  (Warmup)\nChain 4: Iteration:  308 / 2000 [ 15%]  (Warmup)\nChain 4: Iteration:  309 / 2000 [ 15%]  (Warmup)\nChain 4: Iteration:  310 / 2000 [ 15%]  (Warmup)\nChain 4: Iteration:  311 / 2000 [ 15%]  (Warmup)\nChain 4: Iteration:  312 / 2000 [ 15%]  (Warmup)\nChain 4: Iteration:  313 / 2000 [ 15%]  (Warmup)\nChain 4: Iteration:  314 / 2000 [ 15%]  (Warmup)\nChain 4: Iteration:  315 / 2000 [ 15%]  (Warmup)\nChain 4: Iteration:  316 / 2000 [ 15%]  (Warmup)\nChain 4: Iteration:  317 / 2000 [ 15%]  (Warmup)\nChain 4: Iteration:  318 / 2000 [ 15%]  (Warmup)\nChain 4: Iteration:  319 / 2000 [ 15%]  (Warmup)\nChain 4: Iteration:  320 / 2000 [ 16%]  (Warmup)\nChain 4: Iteration:  321 / 2000 [ 16%]  (Warmup)\nChain 4: Iteration:  322 / 2000 [ 16%]  (Warmup)\nChain 4: Iteration:  323 / 2000 [ 16%]  (Warmup)\nChain 4: Iteration:  324 / 2000 [ 16%]  (Warmup)\nChain 4: Iteration:  325 / 2000 [ 16%]  (Warmup)\nChain 4: Iteration:  326 / 2000 [ 16%]  (Warmup)\nChain 4: Iteration:  327 / 2000 [ 16%]  (Warmup)\nChain 4: Iteration:  328 / 2000 [ 16%]  (Warmup)\nChain 4: Iteration:  329 / 2000 [ 16%]  (Warmup)\nChain 4: Iteration:  330 / 2000 [ 16%]  (Warmup)\nChain 4: Iteration:  331 / 2000 [ 16%]  (Warmup)\nChain 4: Iteration:  332 / 2000 [ 16%]  (Warmup)\nChain 4: Iteration:  333 / 2000 [ 16%]  (Warmup)\nChain 4: Iteration:  334 / 2000 [ 16%]  (Warmup)\nChain 4: Iteration:  335 / 2000 [ 16%]  (Warmup)\nChain 4: Iteration:  336 / 2000 [ 16%]  (Warmup)\nChain 4: Iteration:  337 / 2000 [ 16%]  (Warmup)\nChain 4: Iteration:  338 / 2000 [ 16%]  (Warmup)\nChain 4: Iteration:  339 / 2000 [ 16%]  (Warmup)\nChain 4: Iteration:  340 / 2000 [ 17%]  (Warmup)\nChain 4: Iteration:  341 / 2000 [ 17%]  (Warmup)\nChain 4: Iteration:  342 / 2000 [ 17%]  (Warmup)\nChain 4: Iteration:  343 / 2000 [ 17%]  (Warmup)\nChain 4: Iteration:  344 / 2000 [ 17%]  (Warmup)\nChain 4: Iteration:  345 / 2000 [ 17%]  (Warmup)\nChain 4: Iteration:  346 / 2000 [ 17%]  (Warmup)\nChain 4: Iteration:  347 / 2000 [ 17%]  (Warmup)\nChain 4: Iteration:  348 / 2000 [ 17%]  (Warmup)\nChain 4: Iteration:  349 / 2000 [ 17%]  (Warmup)\nChain 4: Iteration:  350 / 2000 [ 17%]  (Warmup)\nChain 4: Iteration:  351 / 2000 [ 17%]  (Warmup)\nChain 4: Iteration:  352 / 2000 [ 17%]  (Warmup)\nChain 4: Iteration:  353 / 2000 [ 17%]  (Warmup)\nChain 4: Iteration:  354 / 2000 [ 17%]  (Warmup)\nChain 4: Iteration:  355 / 2000 [ 17%]  (Warmup)\nChain 4: Iteration:  356 / 2000 [ 17%]  (Warmup)\nChain 4: Iteration:  357 / 2000 [ 17%]  (Warmup)\nChain 4: Iteration:  358 / 2000 [ 17%]  (Warmup)\nChain 4: Iteration:  359 / 2000 [ 17%]  (Warmup)\nChain 4: Iteration:  360 / 2000 [ 18%]  (Warmup)\nChain 4: Iteration:  361 / 2000 [ 18%]  (Warmup)\nChain 4: Iteration:  362 / 2000 [ 18%]  (Warmup)\nChain 4: Iteration:  363 / 2000 [ 18%]  (Warmup)\nChain 4: Iteration:  364 / 2000 [ 18%]  (Warmup)\nChain 4: Iteration:  365 / 2000 [ 18%]  (Warmup)\nChain 4: Iteration:  366 / 2000 [ 18%]  (Warmup)\nChain 4: Iteration:  367 / 2000 [ 18%]  (Warmup)\nChain 4: Iteration:  368 / 2000 [ 18%]  (Warmup)\nChain 4: Iteration:  369 / 2000 [ 18%]  (Warmup)\nChain 4: Iteration:  370 / 2000 [ 18%]  (Warmup)\nChain 4: Iteration:  371 / 2000 [ 18%]  (Warmup)\nChain 4: Iteration:  372 / 2000 [ 18%]  (Warmup)\nChain 4: Iteration:  373 / 2000 [ 18%]  (Warmup)\nChain 4: Iteration:  374 / 2000 [ 18%]  (Warmup)\nChain 4: Iteration:  375 / 2000 [ 18%]  (Warmup)\nChain 4: Iteration:  376 / 2000 [ 18%]  (Warmup)\nChain 4: Iteration:  377 / 2000 [ 18%]  (Warmup)\nChain 4: Iteration:  378 / 2000 [ 18%]  (Warmup)\nChain 4: Iteration:  379 / 2000 [ 18%]  (Warmup)\nChain 4: Iteration:  380 / 2000 [ 19%]  (Warmup)\nChain 4: Iteration:  381 / 2000 [ 19%]  (Warmup)\nChain 4: Iteration:  382 / 2000 [ 19%]  (Warmup)\nChain 4: Iteration:  383 / 2000 [ 19%]  (Warmup)\nChain 4: Iteration:  384 / 2000 [ 19%]  (Warmup)\nChain 4: Iteration:  385 / 2000 [ 19%]  (Warmup)\nChain 4: Iteration:  386 / 2000 [ 19%]  (Warmup)\nChain 4: Iteration:  387 / 2000 [ 19%]  (Warmup)\nChain 4: Iteration:  388 / 2000 [ 19%]  (Warmup)\nChain 4: Iteration:  389 / 2000 [ 19%]  (Warmup)\nChain 4: Iteration:  390 / 2000 [ 19%]  (Warmup)\nChain 4: Iteration:  391 / 2000 [ 19%]  (Warmup)\nChain 4: Iteration:  392 / 2000 [ 19%]  (Warmup)\nChain 4: Iteration:  393 / 2000 [ 19%]  (Warmup)\nChain 4: Iteration:  394 / 2000 [ 19%]  (Warmup)\nChain 4: Iteration:  395 / 2000 [ 19%]  (Warmup)\nChain 4: Iteration:  396 / 2000 [ 19%]  (Warmup)\nChain 4: Iteration:  397 / 2000 [ 19%]  (Warmup)\nChain 4: Iteration:  398 / 2000 [ 19%]  (Warmup)\nChain 4: Iteration:  399 / 2000 [ 19%]  (Warmup)\nChain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 4: Iteration:  401 / 2000 [ 20%]  (Warmup)\nChain 4: Iteration:  402 / 2000 [ 20%]  (Warmup)\nChain 4: Iteration:  403 / 2000 [ 20%]  (Warmup)\nChain 4: Iteration:  404 / 2000 [ 20%]  (Warmup)\nChain 4: Iteration:  405 / 2000 [ 20%]  (Warmup)\nChain 4: Iteration:  406 / 2000 [ 20%]  (Warmup)\nChain 4: Iteration:  407 / 2000 [ 20%]  (Warmup)\nChain 4: Iteration:  408 / 2000 [ 20%]  (Warmup)\nChain 4: Iteration:  409 / 2000 [ 20%]  (Warmup)\nChain 4: Iteration:  410 / 2000 [ 20%]  (Warmup)\nChain 4: Iteration:  411 / 2000 [ 20%]  (Warmup)\nChain 4: Iteration:  412 / 2000 [ 20%]  (Warmup)\nChain 4: Iteration:  413 / 2000 [ 20%]  (Warmup)\nChain 4: Iteration:  414 / 2000 [ 20%]  (Warmup)\nChain 4: Iteration:  415 / 2000 [ 20%]  (Warmup)\nChain 4: Iteration:  416 / 2000 [ 20%]  (Warmup)\nChain 4: Iteration:  417 / 2000 [ 20%]  (Warmup)\nChain 4: Iteration:  418 / 2000 [ 20%]  (Warmup)\nChain 4: Iteration:  419 / 2000 [ 20%]  (Warmup)\nChain 4: Iteration:  420 / 2000 [ 21%]  (Warmup)\nChain 4: Iteration:  421 / 2000 [ 21%]  (Warmup)\nChain 4: Iteration:  422 / 2000 [ 21%]  (Warmup)\nChain 4: Iteration:  423 / 2000 [ 21%]  (Warmup)\nChain 4: Iteration:  424 / 2000 [ 21%]  (Warmup)\nChain 4: Iteration:  425 / 2000 [ 21%]  (Warmup)\nChain 4: Iteration:  426 / 2000 [ 21%]  (Warmup)\nChain 4: Iteration:  427 / 2000 [ 21%]  (Warmup)\nChain 4: Iteration:  428 / 2000 [ 21%]  (Warmup)\nChain 4: Iteration:  429 / 2000 [ 21%]  (Warmup)\nChain 4: Iteration:  430 / 2000 [ 21%]  (Warmup)\nChain 4: Iteration:  431 / 2000 [ 21%]  (Warmup)\nChain 4: Iteration:  432 / 2000 [ 21%]  (Warmup)\nChain 4: Iteration:  433 / 2000 [ 21%]  (Warmup)\nChain 4: Iteration:  434 / 2000 [ 21%]  (Warmup)\nChain 4: Iteration:  435 / 2000 [ 21%]  (Warmup)\nChain 4: Iteration:  436 / 2000 [ 21%]  (Warmup)\nChain 4: Iteration:  437 / 2000 [ 21%]  (Warmup)\nChain 4: Iteration:  438 / 2000 [ 21%]  (Warmup)\nChain 4: Iteration:  439 / 2000 [ 21%]  (Warmup)\nChain 4: Iteration:  440 / 2000 [ 22%]  (Warmup)\nChain 4: Iteration:  441 / 2000 [ 22%]  (Warmup)\nChain 4: Iteration:  442 / 2000 [ 22%]  (Warmup)\nChain 4: Iteration:  443 / 2000 [ 22%]  (Warmup)\nChain 4: Iteration:  444 / 2000 [ 22%]  (Warmup)\nChain 4: Iteration:  445 / 2000 [ 22%]  (Warmup)\nChain 4: Iteration:  446 / 2000 [ 22%]  (Warmup)\nChain 4: Iteration:  447 / 2000 [ 22%]  (Warmup)\nChain 4: Iteration:  448 / 2000 [ 22%]  (Warmup)\nChain 4: Iteration:  449 / 2000 [ 22%]  (Warmup)\nChain 4: Iteration:  450 / 2000 [ 22%]  (Warmup)\nChain 4: Iteration:  451 / 2000 [ 22%]  (Warmup)\nChain 4: Iteration:  452 / 2000 [ 22%]  (Warmup)\nChain 4: Iteration:  453 / 2000 [ 22%]  (Warmup)\nChain 4: Iteration:  454 / 2000 [ 22%]  (Warmup)\nChain 4: Iteration:  455 / 2000 [ 22%]  (Warmup)\nChain 4: Iteration:  456 / 2000 [ 22%]  (Warmup)\nChain 4: Iteration:  457 / 2000 [ 22%]  (Warmup)\nChain 4: Iteration:  458 / 2000 [ 22%]  (Warmup)\nChain 4: Iteration:  459 / 2000 [ 22%]  (Warmup)\nChain 4: Iteration:  460 / 2000 [ 23%]  (Warmup)\nChain 4: Iteration:  461 / 2000 [ 23%]  (Warmup)\nChain 4: Iteration:  462 / 2000 [ 23%]  (Warmup)\nChain 4: Iteration:  463 / 2000 [ 23%]  (Warmup)\nChain 4: Iteration:  464 / 2000 [ 23%]  (Warmup)\nChain 4: Iteration:  465 / 2000 [ 23%]  (Warmup)\nChain 4: Iteration:  466 / 2000 [ 23%]  (Warmup)\nChain 4: Iteration:  467 / 2000 [ 23%]  (Warmup)\nChain 4: Iteration:  468 / 2000 [ 23%]  (Warmup)\nChain 4: Iteration:  469 / 2000 [ 23%]  (Warmup)\nChain 4: Iteration:  470 / 2000 [ 23%]  (Warmup)\nChain 4: Iteration:  471 / 2000 [ 23%]  (Warmup)\nChain 4: Iteration:  472 / 2000 [ 23%]  (Warmup)\nChain 4: Iteration:  473 / 2000 [ 23%]  (Warmup)\nChain 4: Iteration:  474 / 2000 [ 23%]  (Warmup)\nChain 4: Iteration:  475 / 2000 [ 23%]  (Warmup)\nChain 4: Iteration:  476 / 2000 [ 23%]  (Warmup)\nChain 4: Iteration:  477 / 2000 [ 23%]  (Warmup)\nChain 4: Iteration:  478 / 2000 [ 23%]  (Warmup)\nChain 4: Iteration:  479 / 2000 [ 23%]  (Warmup)\nChain 4: Iteration:  480 / 2000 [ 24%]  (Warmup)\nChain 4: Iteration:  481 / 2000 [ 24%]  (Warmup)\nChain 4: Iteration:  482 / 2000 [ 24%]  (Warmup)\nChain 4: Iteration:  483 / 2000 [ 24%]  (Warmup)\nChain 4: Iteration:  484 / 2000 [ 24%]  (Warmup)\nChain 4: Iteration:  485 / 2000 [ 24%]  (Warmup)\nChain 4: Iteration:  486 / 2000 [ 24%]  (Warmup)\nChain 4: Iteration:  487 / 2000 [ 24%]  (Warmup)\nChain 4: Iteration:  488 / 2000 [ 24%]  (Warmup)\nChain 4: Iteration:  489 / 2000 [ 24%]  (Warmup)\nChain 4: Iteration:  490 / 2000 [ 24%]  (Warmup)\nChain 4: Iteration:  491 / 2000 [ 24%]  (Warmup)\nChain 4: Iteration:  492 / 2000 [ 24%]  (Warmup)\nChain 4: Iteration:  493 / 2000 [ 24%]  (Warmup)\nChain 4: Iteration:  494 / 2000 [ 24%]  (Warmup)\nChain 4: Iteration:  495 / 2000 [ 24%]  (Warmup)\nChain 4: Iteration:  496 / 2000 [ 24%]  (Warmup)\nChain 4: Iteration:  497 / 2000 [ 24%]  (Warmup)\nChain 4: Iteration:  498 / 2000 [ 24%]  (Warmup)\nChain 4: Iteration:  499 / 2000 [ 24%]  (Warmup)\nChain 4: Iteration:  500 / 2000 [ 25%]  (Warmup)\nChain 4: Iteration:  501 / 2000 [ 25%]  (Warmup)\nChain 4: Iteration:  502 / 2000 [ 25%]  (Warmup)\nChain 4: Iteration:  503 / 2000 [ 25%]  (Warmup)\nChain 4: Iteration:  504 / 2000 [ 25%]  (Warmup)\nChain 4: Iteration:  505 / 2000 [ 25%]  (Warmup)\nChain 4: Iteration:  506 / 2000 [ 25%]  (Warmup)\nChain 4: Iteration:  507 / 2000 [ 25%]  (Warmup)\nChain 4: Iteration:  508 / 2000 [ 25%]  (Warmup)\nChain 4: Iteration:  509 / 2000 [ 25%]  (Warmup)\nChain 4: Iteration:  510 / 2000 [ 25%]  (Warmup)\nChain 4: Iteration:  511 / 2000 [ 25%]  (Warmup)\nChain 4: Iteration:  512 / 2000 [ 25%]  (Warmup)\nChain 4: Iteration:  513 / 2000 [ 25%]  (Warmup)\nChain 4: Iteration:  514 / 2000 [ 25%]  (Warmup)\nChain 4: Iteration:  515 / 2000 [ 25%]  (Warmup)\nChain 4: Iteration:  516 / 2000 [ 25%]  (Warmup)\nChain 4: Iteration:  517 / 2000 [ 25%]  (Warmup)\nChain 4: Iteration:  518 / 2000 [ 25%]  (Warmup)\nChain 4: Iteration:  519 / 2000 [ 25%]  (Warmup)\nChain 4: Iteration:  520 / 2000 [ 26%]  (Warmup)\nChain 4: Iteration:  521 / 2000 [ 26%]  (Warmup)\nChain 4: Iteration:  522 / 2000 [ 26%]  (Warmup)\nChain 4: Iteration:  523 / 2000 [ 26%]  (Warmup)\nChain 4: Iteration:  524 / 2000 [ 26%]  (Warmup)\nChain 4: Iteration:  525 / 2000 [ 26%]  (Warmup)\nChain 4: Iteration:  526 / 2000 [ 26%]  (Warmup)\nChain 4: Iteration:  527 / 2000 [ 26%]  (Warmup)\nChain 4: Iteration:  528 / 2000 [ 26%]  (Warmup)\nChain 4: Iteration:  529 / 2000 [ 26%]  (Warmup)\nChain 4: Iteration:  530 / 2000 [ 26%]  (Warmup)\nChain 4: Iteration:  531 / 2000 [ 26%]  (Warmup)\nChain 4: Iteration:  532 / 2000 [ 26%]  (Warmup)\nChain 4: Iteration:  533 / 2000 [ 26%]  (Warmup)\nChain 4: Iteration:  534 / 2000 [ 26%]  (Warmup)\nChain 4: Iteration:  535 / 2000 [ 26%]  (Warmup)\nChain 4: Iteration:  536 / 2000 [ 26%]  (Warmup)\nChain 4: Iteration:  537 / 2000 [ 26%]  (Warmup)\nChain 4: Iteration:  538 / 2000 [ 26%]  (Warmup)\nChain 4: Iteration:  539 / 2000 [ 26%]  (Warmup)\nChain 4: Iteration:  540 / 2000 [ 27%]  (Warmup)\nChain 4: Iteration:  541 / 2000 [ 27%]  (Warmup)\nChain 4: Iteration:  542 / 2000 [ 27%]  (Warmup)\nChain 4: Iteration:  543 / 2000 [ 27%]  (Warmup)\nChain 4: Iteration:  544 / 2000 [ 27%]  (Warmup)\nChain 4: Iteration:  545 / 2000 [ 27%]  (Warmup)\nChain 4: Iteration:  546 / 2000 [ 27%]  (Warmup)\nChain 4: Iteration:  547 / 2000 [ 27%]  (Warmup)\nChain 4: Iteration:  548 / 2000 [ 27%]  (Warmup)\nChain 4: Iteration:  549 / 2000 [ 27%]  (Warmup)\nChain 4: Iteration:  550 / 2000 [ 27%]  (Warmup)\nChain 4: Iteration:  551 / 2000 [ 27%]  (Warmup)\nChain 4: Iteration:  552 / 2000 [ 27%]  (Warmup)\nChain 4: Iteration:  553 / 2000 [ 27%]  (Warmup)\nChain 4: Iteration:  554 / 2000 [ 27%]  (Warmup)\nChain 4: Iteration:  555 / 2000 [ 27%]  (Warmup)\nChain 4: Iteration:  556 / 2000 [ 27%]  (Warmup)\nChain 4: Iteration:  557 / 2000 [ 27%]  (Warmup)\nChain 4: Iteration:  558 / 2000 [ 27%]  (Warmup)\nChain 4: Iteration:  559 / 2000 [ 27%]  (Warmup)\nChain 4: Iteration:  560 / 2000 [ 28%]  (Warmup)\nChain 4: Iteration:  561 / 2000 [ 28%]  (Warmup)\nChain 4: Iteration:  562 / 2000 [ 28%]  (Warmup)\nChain 4: Iteration:  563 / 2000 [ 28%]  (Warmup)\nChain 4: Iteration:  564 / 2000 [ 28%]  (Warmup)\nChain 4: Iteration:  565 / 2000 [ 28%]  (Warmup)\nChain 4: Iteration:  566 / 2000 [ 28%]  (Warmup)\nChain 4: Iteration:  567 / 2000 [ 28%]  (Warmup)\nChain 4: Iteration:  568 / 2000 [ 28%]  (Warmup)\nChain 4: Iteration:  569 / 2000 [ 28%]  (Warmup)\nChain 4: Iteration:  570 / 2000 [ 28%]  (Warmup)\nChain 4: Iteration:  571 / 2000 [ 28%]  (Warmup)\nChain 4: Iteration:  572 / 2000 [ 28%]  (Warmup)\nChain 4: Iteration:  573 / 2000 [ 28%]  (Warmup)\nChain 4: Iteration:  574 / 2000 [ 28%]  (Warmup)\nChain 4: Iteration:  575 / 2000 [ 28%]  (Warmup)\nChain 4: Iteration:  576 / 2000 [ 28%]  (Warmup)\nChain 4: Iteration:  577 / 2000 [ 28%]  (Warmup)\nChain 4: Iteration:  578 / 2000 [ 28%]  (Warmup)\nChain 4: Iteration:  579 / 2000 [ 28%]  (Warmup)\nChain 4: Iteration:  580 / 2000 [ 29%]  (Warmup)\nChain 4: Iteration:  581 / 2000 [ 29%]  (Warmup)\nChain 4: Iteration:  582 / 2000 [ 29%]  (Warmup)\nChain 4: Iteration:  583 / 2000 [ 29%]  (Warmup)\nChain 4: Iteration:  584 / 2000 [ 29%]  (Warmup)\nChain 4: Iteration:  585 / 2000 [ 29%]  (Warmup)\nChain 4: Iteration:  586 / 2000 [ 29%]  (Warmup)\nChain 4: Iteration:  587 / 2000 [ 29%]  (Warmup)\nChain 4: Iteration:  588 / 2000 [ 29%]  (Warmup)\nChain 4: Iteration:  589 / 2000 [ 29%]  (Warmup)\nChain 4: Iteration:  590 / 2000 [ 29%]  (Warmup)\nChain 4: Iteration:  591 / 2000 [ 29%]  (Warmup)\nChain 4: Iteration:  592 / 2000 [ 29%]  (Warmup)\nChain 4: Iteration:  593 / 2000 [ 29%]  (Warmup)\nChain 4: Iteration:  594 / 2000 [ 29%]  (Warmup)\nChain 4: Iteration:  595 / 2000 [ 29%]  (Warmup)\nChain 4: Iteration:  596 / 2000 [ 29%]  (Warmup)\nChain 4: Iteration:  597 / 2000 [ 29%]  (Warmup)\nChain 4: Iteration:  598 / 2000 [ 29%]  (Warmup)\nChain 4: Iteration:  599 / 2000 [ 29%]  (Warmup)\nChain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 4: Iteration:  601 / 2000 [ 30%]  (Warmup)\nChain 4: Iteration:  602 / 2000 [ 30%]  (Warmup)\nChain 4: Iteration:  603 / 2000 [ 30%]  (Warmup)\nChain 4: Iteration:  604 / 2000 [ 30%]  (Warmup)\nChain 4: Iteration:  605 / 2000 [ 30%]  (Warmup)\nChain 4: Iteration:  606 / 2000 [ 30%]  (Warmup)\nChain 4: Iteration:  607 / 2000 [ 30%]  (Warmup)\nChain 4: Iteration:  608 / 2000 [ 30%]  (Warmup)\nChain 4: Iteration:  609 / 2000 [ 30%]  (Warmup)\nChain 4: Iteration:  610 / 2000 [ 30%]  (Warmup)\nChain 4: Iteration:  611 / 2000 [ 30%]  (Warmup)\nChain 4: Iteration:  612 / 2000 [ 30%]  (Warmup)\nChain 4: Iteration:  613 / 2000 [ 30%]  (Warmup)\nChain 4: Iteration:  614 / 2000 [ 30%]  (Warmup)\nChain 4: Iteration:  615 / 2000 [ 30%]  (Warmup)\nChain 4: Iteration:  616 / 2000 [ 30%]  (Warmup)\nChain 4: Iteration:  617 / 2000 [ 30%]  (Warmup)\nChain 4: Iteration:  618 / 2000 [ 30%]  (Warmup)\nChain 4: Iteration:  619 / 2000 [ 30%]  (Warmup)\nChain 4: Iteration:  620 / 2000 [ 31%]  (Warmup)\nChain 4: Iteration:  621 / 2000 [ 31%]  (Warmup)\nChain 4: Iteration:  622 / 2000 [ 31%]  (Warmup)\nChain 4: Iteration:  623 / 2000 [ 31%]  (Warmup)\nChain 4: Iteration:  624 / 2000 [ 31%]  (Warmup)\nChain 4: Iteration:  625 / 2000 [ 31%]  (Warmup)\nChain 4: Iteration:  626 / 2000 [ 31%]  (Warmup)\nChain 4: Iteration:  627 / 2000 [ 31%]  (Warmup)\nChain 4: Iteration:  628 / 2000 [ 31%]  (Warmup)\nChain 4: Iteration:  629 / 2000 [ 31%]  (Warmup)\nChain 4: Iteration:  630 / 2000 [ 31%]  (Warmup)\nChain 4: Iteration:  631 / 2000 [ 31%]  (Warmup)\nChain 4: Iteration:  632 / 2000 [ 31%]  (Warmup)\nChain 4: Iteration:  633 / 2000 [ 31%]  (Warmup)\nChain 4: Iteration:  634 / 2000 [ 31%]  (Warmup)\nChain 4: Iteration:  635 / 2000 [ 31%]  (Warmup)\nChain 4: Iteration:  636 / 2000 [ 31%]  (Warmup)\nChain 4: Iteration:  637 / 2000 [ 31%]  (Warmup)\nChain 4: Iteration:  638 / 2000 [ 31%]  (Warmup)\nChain 4: Iteration:  639 / 2000 [ 31%]  (Warmup)\nChain 4: Iteration:  640 / 2000 [ 32%]  (Warmup)\nChain 4: Iteration:  641 / 2000 [ 32%]  (Warmup)\nChain 4: Iteration:  642 / 2000 [ 32%]  (Warmup)\nChain 4: Iteration:  643 / 2000 [ 32%]  (Warmup)\nChain 4: Iteration:  644 / 2000 [ 32%]  (Warmup)\nChain 4: Iteration:  645 / 2000 [ 32%]  (Warmup)\nChain 4: Iteration:  646 / 2000 [ 32%]  (Warmup)\nChain 4: Iteration:  647 / 2000 [ 32%]  (Warmup)\nChain 4: Iteration:  648 / 2000 [ 32%]  (Warmup)\nChain 4: Iteration:  649 / 2000 [ 32%]  (Warmup)\nChain 4: Iteration:  650 / 2000 [ 32%]  (Warmup)\nChain 4: Iteration:  651 / 2000 [ 32%]  (Warmup)\nChain 4: Iteration:  652 / 2000 [ 32%]  (Warmup)\nChain 4: Iteration:  653 / 2000 [ 32%]  (Warmup)\nChain 4: Iteration:  654 / 2000 [ 32%]  (Warmup)\nChain 4: Iteration:  655 / 2000 [ 32%]  (Warmup)\nChain 4: Iteration:  656 / 2000 [ 32%]  (Warmup)\nChain 4: Iteration:  657 / 2000 [ 32%]  (Warmup)\nChain 4: Iteration:  658 / 2000 [ 32%]  (Warmup)\nChain 4: Iteration:  659 / 2000 [ 32%]  (Warmup)\nChain 4: Iteration:  660 / 2000 [ 33%]  (Warmup)\nChain 4: Iteration:  661 / 2000 [ 33%]  (Warmup)\nChain 4: Iteration:  662 / 2000 [ 33%]  (Warmup)\nChain 4: Iteration:  663 / 2000 [ 33%]  (Warmup)\nChain 4: Iteration:  664 / 2000 [ 33%]  (Warmup)\nChain 4: Iteration:  665 / 2000 [ 33%]  (Warmup)\nChain 4: Iteration:  666 / 2000 [ 33%]  (Warmup)\nChain 4: Iteration:  667 / 2000 [ 33%]  (Warmup)\nChain 4: Iteration:  668 / 2000 [ 33%]  (Warmup)\nChain 4: Iteration:  669 / 2000 [ 33%]  (Warmup)\nChain 4: Iteration:  670 / 2000 [ 33%]  (Warmup)\nChain 4: Iteration:  671 / 2000 [ 33%]  (Warmup)\nChain 4: Iteration:  672 / 2000 [ 33%]  (Warmup)\nChain 4: Iteration:  673 / 2000 [ 33%]  (Warmup)\nChain 4: Iteration:  674 / 2000 [ 33%]  (Warmup)\nChain 4: Iteration:  675 / 2000 [ 33%]  (Warmup)\nChain 4: Iteration:  676 / 2000 [ 33%]  (Warmup)\nChain 4: Iteration:  677 / 2000 [ 33%]  (Warmup)\nChain 4: Iteration:  678 / 2000 [ 33%]  (Warmup)\nChain 4: Iteration:  679 / 2000 [ 33%]  (Warmup)\nChain 4: Iteration:  680 / 2000 [ 34%]  (Warmup)\nChain 4: Iteration:  681 / 2000 [ 34%]  (Warmup)\nChain 4: Iteration:  682 / 2000 [ 34%]  (Warmup)\nChain 4: Iteration:  683 / 2000 [ 34%]  (Warmup)\nChain 4: Iteration:  684 / 2000 [ 34%]  (Warmup)\nChain 4: Iteration:  685 / 2000 [ 34%]  (Warmup)\nChain 4: Iteration:  686 / 2000 [ 34%]  (Warmup)\nChain 4: Iteration:  687 / 2000 [ 34%]  (Warmup)\nChain 4: Iteration:  688 / 2000 [ 34%]  (Warmup)\nChain 4: Iteration:  689 / 2000 [ 34%]  (Warmup)\nChain 4: Iteration:  690 / 2000 [ 34%]  (Warmup)\nChain 4: Iteration:  691 / 2000 [ 34%]  (Warmup)\nChain 4: Iteration:  692 / 2000 [ 34%]  (Warmup)\nChain 4: Iteration:  693 / 2000 [ 34%]  (Warmup)\nChain 4: Iteration:  694 / 2000 [ 34%]  (Warmup)\nChain 4: Iteration:  695 / 2000 [ 34%]  (Warmup)\nChain 4: Iteration:  696 / 2000 [ 34%]  (Warmup)\nChain 4: Iteration:  697 / 2000 [ 34%]  (Warmup)\nChain 4: Iteration:  698 / 2000 [ 34%]  (Warmup)\nChain 4: Iteration:  699 / 2000 [ 34%]  (Warmup)\nChain 4: Iteration:  700 / 2000 [ 35%]  (Warmup)\nChain 4: Iteration:  701 / 2000 [ 35%]  (Warmup)\nChain 4: Iteration:  702 / 2000 [ 35%]  (Warmup)\nChain 4: Iteration:  703 / 2000 [ 35%]  (Warmup)\nChain 4: Iteration:  704 / 2000 [ 35%]  (Warmup)\nChain 4: Iteration:  705 / 2000 [ 35%]  (Warmup)\nChain 4: Iteration:  706 / 2000 [ 35%]  (Warmup)\nChain 4: Iteration:  707 / 2000 [ 35%]  (Warmup)\nChain 4: Iteration:  708 / 2000 [ 35%]  (Warmup)\nChain 4: Iteration:  709 / 2000 [ 35%]  (Warmup)\nChain 4: Iteration:  710 / 2000 [ 35%]  (Warmup)\nChain 4: Iteration:  711 / 2000 [ 35%]  (Warmup)\nChain 4: Iteration:  712 / 2000 [ 35%]  (Warmup)\nChain 4: Iteration:  713 / 2000 [ 35%]  (Warmup)\nChain 4: Iteration:  714 / 2000 [ 35%]  (Warmup)\nChain 4: Iteration:  715 / 2000 [ 35%]  (Warmup)\nChain 4: Iteration:  716 / 2000 [ 35%]  (Warmup)\nChain 4: Iteration:  717 / 2000 [ 35%]  (Warmup)\nChain 4: Iteration:  718 / 2000 [ 35%]  (Warmup)\nChain 4: Iteration:  719 / 2000 [ 35%]  (Warmup)\nChain 4: Iteration:  720 / 2000 [ 36%]  (Warmup)\nChain 4: Iteration:  721 / 2000 [ 36%]  (Warmup)\nChain 4: Iteration:  722 / 2000 [ 36%]  (Warmup)\nChain 4: Iteration:  723 / 2000 [ 36%]  (Warmup)\nChain 4: Iteration:  724 / 2000 [ 36%]  (Warmup)\nChain 4: Iteration:  725 / 2000 [ 36%]  (Warmup)\nChain 4: Iteration:  726 / 2000 [ 36%]  (Warmup)\nChain 4: Iteration:  727 / 2000 [ 36%]  (Warmup)\nChain 4: Iteration:  728 / 2000 [ 36%]  (Warmup)\nChain 4: Iteration:  729 / 2000 [ 36%]  (Warmup)\nChain 4: Iteration:  730 / 2000 [ 36%]  (Warmup)\nChain 4: Iteration:  731 / 2000 [ 36%]  (Warmup)\nChain 4: Iteration:  732 / 2000 [ 36%]  (Warmup)\nChain 4: Iteration:  733 / 2000 [ 36%]  (Warmup)\nChain 4: Iteration:  734 / 2000 [ 36%]  (Warmup)\nChain 4: Iteration:  735 / 2000 [ 36%]  (Warmup)\nChain 4: Iteration:  736 / 2000 [ 36%]  (Warmup)\nChain 4: Iteration:  737 / 2000 [ 36%]  (Warmup)\nChain 4: Iteration:  738 / 2000 [ 36%]  (Warmup)\nChain 4: Iteration:  739 / 2000 [ 36%]  (Warmup)\nChain 4: Iteration:  740 / 2000 [ 37%]  (Warmup)\nChain 4: Iteration:  741 / 2000 [ 37%]  (Warmup)\nChain 4: Iteration:  742 / 2000 [ 37%]  (Warmup)\nChain 4: Iteration:  743 / 2000 [ 37%]  (Warmup)\nChain 4: Iteration:  744 / 2000 [ 37%]  (Warmup)\nChain 4: Iteration:  745 / 2000 [ 37%]  (Warmup)\nChain 4: Iteration:  746 / 2000 [ 37%]  (Warmup)\nChain 4: Iteration:  747 / 2000 [ 37%]  (Warmup)\nChain 4: Iteration:  748 / 2000 [ 37%]  (Warmup)\nChain 4: Iteration:  749 / 2000 [ 37%]  (Warmup)\nChain 4: Iteration:  750 / 2000 [ 37%]  (Warmup)\nChain 4: Iteration:  751 / 2000 [ 37%]  (Warmup)\nChain 4: Iteration:  752 / 2000 [ 37%]  (Warmup)\nChain 4: Iteration:  753 / 2000 [ 37%]  (Warmup)\nChain 4: Iteration:  754 / 2000 [ 37%]  (Warmup)\nChain 4: Iteration:  755 / 2000 [ 37%]  (Warmup)\nChain 4: Iteration:  756 / 2000 [ 37%]  (Warmup)\nChain 4: Iteration:  757 / 2000 [ 37%]  (Warmup)\nChain 4: Iteration:  758 / 2000 [ 37%]  (Warmup)\nChain 4: Iteration:  759 / 2000 [ 37%]  (Warmup)\nChain 4: Iteration:  760 / 2000 [ 38%]  (Warmup)\nChain 4: Iteration:  761 / 2000 [ 38%]  (Warmup)\nChain 4: Iteration:  762 / 2000 [ 38%]  (Warmup)\nChain 4: Iteration:  763 / 2000 [ 38%]  (Warmup)\nChain 4: Iteration:  764 / 2000 [ 38%]  (Warmup)\nChain 4: Iteration:  765 / 2000 [ 38%]  (Warmup)\nChain 4: Iteration:  766 / 2000 [ 38%]  (Warmup)\nChain 4: Iteration:  767 / 2000 [ 38%]  (Warmup)\nChain 4: Iteration:  768 / 2000 [ 38%]  (Warmup)\nChain 4: Iteration:  769 / 2000 [ 38%]  (Warmup)\nChain 4: Iteration:  770 / 2000 [ 38%]  (Warmup)\nChain 4: Iteration:  771 / 2000 [ 38%]  (Warmup)\nChain 4: Iteration:  772 / 2000 [ 38%]  (Warmup)\nChain 4: Iteration:  773 / 2000 [ 38%]  (Warmup)\nChain 4: Iteration:  774 / 2000 [ 38%]  (Warmup)\nChain 4: Iteration:  775 / 2000 [ 38%]  (Warmup)\nChain 4: Iteration:  776 / 2000 [ 38%]  (Warmup)\nChain 4: Iteration:  777 / 2000 [ 38%]  (Warmup)\nChain 4: Iteration:  778 / 2000 [ 38%]  (Warmup)\nChain 4: Iteration:  779 / 2000 [ 38%]  (Warmup)\nChain 4: Iteration:  780 / 2000 [ 39%]  (Warmup)\nChain 4: Iteration:  781 / 2000 [ 39%]  (Warmup)\nChain 4: Iteration:  782 / 2000 [ 39%]  (Warmup)\nChain 4: Iteration:  783 / 2000 [ 39%]  (Warmup)\nChain 4: Iteration:  784 / 2000 [ 39%]  (Warmup)\nChain 4: Iteration:  785 / 2000 [ 39%]  (Warmup)\nChain 4: Iteration:  786 / 2000 [ 39%]  (Warmup)\nChain 4: Iteration:  787 / 2000 [ 39%]  (Warmup)\nChain 4: Iteration:  788 / 2000 [ 39%]  (Warmup)\nChain 4: Iteration:  789 / 2000 [ 39%]  (Warmup)\nChain 4: Iteration:  790 / 2000 [ 39%]  (Warmup)\nChain 4: Iteration:  791 / 2000 [ 39%]  (Warmup)\nChain 4: Iteration:  792 / 2000 [ 39%]  (Warmup)\nChain 4: Iteration:  793 / 2000 [ 39%]  (Warmup)\nChain 4: Iteration:  794 / 2000 [ 39%]  (Warmup)\nChain 4: Iteration:  795 / 2000 [ 39%]  (Warmup)\nChain 4: Iteration:  796 / 2000 [ 39%]  (Warmup)\nChain 4: Iteration:  797 / 2000 [ 39%]  (Warmup)\nChain 4: Iteration:  798 / 2000 [ 39%]  (Warmup)\nChain 4: Iteration:  799 / 2000 [ 39%]  (Warmup)\nChain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 4: Iteration:  801 / 2000 [ 40%]  (Warmup)\nChain 4: Iteration:  802 / 2000 [ 40%]  (Warmup)\nChain 4: Iteration:  803 / 2000 [ 40%]  (Warmup)\nChain 4: Iteration:  804 / 2000 [ 40%]  (Warmup)\nChain 4: Iteration:  805 / 2000 [ 40%]  (Warmup)\nChain 4: Iteration:  806 / 2000 [ 40%]  (Warmup)\nChain 4: Iteration:  807 / 2000 [ 40%]  (Warmup)\nChain 4: Iteration:  808 / 2000 [ 40%]  (Warmup)\nChain 4: Iteration:  809 / 2000 [ 40%]  (Warmup)\nChain 4: Iteration:  810 / 2000 [ 40%]  (Warmup)\nChain 4: Iteration:  811 / 2000 [ 40%]  (Warmup)\nChain 4: Iteration:  812 / 2000 [ 40%]  (Warmup)\nChain 4: Iteration:  813 / 2000 [ 40%]  (Warmup)\nChain 4: Iteration:  814 / 2000 [ 40%]  (Warmup)\nChain 4: Iteration:  815 / 2000 [ 40%]  (Warmup)\nChain 4: Iteration:  816 / 2000 [ 40%]  (Warmup)\nChain 4: Iteration:  817 / 2000 [ 40%]  (Warmup)\nChain 4: Iteration:  818 / 2000 [ 40%]  (Warmup)\nChain 4: Iteration:  819 / 2000 [ 40%]  (Warmup)\nChain 4: Iteration:  820 / 2000 [ 41%]  (Warmup)\nChain 4: Iteration:  821 / 2000 [ 41%]  (Warmup)\nChain 4: Iteration:  822 / 2000 [ 41%]  (Warmup)\nChain 4: Iteration:  823 / 2000 [ 41%]  (Warmup)\nChain 4: Iteration:  824 / 2000 [ 41%]  (Warmup)\nChain 4: Iteration:  825 / 2000 [ 41%]  (Warmup)\nChain 4: Iteration:  826 / 2000 [ 41%]  (Warmup)\nChain 4: Iteration:  827 / 2000 [ 41%]  (Warmup)\nChain 4: Iteration:  828 / 2000 [ 41%]  (Warmup)\nChain 4: Iteration:  829 / 2000 [ 41%]  (Warmup)\nChain 4: Iteration:  830 / 2000 [ 41%]  (Warmup)\nChain 4: Iteration:  831 / 2000 [ 41%]  (Warmup)\nChain 4: Iteration:  832 / 2000 [ 41%]  (Warmup)\nChain 4: Iteration:  833 / 2000 [ 41%]  (Warmup)\nChain 4: Iteration:  834 / 2000 [ 41%]  (Warmup)\nChain 4: Iteration:  835 / 2000 [ 41%]  (Warmup)\nChain 4: Iteration:  836 / 2000 [ 41%]  (Warmup)\nChain 4: Iteration:  837 / 2000 [ 41%]  (Warmup)\nChain 4: Iteration:  838 / 2000 [ 41%]  (Warmup)\nChain 4: Iteration:  839 / 2000 [ 41%]  (Warmup)\nChain 4: Iteration:  840 / 2000 [ 42%]  (Warmup)\nChain 4: Iteration:  841 / 2000 [ 42%]  (Warmup)\nChain 4: Iteration:  842 / 2000 [ 42%]  (Warmup)\nChain 4: Iteration:  843 / 2000 [ 42%]  (Warmup)\nChain 4: Iteration:  844 / 2000 [ 42%]  (Warmup)\nChain 4: Iteration:  845 / 2000 [ 42%]  (Warmup)\nChain 4: Iteration:  846 / 2000 [ 42%]  (Warmup)\nChain 4: Iteration:  847 / 2000 [ 42%]  (Warmup)\nChain 4: Iteration:  848 / 2000 [ 42%]  (Warmup)\nChain 4: Iteration:  849 / 2000 [ 42%]  (Warmup)\nChain 4: Iteration:  850 / 2000 [ 42%]  (Warmup)\nChain 4: Iteration:  851 / 2000 [ 42%]  (Warmup)\nChain 4: Iteration:  852 / 2000 [ 42%]  (Warmup)\nChain 4: Iteration:  853 / 2000 [ 42%]  (Warmup)\nChain 4: Iteration:  854 / 2000 [ 42%]  (Warmup)\nChain 4: Iteration:  855 / 2000 [ 42%]  (Warmup)\nChain 4: Iteration:  856 / 2000 [ 42%]  (Warmup)\nChain 4: Iteration:  857 / 2000 [ 42%]  (Warmup)\nChain 4: Iteration:  858 / 2000 [ 42%]  (Warmup)\nChain 4: Iteration:  859 / 2000 [ 42%]  (Warmup)\nChain 4: Iteration:  860 / 2000 [ 43%]  (Warmup)\nChain 4: Iteration:  861 / 2000 [ 43%]  (Warmup)\nChain 4: Iteration:  862 / 2000 [ 43%]  (Warmup)\nChain 4: Iteration:  863 / 2000 [ 43%]  (Warmup)\nChain 4: Iteration:  864 / 2000 [ 43%]  (Warmup)\nChain 4: Iteration:  865 / 2000 [ 43%]  (Warmup)\nChain 4: Iteration:  866 / 2000 [ 43%]  (Warmup)\nChain 4: Iteration:  867 / 2000 [ 43%]  (Warmup)\nChain 4: Iteration:  868 / 2000 [ 43%]  (Warmup)\nChain 4: Iteration:  869 / 2000 [ 43%]  (Warmup)\nChain 4: Iteration:  870 / 2000 [ 43%]  (Warmup)\nChain 4: Iteration:  871 / 2000 [ 43%]  (Warmup)\nChain 4: Iteration:  872 / 2000 [ 43%]  (Warmup)\nChain 4: Iteration:  873 / 2000 [ 43%]  (Warmup)\nChain 4: Iteration:  874 / 2000 [ 43%]  (Warmup)\nChain 4: Iteration:  875 / 2000 [ 43%]  (Warmup)\nChain 4: Iteration:  876 / 2000 [ 43%]  (Warmup)\nChain 4: Iteration:  877 / 2000 [ 43%]  (Warmup)\nChain 4: Iteration:  878 / 2000 [ 43%]  (Warmup)\nChain 4: Iteration:  879 / 2000 [ 43%]  (Warmup)\nChain 4: Iteration:  880 / 2000 [ 44%]  (Warmup)\nChain 4: Iteration:  881 / 2000 [ 44%]  (Warmup)\nChain 4: Iteration:  882 / 2000 [ 44%]  (Warmup)\nChain 4: Iteration:  883 / 2000 [ 44%]  (Warmup)\nChain 4: Iteration:  884 / 2000 [ 44%]  (Warmup)\nChain 4: Iteration:  885 / 2000 [ 44%]  (Warmup)\nChain 4: Iteration:  886 / 2000 [ 44%]  (Warmup)\nChain 4: Iteration:  887 / 2000 [ 44%]  (Warmup)\nChain 4: Iteration:  888 / 2000 [ 44%]  (Warmup)\nChain 4: Iteration:  889 / 2000 [ 44%]  (Warmup)\nChain 4: Iteration:  890 / 2000 [ 44%]  (Warmup)\nChain 4: Iteration:  891 / 2000 [ 44%]  (Warmup)\nChain 4: Iteration:  892 / 2000 [ 44%]  (Warmup)\nChain 4: Iteration:  893 / 2000 [ 44%]  (Warmup)\nChain 4: Iteration:  894 / 2000 [ 44%]  (Warmup)\nChain 4: Iteration:  895 / 2000 [ 44%]  (Warmup)\nChain 4: Iteration:  896 / 2000 [ 44%]  (Warmup)\nChain 4: Iteration:  897 / 2000 [ 44%]  (Warmup)\nChain 4: Iteration:  898 / 2000 [ 44%]  (Warmup)\nChain 4: Iteration:  899 / 2000 [ 44%]  (Warmup)\nChain 4: Iteration:  900 / 2000 [ 45%]  (Warmup)\nChain 4: Iteration:  901 / 2000 [ 45%]  (Warmup)\nChain 4: Iteration:  902 / 2000 [ 45%]  (Warmup)\nChain 4: Iteration:  903 / 2000 [ 45%]  (Warmup)\nChain 4: Iteration:  904 / 2000 [ 45%]  (Warmup)\nChain 4: Iteration:  905 / 2000 [ 45%]  (Warmup)\nChain 4: Iteration:  906 / 2000 [ 45%]  (Warmup)\nChain 4: Iteration:  907 / 2000 [ 45%]  (Warmup)\nChain 4: Iteration:  908 / 2000 [ 45%]  (Warmup)\nChain 4: Iteration:  909 / 2000 [ 45%]  (Warmup)\nChain 4: Iteration:  910 / 2000 [ 45%]  (Warmup)\nChain 4: Iteration:  911 / 2000 [ 45%]  (Warmup)\nChain 4: Iteration:  912 / 2000 [ 45%]  (Warmup)\nChain 4: Iteration:  913 / 2000 [ 45%]  (Warmup)\nChain 4: Iteration:  914 / 2000 [ 45%]  (Warmup)\nChain 4: Iteration:  915 / 2000 [ 45%]  (Warmup)\nChain 4: Iteration:  916 / 2000 [ 45%]  (Warmup)\nChain 4: Iteration:  917 / 2000 [ 45%]  (Warmup)\nChain 4: Iteration:  918 / 2000 [ 45%]  (Warmup)\nChain 4: Iteration:  919 / 2000 [ 45%]  (Warmup)\nChain 4: Iteration:  920 / 2000 [ 46%]  (Warmup)\nChain 4: Iteration:  921 / 2000 [ 46%]  (Warmup)\nChain 4: Iteration:  922 / 2000 [ 46%]  (Warmup)\nChain 4: Iteration:  923 / 2000 [ 46%]  (Warmup)\nChain 4: Iteration:  924 / 2000 [ 46%]  (Warmup)\nChain 4: Iteration:  925 / 2000 [ 46%]  (Warmup)\nChain 4: Iteration:  926 / 2000 [ 46%]  (Warmup)\nChain 4: Iteration:  927 / 2000 [ 46%]  (Warmup)\nChain 4: Iteration:  928 / 2000 [ 46%]  (Warmup)\nChain 4: Iteration:  929 / 2000 [ 46%]  (Warmup)\nChain 4: Iteration:  930 / 2000 [ 46%]  (Warmup)\nChain 4: Iteration:  931 / 2000 [ 46%]  (Warmup)\nChain 4: Iteration:  932 / 2000 [ 46%]  (Warmup)\nChain 4: Iteration:  933 / 2000 [ 46%]  (Warmup)\nChain 4: Iteration:  934 / 2000 [ 46%]  (Warmup)\nChain 4: Iteration:  935 / 2000 [ 46%]  (Warmup)\nChain 4: Iteration:  936 / 2000 [ 46%]  (Warmup)\nChain 4: Iteration:  937 / 2000 [ 46%]  (Warmup)\nChain 4: Iteration:  938 / 2000 [ 46%]  (Warmup)\nChain 4: Iteration:  939 / 2000 [ 46%]  (Warmup)\nChain 4: Iteration:  940 / 2000 [ 47%]  (Warmup)\nChain 4: Iteration:  941 / 2000 [ 47%]  (Warmup)\nChain 4: Iteration:  942 / 2000 [ 47%]  (Warmup)\nChain 4: Iteration:  943 / 2000 [ 47%]  (Warmup)\nChain 4: Iteration:  944 / 2000 [ 47%]  (Warmup)\nChain 4: Iteration:  945 / 2000 [ 47%]  (Warmup)\nChain 4: Iteration:  946 / 2000 [ 47%]  (Warmup)\nChain 4: Iteration:  947 / 2000 [ 47%]  (Warmup)\nChain 4: Iteration:  948 / 2000 [ 47%]  (Warmup)\nChain 4: Iteration:  949 / 2000 [ 47%]  (Warmup)\nChain 4: Iteration:  950 / 2000 [ 47%]  (Warmup)\nChain 4: Iteration:  951 / 2000 [ 47%]  (Warmup)\nChain 4: Iteration:  952 / 2000 [ 47%]  (Warmup)\nChain 4: Iteration:  953 / 2000 [ 47%]  (Warmup)\nChain 4: Iteration:  954 / 2000 [ 47%]  (Warmup)\nChain 4: Iteration:  955 / 2000 [ 47%]  (Warmup)\nChain 4: Iteration:  956 / 2000 [ 47%]  (Warmup)\nChain 4: Iteration:  957 / 2000 [ 47%]  (Warmup)\nChain 4: Iteration:  958 / 2000 [ 47%]  (Warmup)\nChain 4: Iteration:  959 / 2000 [ 47%]  (Warmup)\nChain 4: Iteration:  960 / 2000 [ 48%]  (Warmup)\nChain 4: Iteration:  961 / 2000 [ 48%]  (Warmup)\nChain 4: Iteration:  962 / 2000 [ 48%]  (Warmup)\nChain 4: Iteration:  963 / 2000 [ 48%]  (Warmup)\nChain 4: Iteration:  964 / 2000 [ 48%]  (Warmup)\nChain 4: Iteration:  965 / 2000 [ 48%]  (Warmup)\nChain 4: Iteration:  966 / 2000 [ 48%]  (Warmup)\nChain 4: Iteration:  967 / 2000 [ 48%]  (Warmup)\nChain 4: Iteration:  968 / 2000 [ 48%]  (Warmup)\nChain 4: Iteration:  969 / 2000 [ 48%]  (Warmup)\nChain 4: Iteration:  970 / 2000 [ 48%]  (Warmup)\nChain 4: Iteration:  971 / 2000 [ 48%]  (Warmup)\nChain 4: Iteration:  972 / 2000 [ 48%]  (Warmup)\nChain 4: Iteration:  973 / 2000 [ 48%]  (Warmup)\nChain 4: Iteration:  974 / 2000 [ 48%]  (Warmup)\nChain 4: Iteration:  975 / 2000 [ 48%]  (Warmup)\nChain 4: Iteration:  976 / 2000 [ 48%]  (Warmup)\nChain 4: Iteration:  977 / 2000 [ 48%]  (Warmup)\nChain 4: Iteration:  978 / 2000 [ 48%]  (Warmup)\nChain 4: Iteration:  979 / 2000 [ 48%]  (Warmup)\nChain 4: Iteration:  980 / 2000 [ 49%]  (Warmup)\nChain 4: Iteration:  981 / 2000 [ 49%]  (Warmup)\nChain 4: Iteration:  982 / 2000 [ 49%]  (Warmup)\nChain 4: Iteration:  983 / 2000 [ 49%]  (Warmup)\nChain 4: Iteration:  984 / 2000 [ 49%]  (Warmup)\nChain 4: Iteration:  985 / 2000 [ 49%]  (Warmup)\nChain 4: Iteration:  986 / 2000 [ 49%]  (Warmup)\nChain 4: Iteration:  987 / 2000 [ 49%]  (Warmup)\nChain 4: Iteration:  988 / 2000 [ 49%]  (Warmup)\nChain 4: Iteration:  989 / 2000 [ 49%]  (Warmup)\nChain 4: Iteration:  990 / 2000 [ 49%]  (Warmup)\nChain 4: Iteration:  991 / 2000 [ 49%]  (Warmup)\nChain 4: Iteration:  992 / 2000 [ 49%]  (Warmup)\nChain 4: Iteration:  993 / 2000 [ 49%]  (Warmup)\nChain 4: Iteration:  994 / 2000 [ 49%]  (Warmup)\nChain 4: Iteration:  995 / 2000 [ 49%]  (Warmup)\nChain 4: Iteration:  996 / 2000 [ 49%]  (Warmup)\nChain 4: Iteration:  997 / 2000 [ 49%]  (Warmup)\nChain 4: Iteration:  998 / 2000 [ 49%]  (Warmup)\nChain 4: Iteration:  999 / 2000 [ 49%]  (Warmup)\nChain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 4: Iteration: 1002 / 2000 [ 50%]  (Sampling)\nChain 4: Iteration: 1003 / 2000 [ 50%]  (Sampling)\nChain 4: Iteration: 1004 / 2000 [ 50%]  (Sampling)\nChain 4: Iteration: 1005 / 2000 [ 50%]  (Sampling)\nChain 4: Iteration: 1006 / 2000 [ 50%]  (Sampling)\nChain 4: Iteration: 1007 / 2000 [ 50%]  (Sampling)\nChain 4: Iteration: 1008 / 2000 [ 50%]  (Sampling)\nChain 4: Iteration: 1009 / 2000 [ 50%]  (Sampling)\nChain 4: Iteration: 1010 / 2000 [ 50%]  (Sampling)\nChain 4: Iteration: 1011 / 2000 [ 50%]  (Sampling)\nChain 4: Iteration: 1012 / 2000 [ 50%]  (Sampling)\nChain 4: Iteration: 1013 / 2000 [ 50%]  (Sampling)\nChain 4: Iteration: 1014 / 2000 [ 50%]  (Sampling)\nChain 4: Iteration: 1015 / 2000 [ 50%]  (Sampling)\nChain 4: Iteration: 1016 / 2000 [ 50%]  (Sampling)\nChain 4: Iteration: 1017 / 2000 [ 50%]  (Sampling)\nChain 4: Iteration: 1018 / 2000 [ 50%]  (Sampling)\nChain 4: Iteration: 1019 / 2000 [ 50%]  (Sampling)\nChain 4: Iteration: 1020 / 2000 [ 51%]  (Sampling)\nChain 4: Iteration: 1021 / 2000 [ 51%]  (Sampling)\nChain 4: Iteration: 1022 / 2000 [ 51%]  (Sampling)\nChain 4: Iteration: 1023 / 2000 [ 51%]  (Sampling)\nChain 4: Iteration: 1024 / 2000 [ 51%]  (Sampling)\nChain 4: Iteration: 1025 / 2000 [ 51%]  (Sampling)\nChain 4: Iteration: 1026 / 2000 [ 51%]  (Sampling)\nChain 4: Iteration: 1027 / 2000 [ 51%]  (Sampling)\nChain 4: Iteration: 1028 / 2000 [ 51%]  (Sampling)\nChain 4: Iteration: 1029 / 2000 [ 51%]  (Sampling)\nChain 4: Iteration: 1030 / 2000 [ 51%]  (Sampling)\nChain 4: Iteration: 1031 / 2000 [ 51%]  (Sampling)\nChain 4: Iteration: 1032 / 2000 [ 51%]  (Sampling)\nChain 4: Iteration: 1033 / 2000 [ 51%]  (Sampling)\nChain 4: Iteration: 1034 / 2000 [ 51%]  (Sampling)\nChain 4: Iteration: 1035 / 2000 [ 51%]  (Sampling)\nChain 4: Iteration: 1036 / 2000 [ 51%]  (Sampling)\nChain 4: Iteration: 1037 / 2000 [ 51%]  (Sampling)\nChain 4: Iteration: 1038 / 2000 [ 51%]  (Sampling)\nChain 4: Iteration: 1039 / 2000 [ 51%]  (Sampling)\nChain 4: Iteration: 1040 / 2000 [ 52%]  (Sampling)\nChain 4: Iteration: 1041 / 2000 [ 52%]  (Sampling)\nChain 4: Iteration: 1042 / 2000 [ 52%]  (Sampling)\nChain 4: Iteration: 1043 / 2000 [ 52%]  (Sampling)\nChain 4: Iteration: 1044 / 2000 [ 52%]  (Sampling)\nChain 4: Iteration: 1045 / 2000 [ 52%]  (Sampling)\nChain 4: Iteration: 1046 / 2000 [ 52%]  (Sampling)\nChain 4: Iteration: 1047 / 2000 [ 52%]  (Sampling)\nChain 4: Iteration: 1048 / 2000 [ 52%]  (Sampling)\nChain 4: Iteration: 1049 / 2000 [ 52%]  (Sampling)\nChain 4: Iteration: 1050 / 2000 [ 52%]  (Sampling)\nChain 4: Iteration: 1051 / 2000 [ 52%]  (Sampling)\nChain 4: Iteration: 1052 / 2000 [ 52%]  (Sampling)\nChain 4: Iteration: 1053 / 2000 [ 52%]  (Sampling)\nChain 4: Iteration: 1054 / 2000 [ 52%]  (Sampling)\nChain 4: Iteration: 1055 / 2000 [ 52%]  (Sampling)\nChain 4: Iteration: 1056 / 2000 [ 52%]  (Sampling)\nChain 4: Iteration: 1057 / 2000 [ 52%]  (Sampling)\nChain 4: Iteration: 1058 / 2000 [ 52%]  (Sampling)\nChain 4: Iteration: 1059 / 2000 [ 52%]  (Sampling)\nChain 4: Iteration: 1060 / 2000 [ 53%]  (Sampling)\nChain 4: Iteration: 1061 / 2000 [ 53%]  (Sampling)\nChain 4: Iteration: 1062 / 2000 [ 53%]  (Sampling)\nChain 4: Iteration: 1063 / 2000 [ 53%]  (Sampling)\nChain 4: Iteration: 1064 / 2000 [ 53%]  (Sampling)\nChain 4: Iteration: 1065 / 2000 [ 53%]  (Sampling)\nChain 4: Iteration: 1066 / 2000 [ 53%]  (Sampling)\nChain 4: Iteration: 1067 / 2000 [ 53%]  (Sampling)\nChain 4: Iteration: 1068 / 2000 [ 53%]  (Sampling)\nChain 4: Iteration: 1069 / 2000 [ 53%]  (Sampling)\nChain 4: Iteration: 1070 / 2000 [ 53%]  (Sampling)\nChain 4: Iteration: 1071 / 2000 [ 53%]  (Sampling)\nChain 4: Iteration: 1072 / 2000 [ 53%]  (Sampling)\nChain 4: Iteration: 1073 / 2000 [ 53%]  (Sampling)\nChain 4: Iteration: 1074 / 2000 [ 53%]  (Sampling)\nChain 4: Iteration: 1075 / 2000 [ 53%]  (Sampling)\nChain 4: Iteration: 1076 / 2000 [ 53%]  (Sampling)\nChain 4: Iteration: 1077 / 2000 [ 53%]  (Sampling)\nChain 4: Iteration: 1078 / 2000 [ 53%]  (Sampling)\nChain 4: Iteration: 1079 / 2000 [ 53%]  (Sampling)\nChain 4: Iteration: 1080 / 2000 [ 54%]  (Sampling)\nChain 4: Iteration: 1081 / 2000 [ 54%]  (Sampling)\nChain 4: Iteration: 1082 / 2000 [ 54%]  (Sampling)\nChain 4: Iteration: 1083 / 2000 [ 54%]  (Sampling)\nChain 4: Iteration: 1084 / 2000 [ 54%]  (Sampling)\nChain 4: Iteration: 1085 / 2000 [ 54%]  (Sampling)\nChain 4: Iteration: 1086 / 2000 [ 54%]  (Sampling)\nChain 4: Iteration: 1087 / 2000 [ 54%]  (Sampling)\nChain 4: Iteration: 1088 / 2000 [ 54%]  (Sampling)\nChain 4: Iteration: 1089 / 2000 [ 54%]  (Sampling)\nChain 4: Iteration: 1090 / 2000 [ 54%]  (Sampling)\nChain 4: Iteration: 1091 / 2000 [ 54%]  (Sampling)\nChain 4: Iteration: 1092 / 2000 [ 54%]  (Sampling)\nChain 4: Iteration: 1093 / 2000 [ 54%]  (Sampling)\nChain 4: Iteration: 1094 / 2000 [ 54%]  (Sampling)\nChain 4: Iteration: 1095 / 2000 [ 54%]  (Sampling)\nChain 4: Iteration: 1096 / 2000 [ 54%]  (Sampling)\nChain 4: Iteration: 1097 / 2000 [ 54%]  (Sampling)\nChain 4: Iteration: 1098 / 2000 [ 54%]  (Sampling)\nChain 4: Iteration: 1099 / 2000 [ 54%]  (Sampling)\nChain 4: Iteration: 1100 / 2000 [ 55%]  (Sampling)\nChain 4: Iteration: 1101 / 2000 [ 55%]  (Sampling)\nChain 4: Iteration: 1102 / 2000 [ 55%]  (Sampling)\nChain 4: Iteration: 1103 / 2000 [ 55%]  (Sampling)\nChain 4: Iteration: 1104 / 2000 [ 55%]  (Sampling)\nChain 4: Iteration: 1105 / 2000 [ 55%]  (Sampling)\nChain 4: Iteration: 1106 / 2000 [ 55%]  (Sampling)\nChain 4: Iteration: 1107 / 2000 [ 55%]  (Sampling)\nChain 4: Iteration: 1108 / 2000 [ 55%]  (Sampling)\nChain 4: Iteration: 1109 / 2000 [ 55%]  (Sampling)\nChain 4: Iteration: 1110 / 2000 [ 55%]  (Sampling)\nChain 4: Iteration: 1111 / 2000 [ 55%]  (Sampling)\nChain 4: Iteration: 1112 / 2000 [ 55%]  (Sampling)\nChain 4: Iteration: 1113 / 2000 [ 55%]  (Sampling)\nChain 4: Iteration: 1114 / 2000 [ 55%]  (Sampling)\nChain 4: Iteration: 1115 / 2000 [ 55%]  (Sampling)\nChain 4: Iteration: 1116 / 2000 [ 55%]  (Sampling)\nChain 4: Iteration: 1117 / 2000 [ 55%]  (Sampling)\nChain 4: Iteration: 1118 / 2000 [ 55%]  (Sampling)\nChain 4: Iteration: 1119 / 2000 [ 55%]  (Sampling)\nChain 4: Iteration: 1120 / 2000 [ 56%]  (Sampling)\nChain 4: Iteration: 1121 / 2000 [ 56%]  (Sampling)\nChain 4: Iteration: 1122 / 2000 [ 56%]  (Sampling)\nChain 4: Iteration: 1123 / 2000 [ 56%]  (Sampling)\nChain 4: Iteration: 1124 / 2000 [ 56%]  (Sampling)\nChain 4: Iteration: 1125 / 2000 [ 56%]  (Sampling)\nChain 4: Iteration: 1126 / 2000 [ 56%]  (Sampling)\nChain 4: Iteration: 1127 / 2000 [ 56%]  (Sampling)\nChain 4: Iteration: 1128 / 2000 [ 56%]  (Sampling)\nChain 4: Iteration: 1129 / 2000 [ 56%]  (Sampling)\nChain 4: Iteration: 1130 / 2000 [ 56%]  (Sampling)\nChain 4: Iteration: 1131 / 2000 [ 56%]  (Sampling)\nChain 4: Iteration: 1132 / 2000 [ 56%]  (Sampling)\nChain 4: Iteration: 1133 / 2000 [ 56%]  (Sampling)\nChain 4: Iteration: 1134 / 2000 [ 56%]  (Sampling)\nChain 4: Iteration: 1135 / 2000 [ 56%]  (Sampling)\nChain 4: Iteration: 1136 / 2000 [ 56%]  (Sampling)\nChain 4: Iteration: 1137 / 2000 [ 56%]  (Sampling)\nChain 4: Iteration: 1138 / 2000 [ 56%]  (Sampling)\nChain 4: Iteration: 1139 / 2000 [ 56%]  (Sampling)\nChain 4: Iteration: 1140 / 2000 [ 57%]  (Sampling)\nChain 4: Iteration: 1141 / 2000 [ 57%]  (Sampling)\nChain 4: Iteration: 1142 / 2000 [ 57%]  (Sampling)\nChain 4: Iteration: 1143 / 2000 [ 57%]  (Sampling)\nChain 4: Iteration: 1144 / 2000 [ 57%]  (Sampling)\nChain 4: Iteration: 1145 / 2000 [ 57%]  (Sampling)\nChain 4: Iteration: 1146 / 2000 [ 57%]  (Sampling)\nChain 4: Iteration: 1147 / 2000 [ 57%]  (Sampling)\nChain 4: Iteration: 1148 / 2000 [ 57%]  (Sampling)\nChain 4: Iteration: 1149 / 2000 [ 57%]  (Sampling)\nChain 4: Iteration: 1150 / 2000 [ 57%]  (Sampling)\nChain 4: Iteration: 1151 / 2000 [ 57%]  (Sampling)\nChain 4: Iteration: 1152 / 2000 [ 57%]  (Sampling)\nChain 4: Iteration: 1153 / 2000 [ 57%]  (Sampling)\nChain 4: Iteration: 1154 / 2000 [ 57%]  (Sampling)\nChain 4: Iteration: 1155 / 2000 [ 57%]  (Sampling)\nChain 4: Iteration: 1156 / 2000 [ 57%]  (Sampling)\nChain 4: Iteration: 1157 / 2000 [ 57%]  (Sampling)\nChain 4: Iteration: 1158 / 2000 [ 57%]  (Sampling)\nChain 4: Iteration: 1159 / 2000 [ 57%]  (Sampling)\nChain 4: Iteration: 1160 / 2000 [ 58%]  (Sampling)\nChain 4: Iteration: 1161 / 2000 [ 58%]  (Sampling)\nChain 4: Iteration: 1162 / 2000 [ 58%]  (Sampling)\nChain 4: Iteration: 1163 / 2000 [ 58%]  (Sampling)\nChain 4: Iteration: 1164 / 2000 [ 58%]  (Sampling)\nChain 4: Iteration: 1165 / 2000 [ 58%]  (Sampling)\nChain 4: Iteration: 1166 / 2000 [ 58%]  (Sampling)\nChain 4: Iteration: 1167 / 2000 [ 58%]  (Sampling)\nChain 4: Iteration: 1168 / 2000 [ 58%]  (Sampling)\nChain 4: Iteration: 1169 / 2000 [ 58%]  (Sampling)\nChain 4: Iteration: 1170 / 2000 [ 58%]  (Sampling)\nChain 4: Iteration: 1171 / 2000 [ 58%]  (Sampling)\nChain 4: Iteration: 1172 / 2000 [ 58%]  (Sampling)\nChain 4: Iteration: 1173 / 2000 [ 58%]  (Sampling)\nChain 4: Iteration: 1174 / 2000 [ 58%]  (Sampling)\nChain 4: Iteration: 1175 / 2000 [ 58%]  (Sampling)\nChain 4: Iteration: 1176 / 2000 [ 58%]  (Sampling)\nChain 4: Iteration: 1177 / 2000 [ 58%]  (Sampling)\nChain 4: Iteration: 1178 / 2000 [ 58%]  (Sampling)\nChain 4: Iteration: 1179 / 2000 [ 58%]  (Sampling)\nChain 4: Iteration: 1180 / 2000 [ 59%]  (Sampling)\nChain 4: Iteration: 1181 / 2000 [ 59%]  (Sampling)\nChain 4: Iteration: 1182 / 2000 [ 59%]  (Sampling)\nChain 4: Iteration: 1183 / 2000 [ 59%]  (Sampling)\nChain 4: Iteration: 1184 / 2000 [ 59%]  (Sampling)\nChain 4: Iteration: 1185 / 2000 [ 59%]  (Sampling)\nChain 4: Iteration: 1186 / 2000 [ 59%]  (Sampling)\nChain 4: Iteration: 1187 / 2000 [ 59%]  (Sampling)\nChain 4: Iteration: 1188 / 2000 [ 59%]  (Sampling)\nChain 4: Iteration: 1189 / 2000 [ 59%]  (Sampling)\nChain 4: Iteration: 1190 / 2000 [ 59%]  (Sampling)\nChain 4: Iteration: 1191 / 2000 [ 59%]  (Sampling)\nChain 4: Iteration: 1192 / 2000 [ 59%]  (Sampling)\nChain 4: Iteration: 1193 / 2000 [ 59%]  (Sampling)\nChain 4: Iteration: 1194 / 2000 [ 59%]  (Sampling)\nChain 4: Iteration: 1195 / 2000 [ 59%]  (Sampling)\nChain 4: Iteration: 1196 / 2000 [ 59%]  (Sampling)\nChain 4: Iteration: 1197 / 2000 [ 59%]  (Sampling)\nChain 4: Iteration: 1198 / 2000 [ 59%]  (Sampling)\nChain 4: Iteration: 1199 / 2000 [ 59%]  (Sampling)\nChain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 4: Iteration: 1201 / 2000 [ 60%]  (Sampling)\nChain 4: Iteration: 1202 / 2000 [ 60%]  (Sampling)\nChain 4: Iteration: 1203 / 2000 [ 60%]  (Sampling)\nChain 4: Iteration: 1204 / 2000 [ 60%]  (Sampling)\nChain 4: Iteration: 1205 / 2000 [ 60%]  (Sampling)\nChain 4: Iteration: 1206 / 2000 [ 60%]  (Sampling)\nChain 4: Iteration: 1207 / 2000 [ 60%]  (Sampling)\nChain 4: Iteration: 1208 / 2000 [ 60%]  (Sampling)\nChain 4: Iteration: 1209 / 2000 [ 60%]  (Sampling)\nChain 4: Iteration: 1210 / 2000 [ 60%]  (Sampling)\nChain 4: Iteration: 1211 / 2000 [ 60%]  (Sampling)\nChain 4: Iteration: 1212 / 2000 [ 60%]  (Sampling)\nChain 4: Iteration: 1213 / 2000 [ 60%]  (Sampling)\nChain 4: Iteration: 1214 / 2000 [ 60%]  (Sampling)\nChain 4: Iteration: 1215 / 2000 [ 60%]  (Sampling)\nChain 4: Iteration: 1216 / 2000 [ 60%]  (Sampling)\nChain 4: Iteration: 1217 / 2000 [ 60%]  (Sampling)\nChain 4: Iteration: 1218 / 2000 [ 60%]  (Sampling)\nChain 4: Iteration: 1219 / 2000 [ 60%]  (Sampling)\nChain 4: Iteration: 1220 / 2000 [ 61%]  (Sampling)\nChain 4: Iteration: 1221 / 2000 [ 61%]  (Sampling)\nChain 4: Iteration: 1222 / 2000 [ 61%]  (Sampling)\nChain 4: Iteration: 1223 / 2000 [ 61%]  (Sampling)\nChain 4: Iteration: 1224 / 2000 [ 61%]  (Sampling)\nChain 4: Iteration: 1225 / 2000 [ 61%]  (Sampling)\nChain 4: Iteration: 1226 / 2000 [ 61%]  (Sampling)\nChain 4: Iteration: 1227 / 2000 [ 61%]  (Sampling)\nChain 4: Iteration: 1228 / 2000 [ 61%]  (Sampling)\nChain 4: Iteration: 1229 / 2000 [ 61%]  (Sampling)\nChain 4: Iteration: 1230 / 2000 [ 61%]  (Sampling)\nChain 4: Iteration: 1231 / 2000 [ 61%]  (Sampling)\nChain 4: Iteration: 1232 / 2000 [ 61%]  (Sampling)\nChain 4: Iteration: 1233 / 2000 [ 61%]  (Sampling)\nChain 4: Iteration: 1234 / 2000 [ 61%]  (Sampling)\nChain 4: Iteration: 1235 / 2000 [ 61%]  (Sampling)\nChain 4: Iteration: 1236 / 2000 [ 61%]  (Sampling)\nChain 4: Iteration: 1237 / 2000 [ 61%]  (Sampling)\nChain 4: Iteration: 1238 / 2000 [ 61%]  (Sampling)\nChain 4: Iteration: 1239 / 2000 [ 61%]  (Sampling)\nChain 4: Iteration: 1240 / 2000 [ 62%]  (Sampling)\nChain 4: Iteration: 1241 / 2000 [ 62%]  (Sampling)\nChain 4: Iteration: 1242 / 2000 [ 62%]  (Sampling)\nChain 4: Iteration: 1243 / 2000 [ 62%]  (Sampling)\nChain 4: Iteration: 1244 / 2000 [ 62%]  (Sampling)\nChain 4: Iteration: 1245 / 2000 [ 62%]  (Sampling)\nChain 4: Iteration: 1246 / 2000 [ 62%]  (Sampling)\nChain 4: Iteration: 1247 / 2000 [ 62%]  (Sampling)\nChain 4: Iteration: 1248 / 2000 [ 62%]  (Sampling)\nChain 4: Iteration: 1249 / 2000 [ 62%]  (Sampling)\nChain 4: Iteration: 1250 / 2000 [ 62%]  (Sampling)\nChain 4: Iteration: 1251 / 2000 [ 62%]  (Sampling)\nChain 4: Iteration: 1252 / 2000 [ 62%]  (Sampling)\nChain 4: Iteration: 1253 / 2000 [ 62%]  (Sampling)\nChain 4: Iteration: 1254 / 2000 [ 62%]  (Sampling)\nChain 4: Iteration: 1255 / 2000 [ 62%]  (Sampling)\nChain 4: Iteration: 1256 / 2000 [ 62%]  (Sampling)\nChain 4: Iteration: 1257 / 2000 [ 62%]  (Sampling)\nChain 4: Iteration: 1258 / 2000 [ 62%]  (Sampling)\nChain 4: Iteration: 1259 / 2000 [ 62%]  (Sampling)\nChain 4: Iteration: 1260 / 2000 [ 63%]  (Sampling)\nChain 4: Iteration: 1261 / 2000 [ 63%]  (Sampling)\nChain 4: Iteration: 1262 / 2000 [ 63%]  (Sampling)\nChain 4: Iteration: 1263 / 2000 [ 63%]  (Sampling)\nChain 4: Iteration: 1264 / 2000 [ 63%]  (Sampling)\nChain 4: Iteration: 1265 / 2000 [ 63%]  (Sampling)\nChain 4: Iteration: 1266 / 2000 [ 63%]  (Sampling)\nChain 4: Iteration: 1267 / 2000 [ 63%]  (Sampling)\nChain 4: Iteration: 1268 / 2000 [ 63%]  (Sampling)\nChain 4: Iteration: 1269 / 2000 [ 63%]  (Sampling)\nChain 4: Iteration: 1270 / 2000 [ 63%]  (Sampling)\nChain 4: Iteration: 1271 / 2000 [ 63%]  (Sampling)\nChain 4: Iteration: 1272 / 2000 [ 63%]  (Sampling)\nChain 4: Iteration: 1273 / 2000 [ 63%]  (Sampling)\nChain 4: Iteration: 1274 / 2000 [ 63%]  (Sampling)\nChain 4: Iteration: 1275 / 2000 [ 63%]  (Sampling)\nChain 4: Iteration: 1276 / 2000 [ 63%]  (Sampling)\nChain 4: Iteration: 1277 / 2000 [ 63%]  (Sampling)\nChain 4: Iteration: 1278 / 2000 [ 63%]  (Sampling)\nChain 4: Iteration: 1279 / 2000 [ 63%]  (Sampling)\nChain 4: Iteration: 1280 / 2000 [ 64%]  (Sampling)\nChain 4: Iteration: 1281 / 2000 [ 64%]  (Sampling)\nChain 4: Iteration: 1282 / 2000 [ 64%]  (Sampling)\nChain 4: Iteration: 1283 / 2000 [ 64%]  (Sampling)\nChain 4: Iteration: 1284 / 2000 [ 64%]  (Sampling)\nChain 4: Iteration: 1285 / 2000 [ 64%]  (Sampling)\nChain 4: Iteration: 1286 / 2000 [ 64%]  (Sampling)\nChain 4: Iteration: 1287 / 2000 [ 64%]  (Sampling)\nChain 4: Iteration: 1288 / 2000 [ 64%]  (Sampling)\nChain 4: Iteration: 1289 / 2000 [ 64%]  (Sampling)\nChain 4: Iteration: 1290 / 2000 [ 64%]  (Sampling)\nChain 4: Iteration: 1291 / 2000 [ 64%]  (Sampling)\nChain 4: Iteration: 1292 / 2000 [ 64%]  (Sampling)\nChain 4: Iteration: 1293 / 2000 [ 64%]  (Sampling)\nChain 4: Iteration: 1294 / 2000 [ 64%]  (Sampling)\nChain 4: Iteration: 1295 / 2000 [ 64%]  (Sampling)\nChain 4: Iteration: 1296 / 2000 [ 64%]  (Sampling)\nChain 4: Iteration: 1297 / 2000 [ 64%]  (Sampling)\nChain 4: Iteration: 1298 / 2000 [ 64%]  (Sampling)\nChain 4: Iteration: 1299 / 2000 [ 64%]  (Sampling)\nChain 4: Iteration: 1300 / 2000 [ 65%]  (Sampling)\nChain 4: Iteration: 1301 / 2000 [ 65%]  (Sampling)\nChain 4: Iteration: 1302 / 2000 [ 65%]  (Sampling)\nChain 4: Iteration: 1303 / 2000 [ 65%]  (Sampling)\nChain 4: Iteration: 1304 / 2000 [ 65%]  (Sampling)\nChain 4: Iteration: 1305 / 2000 [ 65%]  (Sampling)\nChain 4: Iteration: 1306 / 2000 [ 65%]  (Sampling)\nChain 4: Iteration: 1307 / 2000 [ 65%]  (Sampling)\nChain 4: Iteration: 1308 / 2000 [ 65%]  (Sampling)\nChain 4: Iteration: 1309 / 2000 [ 65%]  (Sampling)\nChain 4: Iteration: 1310 / 2000 [ 65%]  (Sampling)\nChain 4: Iteration: 1311 / 2000 [ 65%]  (Sampling)\nChain 4: Iteration: 1312 / 2000 [ 65%]  (Sampling)\nChain 4: Iteration: 1313 / 2000 [ 65%]  (Sampling)\nChain 4: Iteration: 1314 / 2000 [ 65%]  (Sampling)\nChain 4: Iteration: 1315 / 2000 [ 65%]  (Sampling)\nChain 4: Iteration: 1316 / 2000 [ 65%]  (Sampling)\nChain 4: Iteration: 1317 / 2000 [ 65%]  (Sampling)\nChain 4: Iteration: 1318 / 2000 [ 65%]  (Sampling)\nChain 4: Iteration: 1319 / 2000 [ 65%]  (Sampling)\nChain 4: Iteration: 1320 / 2000 [ 66%]  (Sampling)\nChain 4: Iteration: 1321 / 2000 [ 66%]  (Sampling)\nChain 4: Iteration: 1322 / 2000 [ 66%]  (Sampling)\nChain 4: Iteration: 1323 / 2000 [ 66%]  (Sampling)\nChain 4: Iteration: 1324 / 2000 [ 66%]  (Sampling)\nChain 4: Iteration: 1325 / 2000 [ 66%]  (Sampling)\nChain 4: Iteration: 1326 / 2000 [ 66%]  (Sampling)\nChain 4: Iteration: 1327 / 2000 [ 66%]  (Sampling)\nChain 4: Iteration: 1328 / 2000 [ 66%]  (Sampling)\nChain 4: Iteration: 1329 / 2000 [ 66%]  (Sampling)\nChain 4: Iteration: 1330 / 2000 [ 66%]  (Sampling)\nChain 4: Iteration: 1331 / 2000 [ 66%]  (Sampling)\nChain 4: Iteration: 1332 / 2000 [ 66%]  (Sampling)\nChain 4: Iteration: 1333 / 2000 [ 66%]  (Sampling)\nChain 4: Iteration: 1334 / 2000 [ 66%]  (Sampling)\nChain 4: Iteration: 1335 / 2000 [ 66%]  (Sampling)\nChain 4: Iteration: 1336 / 2000 [ 66%]  (Sampling)\nChain 4: Iteration: 1337 / 2000 [ 66%]  (Sampling)\nChain 4: Iteration: 1338 / 2000 [ 66%]  (Sampling)\nChain 4: Iteration: 1339 / 2000 [ 66%]  (Sampling)\nChain 4: Iteration: 1340 / 2000 [ 67%]  (Sampling)\nChain 4: Iteration: 1341 / 2000 [ 67%]  (Sampling)\nChain 4: Iteration: 1342 / 2000 [ 67%]  (Sampling)\nChain 4: Iteration: 1343 / 2000 [ 67%]  (Sampling)\nChain 4: Iteration: 1344 / 2000 [ 67%]  (Sampling)\nChain 4: Iteration: 1345 / 2000 [ 67%]  (Sampling)\nChain 4: Iteration: 1346 / 2000 [ 67%]  (Sampling)\nChain 4: Iteration: 1347 / 2000 [ 67%]  (Sampling)\nChain 4: Iteration: 1348 / 2000 [ 67%]  (Sampling)\nChain 4: Iteration: 1349 / 2000 [ 67%]  (Sampling)\nChain 4: Iteration: 1350 / 2000 [ 67%]  (Sampling)\nChain 4: Iteration: 1351 / 2000 [ 67%]  (Sampling)\nChain 4: Iteration: 1352 / 2000 [ 67%]  (Sampling)\nChain 4: Iteration: 1353 / 2000 [ 67%]  (Sampling)\nChain 4: Iteration: 1354 / 2000 [ 67%]  (Sampling)\nChain 4: Iteration: 1355 / 2000 [ 67%]  (Sampling)\nChain 4: Iteration: 1356 / 2000 [ 67%]  (Sampling)\nChain 4: Iteration: 1357 / 2000 [ 67%]  (Sampling)\nChain 4: Iteration: 1358 / 2000 [ 67%]  (Sampling)\nChain 4: Iteration: 1359 / 2000 [ 67%]  (Sampling)\nChain 4: Iteration: 1360 / 2000 [ 68%]  (Sampling)\nChain 4: Iteration: 1361 / 2000 [ 68%]  (Sampling)\nChain 4: Iteration: 1362 / 2000 [ 68%]  (Sampling)\nChain 4: Iteration: 1363 / 2000 [ 68%]  (Sampling)\nChain 4: Iteration: 1364 / 2000 [ 68%]  (Sampling)\nChain 4: Iteration: 1365 / 2000 [ 68%]  (Sampling)\nChain 4: Iteration: 1366 / 2000 [ 68%]  (Sampling)\nChain 4: Iteration: 1367 / 2000 [ 68%]  (Sampling)\nChain 4: Iteration: 1368 / 2000 [ 68%]  (Sampling)\nChain 4: Iteration: 1369 / 2000 [ 68%]  (Sampling)\nChain 4: Iteration: 1370 / 2000 [ 68%]  (Sampling)\nChain 4: Iteration: 1371 / 2000 [ 68%]  (Sampling)\nChain 4: Iteration: 1372 / 2000 [ 68%]  (Sampling)\nChain 4: Iteration: 1373 / 2000 [ 68%]  (Sampling)\nChain 4: Iteration: 1374 / 2000 [ 68%]  (Sampling)\nChain 4: Iteration: 1375 / 2000 [ 68%]  (Sampling)\nChain 4: Iteration: 1376 / 2000 [ 68%]  (Sampling)\nChain 4: Iteration: 1377 / 2000 [ 68%]  (Sampling)\nChain 4: Iteration: 1378 / 2000 [ 68%]  (Sampling)\nChain 4: Iteration: 1379 / 2000 [ 68%]  (Sampling)\nChain 4: Iteration: 1380 / 2000 [ 69%]  (Sampling)\nChain 4: Iteration: 1381 / 2000 [ 69%]  (Sampling)\nChain 4: Iteration: 1382 / 2000 [ 69%]  (Sampling)\nChain 4: Iteration: 1383 / 2000 [ 69%]  (Sampling)\nChain 4: Iteration: 1384 / 2000 [ 69%]  (Sampling)\nChain 4: Iteration: 1385 / 2000 [ 69%]  (Sampling)\nChain 4: Iteration: 1386 / 2000 [ 69%]  (Sampling)\nChain 4: Iteration: 1387 / 2000 [ 69%]  (Sampling)\nChain 4: Iteration: 1388 / 2000 [ 69%]  (Sampling)\nChain 4: Iteration: 1389 / 2000 [ 69%]  (Sampling)\nChain 4: Iteration: 1390 / 2000 [ 69%]  (Sampling)\nChain 4: Iteration: 1391 / 2000 [ 69%]  (Sampling)\nChain 4: Iteration: 1392 / 2000 [ 69%]  (Sampling)\nChain 4: Iteration: 1393 / 2000 [ 69%]  (Sampling)\nChain 4: Iteration: 1394 / 2000 [ 69%]  (Sampling)\nChain 4: Iteration: 1395 / 2000 [ 69%]  (Sampling)\nChain 4: Iteration: 1396 / 2000 [ 69%]  (Sampling)\nChain 4: Iteration: 1397 / 2000 [ 69%]  (Sampling)\nChain 4: Iteration: 1398 / 2000 [ 69%]  (Sampling)\nChain 4: Iteration: 1399 / 2000 [ 69%]  (Sampling)\nChain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 4: Iteration: 1401 / 2000 [ 70%]  (Sampling)\nChain 4: Iteration: 1402 / 2000 [ 70%]  (Sampling)\nChain 4: Iteration: 1403 / 2000 [ 70%]  (Sampling)\nChain 4: Iteration: 1404 / 2000 [ 70%]  (Sampling)\nChain 4: Iteration: 1405 / 2000 [ 70%]  (Sampling)\nChain 4: Iteration: 1406 / 2000 [ 70%]  (Sampling)\nChain 4: Iteration: 1407 / 2000 [ 70%]  (Sampling)\nChain 4: Iteration: 1408 / 2000 [ 70%]  (Sampling)\nChain 4: Iteration: 1409 / 2000 [ 70%]  (Sampling)\nChain 4: Iteration: 1410 / 2000 [ 70%]  (Sampling)\nChain 4: Iteration: 1411 / 2000 [ 70%]  (Sampling)\nChain 4: Iteration: 1412 / 2000 [ 70%]  (Sampling)\nChain 4: Iteration: 1413 / 2000 [ 70%]  (Sampling)\nChain 4: Iteration: 1414 / 2000 [ 70%]  (Sampling)\nChain 4: Iteration: 1415 / 2000 [ 70%]  (Sampling)\nChain 4: Iteration: 1416 / 2000 [ 70%]  (Sampling)\nChain 4: Iteration: 1417 / 2000 [ 70%]  (Sampling)\nChain 4: Iteration: 1418 / 2000 [ 70%]  (Sampling)\nChain 4: Iteration: 1419 / 2000 [ 70%]  (Sampling)\nChain 4: Iteration: 1420 / 2000 [ 71%]  (Sampling)\nChain 4: Iteration: 1421 / 2000 [ 71%]  (Sampling)\nChain 4: Iteration: 1422 / 2000 [ 71%]  (Sampling)\nChain 4: Iteration: 1423 / 2000 [ 71%]  (Sampling)\nChain 4: Iteration: 1424 / 2000 [ 71%]  (Sampling)\nChain 4: Iteration: 1425 / 2000 [ 71%]  (Sampling)\nChain 4: Iteration: 1426 / 2000 [ 71%]  (Sampling)\nChain 4: Iteration: 1427 / 2000 [ 71%]  (Sampling)\nChain 4: Iteration: 1428 / 2000 [ 71%]  (Sampling)\nChain 4: Iteration: 1429 / 2000 [ 71%]  (Sampling)\nChain 4: Iteration: 1430 / 2000 [ 71%]  (Sampling)\nChain 4: Iteration: 1431 / 2000 [ 71%]  (Sampling)\nChain 4: Iteration: 1432 / 2000 [ 71%]  (Sampling)\nChain 4: Iteration: 1433 / 2000 [ 71%]  (Sampling)\nChain 4: Iteration: 1434 / 2000 [ 71%]  (Sampling)\nChain 4: Iteration: 1435 / 2000 [ 71%]  (Sampling)\nChain 4: Iteration: 1436 / 2000 [ 71%]  (Sampling)\nChain 4: Iteration: 1437 / 2000 [ 71%]  (Sampling)\nChain 4: Iteration: 1438 / 2000 [ 71%]  (Sampling)\nChain 4: Iteration: 1439 / 2000 [ 71%]  (Sampling)\nChain 4: Iteration: 1440 / 2000 [ 72%]  (Sampling)\nChain 4: Iteration: 1441 / 2000 [ 72%]  (Sampling)\nChain 4: Iteration: 1442 / 2000 [ 72%]  (Sampling)\nChain 4: Iteration: 1443 / 2000 [ 72%]  (Sampling)\nChain 4: Iteration: 1444 / 2000 [ 72%]  (Sampling)\nChain 4: Iteration: 1445 / 2000 [ 72%]  (Sampling)\nChain 4: Iteration: 1446 / 2000 [ 72%]  (Sampling)\nChain 4: Iteration: 1447 / 2000 [ 72%]  (Sampling)\nChain 4: Iteration: 1448 / 2000 [ 72%]  (Sampling)\nChain 4: Iteration: 1449 / 2000 [ 72%]  (Sampling)\nChain 4: Iteration: 1450 / 2000 [ 72%]  (Sampling)\nChain 4: Iteration: 1451 / 2000 [ 72%]  (Sampling)\nChain 4: Iteration: 1452 / 2000 [ 72%]  (Sampling)\nChain 4: Iteration: 1453 / 2000 [ 72%]  (Sampling)\nChain 4: Iteration: 1454 / 2000 [ 72%]  (Sampling)\nChain 4: Iteration: 1455 / 2000 [ 72%]  (Sampling)\nChain 4: Iteration: 1456 / 2000 [ 72%]  (Sampling)\nChain 4: Iteration: 1457 / 2000 [ 72%]  (Sampling)\nChain 4: Iteration: 1458 / 2000 [ 72%]  (Sampling)\nChain 4: Iteration: 1459 / 2000 [ 72%]  (Sampling)\nChain 4: Iteration: 1460 / 2000 [ 73%]  (Sampling)\nChain 4: Iteration: 1461 / 2000 [ 73%]  (Sampling)\nChain 4: Iteration: 1462 / 2000 [ 73%]  (Sampling)\nChain 4: Iteration: 1463 / 2000 [ 73%]  (Sampling)\nChain 4: Iteration: 1464 / 2000 [ 73%]  (Sampling)\nChain 4: Iteration: 1465 / 2000 [ 73%]  (Sampling)\nChain 4: Iteration: 1466 / 2000 [ 73%]  (Sampling)\nChain 4: Iteration: 1467 / 2000 [ 73%]  (Sampling)\nChain 4: Iteration: 1468 / 2000 [ 73%]  (Sampling)\nChain 4: Iteration: 1469 / 2000 [ 73%]  (Sampling)\nChain 4: Iteration: 1470 / 2000 [ 73%]  (Sampling)\nChain 4: Iteration: 1471 / 2000 [ 73%]  (Sampling)\nChain 4: Iteration: 1472 / 2000 [ 73%]  (Sampling)\nChain 4: Iteration: 1473 / 2000 [ 73%]  (Sampling)\nChain 4: Iteration: 1474 / 2000 [ 73%]  (Sampling)\nChain 4: Iteration: 1475 / 2000 [ 73%]  (Sampling)\nChain 4: Iteration: 1476 / 2000 [ 73%]  (Sampling)\nChain 4: Iteration: 1477 / 2000 [ 73%]  (Sampling)\nChain 4: Iteration: 1478 / 2000 [ 73%]  (Sampling)\nChain 4: Iteration: 1479 / 2000 [ 73%]  (Sampling)\nChain 4: Iteration: 1480 / 2000 [ 74%]  (Sampling)\nChain 4: Iteration: 1481 / 2000 [ 74%]  (Sampling)\nChain 4: Iteration: 1482 / 2000 [ 74%]  (Sampling)\nChain 4: Iteration: 1483 / 2000 [ 74%]  (Sampling)\nChain 4: Iteration: 1484 / 2000 [ 74%]  (Sampling)\nChain 4: Iteration: 1485 / 2000 [ 74%]  (Sampling)\nChain 4: Iteration: 1486 / 2000 [ 74%]  (Sampling)\nChain 4: Iteration: 1487 / 2000 [ 74%]  (Sampling)\nChain 4: Iteration: 1488 / 2000 [ 74%]  (Sampling)\nChain 4: Iteration: 1489 / 2000 [ 74%]  (Sampling)\nChain 4: Iteration: 1490 / 2000 [ 74%]  (Sampling)\nChain 4: Iteration: 1491 / 2000 [ 74%]  (Sampling)\nChain 4: Iteration: 1492 / 2000 [ 74%]  (Sampling)\nChain 4: Iteration: 1493 / 2000 [ 74%]  (Sampling)\nChain 4: Iteration: 1494 / 2000 [ 74%]  (Sampling)\nChain 4: Iteration: 1495 / 2000 [ 74%]  (Sampling)\nChain 4: Iteration: 1496 / 2000 [ 74%]  (Sampling)\nChain 4: Iteration: 1497 / 2000 [ 74%]  (Sampling)\nChain 4: Iteration: 1498 / 2000 [ 74%]  (Sampling)\nChain 4: Iteration: 1499 / 2000 [ 74%]  (Sampling)\nChain 4: Iteration: 1500 / 2000 [ 75%]  (Sampling)\nChain 4: Iteration: 1501 / 2000 [ 75%]  (Sampling)\nChain 4: Iteration: 1502 / 2000 [ 75%]  (Sampling)\nChain 4: Iteration: 1503 / 2000 [ 75%]  (Sampling)\nChain 4: Iteration: 1504 / 2000 [ 75%]  (Sampling)\nChain 4: Iteration: 1505 / 2000 [ 75%]  (Sampling)\nChain 4: Iteration: 1506 / 2000 [ 75%]  (Sampling)\nChain 4: Iteration: 1507 / 2000 [ 75%]  (Sampling)\nChain 4: Iteration: 1508 / 2000 [ 75%]  (Sampling)\nChain 4: Iteration: 1509 / 2000 [ 75%]  (Sampling)\nChain 4: Iteration: 1510 / 2000 [ 75%]  (Sampling)\nChain 4: Iteration: 1511 / 2000 [ 75%]  (Sampling)\nChain 4: Iteration: 1512 / 2000 [ 75%]  (Sampling)\nChain 4: Iteration: 1513 / 2000 [ 75%]  (Sampling)\nChain 4: Iteration: 1514 / 2000 [ 75%]  (Sampling)\nChain 4: Iteration: 1515 / 2000 [ 75%]  (Sampling)\nChain 4: Iteration: 1516 / 2000 [ 75%]  (Sampling)\nChain 4: Iteration: 1517 / 2000 [ 75%]  (Sampling)\nChain 4: Iteration: 1518 / 2000 [ 75%]  (Sampling)\nChain 4: Iteration: 1519 / 2000 [ 75%]  (Sampling)\nChain 4: Iteration: 1520 / 2000 [ 76%]  (Sampling)\nChain 4: Iteration: 1521 / 2000 [ 76%]  (Sampling)\nChain 4: Iteration: 1522 / 2000 [ 76%]  (Sampling)\nChain 4: Iteration: 1523 / 2000 [ 76%]  (Sampling)\nChain 4: Iteration: 1524 / 2000 [ 76%]  (Sampling)\nChain 4: Iteration: 1525 / 2000 [ 76%]  (Sampling)\nChain 4: Iteration: 1526 / 2000 [ 76%]  (Sampling)\nChain 4: Iteration: 1527 / 2000 [ 76%]  (Sampling)\nChain 4: Iteration: 1528 / 2000 [ 76%]  (Sampling)\nChain 4: Iteration: 1529 / 2000 [ 76%]  (Sampling)\nChain 4: Iteration: 1530 / 2000 [ 76%]  (Sampling)\nChain 4: Iteration: 1531 / 2000 [ 76%]  (Sampling)\nChain 4: Iteration: 1532 / 2000 [ 76%]  (Sampling)\nChain 4: Iteration: 1533 / 2000 [ 76%]  (Sampling)\nChain 4: Iteration: 1534 / 2000 [ 76%]  (Sampling)\nChain 4: Iteration: 1535 / 2000 [ 76%]  (Sampling)\nChain 4: Iteration: 1536 / 2000 [ 76%]  (Sampling)\nChain 4: Iteration: 1537 / 2000 [ 76%]  (Sampling)\nChain 4: Iteration: 1538 / 2000 [ 76%]  (Sampling)\nChain 4: Iteration: 1539 / 2000 [ 76%]  (Sampling)\nChain 4: Iteration: 1540 / 2000 [ 77%]  (Sampling)\nChain 4: Iteration: 1541 / 2000 [ 77%]  (Sampling)\nChain 4: Iteration: 1542 / 2000 [ 77%]  (Sampling)\nChain 4: Iteration: 1543 / 2000 [ 77%]  (Sampling)\nChain 4: Iteration: 1544 / 2000 [ 77%]  (Sampling)\nChain 4: Iteration: 1545 / 2000 [ 77%]  (Sampling)\nChain 4: Iteration: 1546 / 2000 [ 77%]  (Sampling)\nChain 4: Iteration: 1547 / 2000 [ 77%]  (Sampling)\nChain 4: Iteration: 1548 / 2000 [ 77%]  (Sampling)\nChain 4: Iteration: 1549 / 2000 [ 77%]  (Sampling)\nChain 4: Iteration: 1550 / 2000 [ 77%]  (Sampling)\nChain 4: Iteration: 1551 / 2000 [ 77%]  (Sampling)\nChain 4: Iteration: 1552 / 2000 [ 77%]  (Sampling)\nChain 4: Iteration: 1553 / 2000 [ 77%]  (Sampling)\nChain 4: Iteration: 1554 / 2000 [ 77%]  (Sampling)\nChain 4: Iteration: 1555 / 2000 [ 77%]  (Sampling)\nChain 4: Iteration: 1556 / 2000 [ 77%]  (Sampling)\nChain 4: Iteration: 1557 / 2000 [ 77%]  (Sampling)\nChain 4: Iteration: 1558 / 2000 [ 77%]  (Sampling)\nChain 4: Iteration: 1559 / 2000 [ 77%]  (Sampling)\nChain 4: Iteration: 1560 / 2000 [ 78%]  (Sampling)\nChain 4: Iteration: 1561 / 2000 [ 78%]  (Sampling)\nChain 4: Iteration: 1562 / 2000 [ 78%]  (Sampling)\nChain 4: Iteration: 1563 / 2000 [ 78%]  (Sampling)\nChain 4: Iteration: 1564 / 2000 [ 78%]  (Sampling)\nChain 4: Iteration: 1565 / 2000 [ 78%]  (Sampling)\nChain 4: Iteration: 1566 / 2000 [ 78%]  (Sampling)\nChain 4: Iteration: 1567 / 2000 [ 78%]  (Sampling)\nChain 4: Iteration: 1568 / 2000 [ 78%]  (Sampling)\nChain 4: Iteration: 1569 / 2000 [ 78%]  (Sampling)\nChain 4: Iteration: 1570 / 2000 [ 78%]  (Sampling)\nChain 4: Iteration: 1571 / 2000 [ 78%]  (Sampling)\nChain 4: Iteration: 1572 / 2000 [ 78%]  (Sampling)\nChain 4: Iteration: 1573 / 2000 [ 78%]  (Sampling)\nChain 4: Iteration: 1574 / 2000 [ 78%]  (Sampling)\nChain 4: Iteration: 1575 / 2000 [ 78%]  (Sampling)\nChain 4: Iteration: 1576 / 2000 [ 78%]  (Sampling)\nChain 4: Iteration: 1577 / 2000 [ 78%]  (Sampling)\nChain 4: Iteration: 1578 / 2000 [ 78%]  (Sampling)\nChain 4: Iteration: 1579 / 2000 [ 78%]  (Sampling)\nChain 4: Iteration: 1580 / 2000 [ 79%]  (Sampling)\nChain 4: Iteration: 1581 / 2000 [ 79%]  (Sampling)\nChain 4: Iteration: 1582 / 2000 [ 79%]  (Sampling)\nChain 4: Iteration: 1583 / 2000 [ 79%]  (Sampling)\nChain 4: Iteration: 1584 / 2000 [ 79%]  (Sampling)\nChain 4: Iteration: 1585 / 2000 [ 79%]  (Sampling)\nChain 4: Iteration: 1586 / 2000 [ 79%]  (Sampling)\nChain 4: Iteration: 1587 / 2000 [ 79%]  (Sampling)\nChain 4: Iteration: 1588 / 2000 [ 79%]  (Sampling)\nChain 4: Iteration: 1589 / 2000 [ 79%]  (Sampling)\nChain 4: Iteration: 1590 / 2000 [ 79%]  (Sampling)\nChain 4: Iteration: 1591 / 2000 [ 79%]  (Sampling)\nChain 4: Iteration: 1592 / 2000 [ 79%]  (Sampling)\nChain 4: Iteration: 1593 / 2000 [ 79%]  (Sampling)\nChain 4: Iteration: 1594 / 2000 [ 79%]  (Sampling)\nChain 4: Iteration: 1595 / 2000 [ 79%]  (Sampling)\nChain 4: Iteration: 1596 / 2000 [ 79%]  (Sampling)\nChain 4: Iteration: 1597 / 2000 [ 79%]  (Sampling)\nChain 4: Iteration: 1598 / 2000 [ 79%]  (Sampling)\nChain 4: Iteration: 1599 / 2000 [ 79%]  (Sampling)\nChain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 4: Iteration: 1601 / 2000 [ 80%]  (Sampling)\nChain 4: Iteration: 1602 / 2000 [ 80%]  (Sampling)\nChain 4: Iteration: 1603 / 2000 [ 80%]  (Sampling)\nChain 4: Iteration: 1604 / 2000 [ 80%]  (Sampling)\nChain 4: Iteration: 1605 / 2000 [ 80%]  (Sampling)\nChain 4: Iteration: 1606 / 2000 [ 80%]  (Sampling)\nChain 4: Iteration: 1607 / 2000 [ 80%]  (Sampling)\nChain 4: Iteration: 1608 / 2000 [ 80%]  (Sampling)\nChain 4: Iteration: 1609 / 2000 [ 80%]  (Sampling)\nChain 4: Iteration: 1610 / 2000 [ 80%]  (Sampling)\nChain 4: Iteration: 1611 / 2000 [ 80%]  (Sampling)\nChain 4: Iteration: 1612 / 2000 [ 80%]  (Sampling)\nChain 4: Iteration: 1613 / 2000 [ 80%]  (Sampling)\nChain 4: Iteration: 1614 / 2000 [ 80%]  (Sampling)\nChain 4: Iteration: 1615 / 2000 [ 80%]  (Sampling)\nChain 4: Iteration: 1616 / 2000 [ 80%]  (Sampling)\nChain 4: Iteration: 1617 / 2000 [ 80%]  (Sampling)\nChain 4: Iteration: 1618 / 2000 [ 80%]  (Sampling)\nChain 4: Iteration: 1619 / 2000 [ 80%]  (Sampling)\nChain 4: Iteration: 1620 / 2000 [ 81%]  (Sampling)\nChain 4: Iteration: 1621 / 2000 [ 81%]  (Sampling)\nChain 4: Iteration: 1622 / 2000 [ 81%]  (Sampling)\nChain 4: Iteration: 1623 / 2000 [ 81%]  (Sampling)\nChain 4: Iteration: 1624 / 2000 [ 81%]  (Sampling)\nChain 4: Iteration: 1625 / 2000 [ 81%]  (Sampling)\nChain 4: Iteration: 1626 / 2000 [ 81%]  (Sampling)\nChain 4: Iteration: 1627 / 2000 [ 81%]  (Sampling)\nChain 4: Iteration: 1628 / 2000 [ 81%]  (Sampling)\nChain 4: Iteration: 1629 / 2000 [ 81%]  (Sampling)\nChain 4: Iteration: 1630 / 2000 [ 81%]  (Sampling)\nChain 4: Iteration: 1631 / 2000 [ 81%]  (Sampling)\nChain 4: Iteration: 1632 / 2000 [ 81%]  (Sampling)\nChain 4: Iteration: 1633 / 2000 [ 81%]  (Sampling)\nChain 4: Iteration: 1634 / 2000 [ 81%]  (Sampling)\nChain 4: Iteration: 1635 / 2000 [ 81%]  (Sampling)\nChain 4: Iteration: 1636 / 2000 [ 81%]  (Sampling)\nChain 4: Iteration: 1637 / 2000 [ 81%]  (Sampling)\nChain 4: Iteration: 1638 / 2000 [ 81%]  (Sampling)\nChain 4: Iteration: 1639 / 2000 [ 81%]  (Sampling)\nChain 4: Iteration: 1640 / 2000 [ 82%]  (Sampling)\nChain 4: Iteration: 1641 / 2000 [ 82%]  (Sampling)\nChain 4: Iteration: 1642 / 2000 [ 82%]  (Sampling)\nChain 4: Iteration: 1643 / 2000 [ 82%]  (Sampling)\nChain 4: Iteration: 1644 / 2000 [ 82%]  (Sampling)\nChain 4: Iteration: 1645 / 2000 [ 82%]  (Sampling)\nChain 4: Iteration: 1646 / 2000 [ 82%]  (Sampling)\nChain 4: Iteration: 1647 / 2000 [ 82%]  (Sampling)\nChain 4: Iteration: 1648 / 2000 [ 82%]  (Sampling)\nChain 4: Iteration: 1649 / 2000 [ 82%]  (Sampling)\nChain 4: Iteration: 1650 / 2000 [ 82%]  (Sampling)\nChain 4: Iteration: 1651 / 2000 [ 82%]  (Sampling)\nChain 4: Iteration: 1652 / 2000 [ 82%]  (Sampling)\nChain 4: Iteration: 1653 / 2000 [ 82%]  (Sampling)\nChain 4: Iteration: 1654 / 2000 [ 82%]  (Sampling)\nChain 4: Iteration: 1655 / 2000 [ 82%]  (Sampling)\nChain 4: Iteration: 1656 / 2000 [ 82%]  (Sampling)\nChain 4: Iteration: 1657 / 2000 [ 82%]  (Sampling)\nChain 4: Iteration: 1658 / 2000 [ 82%]  (Sampling)\nChain 4: Iteration: 1659 / 2000 [ 82%]  (Sampling)\nChain 4: Iteration: 1660 / 2000 [ 83%]  (Sampling)\nChain 4: Iteration: 1661 / 2000 [ 83%]  (Sampling)\nChain 4: Iteration: 1662 / 2000 [ 83%]  (Sampling)\nChain 4: Iteration: 1663 / 2000 [ 83%]  (Sampling)\nChain 4: Iteration: 1664 / 2000 [ 83%]  (Sampling)\nChain 4: Iteration: 1665 / 2000 [ 83%]  (Sampling)\nChain 4: Iteration: 1666 / 2000 [ 83%]  (Sampling)\nChain 4: Iteration: 1667 / 2000 [ 83%]  (Sampling)\nChain 4: Iteration: 1668 / 2000 [ 83%]  (Sampling)\nChain 4: Iteration: 1669 / 2000 [ 83%]  (Sampling)\nChain 4: Iteration: 1670 / 2000 [ 83%]  (Sampling)\nChain 4: Iteration: 1671 / 2000 [ 83%]  (Sampling)\nChain 4: Iteration: 1672 / 2000 [ 83%]  (Sampling)\nChain 4: Iteration: 1673 / 2000 [ 83%]  (Sampling)\nChain 4: Iteration: 1674 / 2000 [ 83%]  (Sampling)\nChain 4: Iteration: 1675 / 2000 [ 83%]  (Sampling)\nChain 4: Iteration: 1676 / 2000 [ 83%]  (Sampling)\nChain 4: Iteration: 1677 / 2000 [ 83%]  (Sampling)\nChain 4: Iteration: 1678 / 2000 [ 83%]  (Sampling)\nChain 4: Iteration: 1679 / 2000 [ 83%]  (Sampling)\nChain 4: Iteration: 1680 / 2000 [ 84%]  (Sampling)\nChain 4: Iteration: 1681 / 2000 [ 84%]  (Sampling)\nChain 4: Iteration: 1682 / 2000 [ 84%]  (Sampling)\nChain 4: Iteration: 1683 / 2000 [ 84%]  (Sampling)\nChain 4: Iteration: 1684 / 2000 [ 84%]  (Sampling)\nChain 4: Iteration: 1685 / 2000 [ 84%]  (Sampling)\nChain 4: Iteration: 1686 / 2000 [ 84%]  (Sampling)\nChain 4: Iteration: 1687 / 2000 [ 84%]  (Sampling)\nChain 4: Iteration: 1688 / 2000 [ 84%]  (Sampling)\nChain 4: Iteration: 1689 / 2000 [ 84%]  (Sampling)\nChain 4: Iteration: 1690 / 2000 [ 84%]  (Sampling)\nChain 4: Iteration: 1691 / 2000 [ 84%]  (Sampling)\nChain 4: Iteration: 1692 / 2000 [ 84%]  (Sampling)\nChain 4: Iteration: 1693 / 2000 [ 84%]  (Sampling)\nChain 4: Iteration: 1694 / 2000 [ 84%]  (Sampling)\nChain 4: Iteration: 1695 / 2000 [ 84%]  (Sampling)\nChain 4: Iteration: 1696 / 2000 [ 84%]  (Sampling)\nChain 4: Iteration: 1697 / 2000 [ 84%]  (Sampling)\nChain 4: Iteration: 1698 / 2000 [ 84%]  (Sampling)\nChain 4: Iteration: 1699 / 2000 [ 84%]  (Sampling)\nChain 4: Iteration: 1700 / 2000 [ 85%]  (Sampling)\nChain 4: Iteration: 1701 / 2000 [ 85%]  (Sampling)\nChain 4: Iteration: 1702 / 2000 [ 85%]  (Sampling)\nChain 4: Iteration: 1703 / 2000 [ 85%]  (Sampling)\nChain 4: Iteration: 1704 / 2000 [ 85%]  (Sampling)\nChain 4: Iteration: 1705 / 2000 [ 85%]  (Sampling)\nChain 4: Iteration: 1706 / 2000 [ 85%]  (Sampling)\nChain 4: Iteration: 1707 / 2000 [ 85%]  (Sampling)\nChain 4: Iteration: 1708 / 2000 [ 85%]  (Sampling)\nChain 4: Iteration: 1709 / 2000 [ 85%]  (Sampling)\nChain 4: Iteration: 1710 / 2000 [ 85%]  (Sampling)\nChain 4: Iteration: 1711 / 2000 [ 85%]  (Sampling)\nChain 4: Iteration: 1712 / 2000 [ 85%]  (Sampling)\nChain 4: Iteration: 1713 / 2000 [ 85%]  (Sampling)\nChain 4: Iteration: 1714 / 2000 [ 85%]  (Sampling)\nChain 4: Iteration: 1715 / 2000 [ 85%]  (Sampling)\nChain 4: Iteration: 1716 / 2000 [ 85%]  (Sampling)\nChain 4: Iteration: 1717 / 2000 [ 85%]  (Sampling)\nChain 4: Iteration: 1718 / 2000 [ 85%]  (Sampling)\nChain 4: Iteration: 1719 / 2000 [ 85%]  (Sampling)\nChain 4: Iteration: 1720 / 2000 [ 86%]  (Sampling)\nChain 4: Iteration: 1721 / 2000 [ 86%]  (Sampling)\nChain 4: Iteration: 1722 / 2000 [ 86%]  (Sampling)\nChain 4: Iteration: 1723 / 2000 [ 86%]  (Sampling)\nChain 4: Iteration: 1724 / 2000 [ 86%]  (Sampling)\nChain 4: Iteration: 1725 / 2000 [ 86%]  (Sampling)\nChain 4: Iteration: 1726 / 2000 [ 86%]  (Sampling)\nChain 4: Iteration: 1727 / 2000 [ 86%]  (Sampling)\nChain 4: Iteration: 1728 / 2000 [ 86%]  (Sampling)\nChain 4: Iteration: 1729 / 2000 [ 86%]  (Sampling)\nChain 4: Iteration: 1730 / 2000 [ 86%]  (Sampling)\nChain 4: Iteration: 1731 / 2000 [ 86%]  (Sampling)\nChain 4: Iteration: 1732 / 2000 [ 86%]  (Sampling)\nChain 4: Iteration: 1733 / 2000 [ 86%]  (Sampling)\nChain 4: Iteration: 1734 / 2000 [ 86%]  (Sampling)\nChain 4: Iteration: 1735 / 2000 [ 86%]  (Sampling)\nChain 4: Iteration: 1736 / 2000 [ 86%]  (Sampling)\nChain 4: Iteration: 1737 / 2000 [ 86%]  (Sampling)\nChain 4: Iteration: 1738 / 2000 [ 86%]  (Sampling)\nChain 4: Iteration: 1739 / 2000 [ 86%]  (Sampling)\nChain 4: Iteration: 1740 / 2000 [ 87%]  (Sampling)\nChain 4: Iteration: 1741 / 2000 [ 87%]  (Sampling)\nChain 4: Iteration: 1742 / 2000 [ 87%]  (Sampling)\nChain 4: Iteration: 1743 / 2000 [ 87%]  (Sampling)\nChain 4: Iteration: 1744 / 2000 [ 87%]  (Sampling)\nChain 4: Iteration: 1745 / 2000 [ 87%]  (Sampling)\nChain 4: Iteration: 1746 / 2000 [ 87%]  (Sampling)\nChain 4: Iteration: 1747 / 2000 [ 87%]  (Sampling)\nChain 4: Iteration: 1748 / 2000 [ 87%]  (Sampling)\nChain 4: Iteration: 1749 / 2000 [ 87%]  (Sampling)\nChain 4: Iteration: 1750 / 2000 [ 87%]  (Sampling)\nChain 4: Iteration: 1751 / 2000 [ 87%]  (Sampling)\nChain 4: Iteration: 1752 / 2000 [ 87%]  (Sampling)\nChain 4: Iteration: 1753 / 2000 [ 87%]  (Sampling)\nChain 4: Iteration: 1754 / 2000 [ 87%]  (Sampling)\nChain 4: Iteration: 1755 / 2000 [ 87%]  (Sampling)\nChain 4: Iteration: 1756 / 2000 [ 87%]  (Sampling)\nChain 4: Iteration: 1757 / 2000 [ 87%]  (Sampling)\nChain 4: Iteration: 1758 / 2000 [ 87%]  (Sampling)\nChain 4: Iteration: 1759 / 2000 [ 87%]  (Sampling)\nChain 4: Iteration: 1760 / 2000 [ 88%]  (Sampling)\nChain 4: Iteration: 1761 / 2000 [ 88%]  (Sampling)\nChain 4: Iteration: 1762 / 2000 [ 88%]  (Sampling)\nChain 4: Iteration: 1763 / 2000 [ 88%]  (Sampling)\nChain 4: Iteration: 1764 / 2000 [ 88%]  (Sampling)\nChain 4: Iteration: 1765 / 2000 [ 88%]  (Sampling)\nChain 4: Iteration: 1766 / 2000 [ 88%]  (Sampling)\nChain 4: Iteration: 1767 / 2000 [ 88%]  (Sampling)\nChain 4: Iteration: 1768 / 2000 [ 88%]  (Sampling)\nChain 4: Iteration: 1769 / 2000 [ 88%]  (Sampling)\nChain 4: Iteration: 1770 / 2000 [ 88%]  (Sampling)\nChain 4: Iteration: 1771 / 2000 [ 88%]  (Sampling)\nChain 4: Iteration: 1772 / 2000 [ 88%]  (Sampling)\nChain 4: Iteration: 1773 / 2000 [ 88%]  (Sampling)\nChain 4: Iteration: 1774 / 2000 [ 88%]  (Sampling)\nChain 4: Iteration: 1775 / 2000 [ 88%]  (Sampling)\nChain 4: Iteration: 1776 / 2000 [ 88%]  (Sampling)\nChain 4: Iteration: 1777 / 2000 [ 88%]  (Sampling)\nChain 4: Iteration: 1778 / 2000 [ 88%]  (Sampling)\nChain 4: Iteration: 1779 / 2000 [ 88%]  (Sampling)\nChain 4: Iteration: 1780 / 2000 [ 89%]  (Sampling)\nChain 4: Iteration: 1781 / 2000 [ 89%]  (Sampling)\nChain 4: Iteration: 1782 / 2000 [ 89%]  (Sampling)\nChain 4: Iteration: 1783 / 2000 [ 89%]  (Sampling)\nChain 4: Iteration: 1784 / 2000 [ 89%]  (Sampling)\nChain 4: Iteration: 1785 / 2000 [ 89%]  (Sampling)\nChain 4: Iteration: 1786 / 2000 [ 89%]  (Sampling)\nChain 4: Iteration: 1787 / 2000 [ 89%]  (Sampling)\nChain 4: Iteration: 1788 / 2000 [ 89%]  (Sampling)\nChain 4: Iteration: 1789 / 2000 [ 89%]  (Sampling)\nChain 4: Iteration: 1790 / 2000 [ 89%]  (Sampling)\nChain 4: Iteration: 1791 / 2000 [ 89%]  (Sampling)\nChain 4: Iteration: 1792 / 2000 [ 89%]  (Sampling)\nChain 4: Iteration: 1793 / 2000 [ 89%]  (Sampling)\nChain 4: Iteration: 1794 / 2000 [ 89%]  (Sampling)\nChain 4: Iteration: 1795 / 2000 [ 89%]  (Sampling)\nChain 4: Iteration: 1796 / 2000 [ 89%]  (Sampling)\nChain 4: Iteration: 1797 / 2000 [ 89%]  (Sampling)\nChain 4: Iteration: 1798 / 2000 [ 89%]  (Sampling)\nChain 4: Iteration: 1799 / 2000 [ 89%]  (Sampling)\nChain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 4: Iteration: 1801 / 2000 [ 90%]  (Sampling)\nChain 4: Iteration: 1802 / 2000 [ 90%]  (Sampling)\nChain 4: Iteration: 1803 / 2000 [ 90%]  (Sampling)\nChain 4: Iteration: 1804 / 2000 [ 90%]  (Sampling)\nChain 4: Iteration: 1805 / 2000 [ 90%]  (Sampling)\nChain 4: Iteration: 1806 / 2000 [ 90%]  (Sampling)\nChain 4: Iteration: 1807 / 2000 [ 90%]  (Sampling)\nChain 4: Iteration: 1808 / 2000 [ 90%]  (Sampling)\nChain 4: Iteration: 1809 / 2000 [ 90%]  (Sampling)\nChain 4: Iteration: 1810 / 2000 [ 90%]  (Sampling)\nChain 4: Iteration: 1811 / 2000 [ 90%]  (Sampling)\nChain 4: Iteration: 1812 / 2000 [ 90%]  (Sampling)\nChain 4: Iteration: 1813 / 2000 [ 90%]  (Sampling)\nChain 4: Iteration: 1814 / 2000 [ 90%]  (Sampling)\nChain 4: Iteration: 1815 / 2000 [ 90%]  (Sampling)\nChain 4: Iteration: 1816 / 2000 [ 90%]  (Sampling)\nChain 4: Iteration: 1817 / 2000 [ 90%]  (Sampling)\nChain 4: Iteration: 1818 / 2000 [ 90%]  (Sampling)\nChain 4: Iteration: 1819 / 2000 [ 90%]  (Sampling)\nChain 4: Iteration: 1820 / 2000 [ 91%]  (Sampling)\nChain 4: Iteration: 1821 / 2000 [ 91%]  (Sampling)\nChain 4: Iteration: 1822 / 2000 [ 91%]  (Sampling)\nChain 4: Iteration: 1823 / 2000 [ 91%]  (Sampling)\nChain 4: Iteration: 1824 / 2000 [ 91%]  (Sampling)\nChain 4: Iteration: 1825 / 2000 [ 91%]  (Sampling)\nChain 4: Iteration: 1826 / 2000 [ 91%]  (Sampling)\nChain 4: Iteration: 1827 / 2000 [ 91%]  (Sampling)\nChain 4: Iteration: 1828 / 2000 [ 91%]  (Sampling)\nChain 4: Iteration: 1829 / 2000 [ 91%]  (Sampling)\nChain 4: Iteration: 1830 / 2000 [ 91%]  (Sampling)\nChain 4: Iteration: 1831 / 2000 [ 91%]  (Sampling)\nChain 4: Iteration: 1832 / 2000 [ 91%]  (Sampling)\nChain 4: Iteration: 1833 / 2000 [ 91%]  (Sampling)\nChain 4: Iteration: 1834 / 2000 [ 91%]  (Sampling)\nChain 4: Iteration: 1835 / 2000 [ 91%]  (Sampling)\nChain 4: Iteration: 1836 / 2000 [ 91%]  (Sampling)\nChain 4: Iteration: 1837 / 2000 [ 91%]  (Sampling)\nChain 4: Iteration: 1838 / 2000 [ 91%]  (Sampling)\nChain 4: Iteration: 1839 / 2000 [ 91%]  (Sampling)\nChain 4: Iteration: 1840 / 2000 [ 92%]  (Sampling)\nChain 4: Iteration: 1841 / 2000 [ 92%]  (Sampling)\nChain 4: Iteration: 1842 / 2000 [ 92%]  (Sampling)\nChain 4: Iteration: 1843 / 2000 [ 92%]  (Sampling)\nChain 4: Iteration: 1844 / 2000 [ 92%]  (Sampling)\nChain 4: Iteration: 1845 / 2000 [ 92%]  (Sampling)\nChain 4: Iteration: 1846 / 2000 [ 92%]  (Sampling)\nChain 4: Iteration: 1847 / 2000 [ 92%]  (Sampling)\nChain 4: Iteration: 1848 / 2000 [ 92%]  (Sampling)\nChain 4: Iteration: 1849 / 2000 [ 92%]  (Sampling)\nChain 4: Iteration: 1850 / 2000 [ 92%]  (Sampling)\nChain 4: Iteration: 1851 / 2000 [ 92%]  (Sampling)\nChain 4: Iteration: 1852 / 2000 [ 92%]  (Sampling)\nChain 4: Iteration: 1853 / 2000 [ 92%]  (Sampling)\nChain 4: Iteration: 1854 / 2000 [ 92%]  (Sampling)\nChain 4: Iteration: 1855 / 2000 [ 92%]  (Sampling)\nChain 4: Iteration: 1856 / 2000 [ 92%]  (Sampling)\nChain 4: Iteration: 1857 / 2000 [ 92%]  (Sampling)\nChain 4: Iteration: 1858 / 2000 [ 92%]  (Sampling)\nChain 4: Iteration: 1859 / 2000 [ 92%]  (Sampling)\nChain 4: Iteration: 1860 / 2000 [ 93%]  (Sampling)\nChain 4: Iteration: 1861 / 2000 [ 93%]  (Sampling)\nChain 4: Iteration: 1862 / 2000 [ 93%]  (Sampling)\nChain 4: Iteration: 1863 / 2000 [ 93%]  (Sampling)\nChain 4: Iteration: 1864 / 2000 [ 93%]  (Sampling)\nChain 4: Iteration: 1865 / 2000 [ 93%]  (Sampling)\nChain 4: Iteration: 1866 / 2000 [ 93%]  (Sampling)\nChain 4: Iteration: 1867 / 2000 [ 93%]  (Sampling)\nChain 4: Iteration: 1868 / 2000 [ 93%]  (Sampling)\nChain 4: Iteration: 1869 / 2000 [ 93%]  (Sampling)\nChain 4: Iteration: 1870 / 2000 [ 93%]  (Sampling)\nChain 4: Iteration: 1871 / 2000 [ 93%]  (Sampling)\nChain 4: Iteration: 1872 / 2000 [ 93%]  (Sampling)\nChain 4: Iteration: 1873 / 2000 [ 93%]  (Sampling)\nChain 4: Iteration: 1874 / 2000 [ 93%]  (Sampling)\nChain 4: Iteration: 1875 / 2000 [ 93%]  (Sampling)\nChain 4: Iteration: 1876 / 2000 [ 93%]  (Sampling)\nChain 4: Iteration: 1877 / 2000 [ 93%]  (Sampling)\nChain 4: Iteration: 1878 / 2000 [ 93%]  (Sampling)\nChain 4: Iteration: 1879 / 2000 [ 93%]  (Sampling)\nChain 4: Iteration: 1880 / 2000 [ 94%]  (Sampling)\nChain 4: Iteration: 1881 / 2000 [ 94%]  (Sampling)\nChain 4: Iteration: 1882 / 2000 [ 94%]  (Sampling)\nChain 4: Iteration: 1883 / 2000 [ 94%]  (Sampling)\nChain 4: Iteration: 1884 / 2000 [ 94%]  (Sampling)\nChain 4: Iteration: 1885 / 2000 [ 94%]  (Sampling)\nChain 4: Iteration: 1886 / 2000 [ 94%]  (Sampling)\nChain 4: Iteration: 1887 / 2000 [ 94%]  (Sampling)\nChain 4: Iteration: 1888 / 2000 [ 94%]  (Sampling)\nChain 4: Iteration: 1889 / 2000 [ 94%]  (Sampling)\nChain 4: Iteration: 1890 / 2000 [ 94%]  (Sampling)\nChain 4: Iteration: 1891 / 2000 [ 94%]  (Sampling)\nChain 4: Iteration: 1892 / 2000 [ 94%]  (Sampling)\nChain 4: Iteration: 1893 / 2000 [ 94%]  (Sampling)\nChain 4: Iteration: 1894 / 2000 [ 94%]  (Sampling)\nChain 4: Iteration: 1895 / 2000 [ 94%]  (Sampling)\nChain 4: Iteration: 1896 / 2000 [ 94%]  (Sampling)\nChain 4: Iteration: 1897 / 2000 [ 94%]  (Sampling)\nChain 4: Iteration: 1898 / 2000 [ 94%]  (Sampling)\nChain 4: Iteration: 1899 / 2000 [ 94%]  (Sampling)\nChain 4: Iteration: 1900 / 2000 [ 95%]  (Sampling)\nChain 4: Iteration: 1901 / 2000 [ 95%]  (Sampling)\nChain 4: Iteration: 1902 / 2000 [ 95%]  (Sampling)\nChain 4: Iteration: 1903 / 2000 [ 95%]  (Sampling)\nChain 4: Iteration: 1904 / 2000 [ 95%]  (Sampling)\nChain 4: Iteration: 1905 / 2000 [ 95%]  (Sampling)\nChain 4: Iteration: 1906 / 2000 [ 95%]  (Sampling)\nChain 4: Iteration: 1907 / 2000 [ 95%]  (Sampling)\nChain 4: Iteration: 1908 / 2000 [ 95%]  (Sampling)\nChain 4: Iteration: 1909 / 2000 [ 95%]  (Sampling)\nChain 4: Iteration: 1910 / 2000 [ 95%]  (Sampling)\nChain 4: Iteration: 1911 / 2000 [ 95%]  (Sampling)\nChain 4: Iteration: 1912 / 2000 [ 95%]  (Sampling)\nChain 4: Iteration: 1913 / 2000 [ 95%]  (Sampling)\nChain 4: Iteration: 1914 / 2000 [ 95%]  (Sampling)\nChain 4: Iteration: 1915 / 2000 [ 95%]  (Sampling)\nChain 4: Iteration: 1916 / 2000 [ 95%]  (Sampling)\nChain 4: Iteration: 1917 / 2000 [ 95%]  (Sampling)\nChain 4: Iteration: 1918 / 2000 [ 95%]  (Sampling)\nChain 4: Iteration: 1919 / 2000 [ 95%]  (Sampling)\nChain 4: Iteration: 1920 / 2000 [ 96%]  (Sampling)\nChain 4: Iteration: 1921 / 2000 [ 96%]  (Sampling)\nChain 4: Iteration: 1922 / 2000 [ 96%]  (Sampling)\nChain 4: Iteration: 1923 / 2000 [ 96%]  (Sampling)\nChain 4: Iteration: 1924 / 2000 [ 96%]  (Sampling)\nChain 4: Iteration: 1925 / 2000 [ 96%]  (Sampling)\nChain 4: Iteration: 1926 / 2000 [ 96%]  (Sampling)\nChain 4: Iteration: 1927 / 2000 [ 96%]  (Sampling)\nChain 4: Iteration: 1928 / 2000 [ 96%]  (Sampling)\nChain 4: Iteration: 1929 / 2000 [ 96%]  (Sampling)\nChain 4: Iteration: 1930 / 2000 [ 96%]  (Sampling)\nChain 4: Iteration: 1931 / 2000 [ 96%]  (Sampling)\nChain 4: Iteration: 1932 / 2000 [ 96%]  (Sampling)\nChain 4: Iteration: 1933 / 2000 [ 96%]  (Sampling)\nChain 4: Iteration: 1934 / 2000 [ 96%]  (Sampling)\nChain 4: Iteration: 1935 / 2000 [ 96%]  (Sampling)\nChain 4: Iteration: 1936 / 2000 [ 96%]  (Sampling)\nChain 4: Iteration: 1937 / 2000 [ 96%]  (Sampling)\nChain 4: Iteration: 1938 / 2000 [ 96%]  (Sampling)\nChain 4: Iteration: 1939 / 2000 [ 96%]  (Sampling)\nChain 4: Iteration: 1940 / 2000 [ 97%]  (Sampling)\nChain 4: Iteration: 1941 / 2000 [ 97%]  (Sampling)\nChain 4: Iteration: 1942 / 2000 [ 97%]  (Sampling)\nChain 4: Iteration: 1943 / 2000 [ 97%]  (Sampling)\nChain 4: Iteration: 1944 / 2000 [ 97%]  (Sampling)\nChain 4: Iteration: 1945 / 2000 [ 97%]  (Sampling)\nChain 4: Iteration: 1946 / 2000 [ 97%]  (Sampling)\nChain 4: Iteration: 1947 / 2000 [ 97%]  (Sampling)\nChain 4: Iteration: 1948 / 2000 [ 97%]  (Sampling)\nChain 4: Iteration: 1949 / 2000 [ 97%]  (Sampling)\nChain 4: Iteration: 1950 / 2000 [ 97%]  (Sampling)\nChain 4: Iteration: 1951 / 2000 [ 97%]  (Sampling)\nChain 4: Iteration: 1952 / 2000 [ 97%]  (Sampling)\nChain 4: Iteration: 1953 / 2000 [ 97%]  (Sampling)\nChain 4: Iteration: 1954 / 2000 [ 97%]  (Sampling)\nChain 4: Iteration: 1955 / 2000 [ 97%]  (Sampling)\nChain 4: Iteration: 1956 / 2000 [ 97%]  (Sampling)\nChain 4: Iteration: 1957 / 2000 [ 97%]  (Sampling)\nChain 4: Iteration: 1958 / 2000 [ 97%]  (Sampling)\nChain 4: Iteration: 1959 / 2000 [ 97%]  (Sampling)\nChain 4: Iteration: 1960 / 2000 [ 98%]  (Sampling)\nChain 4: Iteration: 1961 / 2000 [ 98%]  (Sampling)\nChain 4: Iteration: 1962 / 2000 [ 98%]  (Sampling)\nChain 4: Iteration: 1963 / 2000 [ 98%]  (Sampling)\nChain 4: Iteration: 1964 / 2000 [ 98%]  (Sampling)\nChain 4: Iteration: 1965 / 2000 [ 98%]  (Sampling)\nChain 4: Iteration: 1966 / 2000 [ 98%]  (Sampling)\nChain 4: Iteration: 1967 / 2000 [ 98%]  (Sampling)\nChain 4: Iteration: 1968 / 2000 [ 98%]  (Sampling)\nChain 4: Iteration: 1969 / 2000 [ 98%]  (Sampling)\nChain 4: Iteration: 1970 / 2000 [ 98%]  (Sampling)\nChain 4: Iteration: 1971 / 2000 [ 98%]  (Sampling)\nChain 4: Iteration: 1972 / 2000 [ 98%]  (Sampling)\nChain 4: Iteration: 1973 / 2000 [ 98%]  (Sampling)\nChain 4: Iteration: 1974 / 2000 [ 98%]  (Sampling)\nChain 4: Iteration: 1975 / 2000 [ 98%]  (Sampling)\nChain 4: Iteration: 1976 / 2000 [ 98%]  (Sampling)\nChain 4: Iteration: 1977 / 2000 [ 98%]  (Sampling)\nChain 4: Iteration: 1978 / 2000 [ 98%]  (Sampling)\nChain 4: Iteration: 1979 / 2000 [ 98%]  (Sampling)\nChain 4: Iteration: 1980 / 2000 [ 99%]  (Sampling)\nChain 4: Iteration: 1981 / 2000 [ 99%]  (Sampling)\nChain 4: Iteration: 1982 / 2000 [ 99%]  (Sampling)\nChain 4: Iteration: 1983 / 2000 [ 99%]  (Sampling)\nChain 4: Iteration: 1984 / 2000 [ 99%]  (Sampling)\nChain 4: Iteration: 1985 / 2000 [ 99%]  (Sampling)\nChain 4: Iteration: 1986 / 2000 [ 99%]  (Sampling)\nChain 4: Iteration: 1987 / 2000 [ 99%]  (Sampling)\nChain 4: Iteration: 1988 / 2000 [ 99%]  (Sampling)\nChain 4: Iteration: 1989 / 2000 [ 99%]  (Sampling)\nChain 4: Iteration: 1990 / 2000 [ 99%]  (Sampling)\nChain 4: Iteration: 1991 / 2000 [ 99%]  (Sampling)\nChain 4: Iteration: 1992 / 2000 [ 99%]  (Sampling)\nChain 4: Iteration: 1993 / 2000 [ 99%]  (Sampling)\nChain 4: Iteration: 1994 / 2000 [ 99%]  (Sampling)\nChain 4: Iteration: 1995 / 2000 [ 99%]  (Sampling)\nChain 4: Iteration: 1996 / 2000 [ 99%]  (Sampling)\nChain 4: Iteration: 1997 / 2000 [ 99%]  (Sampling)\nChain 4: Iteration: 1998 / 2000 [ 99%]  (Sampling)\nChain 4: Iteration: 1999 / 2000 [ 99%]  (Sampling)\nChain 4: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 4: \nChain 4:  Elapsed Time: 0.049 seconds (Warm-up)\nChain 4:                0.038 seconds (Sampling)\nChain 4:                0.087 seconds (Total)\nChain 4: \n\n\nAl imprimir el objeto que devuelve stan() se puede encontrar un resumen del posterior, incluyendo medidas de diagnóstico como el tamaño de muestra efectivo y \\(\\hat{R}\\).\n\nmodelo_1_fit\n\nInference for Stan model: anon_model.\n4 chains, each with iter=2000; warmup=1000; thin=1; \npost-warmup draws per chain=1000, total post-warmup draws=4000.\n\n        mean se_mean   sd   2.5%    25%    50%    75%  97.5% n_eff Rhat\nbeta0   5.44    0.01 0.47   4.46   5.13   5.45   5.75   6.33  1408    1\nbeta1   0.29    0.00 0.04   0.21   0.26   0.29   0.31   0.37  1482    1\nsigma   1.20    0.00 0.13   0.98   1.11   1.19   1.28   1.48  1932    1\nlp__  -34.57    0.04 1.29 -37.92 -35.14 -34.24 -33.64 -33.13  1210    1\n\nSamples were drawn using NUTS(diag_e) at Wed May 21 11:17:11 2025.\nFor each parameter, n_eff is a crude measure of effective sample size,\nand Rhat is the potential scale reduction factor on split chains (at \nconvergence, Rhat=1).\n\n\nPara continuar explorando el posterior y calcular cantidades de interés, conviene trabajar con las muestras en un data.frame. Para ello resulta fundamental la función extract() que extrae las muestras del posterior y las devuelve en una lista.\n\ndf_draws_1 &lt;- as.data.frame(extract(modelo_1_fit))\nhead(df_draws_1)\n\n     beta0     beta1     sigma      lp__\n1 5.868152 0.2284795 1.2820049 -35.65002\n2 5.702197 0.2618678 1.0917051 -33.52848\n3 5.223243 0.3034108 1.1549826 -33.12978\n4 4.978928 0.3444954 0.9722214 -37.33767\n5 4.756652 0.3424234 1.2575343 -34.29626\n6 4.773764 0.3139272 1.0322909 -37.42921\n\n\nLo siguiente es un enfoque posible, aunque bastante manual, para graficar los posteriors marginales de los parámetros del modelo.\n\ndf_draws_long_1 &lt;- df_draws_1 |&gt;\n  select(beta0, beta1, sigma) |&gt;\n  tidyr::pivot_longer(c(\"beta0\", \"beta1\", \"sigma\"), names_to = \"parametro\")\n\nggplot(df_draws_long_1) +\n  geom_histogram(aes(x = value, y = after_stat(density)), bins = 40) +\n  facet_wrap(~ parametro, scales = \"free\") +\n  labs(x = \"Valor\", y = \"Densidad\")\n\n\n\n\n\n\n\n\nUn primer paso en el análisis de un modelo lineal simple es visualizar la recta de regresión estimada.\n\n# Calcular la media a posteriori del intercepto y la pendiente\nintercept_mean &lt;- mean(df_draws_1$beta0)\nslope_mean &lt;- mean(df_draws_1$beta1)\n\n# Utilizar estos dos valores para graficar la media de la recta de regresión\nggplot(df_sales) +\n  geom_point(aes(x = x, y = y), alpha = 0.6, size = 2) +\n  geom_abline(\n    intercept = intercept_mean,\n    slope = slope_mean,\n    linewidth = 1,\n    color = \"red\"\n  ) +\n  labs(x = \"Publicidad ($)\", y = \"Ventas ($)\")\n\n\n\n\n\n\n\n\nPero no hay que olvidar que estamos trabajando con un modelo bayesiano. Por lo tanto, más que visualizar una recta basada en la media de las distribuciones marginales, es mejor visualizar las rectas asociadas a muestras del posterior. Esto también brinda una idea de la variabilidad en la estimación de la recta.\n\nggplot(df_sales) +\n  geom_point(aes(x = x, y = y), alpha = 0.6, size = 2) +\n  geom_abline(\n    aes(intercept = beta0, slope = beta1),\n    alpha = 0.3,\n    color = \"gray30\",\n1    data = df_draws_1[sample(nrow(df_draws_1), 40), ]\n  ) +\n  geom_abline(\n    intercept = intercept_mean,\n    slope = slope_mean,\n    linewidth = 1,\n    color = \"red\"\n  ) +\n  labs(x = \"Publicidad ($)\", y = \"Ventas ($)\")\n\n\n1\n\nSe utiliza sample(nrow(df_draws_1), 40) para seleccionar 40 muestras del posterior al azar. La figura resultaría muy difícil de leer si se intenta visualizar las rectas asociadas a muchas más muestras.\n\n\n\n\n\n\n\n\n\n\n\nTambién es posible utilizar todas las muestras del posterior para obtener bandas de credibilidad para la recta de regresión. A continuación, se obtiene la distribución condicional de \\(\\mu_i\\) para valores de \\(\\text{ventas}_i\\) en una grilla que cubre el rango de valores observados. A partir de esas muestras, se calculan intervalos de credibilidad.\n\n1x_grid &lt;- seq(4, 20, length.out = 100)\n2mu_matrix &lt;- matrix(nrow = 4000, ncol = 100)\n\nfor (i in seq_along(x_grid)) {\n3    mu_matrix[, i] &lt;- df_draws_1$beta0 + df_draws_1$beta1 * x_grid[i]\n}\n\n4mu_mean &lt;- apply(mu_matrix, 2, mean)\n5mu_qts &lt;- t(apply(mu_matrix, 2, function(x) quantile(x, c(0.025, 0.975))))\n6mu_qts2 &lt;- t(apply(mu_matrix, 2, function(x) quantile(x, c(0.25, 0.75))))\n\n# Finalmente, se lamacenan los valores calculados en un data frame\ndata_mu &lt;- data.frame(\n  x = x_grid,\n  y = mu_mean,\n  lower_95 = mu_qts[, 1],\n  upper_95 = mu_qts[, 2],\n  lower_50 = mu_qts2[, 1],\n  upper_50 = mu_qts2[, 2]\n)\n\nhead(data_mu)\n\n\n1\n\nSe crea la grilla de valores para \\(\\text{ventas}\\).\n\n2\n\nSe crea una matriz con tantas filas como muestras del posterior y tantas columnas como cantidad de valores en la grilla. Acá almacenamos los valores de \\(\\mu_i\\).\n\n3\n\nPara cada valor de la grilla, se obtiene la distribución condicional de \\(\\mu_i\\).\n\n4\n\nSe calcula la media a posteriori de \\(\\mu_i\\) para cada valor en la grilla.\n\n5\n\nSe calculan los percentiles de \\(\\mu_i\\) que se corresponden con un intervalo de colas iguales del 95%.\n\n6\n\nSe calculan los percentiles de \\(\\mu_i\\) que se corresponden con un intervalo de colas iguales del 50%.\n\n\n\n\n         x        y lower_95 upper_95 lower_50 upper_50\n1 4.000000 6.591681 5.915330 7.227227 6.381302 6.812441\n2 4.161616 6.638253 5.974315 7.263578 6.431583 6.856311\n3 4.323232 6.684826 6.028767 7.298794 6.481590 6.899142\n4 4.484848 6.731398 6.085649 7.335776 6.531564 6.941987\n5 4.646465 6.777970 6.144327 7.373665 6.581237 6.985055\n6 4.808081 6.824543 6.203625 7.409796 6.630832 7.028219\n\n\nLas bandas de credibilidad se construyen usando geom_ribbon de {ggplot2}.\n\nggplot(df_sales) +\n  geom_ribbon(\n    aes(x, ymin = lower_95, ymax = upper_95),\n    fill = \"grey50\",\n    alpha = 0.6,\n    data = data_mu\n  ) +\n  geom_ribbon(\n    aes(x, ymin = lower_50, ymax = upper_50),\n    fill = \"grey35\",\n    alpha = 0.6,\n    data = data_mu\n  ) +\n  geom_point(aes(x = x, y = y), alpha = 0.6, size = 2) +\n  geom_line(\n    aes(x, y),\n    color = \"firebrick\",\n    data = data_mu\n  ) +\n  labs(x = \"Publicidad ($)\", y = \"Ventas ($)\")\n\n\n\n\n\n\n\n\nFinalmente, se muestra como visualizar la distribución condicional de \\(\\mu_i\\) para un valor particular de la variable predictora \\(\\text{ventas}\\).\n\npublicidad &lt;- 15\nmu &lt;- df_draws_1$beta0 + df_draws_1$beta1 * publicidad\ndata.frame(mu = mu) |&gt;\n    ggplot() +\n    geom_histogram(aes(mu, y = after_stat(density)), bins = 40) +\n    labs(\n        x = expression(mu[i] ~ \" | \" ~ publicidad[i] ~ \" = 15\"),\n        y = \"Densidad\"\n    )\n\n\n\n\n\n\n\n\nAhora se muestra como utilizar distribuciones a priori no uniformes para los parámetros del modelo. El modelo se describe: \\[\n\\begin{aligned}\n\\text{ventas}_i &\\sim \\text{Normal}(\\mu_i, \\sigma^2) \\\\\n\\mu_i &= \\beta_0 + \\beta_1 \\text{publicidad}_i \\\\\n\\beta_0  &\\sim \\text{Normal}(\\overline{\\text{ventas}}, 10^2) \\\\\n\\beta_1  &\\sim \\text{Normal}(0, 0.5^2) \\\\\n\\sigma &\\sim \\text{Normal}^+(5)\n\\end{aligned}\n\\]\ny el programa de Stan que lo implementa es el siguiente\n\ndata {\n  int&lt;lower=0&gt; N;  // Tamaño de muestra\n  vector[N] x;     // Valores del predictor\n  vector[N] y;     // Valores de la respuesta\n  real beta0_mu;   // Media del prior del intercepto\n}\nparameters {\n  // Primero se declaran los parámetros\n  real beta0;\n  real beta1;\n  real&lt;lower=0&gt; sigma;\n}\nmodel {\n  // Luego se especifican sus priors\n  beta0 ~ normal(beta0_mu, 10);  // Acá se pasa el valor de la media\n  beta1 ~ normal(0, 0.5);\n  sigma ~ normal(0, 5);          // Es una 'media-normal' dada la cota inferior arriba\n  // Likelihood\n  y ~ normal(beta0 + beta1 * x, sigma);\n}\n\nLa estructura del código para ajustar el modelo es idéntica a la utilizada anteriormente. La única diferencia es que debemos pasar el valor de la media del prior del intercepto \\(\\beta_0\\).\n\nstan_data &lt;- list(\n    N = nrow(df_sales),          # Cantidad de observaciones\n    x = df_sales$x,              # Publicidad\n    y = df_sales$y,              # Ventas\n    beta0_mu = mean(df_sales$y)  # Media del prior del intercepto\n)\n\nruta_modelo_2 &lt;- here::here(\n    \"recursos\", \"codigo\", \"stan\", \"regresion_lineal\", \"02_modelo_con_priors.stan\"\n)\n\nmodelo_2 &lt;- stan_model(file = ruta_modelo_2)\n\nmodelo_2_fit &lt;- sampling(\n    modelo_2,\n    chains = 4,         # Cantidad de cadenas\n    data = stan_data,   # Datos\n    refresh = 0,        # No mostrar mensajes del sampler\n    seed = 121195       # Para que el resultado sea reproducible\n)\n\nSe puede ver el resumen del posterior\n\nmodelo_2_fit\n\nInference for Stan model: anon_model.\n4 chains, each with iter=2000; warmup=1000; thin=1; \npost-warmup draws per chain=1000, total post-warmup draws=4000.\n\n        mean se_mean   sd   2.5%    25%    50%    75%  97.5% n_eff Rhat\nbeta0   5.47    0.01 0.47   4.59   5.14   5.45   5.78   6.45  1537    1\nbeta1   0.29    0.00 0.04   0.21   0.26   0.29   0.31   0.36  1631    1\nsigma   1.20    0.00 0.12   0.99   1.11   1.19   1.27   1.47  1810    1\nlp__  -34.82    0.03 1.24 -37.88 -35.41 -34.50 -33.88 -33.37  1421    1\n\nSamples were drawn using NUTS(diag_e) at Wed May 21 11:18:06 2025.\nFor each parameter, n_eff is a crude measure of effective sample size,\nand Rhat is the potential scale reduction factor on split chains (at \nconvergence, Rhat=1).\n\n\ny también se podrían crear visualizaciones como las anteriores utilizando el mismo código.\nSe recomienda echar un vistazo a las funciones disponibles en la librería {bayesplot} aquí. Por ejemplo, mcmc_trace(), que permite obtener un traceplot que sirve para evaluar la convergencia y mezcla de las cadenas.\n\nlibrary(bayesplot)\nmcmc_trace(\n  modelo_2_fit,\n  pars = c(\"beta0\", \"beta1\", \"sigma\"),\n  facet_args = list(nrow = 3)\n)",
    "crumbs": [
      "Recursos",
      "Código",
      "14 - Regresión lineal con `{RStan}`"
    ]
  },
  {
    "objectID": "recursos/codigo/13_hmc_gaussian_2d.html",
    "href": "recursos/codigo/13_hmc_gaussian_2d.html",
    "title": "13 - Hamiltonian Monte Carlo para normal bivariada",
    "section": "",
    "text": "Esta sección muestra una implementación de Hamiltonian Monte Carlo para obtener muestras de una distribución \\(\\mathcal{N}(\\mathbf{0}, \\pmb{\\Sigma})\\) con: \\[\n\\pmb{\\Sigma} =\n    \\begin{bmatrix}\n    1 & \\rho \\\\\n    \\rho & 1\n    \\end{bmatrix}\n\\]\nFunciones auxiliares\n\nlibrary(ggplot2)\nlibrary(ggquiver)\nlibrary(mvtnorm)\n\nset.seed(12345)\n\n# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# Funciones auxiliares\n# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\ndlogpdx &lt;- function(x, y, rho) {\n  - (x - rho * y) / (1 - rho ^ 2)\n}\n\ndlogpdy &lt;- function(x, y, rho) {\n  - (y - rho * x) / (1 - rho ^ 2)\n}\n\ndlogp &lt;- function(q, rho) {\n  x &lt;- q[1]\n  y &lt;- q[2]\n  c(dlogpdx(x, y, rho), dlogpdy(x, y, rho))\n}\n\nmake_neg_dlogp &lt;- function(rho) {\n  function(q) -dlogp(q, rho)\n}\n\nmake_neg_logp &lt;- function(rho) {\n  Mu &lt;- rep(0, 2)\n  Sigma &lt;- diag(2)\n  Sigma[Sigma == 0] &lt;- rho\n  function(q) -dmvnorm(q, Mu, Sigma, log = TRUE)\n}\n\nleapfrog &lt;- function(p, q, neg_dlogp, path_length, step_size) {\n  leap_q &lt;- list()\n  leap_p &lt;- list()\n  p &lt;- p - step_size * neg_dlogp(q) / 2\n  for (i in seq_len(round(path_length / step_size) - 1)) {\n    q &lt;- q + step_size * p\n    p &lt;- p - step_size * neg_dlogp(q)\n    leap_q[[i]] &lt;- q\n    leap_p[[i]] &lt;- p\n  }\n  q &lt;- q + step_size * p\n  p &lt;- p - step_size * neg_dlogp(q) / 2\n\n  # Flip del momentum\n  return(list(q = q, p = -p, leap_q = leap_q, leap_p = leap_p))\n}\n\nlappend &lt;- function(l, object) {\n  l[[length(l) + 1]] &lt;- object\n  l\n}\n\nltail &lt;- function(l) {\n  l[[length(l)]]\n}\n\nhmc &lt;- function(\n    neg_logp,\n    neg_dlogp,\n    n_samples,\n    initial_position,\n    path_length = 1,\n    step_size = 0.01\n) {\n\n  leap_p &lt;- list()\n  leap_q &lt;- list()\n  samples &lt;- list(initial_position)\n  n_dimensions &lt;- length(initial_position)\n\n  # Se generan momentums a partir de una MVN(0, 1)\n  # Es de dimension (n_samples, n_dimensions)\n  momentum &lt;- rmvnorm(n_samples, rep(0, n_dimensions), diag(n_dimensions))\n\n  for (i in seq_len(n_samples)) {\n    # Obtener posicion y momentum\n    p_current &lt;- momentum[i, ]\n    q_current &lt;- ltail(samples)\n\n    # Integrar para obtener una nueva posición y momentum\n    integration &lt;- leapfrog(\n      p_current, q_current, neg_dlogp, path_length, step_size\n    )\n\n    p_new &lt;- integration$p\n    q_new &lt;- integration$q\n    leap_p &lt;- lappend(leap_p, integration$leap_p)\n    leap_q &lt;- lappend(leap_q, integration$leap_q)\n\n    # Criterio de aceptacion de Metropolis\n    current_logp &lt;- neg_logp(q_current) - dmvnorm(p_current, log = TRUE)\n    new_logp &lt;- neg_logp(q_new) - dmvnorm(p_new, log = TRUE)\n\n    sample_new &lt;- if(log(runif(1)) &lt; current_logp - new_logp) {\n      q_new\n    } else {\n      sample_current\n    }\n    samples &lt;- lappend(samples, sample_new)\n  }\n  samples &lt;- as.data.frame(do.call(rbind, samples))\n  leap_p &lt;- as.data.frame(do.call(rbind, lapply(leap_p, function(x) do.call(rbind, x))))\n  leap_q &lt;- as.data.frame(do.call(rbind, lapply(leap_q, function(x) do.call(rbind, x))))\n\n  colnames(samples) &lt;- c(\"x\", \"y\")\n  colnames(leap_p) &lt;- c(\"p0\", \"p1\")\n  colnames(leap_q) &lt;- c(\"q0\", \"q1\")\n  list(samples = samples, leap_p = leap_p, leap_q = leap_q)\n}\n\nEjemplo 1: Se grafican trayectorias\n\nneg_logp &lt;- make_neg_logp(0)\nneg_dlogp &lt;- make_neg_dlogp(0)\n\ninitial_position &lt;- c(1.6, -0.6)\nhmc_output &lt;- hmc(neg_logp, neg_dlogp, 10, initial_position, 1.5, 0.01)\n\ndf_samples &lt;- hmc_output$samples\ndf_leap_p &lt;- hmc_output$leap_p\ndf_leap_q &lt;- hmc_output$leap_q\ndf_leap &lt;- cbind(df_leap_p, df_leap_q)\n\ndf_basis &lt;- tidyr::crossing(x1 = seq(-3, 3, 0.1), x2 = seq(-3, 3, 0.1))\nplt &lt;- df_basis |&gt;\n  dplyr::mutate(f = mvtnorm::dmvnorm(df_basis, c(0, 0), diag(2))) |&gt;\n  ggplot() +\n  geom_raster(aes(x = x1, y = x2, fill = f)) +\n  stat_contour(aes(x = x1, y = x2, z = f), col = \"white\", bins = 5) +\n  viridis::scale_fill_viridis() +\n  theme(\n    legend.position = \"none\",\n    plot.title = element_text(hjust = 0.5, size = 18)\n  )\n\nplt +\n  geom_path(aes(x = q0, y = q1), linewidth = 1, data = df_leap) +\n  geom_quiver(\n    aes(x = q0, y = q1, u = p0, v = p1),\n    linewidth = 1,\n    vecsize = 600,\n    data = df_leap[seq(1, nrow(df_leap), by = 30), ]\n  ) +\n  geom_point(\n    aes(x = x, y = y),\n    color = \"red\",\n    size = 2,\n    data = df_samples\n  ) +\n  labs(x = \"x\", y = \"y\") +\n  scale_x_continuous(expand = c(0, 0)) +\n  scale_y_continuous(expand = c(0, 0))\n\n\n\n\nTrayectorias realizadas junto a las 10 muestras obtenidas.\n\n\n\n\nEjemplo 2: Se obtienen muestras\n\nneg_logp &lt;- make_neg_logp(0)\nneg_dlogp &lt;- make_neg_dlogp(0)\n\ninitial_position &lt;- c(1.6, -0.6)\nhmc_output &lt;- hmc(neg_logp, neg_dlogp, 1000, initial_position, 1.5, 0.01)\n\ndf_samples &lt;- hmc_output$samples\ndf_leap_p &lt;- hmc_output$leap_p\ndf_leap_q &lt;- hmc_output$leap_q\n\ndf_basis &lt;- tidyr::crossing(x1 = seq(-3.5, 3.5, 0.1), x2 = seq(-3.5, 3.5, 0.1))\nplt &lt;- df_basis |&gt;\n  dplyr::mutate(f = mvtnorm::dmvnorm(df_basis, c(0, 0), diag(2))) |&gt;\n  ggplot() +\n  geom_raster(aes(x = x1, y = x2, fill = f)) +\n  stat_contour(aes(x = x1, y = x2, z = f), col = \"white\", bins = 5) +\n  viridis::scale_fill_viridis() +\n  theme(\n    legend.position = \"none\",\n    plot.title = element_text(hjust = 0.5, size = 18)\n  )\n\nplt +\n  geom_point(\n    aes(x = x, y = y),\n    size = 2,\n    alpha = 0.7,\n    color = \"red\",\n    data = df_samples\n  ) +\n  labs(x = \"x\", y = \"y\") +\n  scale_x_continuous(expand = c(0, 0)) +\n  scale_y_continuous(expand = c(0, 0))\n\n\n\n\n1000 muestras de una distribución \\(\\mathcal{N}(\\mathbf{0}, \\mathbf{I})\\).\n\n\n\n\nEjemplo 3: Se obtienen muestras con \\(\\rho = 0.6\\)\n\nneg_logp &lt;- make_neg_logp(0.7)\nneg_dlogp &lt;- make_neg_dlogp(0.7)\n\ninitial_position &lt;- c(1.6, -0.6)\nhmc_output &lt;- hmc(neg_logp, neg_dlogp, 1000, initial_position, 1.5, 0.01)\n\ndf_samples &lt;- hmc_output$samples\ndf_leap_p &lt;- hmc_output$leap_p\ndf_leap_q &lt;- hmc_output$leap_q\n\nSigma &lt;- matrix(c(1, 0.7, 0.7, 1), ncol = 2)\n\ndf_basis &lt;- tidyr::crossing(x1 = seq(-3.5, 3.5, 0.1), x2 = seq(-3.5, 3.5, 0.1))\nplt &lt;- df_basis |&gt;\n  dplyr::mutate(f = mvtnorm::dmvnorm(df_basis, c(0, 0), Sigma)) |&gt;\n  ggplot() +\n  geom_raster(aes(x = x1, y = x2, fill = f)) +\n  stat_contour(aes(x = x1, y = x2, z = f), col = \"white\", bins = 5) +\n  viridis::scale_fill_viridis() +\n  theme(\n    legend.position = \"none\",\n    plot.title = element_text(hjust = 0.5, size = 18)\n  )\n\nplt +\n  geom_point(\n    aes(x = x, y = y),\n    size = 2,\n    alpha = 0.7,\n    color = \"red\",\n    data = df_samples\n  ) +\n  labs(x = \"x\", y = \"y\") +\n  scale_x_continuous(expand = c(0, 0)) +\n  scale_y_continuous(expand = c(0, 0))\n\n\n\n\n1000 muestras de una distribución \\(\\mathcal{N}(\\mathbf{0}, 0.7 \\mathbf{I})\\).\n\n\n\ncor(df_samples$x, df_samples$y)\n\n[1] 0.6584974",
    "crumbs": [
      "Recursos",
      "Código",
      "13 - Hamiltonian Monte Carlo para normal bivariada"
    ]
  },
  {
    "objectID": "recursos/codigo/08_encuentro_fotocopiadora.html",
    "href": "recursos/codigo/08_encuentro_fotocopiadora.html",
    "title": "08 - Te veo en la fotocopiadora",
    "section": "",
    "text": "El siguiente programa sirve para responder al ejercicio Te veo en la fotocopiadora de la Práctica 3.\n\nlibrary(ggplot2)\n\n# Para reproducibilidad del gráfico\nset.seed(12345)\n\n# Tiempo que espera cada estudiante\nespera_a &lt;- 10\nespera_b &lt;- 14\n\n# Dos tiempos de llegada posible para cada estudiante\nllegada_a &lt;- runif(1, min = 0, max = 60)\nllegada_b &lt;- runif(1, min = 0, max = 60)\n\n# El intervalo en el que cada estudiante está en la fotocopiadora\nintervalo_a &lt;- c(llegada_a, llegada_a + espera_a)\nintervalo_b &lt;- c(llegada_b, llegada_b + espera_b)\n\n# Lo convertimos a data frame para graficar con ggplot2\ndatos &lt;- data.frame(\n  intervalo = c(intervalo_a, intervalo_b),\n  persona = rep(c(\"A\", \"B\"), each = 2)\n)\n\n# Graficamos con ggplot2\nggplot(datos) +\n  geom_line(\n    aes(x = intervalo, y = persona, color = persona),\n    linewidth = 2\n  ) +\n  geom_vline(xintercept = c(0, 60), linewidth = 0.25, linetype = \"dashed\") +\n  scale_color_manual(values = c(\"#3b78b0\", \"#d1352c\")) +\n  xlim(c(0, 70)) +\n  theme_bw() +\n  theme(\n    panel.grid.minor = element_blank()\n  )\n\n\n\n\n\n\n\n\nSi los intervalos se solapan, significa que hay una línea vertical que cruza a ambos. Esto es lo mismo que decir que hay una \\(C\\) que cumple: \\[\n\\begin{array}{c}\na_1 \\le C \\le a_2 \\\\\nb_1 \\le C \\le b_2\n\\end{array}\n\\]\nSe puede ver que ambas condiciones se cumplen cuando: \\[\na_1 \\le b_2 \\quad \\text{y} \\quad b_1 \\le a_2\n\\]\nEn otras palabras, esto dice que “A llega antes de que B se vaya, y B llega antes de que A se vaya”\nEn este primer caso se tiene:\n\n(intervalo_a[1] &lt;= intervalo_b[2]) & (intervalo_b[1] &lt;= intervalo_a[2])\n\n[1] TRUE\n\n\nY para muchas iteraciones:\n\n# Simular llegadas de A y B\nllegada_a &lt;- runif(10000, min = 0, max = 60)\nllegada_b &lt;- runif(10000, min = 0, max = 60)\n\n# Determinar escenarios donde A y B se encuentran\ncomparaciones &lt;- (\n1  (llegada_a &lt;= llegada_b + espera_b)\n2  & (llegada_b &lt;= llegada_a + espera_a)\n)\nmean(comparaciones)\n\n\n1\n\n\\(A\\) llega antes que \\(B\\) se vaya\n\n2\n\n\\(B\\) llega antes que \\(A\\) se vaya\n\n\n\n\n[1] 0.3494",
    "crumbs": [
      "Recursos",
      "Código",
      "08 - Te veo en la fotocopiadora"
    ]
  },
  {
    "objectID": "practica/practica_02.html",
    "href": "practica/practica_02.html",
    "title": "Práctica - Unidad 2",
    "section": "",
    "text": "Esta sección contiene ejercicios para trabajar con modelos basados en distribuciones conjugadas. En general, los ejercicios requieren cálculos o derivaciones que se pueden realizar a mano. Sin embargo, se promueve fuertemente el uso de la computadora y el lenguaje R para verificar los resultados, mostrar soluciones alternativas y ejercitar el uso de una herramienta que será de suma utilidad a lo largo de todo el curso y de la vida profesional.\n\n💻📌 ¿Quién domina el posterior?\nPara cada una de las situaciones siguientes, se da una distribución a priori Beta para el parámetro \\(\\pi\\) de un ensayo binomial. Para cada escenario, identificar cuál de estos se cumple: el prior tiene mayor influencia en el posterior, los datos tienen más influencia en el posteriori, o la creencia a priori y los datos influyen de manera similar en la creencia a posteriori\n\nPrior: \\(\\pi \\sim \\text{Beta}(1,4)\\), observaciones: \\(y=8\\) éxitos en \\(n=10\\) ensayos.\nPrior: \\(\\pi \\sim \\text{Beta}(20,3)\\), observaciones: \\(y=0\\) éxitos en \\(n=1\\) ensayos.\nPrior: \\(\\pi \\sim \\text{Beta}(4,2)\\), observaciones: \\(y=1\\) éxitos en \\(n=3\\) ensayos.\nPrior: \\(\\pi \\sim \\text{Beta}(3,10)\\), observaciones: \\(y=10\\) éxitos en \\(n=13\\) ensayos.\nPrior: \\(\\pi \\sim \\text{Beta}(20,2)\\), observaciones: \\(y=10\\) éxitos en \\(n=200\\) ensayos.\n\n💻📌 Más o menos certeza\nSea \\(\\theta\\) la proporción de personas que prefieren los perros a los gatos. Suponga que se elige una distribución \\(\\text{Beta}(7,2)\\) para representar la creencia a priori\n\nDe acuerdo al prior ¿cuáles son valores razonables para \\(\\theta\\)?\nSe observa en una encuesta que \\(y=19\\) de \\(n=20\\) personas prefieren perros, ¿cómo cambia eso el conocimiento acerca de \\(\\theta\\)? Comenta en términos de la evolución de la credibilidad media y del grado de certidumbre acerca de \\(\\theta\\).\nSi, en lugar de eso, se determina que \\(y=1\\) de \\(n=20\\) personas prefieren perros, ¿cómo cambia ahora el grado de credibilidad de los diferentes valores de \\(\\theta\\)?\nSi, en lugar de eso, se determina que \\(y=10\\) de \\(n=20\\) personas prefieren perros, ¿cómo cambia ahora el grado de credibilidad de los diferentes valores de \\(\\theta\\)?\n\n💻📌 Pasito a pasito\nSea \\(\\theta\\) la probabilidad de éxito de un evento de interés. Sea \\(\\text{Beta}(2,3)\\) la distribución a priori para \\(\\theta\\). Actualiza la distribución a posteriori para \\(\\theta\\) secuencialmente:\n\nPrimera observación: éxito.\nSegunda observación: éxito.\nTercera observación: fracaso.\nCuarta observación: éxito.\n\n💻📌 Pasitos tras pasitos\nSea \\(\\theta\\) la probabilidad de éxito de un evento de interés. Sea \\(\\text{Beta}(2,3)\\) la distribución a priori para \\(\\theta\\). Actualiza la distribución a posteriori para \\(\\theta\\) secuencialmente dados conjuntos de cinco observaciones:\n\nPrimeras observaciones: tres éxitos.\nSegundas observaciones: un éxito.\nTerceras observaciones: un éxito.\nCuartas observaciónes: dos éxitos.\n\n💻📌 Diferentes observaciones, diferentes posteriors\nUna empresa que fabrica zapatillas está diseñando una publicidad para Instagram. Tres empleados comparten que la creencia a priori para \\(\\pi\\), la probabilidad de que un cliente haga clic en el anuncio cuando lo ve, puede expresarse con una distribución \\(\\text{Beta}(4, 3)\\). No obstante, los tres empleados realizan tres experimentos distintos y por ende tienen acceso a datos diferentes. El primer empleado prueba el anuncio en una persona, que no cliquea el anuncio. El segundo lo prueba en 10 personas, de las cuales 3 cliquean el anuncio. El último lo prueba en 100 personas, 20 de las cuales cliquean el anuncio.\n\nDescriba el entendimiento a priori que los empleados tienen sobre \\(\\pi\\).\nEspecifique la distribución a posteriori de cada uno de los empleados.\nCompare las distribuciones a posteriori de cada empleado.\n\n💻📌 ¿Galletitas o masitas?\nLa UNR reúne cada año a estudiantes provenientes de diferentes localidades. Cuántas cuadras constituyen una distancia “caminable” suele ser motivo de discusión, entre otros. Pero la verdadera grieta está entre la denominación galletitas versus masitas. Un rosarino pone un prior \\(\\text{Beta}(20,2)\\) a la proporción de personas que dicen galletitas, mientras que un oriundo de una localidad del interior dirá que la credibilidad a priori es \\(\\text{Beta}(2,8)\\).\n\nResuma ambas distribuciones a priori y explique con sus palabras lo que implican.\nCon la información de sus compañeros de curso, actualice ambas distribuciones a priori. ¿Es suficiente esa información para acercar ambas posturas?\n\n💻 📌 Mi primera huerta\nEn un campamento de verano para infantes se realizaron actividades que promueven el contacto con la naturaleza. Una de las tareas consistió en germinar semillas de tomate. Josefina plantó 18 semillas en su almaciguera. Al cabo de 5 días, 8 de ellas germinaron. Sea \\(\\theta\\) la probabilidad de que una semilla de tomate germine y sea \\(\\text{Beta}(1, 1)\\) su distribución a priori.\n\n¿Qué información implica el prior sobre la probabilidad de germinación?\nCalcule la media y el desvío estándar a posteriori de \\(\\theta\\) a mano.\nVerifique el cálculo utilizando R.\nObtenga un intervalo de credibilidad del 95% para \\(\\theta\\).\n\n\n\n\n\n\nFoto de Markus Spiske en Unsplash\n\n\n\n\n💻 ¿Quién dijo que el fútbol siempre da revancha?\nEn la final del 2018 de la Copa del Mundo de la FIFA, Francia le ganó a Croacia por 4 a 2. Considere que el número de goles que un equipo hace en un partido puede modelizarse con una distribución de Poisson. Suponga un parámetro \\(\\lambda_F\\) para Francia y uno \\(\\lambda_C\\) para Croacia. Elija una distribución Gamma a priori para el número medio de goles por partido (es decir, \\(\\lambda_F\\) y \\(\\lambda_C\\) compartirán la distribución a priori). \\(\\lambda_F\\) da una idea de la capacidad de Francia de hacer goles (\\(\\lambda_C\\) lo mismo, pero para Croacia).\nEn función del resultado del partido, obtenga las distribuciones a posteriori de \\(\\lambda_F\\) y \\(\\lambda_C\\) y responda utilizando R:\n\n¿Qué probabilidad hay de que Francia fuera un mejor equipo que Croacia?\nSi el mismo partido se jugara de nuevo (cosa que los franceses en aquella oportunidad no pidieron), ¿cuál es la probabilidad de que Francia ganara de nuevo? \n\n\n\n\n\n\n\nAyuda\n\n\n\n\n\nPlantee el modelo: \\[\n\\begin{aligned}\nY_F & \\sim \\text{Poisson}(\\lambda_F) \\\\\nY_C & \\sim \\text{Poisson}(\\lambda_C) \\\\\n\\lambda_F, \\lambda_C &\\sim \\text{Gamma}(\\alpha, \\beta)\n\\end{aligned}\n\\]\nutilizando valores razonables para \\(\\alpha\\) y \\(\\beta\\). Luego obtener el posterior y calcular \\(P(\\lambda_F &gt; \\lambda_C)\\). Para el segundo punto calcular \\(P(\\tilde{y}_F \\mid y_F) &gt; P(\\tilde{y}_C \\mid y_C)\\).\n\n\n\nMirá si me va a pasar a mi…\nDurante el desarrollo de las vacunas contra el COVID-19, un medio anunció para una determinada vacuna una eficacia del 100%.\n\nEn la fase 3 de un ensayo en adolescentes de entre 12 y 15 años, la vacuna BNT162b2 de Pfizer-BioNTech para el COVID-19 demostró una eficacia del 100% y una respuesta robusta de anticuerpos. El ensayo clínico involucró 2260 jóvenes estadounidenses. En el ensayo, 18 casos de COVID-19 fueron observados en el grupo placebo (\\(n=1129\\)) y ninguno en el grupo vacunado (\\(n=1131\\))\n\nEs de esperar que, en un ensayo más grande, aparezca algún caso de COVID-19 en el grupo que recibió el tratamiento. ¿Cómo se estima la probabilidad de algo que aún no ocurrió? \n\n\n\n\n\n\nAyuda\n\n\n\n\n\nSea \\(Y\\) laf cantidad de personas que reciben la vacuna y se contagian COVID-19. Plantee un modelo y obtenga el posterior de \\(\\theta_v\\), la probabilidad de que una persona vacunada se contagie de COVID-19. Luego obtenga la distribución predictiva a posteriori utilizando un \\(N_v\\) grande y calcule \\(P(\\tilde{Y} &gt; 0)\\).\n\n\n\n💻 La regla del tres\nUna estudiante de Licenciatura en Estadística está releyendo su tesina antes de entregarla. Si en 20 páginas encontrase 5 de ellas con al menos un typo, sería razonable estimar que la probabilidad de que una página contenga un typo es \\(\\frac{5}{20} = \\frac{1}{4}\\). ¿Pero qué ocurre si en 20 páginas no encuentra ningún error?\nVerifcar que, partiendo de un prior uniforme, \\(\\frac{3}{N}\\) es una estimación razonable para \\(\\tau\\) (la probabilidad de que una página contenga un typo), siendo \\(N\\) el número de páginas. Para ello, grafique la distribución a posteriori que se obtiene al haber observado 0 typos en 10 páginas y luego halle la probabilidad de que \\(\\tau &lt; \\frac{3}{N}\\) para diferentes valores de \\(N\\) (10, 100, 1000, 10000). \n\n\n\n\n\n\nAyuda\n\n\n\n\n\nConsiderar \\(Y =\\) cantidad de páginas con al menos un typo en \\(N\\) páginas. Asumir que la ocurrencia de typos entre páginas son independientes. El prior uniforme refiere a la probabilidad de cometer al menos un typo en una página, \\(\\tau\\). Para los diferentes valores de \\(N\\), primero encontrar el posterior de \\(\\tau\\) y luego calcular \\(P(\\tau &lt; \\frac{3}{N})\\). ¿Qué se observa?\n\n\n\n¿Tenés alguien para recomendar?\nUna colega quiere comprar un producto por Internet. Tres vendedores ofrecen el mismo producto al mismo precio. Un vendedor tiene 100% de evaluaciones positivas, con 10 reviews. Otro tiene 96% de evaluaciones positivas, con 50 reviews. El último tiene 90% de comentarios positivos, con 200 evaluaciones. ¿Cuál de los tres vendedores le recomendarías? \n💻📌 Bichos\nUn biólogo quiere determinar la densidad de un insecto en su región. Su conocimiento a priori del número promedio de insectos por unidad de área (\\(\\text{m}^2\\)) se puede representar con una distribución Gamma de media 0.50 y desvío estándar 0.25. En una investigación en 20 \\(\\text{m}^2\\) de área, se hallan 3, 2, 5, 1 y 2 insectos en los primeros 5 \\(\\text{m}^2\\) y ninguno en la fracción de tierra restante.\n\nHalle la distribución a posteriori del número medio de insectos por unidad de área.\nHalle la distribución predictiva a posteriori del número de insectos que se espera encontrar en una exploración de un área de 10 \\(\\text{m}^2\\)\n\n\n\n\n\n\nUna gran variedad de insectosImágen de Freepik\n\n\n\n\n\n\n\n\n\n\nAyuda\n\n\n\n\n\n\nUna distribución conveniente para la cantidad de bichos por metro cuadrado dado que se sabe que su promedio tiene un prior Gamma es la Poisson. A partir de eso es sencillo obtener el posterior.\nPrimero resolverlo para 1 metro cuadrado, y luego para 10. Se puede hacer computacionalmente muestreando \\(\\lambda\\) del posterior y obteniendo muestras de la distribución de la respuesta usando los valores muestreados de \\(\\lambda\\). También se puede resolver de manera teórica calculando la integral de la distribución predictiva a posteriori. Para eso tener en cuenta: \\[\n\\begin{aligned}\n\\Gamma(x + 1) &= x! \\\\\n\\int_0^{\\infty} x^b e^{-ax} dx &= \\frac{\\Gamma(b + 1)}{a ^ {b+1}}\n\\end{aligned}\n\\] y que la función de masa de probabiliad la distribución binomial negativa se puede escribir como: \\[\n\\begin{array}{ccc}\np(k, \\mid r, \\pi) = P(X = k) = \\displaystyle \\frac{\\Gamma(k + r)}{\\Gamma(k + 1) \\Gamma({r})}(1 - \\pi)^k \\pi^r,\n& k \\in \\{0, 1, 2, \\dots\\},\n& r &gt; 0\n\\end{array}\n\\]\n\n\n\n\n📌 Alter-ego\nEl profesor Caprista y el profesor Evangetto están dando sus primeros cursos de Estadística Bayesiana. Sus colegas les dijeron que el puntaje promedio en un examen final, \\(\\mu\\), varía normalmente año a año con media 8 y desvío estándar 0.4. Y además, que los puntajes individuales de los estudiantes \\(Y\\) varían normalmente alrededor de \\(\\mu\\) con una desviación estándar de 0.4\n\n¿Cuál es la probabilidad a priori de que un estudiante se saque más de 9 en un examen final?\nEl profesor Caprista toma el examen final y observa que sus 20 estudiantes obtuvieron una nota media de 8.6. Halle la distribución a posteriori de \\(\\mu\\).\nEl profesor Evangetto toma el examen final y observa que sus 20 estudiantes obtuvieron una nota media de 8.2. Halle la distribución a posteriori de \\(\\mu\\).\nCombine las notas de ambos exámenes para obtener la distribución a posteriori de \\(\\mu\\)\n¿Cuál es la probabilidad a posteriori de que un estudiante se saque más de 9 en un examen final?\n\n📌 Inferencia sobre una distribución de Poisson\nLa distribución de masa de probabilidad Poisson se define como\n\\[\n\\begin{array}{lcr}\n\\displaystyle p(x \\mid \\lambda) = \\frac{e^{-\\lambda}\\lambda^x}{x!} &\n\\text{con} &\nx \\in \\{0, 1, 2, \\cdots \\}\n\\end{array}\n\\]\ndonde \\(\\lambda &gt; 0\\) es la cantidad promedio de veces que ocurre el evento de interés en un periodo o espacio determinado.\n\nDerive el estimador de máxima verosimilitud del parámetro \\(\\lambda\\).\nDerive el posterior \\(p(\\lambda \\mid \\boldsymbol{x})\\) suponiendo que el prior sobre \\(\\lambda\\) es \\(\\text{Gamma}(\\lambda \\mid \\alpha, \\beta)\\) con \\(p(\\lambda \\mid \\alpha, \\beta)  \\propto \\lambda^{\\alpha - 1}e ^ {-\\lambda \\beta}\\). Ayuda: El posterior también es una distribución Gamma.\n¿A qué valor tiende la media a posteriori cuando \\(\\alpha \\to 0\\) y \\(\\beta \\to 0\\)? Recuerde que la media de una distribución \\(\\text{Gamma}(\\alpha, \\beta)\\) es \\(\\alpha/\\beta\\). \n\n📌 El modelo Gamma-Poisson\nSea \\(\\lambda\\) la tasa de mensajes de WhatsApp que una persona recibe en una hora. Suponga inicialmente que se cree que la tasa de mensajes por hora tiene media 5 con desvío estándar de 0.25 mensajes.\n\nElija una distribución Gamma que represente adecuadamente lo que se cree acerca de \\(\\lambda\\)\n¿Cuál es la probabilidad a priori de que la tasa de mensajes sea mayor a 10?\n¿Cuántos mensajes se espera que reciba una persona en promedio en una hora?\n\nSe sondea a un grupo de seis personas que recibieron 7, 3, 8, 9, 10 y 12 mensajes en la última hora.\n\nGraficar la función de verosimilitud.\nDeterminar la distribución a posteriori de \\(\\lambda\\).\n¿Cuál es la probabilidad a posteriori de que la tasa de mensajes sea mayor a 10?\n¿Cuántos mensajes se espera ahora que reciba una persona en promedio en una hora?\n¿Cuál es la probabilidad de que una persona reciba mas de 6 mensajes en una hora?\n\n🧩 📌 Inferencia sobre una distribución Uniforme\nConsidere una distribución \\(\\text{Uniforme}(0, \\theta)\\). La función de densidad de probabilidad es\n\\[\np(x) = \\frac{1}{\\theta}\\mathbb{I}(x \\in [0, \\theta])\n\\]\nSea \\(\\boldsymbol{X} = (X_1,..., X_n)\\) un vector de \\(n\\) variables aleatorias independientes e idénticamente distribuidas según \\(p(x)\\)\nInferencia máximo-verosímil\n\n¿Cuál es el estimador máximo verosímil de \\(\\theta\\) (llámelo \\(\\hat{\\theta}_{\\text{MV}}\\))?\n¿Qué probabilidad le asigna el modelo a una nueva observación \\(x_{n + 1}\\) usando \\(\\hat{\\theta}_{\\text{MV}}\\)?\n¿Observa algún problema con el resultado anterior? Si es así, sugiera una alternativa mejor.\n\nInferencia Bayesiana\nEl prior conjugado de la distribución uniforme es la distribución de Pareto.\nSi \\(X \\sim \\text{Pareto}(\\alpha, m)\\), luego \\[\np(x \\mid \\alpha, m) = \\frac{\\alpha m^\\alpha}{x^{\\alpha+1}} \\mathbb{I}(x \\ge m)\n\\]\nSi el prior es una distribución de Pareto, la distribución conjunta de \\(\\theta\\) y \\(\\boldsymbol{X} = (X_1,..., X_n)\\) es \\[\np(\\theta, \\boldsymbol{X})\n    = \\frac{\\alpha m^\\alpha}{\\theta^{n + \\alpha + 1}}\n    \\mathbb{I}(\\theta \\ge \\max(M_x, m))\n\\]\ndonde \\(M_x = \\text{max}(\\boldsymbol{X})\\). La evidencia es: \\[\n\\begin{aligned}\np(\\boldsymbol{X}) &= \\int_{M_x}^\\infty\n                 \\frac{\\alpha m^\\alpha}{\\theta^{n + \\alpha + 1}}\n                 d\\theta \\\\\n&=  \\begin{cases}\n    \\displaystyle \\frac{\\alpha}{(n+\\alpha)m^n} & \\text{Si } M_x \\le m \\\\\n    \\displaystyle \\frac{\\alpha m^\\alpha}{(n+\\alpha)M_x^{n+\\alpha}} & \\text{Si } M_x &gt; m \\\\\n    \\end{cases}\n\\end{aligned}\n\\]\nDerive el posterior y muestre que puede ser expresado como una distribución de Pareto. \n🧩 📌 Inferencia sobre una distribución Exponencial\nEl tiempo de vida de una máquina en años \\(X\\) es modelado con una distribución exponencial con parámetro \\(\\theta\\) desconocido. La función de densidad es: \\[\n\\begin{array}{lcrr}\np(x \\mid \\theta) = \\theta e^{-\\theta x} & \\text{con} & x \\ge 0, & \\theta \\ge 0\n\\end{array}\n\\]\n\nMuestre que el estimador máximo verosímil (MV) es \\(\\hat{\\theta}_\\text{MV} = 1/\\bar{x}\\).\nSuponga que se observan los siguientes tiempos de vida de tres máquinas independientes \\(x_1 = 5\\), \\(x_2 = 6\\), \\(x_3 = 4\\). ¿Cuál es el valor del estimador MV?\nUna experta del área sugiere que \\(\\theta\\) debe tener una distribución a priori que también sea exponencial. \\[\n\\begin{aligned}\n\\theta \\mid \\lambda &\\sim \\text{Exp}(\\lambda) \\\\\np(\\theta \\mid \\lambda) &= \\lambda e^{-\\lambda \\theta}\n\\end{aligned}\n\\] Elija un valor para el hiperparámetro \\(\\lambda\\) de la distribución a priori tal que \\(\\mathbb{E}(\\theta) = 1/3\\). Utilice \\(\\lambda_0\\) para representar al valor.\n¿Cuál es el posterior \\(p(\\theta \\mid \\boldsymbol{x}, \\lambda_0)\\)?\n¿Es la distribución exponencial conjugada con un likelihood exponencial?\nEncuentre la media del posterior, \\(\\mathbb{E}(\\theta \\mid \\boldsymbol{x}, \\lambda_0)\\)\nExplique por que difieren el estimador MV de la media a posteriori. ¿Cuál es más razonable en este ejemplo? \n\n📌 Otras distribuciones conjugadas (I)\nConsidere el siguiente modelo: \\[\n\\begin{array}{l}\nY \\mid \\theta \\sim \\text{Geométrica}(\\theta) \\\\\n\\theta \\sim \\text{Beta}(\\alpha, \\beta)\n\\end{array}\n\\]\ndonde la función de densidad de la distribución geométrica es \\(p(y \\mid \\theta) = \\theta (1 - \\theta)^{y-1}\\) para \\(y \\in {1,2,\\dots}\\)\n\n¿Qué debería ocurrir con la distribución a posteriori de \\(\\theta\\) para poder afirmar que la distribución geométrica es conjugada de la beta?\nDerive la distribución a posteriori de \\(\\theta\\) y concluya.\n\n📌 Otras distribuciones conjugadas (II)\nConsidere el siguiente modelo: \\[\n\\begin{aligned}\nY\\mid\\theta &\\sim \\text{BinomialNeg}(\\theta, m) \\\\\n\\theta &\\sim \\text{Beta}(\\alpha,\\beta)\n\\end{aligned}\n\\]\ndonde la función de densidad de la distribución binomial negativa es: \\[\np(y \\mid \\theta, m) = {y+m-1 \\choose y} \\theta^{m} (1-\\theta)^y\n\\]\nObtenga la distribución a posteriori de \\(\\theta\\).\n🧩 Otras distribuciones conjugadas (III)\nConsidere el siguiente modelo: \\[\nY \\mid \\theta \\sim \\text{Exponencial}(\\theta) = \\text{Gamma}(1,\\theta)\n\\]\ndonde la función de densidad exponencial es \\(p(y \\mid \\theta) = \\theta e^{-\\theta y}\\).\nElija una distribución a priori conjugada de la verosimilitud propuesta y obtenga la expresión para la distribución de probabilidad a posteriori.",
    "crumbs": [
      "Práctica",
      "Unidad 2"
    ]
  },
  {
    "objectID": "practica/practica_02.html#modelos-de-distribuciones-conjugadas",
    "href": "practica/practica_02.html#modelos-de-distribuciones-conjugadas",
    "title": "Práctica - Unidad 2",
    "section": "",
    "text": "Esta sección contiene ejercicios para trabajar con modelos basados en distribuciones conjugadas. En general, los ejercicios requieren cálculos o derivaciones que se pueden realizar a mano. Sin embargo, se promueve fuertemente el uso de la computadora y el lenguaje R para verificar los resultados, mostrar soluciones alternativas y ejercitar el uso de una herramienta que será de suma utilidad a lo largo de todo el curso y de la vida profesional.\n\n💻📌 ¿Quién domina el posterior?\nPara cada una de las situaciones siguientes, se da una distribución a priori Beta para el parámetro \\(\\pi\\) de un ensayo binomial. Para cada escenario, identificar cuál de estos se cumple: el prior tiene mayor influencia en el posterior, los datos tienen más influencia en el posteriori, o la creencia a priori y los datos influyen de manera similar en la creencia a posteriori\n\nPrior: \\(\\pi \\sim \\text{Beta}(1,4)\\), observaciones: \\(y=8\\) éxitos en \\(n=10\\) ensayos.\nPrior: \\(\\pi \\sim \\text{Beta}(20,3)\\), observaciones: \\(y=0\\) éxitos en \\(n=1\\) ensayos.\nPrior: \\(\\pi \\sim \\text{Beta}(4,2)\\), observaciones: \\(y=1\\) éxitos en \\(n=3\\) ensayos.\nPrior: \\(\\pi \\sim \\text{Beta}(3,10)\\), observaciones: \\(y=10\\) éxitos en \\(n=13\\) ensayos.\nPrior: \\(\\pi \\sim \\text{Beta}(20,2)\\), observaciones: \\(y=10\\) éxitos en \\(n=200\\) ensayos.\n\n💻📌 Más o menos certeza\nSea \\(\\theta\\) la proporción de personas que prefieren los perros a los gatos. Suponga que se elige una distribución \\(\\text{Beta}(7,2)\\) para representar la creencia a priori\n\nDe acuerdo al prior ¿cuáles son valores razonables para \\(\\theta\\)?\nSe observa en una encuesta que \\(y=19\\) de \\(n=20\\) personas prefieren perros, ¿cómo cambia eso el conocimiento acerca de \\(\\theta\\)? Comenta en términos de la evolución de la credibilidad media y del grado de certidumbre acerca de \\(\\theta\\).\nSi, en lugar de eso, se determina que \\(y=1\\) de \\(n=20\\) personas prefieren perros, ¿cómo cambia ahora el grado de credibilidad de los diferentes valores de \\(\\theta\\)?\nSi, en lugar de eso, se determina que \\(y=10\\) de \\(n=20\\) personas prefieren perros, ¿cómo cambia ahora el grado de credibilidad de los diferentes valores de \\(\\theta\\)?\n\n💻📌 Pasito a pasito\nSea \\(\\theta\\) la probabilidad de éxito de un evento de interés. Sea \\(\\text{Beta}(2,3)\\) la distribución a priori para \\(\\theta\\). Actualiza la distribución a posteriori para \\(\\theta\\) secuencialmente:\n\nPrimera observación: éxito.\nSegunda observación: éxito.\nTercera observación: fracaso.\nCuarta observación: éxito.\n\n💻📌 Pasitos tras pasitos\nSea \\(\\theta\\) la probabilidad de éxito de un evento de interés. Sea \\(\\text{Beta}(2,3)\\) la distribución a priori para \\(\\theta\\). Actualiza la distribución a posteriori para \\(\\theta\\) secuencialmente dados conjuntos de cinco observaciones:\n\nPrimeras observaciones: tres éxitos.\nSegundas observaciones: un éxito.\nTerceras observaciones: un éxito.\nCuartas observaciónes: dos éxitos.\n\n💻📌 Diferentes observaciones, diferentes posteriors\nUna empresa que fabrica zapatillas está diseñando una publicidad para Instagram. Tres empleados comparten que la creencia a priori para \\(\\pi\\), la probabilidad de que un cliente haga clic en el anuncio cuando lo ve, puede expresarse con una distribución \\(\\text{Beta}(4, 3)\\). No obstante, los tres empleados realizan tres experimentos distintos y por ende tienen acceso a datos diferentes. El primer empleado prueba el anuncio en una persona, que no cliquea el anuncio. El segundo lo prueba en 10 personas, de las cuales 3 cliquean el anuncio. El último lo prueba en 100 personas, 20 de las cuales cliquean el anuncio.\n\nDescriba el entendimiento a priori que los empleados tienen sobre \\(\\pi\\).\nEspecifique la distribución a posteriori de cada uno de los empleados.\nCompare las distribuciones a posteriori de cada empleado.\n\n💻📌 ¿Galletitas o masitas?\nLa UNR reúne cada año a estudiantes provenientes de diferentes localidades. Cuántas cuadras constituyen una distancia “caminable” suele ser motivo de discusión, entre otros. Pero la verdadera grieta está entre la denominación galletitas versus masitas. Un rosarino pone un prior \\(\\text{Beta}(20,2)\\) a la proporción de personas que dicen galletitas, mientras que un oriundo de una localidad del interior dirá que la credibilidad a priori es \\(\\text{Beta}(2,8)\\).\n\nResuma ambas distribuciones a priori y explique con sus palabras lo que implican.\nCon la información de sus compañeros de curso, actualice ambas distribuciones a priori. ¿Es suficiente esa información para acercar ambas posturas?\n\n💻 📌 Mi primera huerta\nEn un campamento de verano para infantes se realizaron actividades que promueven el contacto con la naturaleza. Una de las tareas consistió en germinar semillas de tomate. Josefina plantó 18 semillas en su almaciguera. Al cabo de 5 días, 8 de ellas germinaron. Sea \\(\\theta\\) la probabilidad de que una semilla de tomate germine y sea \\(\\text{Beta}(1, 1)\\) su distribución a priori.\n\n¿Qué información implica el prior sobre la probabilidad de germinación?\nCalcule la media y el desvío estándar a posteriori de \\(\\theta\\) a mano.\nVerifique el cálculo utilizando R.\nObtenga un intervalo de credibilidad del 95% para \\(\\theta\\).\n\n\n\n\n\n\nFoto de Markus Spiske en Unsplash\n\n\n\n\n💻 ¿Quién dijo que el fútbol siempre da revancha?\nEn la final del 2018 de la Copa del Mundo de la FIFA, Francia le ganó a Croacia por 4 a 2. Considere que el número de goles que un equipo hace en un partido puede modelizarse con una distribución de Poisson. Suponga un parámetro \\(\\lambda_F\\) para Francia y uno \\(\\lambda_C\\) para Croacia. Elija una distribución Gamma a priori para el número medio de goles por partido (es decir, \\(\\lambda_F\\) y \\(\\lambda_C\\) compartirán la distribución a priori). \\(\\lambda_F\\) da una idea de la capacidad de Francia de hacer goles (\\(\\lambda_C\\) lo mismo, pero para Croacia).\nEn función del resultado del partido, obtenga las distribuciones a posteriori de \\(\\lambda_F\\) y \\(\\lambda_C\\) y responda utilizando R:\n\n¿Qué probabilidad hay de que Francia fuera un mejor equipo que Croacia?\nSi el mismo partido se jugara de nuevo (cosa que los franceses en aquella oportunidad no pidieron), ¿cuál es la probabilidad de que Francia ganara de nuevo? \n\n\n\n\n\n\n\nAyuda\n\n\n\n\n\nPlantee el modelo: \\[\n\\begin{aligned}\nY_F & \\sim \\text{Poisson}(\\lambda_F) \\\\\nY_C & \\sim \\text{Poisson}(\\lambda_C) \\\\\n\\lambda_F, \\lambda_C &\\sim \\text{Gamma}(\\alpha, \\beta)\n\\end{aligned}\n\\]\nutilizando valores razonables para \\(\\alpha\\) y \\(\\beta\\). Luego obtener el posterior y calcular \\(P(\\lambda_F &gt; \\lambda_C)\\). Para el segundo punto calcular \\(P(\\tilde{y}_F \\mid y_F) &gt; P(\\tilde{y}_C \\mid y_C)\\).\n\n\n\nMirá si me va a pasar a mi…\nDurante el desarrollo de las vacunas contra el COVID-19, un medio anunció para una determinada vacuna una eficacia del 100%.\n\nEn la fase 3 de un ensayo en adolescentes de entre 12 y 15 años, la vacuna BNT162b2 de Pfizer-BioNTech para el COVID-19 demostró una eficacia del 100% y una respuesta robusta de anticuerpos. El ensayo clínico involucró 2260 jóvenes estadounidenses. En el ensayo, 18 casos de COVID-19 fueron observados en el grupo placebo (\\(n=1129\\)) y ninguno en el grupo vacunado (\\(n=1131\\))\n\nEs de esperar que, en un ensayo más grande, aparezca algún caso de COVID-19 en el grupo que recibió el tratamiento. ¿Cómo se estima la probabilidad de algo que aún no ocurrió? \n\n\n\n\n\n\nAyuda\n\n\n\n\n\nSea \\(Y\\) laf cantidad de personas que reciben la vacuna y se contagian COVID-19. Plantee un modelo y obtenga el posterior de \\(\\theta_v\\), la probabilidad de que una persona vacunada se contagie de COVID-19. Luego obtenga la distribución predictiva a posteriori utilizando un \\(N_v\\) grande y calcule \\(P(\\tilde{Y} &gt; 0)\\).\n\n\n\n💻 La regla del tres\nUna estudiante de Licenciatura en Estadística está releyendo su tesina antes de entregarla. Si en 20 páginas encontrase 5 de ellas con al menos un typo, sería razonable estimar que la probabilidad de que una página contenga un typo es \\(\\frac{5}{20} = \\frac{1}{4}\\). ¿Pero qué ocurre si en 20 páginas no encuentra ningún error?\nVerifcar que, partiendo de un prior uniforme, \\(\\frac{3}{N}\\) es una estimación razonable para \\(\\tau\\) (la probabilidad de que una página contenga un typo), siendo \\(N\\) el número de páginas. Para ello, grafique la distribución a posteriori que se obtiene al haber observado 0 typos en 10 páginas y luego halle la probabilidad de que \\(\\tau &lt; \\frac{3}{N}\\) para diferentes valores de \\(N\\) (10, 100, 1000, 10000). \n\n\n\n\n\n\nAyuda\n\n\n\n\n\nConsiderar \\(Y =\\) cantidad de páginas con al menos un typo en \\(N\\) páginas. Asumir que la ocurrencia de typos entre páginas son independientes. El prior uniforme refiere a la probabilidad de cometer al menos un typo en una página, \\(\\tau\\). Para los diferentes valores de \\(N\\), primero encontrar el posterior de \\(\\tau\\) y luego calcular \\(P(\\tau &lt; \\frac{3}{N})\\). ¿Qué se observa?\n\n\n\n¿Tenés alguien para recomendar?\nUna colega quiere comprar un producto por Internet. Tres vendedores ofrecen el mismo producto al mismo precio. Un vendedor tiene 100% de evaluaciones positivas, con 10 reviews. Otro tiene 96% de evaluaciones positivas, con 50 reviews. El último tiene 90% de comentarios positivos, con 200 evaluaciones. ¿Cuál de los tres vendedores le recomendarías? \n💻📌 Bichos\nUn biólogo quiere determinar la densidad de un insecto en su región. Su conocimiento a priori del número promedio de insectos por unidad de área (\\(\\text{m}^2\\)) se puede representar con una distribución Gamma de media 0.50 y desvío estándar 0.25. En una investigación en 20 \\(\\text{m}^2\\) de área, se hallan 3, 2, 5, 1 y 2 insectos en los primeros 5 \\(\\text{m}^2\\) y ninguno en la fracción de tierra restante.\n\nHalle la distribución a posteriori del número medio de insectos por unidad de área.\nHalle la distribución predictiva a posteriori del número de insectos que se espera encontrar en una exploración de un área de 10 \\(\\text{m}^2\\)\n\n\n\n\n\n\nUna gran variedad de insectosImágen de Freepik\n\n\n\n\n\n\n\n\n\n\nAyuda\n\n\n\n\n\n\nUna distribución conveniente para la cantidad de bichos por metro cuadrado dado que se sabe que su promedio tiene un prior Gamma es la Poisson. A partir de eso es sencillo obtener el posterior.\nPrimero resolverlo para 1 metro cuadrado, y luego para 10. Se puede hacer computacionalmente muestreando \\(\\lambda\\) del posterior y obteniendo muestras de la distribución de la respuesta usando los valores muestreados de \\(\\lambda\\). También se puede resolver de manera teórica calculando la integral de la distribución predictiva a posteriori. Para eso tener en cuenta: \\[\n\\begin{aligned}\n\\Gamma(x + 1) &= x! \\\\\n\\int_0^{\\infty} x^b e^{-ax} dx &= \\frac{\\Gamma(b + 1)}{a ^ {b+1}}\n\\end{aligned}\n\\] y que la función de masa de probabiliad la distribución binomial negativa se puede escribir como: \\[\n\\begin{array}{ccc}\np(k, \\mid r, \\pi) = P(X = k) = \\displaystyle \\frac{\\Gamma(k + r)}{\\Gamma(k + 1) \\Gamma({r})}(1 - \\pi)^k \\pi^r,\n& k \\in \\{0, 1, 2, \\dots\\},\n& r &gt; 0\n\\end{array}\n\\]\n\n\n\n\n📌 Alter-ego\nEl profesor Caprista y el profesor Evangetto están dando sus primeros cursos de Estadística Bayesiana. Sus colegas les dijeron que el puntaje promedio en un examen final, \\(\\mu\\), varía normalmente año a año con media 8 y desvío estándar 0.4. Y además, que los puntajes individuales de los estudiantes \\(Y\\) varían normalmente alrededor de \\(\\mu\\) con una desviación estándar de 0.4\n\n¿Cuál es la probabilidad a priori de que un estudiante se saque más de 9 en un examen final?\nEl profesor Caprista toma el examen final y observa que sus 20 estudiantes obtuvieron una nota media de 8.6. Halle la distribución a posteriori de \\(\\mu\\).\nEl profesor Evangetto toma el examen final y observa que sus 20 estudiantes obtuvieron una nota media de 8.2. Halle la distribución a posteriori de \\(\\mu\\).\nCombine las notas de ambos exámenes para obtener la distribución a posteriori de \\(\\mu\\)\n¿Cuál es la probabilidad a posteriori de que un estudiante se saque más de 9 en un examen final?\n\n📌 Inferencia sobre una distribución de Poisson\nLa distribución de masa de probabilidad Poisson se define como\n\\[\n\\begin{array}{lcr}\n\\displaystyle p(x \\mid \\lambda) = \\frac{e^{-\\lambda}\\lambda^x}{x!} &\n\\text{con} &\nx \\in \\{0, 1, 2, \\cdots \\}\n\\end{array}\n\\]\ndonde \\(\\lambda &gt; 0\\) es la cantidad promedio de veces que ocurre el evento de interés en un periodo o espacio determinado.\n\nDerive el estimador de máxima verosimilitud del parámetro \\(\\lambda\\).\nDerive el posterior \\(p(\\lambda \\mid \\boldsymbol{x})\\) suponiendo que el prior sobre \\(\\lambda\\) es \\(\\text{Gamma}(\\lambda \\mid \\alpha, \\beta)\\) con \\(p(\\lambda \\mid \\alpha, \\beta)  \\propto \\lambda^{\\alpha - 1}e ^ {-\\lambda \\beta}\\). Ayuda: El posterior también es una distribución Gamma.\n¿A qué valor tiende la media a posteriori cuando \\(\\alpha \\to 0\\) y \\(\\beta \\to 0\\)? Recuerde que la media de una distribución \\(\\text{Gamma}(\\alpha, \\beta)\\) es \\(\\alpha/\\beta\\). \n\n📌 El modelo Gamma-Poisson\nSea \\(\\lambda\\) la tasa de mensajes de WhatsApp que una persona recibe en una hora. Suponga inicialmente que se cree que la tasa de mensajes por hora tiene media 5 con desvío estándar de 0.25 mensajes.\n\nElija una distribución Gamma que represente adecuadamente lo que se cree acerca de \\(\\lambda\\)\n¿Cuál es la probabilidad a priori de que la tasa de mensajes sea mayor a 10?\n¿Cuántos mensajes se espera que reciba una persona en promedio en una hora?\n\nSe sondea a un grupo de seis personas que recibieron 7, 3, 8, 9, 10 y 12 mensajes en la última hora.\n\nGraficar la función de verosimilitud.\nDeterminar la distribución a posteriori de \\(\\lambda\\).\n¿Cuál es la probabilidad a posteriori de que la tasa de mensajes sea mayor a 10?\n¿Cuántos mensajes se espera ahora que reciba una persona en promedio en una hora?\n¿Cuál es la probabilidad de que una persona reciba mas de 6 mensajes en una hora?\n\n🧩 📌 Inferencia sobre una distribución Uniforme\nConsidere una distribución \\(\\text{Uniforme}(0, \\theta)\\). La función de densidad de probabilidad es\n\\[\np(x) = \\frac{1}{\\theta}\\mathbb{I}(x \\in [0, \\theta])\n\\]\nSea \\(\\boldsymbol{X} = (X_1,..., X_n)\\) un vector de \\(n\\) variables aleatorias independientes e idénticamente distribuidas según \\(p(x)\\)\nInferencia máximo-verosímil\n\n¿Cuál es el estimador máximo verosímil de \\(\\theta\\) (llámelo \\(\\hat{\\theta}_{\\text{MV}}\\))?\n¿Qué probabilidad le asigna el modelo a una nueva observación \\(x_{n + 1}\\) usando \\(\\hat{\\theta}_{\\text{MV}}\\)?\n¿Observa algún problema con el resultado anterior? Si es así, sugiera una alternativa mejor.\n\nInferencia Bayesiana\nEl prior conjugado de la distribución uniforme es la distribución de Pareto.\nSi \\(X \\sim \\text{Pareto}(\\alpha, m)\\), luego \\[\np(x \\mid \\alpha, m) = \\frac{\\alpha m^\\alpha}{x^{\\alpha+1}} \\mathbb{I}(x \\ge m)\n\\]\nSi el prior es una distribución de Pareto, la distribución conjunta de \\(\\theta\\) y \\(\\boldsymbol{X} = (X_1,..., X_n)\\) es \\[\np(\\theta, \\boldsymbol{X})\n    = \\frac{\\alpha m^\\alpha}{\\theta^{n + \\alpha + 1}}\n    \\mathbb{I}(\\theta \\ge \\max(M_x, m))\n\\]\ndonde \\(M_x = \\text{max}(\\boldsymbol{X})\\). La evidencia es: \\[\n\\begin{aligned}\np(\\boldsymbol{X}) &= \\int_{M_x}^\\infty\n                 \\frac{\\alpha m^\\alpha}{\\theta^{n + \\alpha + 1}}\n                 d\\theta \\\\\n&=  \\begin{cases}\n    \\displaystyle \\frac{\\alpha}{(n+\\alpha)m^n} & \\text{Si } M_x \\le m \\\\\n    \\displaystyle \\frac{\\alpha m^\\alpha}{(n+\\alpha)M_x^{n+\\alpha}} & \\text{Si } M_x &gt; m \\\\\n    \\end{cases}\n\\end{aligned}\n\\]\nDerive el posterior y muestre que puede ser expresado como una distribución de Pareto. \n🧩 📌 Inferencia sobre una distribución Exponencial\nEl tiempo de vida de una máquina en años \\(X\\) es modelado con una distribución exponencial con parámetro \\(\\theta\\) desconocido. La función de densidad es: \\[\n\\begin{array}{lcrr}\np(x \\mid \\theta) = \\theta e^{-\\theta x} & \\text{con} & x \\ge 0, & \\theta \\ge 0\n\\end{array}\n\\]\n\nMuestre que el estimador máximo verosímil (MV) es \\(\\hat{\\theta}_\\text{MV} = 1/\\bar{x}\\).\nSuponga que se observan los siguientes tiempos de vida de tres máquinas independientes \\(x_1 = 5\\), \\(x_2 = 6\\), \\(x_3 = 4\\). ¿Cuál es el valor del estimador MV?\nUna experta del área sugiere que \\(\\theta\\) debe tener una distribución a priori que también sea exponencial. \\[\n\\begin{aligned}\n\\theta \\mid \\lambda &\\sim \\text{Exp}(\\lambda) \\\\\np(\\theta \\mid \\lambda) &= \\lambda e^{-\\lambda \\theta}\n\\end{aligned}\n\\] Elija un valor para el hiperparámetro \\(\\lambda\\) de la distribución a priori tal que \\(\\mathbb{E}(\\theta) = 1/3\\). Utilice \\(\\lambda_0\\) para representar al valor.\n¿Cuál es el posterior \\(p(\\theta \\mid \\boldsymbol{x}, \\lambda_0)\\)?\n¿Es la distribución exponencial conjugada con un likelihood exponencial?\nEncuentre la media del posterior, \\(\\mathbb{E}(\\theta \\mid \\boldsymbol{x}, \\lambda_0)\\)\nExplique por que difieren el estimador MV de la media a posteriori. ¿Cuál es más razonable en este ejemplo? \n\n📌 Otras distribuciones conjugadas (I)\nConsidere el siguiente modelo: \\[\n\\begin{array}{l}\nY \\mid \\theta \\sim \\text{Geométrica}(\\theta) \\\\\n\\theta \\sim \\text{Beta}(\\alpha, \\beta)\n\\end{array}\n\\]\ndonde la función de densidad de la distribución geométrica es \\(p(y \\mid \\theta) = \\theta (1 - \\theta)^{y-1}\\) para \\(y \\in {1,2,\\dots}\\)\n\n¿Qué debería ocurrir con la distribución a posteriori de \\(\\theta\\) para poder afirmar que la distribución geométrica es conjugada de la beta?\nDerive la distribución a posteriori de \\(\\theta\\) y concluya.\n\n📌 Otras distribuciones conjugadas (II)\nConsidere el siguiente modelo: \\[\n\\begin{aligned}\nY\\mid\\theta &\\sim \\text{BinomialNeg}(\\theta, m) \\\\\n\\theta &\\sim \\text{Beta}(\\alpha,\\beta)\n\\end{aligned}\n\\]\ndonde la función de densidad de la distribución binomial negativa es: \\[\np(y \\mid \\theta, m) = {y+m-1 \\choose y} \\theta^{m} (1-\\theta)^y\n\\]\nObtenga la distribución a posteriori de \\(\\theta\\).\n🧩 Otras distribuciones conjugadas (III)\nConsidere el siguiente modelo: \\[\nY \\mid \\theta \\sim \\text{Exponencial}(\\theta) = \\text{Gamma}(1,\\theta)\n\\]\ndonde la función de densidad exponencial es \\(p(y \\mid \\theta) = \\theta e^{-\\theta y}\\).\nElija una distribución a priori conjugada de la verosimilitud propuesta y obtenga la expresión para la distribución de probabilidad a posteriori.",
    "crumbs": [
      "Práctica",
      "Unidad 2"
    ]
  },
  {
    "objectID": "practica/practica_02.html#simulaciones",
    "href": "practica/practica_02.html#simulaciones",
    "title": "Práctica - Unidad 2",
    "section": "💻 Simulaciones",
    "text": "💻 Simulaciones\nA diferencia de la sección anterior, que requiere resolver los ejercicios a mano y promueve el uso de la computadora y R de manera complementaria, esta sección contiene ejercicios que deben ser resueltos mediante técnicas de simulación implementadas en R. Es posible que en algunos casos también se pueda obtener una solución analítica. En estos casos, puede resultar de utilidad obtener también una solución a mano para validar el resultado, evaluar el nivel de dificultad y ver que tan intuitivo resultan ambos enfoques.\n\n📌 Entrada en calor\nPara cada una de las siguientes situaciones, hallar los intervalos centrales de credibilidad.\n\nIntervalo del 95% para \\(\\pi\\) siendo \\(\\pi\\mid \\boldsymbol{y} \\sim \\text{Beta}(4,5)\\).\nIntervalo del 60% para \\(\\pi\\) siendo \\(\\pi\\mid \\boldsymbol{y} \\sim \\text{Beta}(4,5)\\).\nIntervalo del 89% para \\(\\lambda\\) siendo \\(\\lambda\\mid \\boldsymbol{y} \\sim \\text{Gamma}(1,8)\\).\nIntervalo del 95% para \\(\\lambda\\) siendo \\(\\lambda\\mid \\boldsymbol{y} \\sim \\text{Gamma}(2,5)\\).\nIntervalo del 81% para \\(\\mu\\) siendo \\(\\mu\\mid \\boldsymbol{y} \\sim \\mathcal{N}(10,2^2)\\).\nIntervalo del 99% para \\(\\pi\\) siendo \\(\\mu\\mid \\boldsymbol{y} \\sim \\mathcal{N}(-3,1^2)\\).\n\nPropiedades frecuentistas de inferencias bayesianas (!!)\nSea una variable \\(Y\\) tal que \\(Y \\mid \\theta \\sim \\text{Binomial}(n, \\theta)\\) y \\(\\theta \\sim \\text{Beta}(1/2, 1/2)\\). Mediante un estudio de simulación calcule la cobertura empírica del intervalo de credibilidad del 95% con \\(n \\in \\{1, 5, 10, 25\\}\\) y \\(\\theta \\in \\{0.05, 0.10, \\dots, 0.50 \\}\\). Describa las propiedades frecuentistas del intervalo de credibilidad bayesiano. \n📌 ¿Te preguntaste alguna vez cuál es la distribución de un p-value?\nConsidere un problema conocido. Se desean comparar dos muestras independientes de tamaño 5 utilizando un test t y utilizando el test de Mann-Whitney.\n\nConsidere el caso en que las dos muestras provienen de poblaciones con igual media y desvío estándar (supongamos normal de media nula y varianza unitaria). Si se repitiera muchas veces el proceso de tomar las muestras y realizar los tests, ¿qué distribución tendrán los p-values obtenidos para cada test?\nConsidere ahora el caso en que las dos muestras provienen de poblaciones con diferente media e igual desvío estándar (\\(\\mathcal{N}(0,1)\\) y \\(\\mathcal{N}(1,1)\\)). Si se repitiera muchas veces el proceso de tomar las muestras y realizar los tests, ¿qué distribución tendrán los p-values obtenidos para cada test?",
    "crumbs": [
      "Práctica",
      "Unidad 2"
    ]
  },
  {
    "objectID": "practica/practica_02.html#elección-de-distribuciones-a-priori",
    "href": "practica/practica_02.html#elección-de-distribuciones-a-priori",
    "title": "Práctica - Unidad 2",
    "section": "Elección de distribuciones a priori",
    "text": "Elección de distribuciones a priori\nEsta útima sección de la práctica tiene como propósito ejercitar el uso de distribuciones de probabilidad como herramienta para reflejar información de un problema determinado.\n\n📌 Esbozar la distribución de las siguientes variables\n\nEl número de personas que compran café en el bar de la facultad asumiendo distribución de Poisson.\nEl peso de perros adultos en kilogramos asumiendo una distribución Uniforme.\nEl peso de elefantes adultos en kilogramos asumiendo una distribución Normal.\nEl peso de humanos adultos en libras asumiendo una distribución asimétrica hacia la derecha.\n\n📌 Verificar los resultados de manera computacional\nPara cada uno cada uno de los ejemplos del ejercicio anterior, graficar la distribución usando R. Seleccionar los parámetros que creas razonable, tomar una muestra aleatoria de tamaño 1000 y graficar la distribución en base a las muestras. ¿Se refleja tu conocimiento del problema en la distribución graficada? Si no, ajustar los parámetros y repetir el proceso hasta que el resultado tenga concuerde con el conocimiento del problema.\n💻 📌 Hay que amigarse con de la distribución Beta\nComparar las siguientes distribuciones a priori.\n\n\\(\\text{Beta}(0.5, 0.5)\\).\n\\(\\text{Beta}(1, 1)\\).\n\\(\\text{Beta}(1, 4)\\).\n\\(\\text{Beta}(5, 1.5)\\).\n\n\n¿En qué se diferencian?\n¿Cuál de ellas es más informativa?\n¿Cómo lo determinaste?\n\n💻 📌 Elicitación de priors\nEn cada una de la situaciones que se describen debajo, ajustar manualmente los parámetros de una distribución \\(\\text{Beta}\\) para que reflejen la información brindada. No siempre existe una única respuesta correcta.\n\nUn amigo se postuló para un empleo en LinkedIn y te dijo: “Diría que tengo una chance del 40% de que me den el trabajo, pero no estoy seguro”. Cuando le preguntamos un poco mas, dijo que estima sus chances entre un 20% y un 60%.\nUn grupo de investigación del CONICET desarrolló una nueva prueba para una enfermedad bastante rara. El grupo espera que esta prueba arroje resultados correctos el 80% de las veces, con una varianza de 0.05.\nEl primo de un amigo es un apasionado de la pesca, lo practica muy seguido, y se dice ser muy bueno. Según comenta tu amigo, en el asado de los Jueves el pescador dijo lo siguiente:\n\n\nSi tengo que hacer un promedio, 9 de cada 10 veces que salgo, vuelvo con algo. Pero últimamente te diría que siempre es 10 de 10. Estoy infalible. La verdad es que soy un crack de la pesca.\n\nAnte el descreimiento de algunos de los comensales supo reconocer que no siempre le fue tan bien:\n\nTuve mis malas rachas, pero nunca menos de 8 pescas de cada 10 salidas.\n\n💻 📌 Efecto de la parametrización\nSea \\(\\theta\\) la probabilidad de éxito en un experimento binomial y sea \\(\\gamma = \\frac{\\theta}{1-\\theta}\\) la chance de éxito. Utilizar simulaciones para explorar los efectos de las siguientes elecciones de distribuciones a priori\n\nSi \\(\\theta \\sim \\text{Uniforme}(0, 1)\\), ¿cuál es el prior inducido para \\(\\gamma\\)?\nSi \\(\\theta \\sim \\text{Beta}(5, 5)\\), ¿cuál es el prior inducido para \\(\\gamma\\)?\nSi \\(\\gamma \\sim \\text{Uniforme}(0, 100)\\), ¿cuál es el prior inducido para \\(\\theta\\)?\nSi \\(\\gamma \\sim \\text{Gamma}(1, 1)\\), ¿cuál es el prior inducido para \\(\\theta\\)?",
    "crumbs": [
      "Práctica",
      "Unidad 2"
    ]
  },
  {
    "objectID": "practica/practica_02.html#teoría-de-la-decisión",
    "href": "practica/practica_02.html#teoría-de-la-decisión",
    "title": "Práctica - Unidad 2",
    "section": "Teoría de la Decisión",
    "text": "Teoría de la Decisión\n\nDada la distribución a posteriori \\(p(\\theta \\mid y)\\), probar que el estimador de Bayes que minimiza la función de pérdida \\(L_1\\) es la mediana de \\(p(\\theta \\mid y)\\). \nSuponga que la distribución a posteriori de \\(\\pi\\), \\(p(\\pi \\mid y)\\), es \\(\\text{Beta}(12,4)\\). Determine mediante simulación el estimador que minimiza la pérdida de Huber: \\[\n\\mathcal{L}(\\delta,\\pi) =\n\\begin{cases}\n\\frac{1}{2} (\\pi - \\delta)^2 \\text{ si } |\\pi - \\delta| \\leq \\alpha \\\\\n\\alpha \\cdot (|\\pi - \\delta|-\\frac{1}{2}\\alpha) \\text{ en cualquier otro caso}\n\\end{cases}\n\\]",
    "crumbs": [
      "Práctica",
      "Unidad 2"
    ]
  },
  {
    "objectID": "practica/practica_02.html#otros",
    "href": "practica/practica_02.html#otros",
    "title": "Práctica - Unidad 2",
    "section": "Otros",
    "text": "Otros\n\n✍️ 📌 Distribución predictiva a posteriori\nConsidere un modelo \\(Y \\mid \\theta \\sim \\text{Binomial}(\\theta, n)\\), donde \\(\\theta\\) puede tomar valores discretos \\(0,\\ 0.1,\\ 0.2,\\ \\dots,\\ 1\\). Se realizaron inferencias sobre \\(\\theta\\) y se obtuvo la distribución a posteriori que se muestra en la parte superior de la Figura 1.\nSe desea obtener la distribución predictiva a posteriori para el número de éxitos \\(\\tilde{y}\\) en \\(n=5\\) nuevas realizaciones del experimento. Cada valor de \\(\\theta\\) da lugar a una posible distribución de \\(\\tilde{y}\\) de acuerdo a la verosimilitud binomial, como se observa en la parte inferior de la Figura 1.\n\n\n\n\n\n\n\n\n\nFigura 1: Distribución a posteriori y distribución predictiva a posteriori\n\n\n\n\n\n\nCalcule \\(p(\\tilde{y}\\mid\\theta)\\) para cada valor posible de \\(\\theta\\) y compruebe que los gráficos de la parte inferior de la Figura 1 son correctos.\nCombine los \\(p(\\tilde{y}\\mid\\theta)\\) ponderando por las probabilidades a posteriori de \\(\\theta\\), \\(p(\\theta\\mid y)\\) para obtener la distribución predictiva a posteriori\nCompare la varianza de una de las \\(p(\\tilde{y}\\mid\\theta)\\) (por ejemplo, la de \\(\\theta=0.5\\)) con la varianza de la distribución predictiva a posteriori ¿qué observa?\nA partir de \\(p(\\theta\\mid y)\\) y de la verosimilitud binomial, obtenga muestras de \\(p(\\tilde{y}\\mid y)\\) y grafique su distribución.",
    "crumbs": [
      "Práctica",
      "Unidad 2"
    ]
  },
  {
    "objectID": "practica/practica_01.html",
    "href": "practica/practica_01.html",
    "title": "Práctica - Unidad 1",
    "section": "",
    "text": "El propósito de esta sección de la práctica es resolver situaciones que impliquen la aplicación de la Regla de Bayes como se presenta tradicionalmente en un curso de Probabilidad.\n\nDemostración\nDemuestra la validez de la siguiente expresión de la Regla de Bayes\n\\[\nP(B_j \\mid A) = \\frac{P(A \\mid B_j) P(B_j)}{\\sum_{k=1}^{K}P(A \\mid B_k) P(B_k)}\n\\]\ndonde \\(A\\) es un evento cualquiera y \\(\\{B_1, \\cdots, B_K\\}\\) forman una partición. \n\n\n\n\n\n\n📌 El test infalible\nEn una población dada, una de cada mil personas tiene una enfermedad. Se toma una persona al azar de la población, se le aplica un test para detectar dicha enfermedad, y el resultado es positivo. El test se caracteriza por dar positivo el 99% de las veces que una persona tiene la enfermedad. Además, dicho test tiene una tasa de falsos positivos del 5%.\n\n¿Cuál es la probabilidad de que la persona tenga efectivamente la enfermedad?\nSi realizamos el mismo análisis una segunda vez sobre el mismo paciente y obtenemos nuevamente positivo,\n\n¿Cuál seria la probabilidad que el paciente esté enfermo?\n¿Y si diera negativo?\n¿Es el prior el mismo cuando se analiza el resultado del segundo análisis que cuando solo se analiza el primero? \n\n\n¿Es verdad que existen los vampiros? Versión Crepúsculo\nEdward quiere probarle a Bella que los vampiros existen. Según Bella, hay una probabilidad del 5% de que los vampiros existan. También cree que la probabilidad de que exista alguien con la piel brillante dado que los vampiros existen es del 70%, y que la probabilidad de que alguien tenga la piel brillante si los vampiros no existen es del 3%. Edward lleva a Bella al bosque y le muestra que de hecho su piel brilla como un 💎 ¿Cuál es la probabilidad que existan los vampiros? \n\n\n\n\n\nRobert Pattinson como Edward en Crepúsculo\n\n\n\n\n📌 Árboles enfermos\nUn vivero de la ciudad se destaca por vender una variedad de árboles nativos, incluyendo al jacarandá, ceibo, ombú, entre otros. Lamentablemente, el 18% de los árboles del vivero estan infectados con moho. Los árboles enfermos se componen en un 15% por jacarandás, 80% de ceibos, y 5% de otras especies. Los árboles sanos se componen por un 20% de jacarandás, 10% de ceibos, y 70% de otras especies. Con el objetivo de monitorear cuanto se propagó la enfermedad, una de las personas que trabaja en el vivero selecciona al azar uno de los árboles para testear.\n\n¿Cuál es la probabilidad a priori de que el árbol tenga moho?\nResulta que el árbol seleccionado es un ceibo. ¿Cuál es la probabilidad de haber seleccionado un ceibo?\n¿Cuál es la probabilidad a posteriori de que el ceibo seleccionado tenga moho?\nCompare las probabilidades a priori y a posteriori de que el árbol tenga moho. ¿Cómo afecta el análisis el saber que el árbol es un ceibo? \n\n\n\n\n\n\nFlor del Ceibo, la flor nacional\n\n\n\n\n📌 Transporte “El Impuntual”\nUna cierta empresa de transporte regional, que decidimos llamar “El Impuntual”, tiene servicios que van desde Rosario hasta Wheelwright varias veces al día, todos los días de la semana. Un 30% de los viajes salen a la mañana, otro 30% salen a la tarde, y el restante 40% salen a la noche. Los pasajeros suelen estar muy frustrados ya que un 25% de los viajes salen tarde. De estos viajes demorados, el 40% corresponden a la mañana, un 50% suceden a la tarde, y el 10% restante ocurre a la noche1.\nLucio y Franco son dos amigos del pueblo, y se volvieron a sus casas en colectivos diferentes.\n\nLucio se fue en uno de los colectivos de la mañana. ¿Cuál es la probabilidad que su viaje esté demorado?\nEl colectivo de Franco no está demorado. ¿Cuál es la probabilidad de que esté viajando en uno de los colectivos de la mañana? \n\n\n\n\n\n\nFoto de Markus Winkler en Unsplash\n\n\n\n\n📌 Bebé panda\nSupongamos que hay dos especies de osos panda. Ambas especies son igual de frecuentes y viven en la misma región. Es más, lucen de la misma forma y comen la misma comida. Aún no existe una prueba genética que pueda diferenciarlos. Lo único que los diferencia es la cantidad de crías que suelen tener. Las madres de la especie A dan luz a mellizos el 10% del tiempo. Y las madres de la especie B dan a luz mellizos el 20% del tiempo. En todos los otros casos, estas madres dan a luz un solo bebé panda.\nUsando un poco la imaginación, supongamos que somos la persona encargada de un programa de reproducción de pandas. Tenemos una panda femenina que acaba de dar a luz a un par de mellizos, pero no sabemos a que especie pertenece.\n\n¿Cuál es la probabilidad que la mamá panda sea de la especie A?\n¿Cuál es la probabilidad que vuelva a tener mellizos en la próxima parición?\nUn tiempo después sos encontramos con que en la segunda parición da a luz a un único bebé panda. ¿Cuál es la probabilidad de que este panda sea de la especie A? \n\n\n\n\n\n\nFoto de Stone Wang en Unsplash\n\n\n\n\n📌 Paraguas\nEstás a punto de subir a un avión rumbo a Mendoza. Querés saber si tenés que llevar un paraguas o no. Llamás a tres amigos que viven en Mendoza y les preguntás si está lloviendo. Cada uno de ellos tiene una probabilidad de \\(2/3\\) de decirte la verdad y \\(1/3\\) de mentirte para hacerte una broma. Los tres responden que sí está lloviendo. ¿Cuál es la probabilidad de que realmente esté lloviendo en las Mendoza? Se puede asumir que en Mendoza llueve en 1 de cada 10 días. \n\n\n\n\n\n\nAyuda\n\n\n\n\n\nSi \\(LLL\\) es “los tres amigos dijeron que llovía”, buscamos \\(P(\\text{lluvia} \\mid LLL) = P(LLL \\mid \\text{lluvia}) P(\\text{lluvia}) / P(LLL)\\).\n\\(P(LLL \\mid \\text{lluvia})\\) es la probabilidad de que ninguno de los tres mienta.\n\\(P(LLL) = P(LLL \\mid \\text{lluvia}) P(\\text{lluvia}) + P(LLL \\mid \\text{no lluvia}) P(\\text{no lluvia})\\).\n\n\n\n📌 Sherlock\nDos personas dejaron rastros de sangre en la escena del crimen. La sangre de Guido, un sospechoso, es analizada y resulta ser de tipo ‘0’. Los rastros de sangre de la escena son de tipo ‘0’ (un tipo común en la población, presente en el 60% de las personas) y de tipo ‘AB’ (un tipo raro, con una frecuencia del 1% en la población). ¿Estos datos representan evidencia de que Guido estaba presente en la escena del crimen? \n\n\n\n\n\nSherlock Holmes, el detective privado\n\n\n\n\n\n\n\n\n\n\nAyuda\n\n\n\n\n\nLlamemos \\(S\\) a “Guido y otra persona estuvieron en la escena del crimen” versus \\(S'\\), “dos personas desconocidas estuvieron en la escena del crimen”. Además, contamos con la evidencia \\(E\\), “observar sangre 0 y sangre AB”.\nQueremos saber qué es mayor, si \\(P(E \\mid S)\\) o \\(P(E \\mid S')\\) (si luego quisiéramos comparar \\(P(S \\mid E)\\) con \\(P(S' \\mid E)\\) necesitaríamos las probabilidades a priori de \\(P(S)\\) y \\(P(S')\\)). Si \\(P(E \\mid S) &lt; P(E \\mid S')\\) entonces los datos aportan evidencia en contra de que Guido estaba en la escena del crimen (\\(E\\) es más probable bajo \\(S'\\) que bajo \\(S\\)).\n\\(P(E \\mid S)\\) es la probabilidad de observar sangre 0 y sangre AB, si Guido estaba en la escena del crimen. Esto es básicamente la probabilidad de encontrar sangre AB (porque la cuota de 0 ya está cubierta por Guido). Eso equivale a tener en la escena del crimen a una de las personas que tienen sangre AB (1/100 de la población). Luego, \\(P(E \\mid S) = 0.01\\).\n\\(P(E \\mid S')\\) es la probabilidad de observar sangre 0 y sangre AB, si Guido no estaba en la escena del crimen. Esto es la probabilidad de que la primera persona tuviera sangre 0 y la segunda AB más la probabilidad de que la primera persona tuviera sangre AB y la segunda sangre 0: \\((60 / 100) \\cdot (1 / 100) + (1 / 100) \\cdot (60 / 100) = 0.012\\). Para las personas que buscan una explicación más formal, esto es un experimento multinomial con tres resultados posibles (\\(x_1\\): grupo 0, \\(x_2\\): grupo AB, \\(x_3\\): otro grupo), la probabilidad de observar \\(x_1 = 1\\), \\(x_2 = 1\\) y \\(x_3 = 0\\) en \\(N = 2\\) intentos (muestras) \\(\\frac{2!}{1!1!0!} p_1^{1} p_2^{1} p_3^{0}\\).\nPara pensarlo intuitivamente: supongamos que hay 200 personas en un pueblo. 120 personas (Guido y 119 más) tienen sangre 0, 2 personas tienen sangre AB y el resto tiene otro tipo. Con Guido en la escena del crimen hay \\(2/200\\) personas que podrían haber estado con él; sin Guido en la escena, pudieron haber estado cualquiera de las 119 personas de sangre 0 con cualquiera de las 2 personas de sangre AB.\n\n\n\nHijos de la probabilidad\nNos encontramos con alguien en la calle y nos dice que tiene dos hijos. Le preguntamos si alguno de ellos es mujer y nos responde que sí. ¿Cuál es la probabilidad de que ambos sean niñas? \nLos Reyes del Rock\nElvis Presley tenía un hermano varón que nació en el mismo parto pero que murió al poco tiempo. ¿Cuál es la probabilidad de que Elvis tuviera un gemelo? Alguna información adicional: en 1935, cuando Elvis nació, 1/3 de los hermanos del mismo parto eran gemelos y 2/3 mellizos; además, la probabilidad de que dos mellizos sean del mismo sexo biológico puede estimarse en 50%, mientras que dos gemelos son siempre del mismo sexo biológico. \n📌 ¿Alguien ordena las medias?\nDos cajones contienen medias. Uno de ellos tiene igual cantidad de medias blancas y negras. El otro contiene un número igual de medias rojas, verdes y azules. Se elige un cajón al azar, se sacan dos medias sin mirar y resultan ser las dos iguales. ¿Cuál es la probabilidad de que las medias sean blancas? Supóngase que sacar la primera media no altera las proporciones. \n\n\n\n\n\n\nAyuda\n\n\n\n\n\nSean los eventos\n\\(C\\): elegir el cajón BN (y no el RVA)\n\\(I\\): elegir un par de medias iguales\n\\(B\\): elegir un par de medias blancas\nMatemáticamente, buscamos \\(P(B \\mid I) = P(B, C \\mid I) + P(B, C' \\mid I)\\) (básicamente estamos marginalizando la variable “cajón”). El segundo término es 0 porque \\(B\\) y \\(C'\\) no pueden darse nunca (nunca voy a sacar un par blanco del cajón RVA). Entonces queda \\(P(B \\mid I) = P(B,C \\mid I) = P(B \\mid C, I) P(C \\mid I)\\), que son los dosvalores que encontramos arriba.\n¿De dónde sale que \\(P(B, C \\mid I) = P(B \\mid C, I) P(C \\mid I)\\)?\n\\(P(B, C \\mid I) = P(B,C,I)/P(I)\\) y el numerador, por regla de la cadena, es \\(P(A) P(C \\mid I) P(B \\mid C,I)\\).\n\n\n\n📌 La Falacia del Fiscal\nSally Clark era una abogada británica que fue erróneamente sentenciada a prisión perpetua en 1999 por la muerte de sus dos hijos bebés. Su hijo mayor, Christopher, murió con 11 semanas en diciembre de 1996 y su hijo más joven, Harry, con 8 semanas en enero de 1998. Durante el juicio, la defensa argumentó que las muertes se debieron al síndrome de muerte súbita del lactante (SIDS). Clark fue condenada a partir del testimonio del pediatra Sir Roy Meadow, quien argumentó en la corte lo siguiente:\n\nEn familias sanas, la chance de muerte por SIDS es de \\(\\frac{1}{8500}\\)\nLa probabilidad de dos muertes por SIDS en la misma familia es aproximadamente \\(\\frac{1}{8500^2} \\approx \\frac{1}{73000000}\\)\nEs, por ende, muy poco probable que Clark sea inocente\n\nLuego de pasar 3 años en prisión, Clark fue liberada en 2003 luego de que se determinara que el testimonio experto de Meadows era equivocado. Dos mujeres, a las cuales el testimonio de Meadows había enviado a prisión, también fueron liberadas.\n\nIdentifica una falla en la probabilidad de \\(\\frac{1}{73000000}\\) dada por Meadows.\nIncluso aceptando el número anterior como correcto, ¿cuál es el problema de interpretar esa probabilidad como la probabilidad de inocencia de Clark?\n\n\n\n\n\n\n\n\nAyuda\n\n\n\n\n\nAún asumiendo que \\(P(E \\mid I)\\) es lo que dice Meadows, esa no es la probabilidad de interés. La probabilidad que interesa es la de inocencia (o culpabilidad) dada la evidencia. Es decir, la probabilidad que nos interesa es \\(P(I \\mid E)\\) que la podemos escribir como \\(P(I \\mid E) = P(E \\mid I) P(I) / P(E)\\).\n\\(P(I \\mid E)\\) es solo similar a \\(P(E \\ mid I)\\) si \\(P(I)\\) es similar a \\(P(E)\\). Este no es el caso, ¿por qué?",
    "crumbs": [
      "Práctica",
      "Unidad 1"
    ]
  },
  {
    "objectID": "practica/practica_01.html#regla-de-bayes",
    "href": "practica/practica_01.html#regla-de-bayes",
    "title": "Práctica - Unidad 1",
    "section": "",
    "text": "El propósito de esta sección de la práctica es resolver situaciones que impliquen la aplicación de la Regla de Bayes como se presenta tradicionalmente en un curso de Probabilidad.\n\nDemostración\nDemuestra la validez de la siguiente expresión de la Regla de Bayes\n\\[\nP(B_j \\mid A) = \\frac{P(A \\mid B_j) P(B_j)}{\\sum_{k=1}^{K}P(A \\mid B_k) P(B_k)}\n\\]\ndonde \\(A\\) es un evento cualquiera y \\(\\{B_1, \\cdots, B_K\\}\\) forman una partición. \n\n\n\n\n\n\n📌 El test infalible\nEn una población dada, una de cada mil personas tiene una enfermedad. Se toma una persona al azar de la población, se le aplica un test para detectar dicha enfermedad, y el resultado es positivo. El test se caracteriza por dar positivo el 99% de las veces que una persona tiene la enfermedad. Además, dicho test tiene una tasa de falsos positivos del 5%.\n\n¿Cuál es la probabilidad de que la persona tenga efectivamente la enfermedad?\nSi realizamos el mismo análisis una segunda vez sobre el mismo paciente y obtenemos nuevamente positivo,\n\n¿Cuál seria la probabilidad que el paciente esté enfermo?\n¿Y si diera negativo?\n¿Es el prior el mismo cuando se analiza el resultado del segundo análisis que cuando solo se analiza el primero? \n\n\n¿Es verdad que existen los vampiros? Versión Crepúsculo\nEdward quiere probarle a Bella que los vampiros existen. Según Bella, hay una probabilidad del 5% de que los vampiros existan. También cree que la probabilidad de que exista alguien con la piel brillante dado que los vampiros existen es del 70%, y que la probabilidad de que alguien tenga la piel brillante si los vampiros no existen es del 3%. Edward lleva a Bella al bosque y le muestra que de hecho su piel brilla como un 💎 ¿Cuál es la probabilidad que existan los vampiros? \n\n\n\n\n\nRobert Pattinson como Edward en Crepúsculo\n\n\n\n\n📌 Árboles enfermos\nUn vivero de la ciudad se destaca por vender una variedad de árboles nativos, incluyendo al jacarandá, ceibo, ombú, entre otros. Lamentablemente, el 18% de los árboles del vivero estan infectados con moho. Los árboles enfermos se componen en un 15% por jacarandás, 80% de ceibos, y 5% de otras especies. Los árboles sanos se componen por un 20% de jacarandás, 10% de ceibos, y 70% de otras especies. Con el objetivo de monitorear cuanto se propagó la enfermedad, una de las personas que trabaja en el vivero selecciona al azar uno de los árboles para testear.\n\n¿Cuál es la probabilidad a priori de que el árbol tenga moho?\nResulta que el árbol seleccionado es un ceibo. ¿Cuál es la probabilidad de haber seleccionado un ceibo?\n¿Cuál es la probabilidad a posteriori de que el ceibo seleccionado tenga moho?\nCompare las probabilidades a priori y a posteriori de que el árbol tenga moho. ¿Cómo afecta el análisis el saber que el árbol es un ceibo? \n\n\n\n\n\n\nFlor del Ceibo, la flor nacional\n\n\n\n\n📌 Transporte “El Impuntual”\nUna cierta empresa de transporte regional, que decidimos llamar “El Impuntual”, tiene servicios que van desde Rosario hasta Wheelwright varias veces al día, todos los días de la semana. Un 30% de los viajes salen a la mañana, otro 30% salen a la tarde, y el restante 40% salen a la noche. Los pasajeros suelen estar muy frustrados ya que un 25% de los viajes salen tarde. De estos viajes demorados, el 40% corresponden a la mañana, un 50% suceden a la tarde, y el 10% restante ocurre a la noche1.\nLucio y Franco son dos amigos del pueblo, y se volvieron a sus casas en colectivos diferentes.\n\nLucio se fue en uno de los colectivos de la mañana. ¿Cuál es la probabilidad que su viaje esté demorado?\nEl colectivo de Franco no está demorado. ¿Cuál es la probabilidad de que esté viajando en uno de los colectivos de la mañana? \n\n\n\n\n\n\nFoto de Markus Winkler en Unsplash\n\n\n\n\n📌 Bebé panda\nSupongamos que hay dos especies de osos panda. Ambas especies son igual de frecuentes y viven en la misma región. Es más, lucen de la misma forma y comen la misma comida. Aún no existe una prueba genética que pueda diferenciarlos. Lo único que los diferencia es la cantidad de crías que suelen tener. Las madres de la especie A dan luz a mellizos el 10% del tiempo. Y las madres de la especie B dan a luz mellizos el 20% del tiempo. En todos los otros casos, estas madres dan a luz un solo bebé panda.\nUsando un poco la imaginación, supongamos que somos la persona encargada de un programa de reproducción de pandas. Tenemos una panda femenina que acaba de dar a luz a un par de mellizos, pero no sabemos a que especie pertenece.\n\n¿Cuál es la probabilidad que la mamá panda sea de la especie A?\n¿Cuál es la probabilidad que vuelva a tener mellizos en la próxima parición?\nUn tiempo después sos encontramos con que en la segunda parición da a luz a un único bebé panda. ¿Cuál es la probabilidad de que este panda sea de la especie A? \n\n\n\n\n\n\nFoto de Stone Wang en Unsplash\n\n\n\n\n📌 Paraguas\nEstás a punto de subir a un avión rumbo a Mendoza. Querés saber si tenés que llevar un paraguas o no. Llamás a tres amigos que viven en Mendoza y les preguntás si está lloviendo. Cada uno de ellos tiene una probabilidad de \\(2/3\\) de decirte la verdad y \\(1/3\\) de mentirte para hacerte una broma. Los tres responden que sí está lloviendo. ¿Cuál es la probabilidad de que realmente esté lloviendo en las Mendoza? Se puede asumir que en Mendoza llueve en 1 de cada 10 días. \n\n\n\n\n\n\nAyuda\n\n\n\n\n\nSi \\(LLL\\) es “los tres amigos dijeron que llovía”, buscamos \\(P(\\text{lluvia} \\mid LLL) = P(LLL \\mid \\text{lluvia}) P(\\text{lluvia}) / P(LLL)\\).\n\\(P(LLL \\mid \\text{lluvia})\\) es la probabilidad de que ninguno de los tres mienta.\n\\(P(LLL) = P(LLL \\mid \\text{lluvia}) P(\\text{lluvia}) + P(LLL \\mid \\text{no lluvia}) P(\\text{no lluvia})\\).\n\n\n\n📌 Sherlock\nDos personas dejaron rastros de sangre en la escena del crimen. La sangre de Guido, un sospechoso, es analizada y resulta ser de tipo ‘0’. Los rastros de sangre de la escena son de tipo ‘0’ (un tipo común en la población, presente en el 60% de las personas) y de tipo ‘AB’ (un tipo raro, con una frecuencia del 1% en la población). ¿Estos datos representan evidencia de que Guido estaba presente en la escena del crimen? \n\n\n\n\n\nSherlock Holmes, el detective privado\n\n\n\n\n\n\n\n\n\n\nAyuda\n\n\n\n\n\nLlamemos \\(S\\) a “Guido y otra persona estuvieron en la escena del crimen” versus \\(S'\\), “dos personas desconocidas estuvieron en la escena del crimen”. Además, contamos con la evidencia \\(E\\), “observar sangre 0 y sangre AB”.\nQueremos saber qué es mayor, si \\(P(E \\mid S)\\) o \\(P(E \\mid S')\\) (si luego quisiéramos comparar \\(P(S \\mid E)\\) con \\(P(S' \\mid E)\\) necesitaríamos las probabilidades a priori de \\(P(S)\\) y \\(P(S')\\)). Si \\(P(E \\mid S) &lt; P(E \\mid S')\\) entonces los datos aportan evidencia en contra de que Guido estaba en la escena del crimen (\\(E\\) es más probable bajo \\(S'\\) que bajo \\(S\\)).\n\\(P(E \\mid S)\\) es la probabilidad de observar sangre 0 y sangre AB, si Guido estaba en la escena del crimen. Esto es básicamente la probabilidad de encontrar sangre AB (porque la cuota de 0 ya está cubierta por Guido). Eso equivale a tener en la escena del crimen a una de las personas que tienen sangre AB (1/100 de la población). Luego, \\(P(E \\mid S) = 0.01\\).\n\\(P(E \\mid S')\\) es la probabilidad de observar sangre 0 y sangre AB, si Guido no estaba en la escena del crimen. Esto es la probabilidad de que la primera persona tuviera sangre 0 y la segunda AB más la probabilidad de que la primera persona tuviera sangre AB y la segunda sangre 0: \\((60 / 100) \\cdot (1 / 100) + (1 / 100) \\cdot (60 / 100) = 0.012\\). Para las personas que buscan una explicación más formal, esto es un experimento multinomial con tres resultados posibles (\\(x_1\\): grupo 0, \\(x_2\\): grupo AB, \\(x_3\\): otro grupo), la probabilidad de observar \\(x_1 = 1\\), \\(x_2 = 1\\) y \\(x_3 = 0\\) en \\(N = 2\\) intentos (muestras) \\(\\frac{2!}{1!1!0!} p_1^{1} p_2^{1} p_3^{0}\\).\nPara pensarlo intuitivamente: supongamos que hay 200 personas en un pueblo. 120 personas (Guido y 119 más) tienen sangre 0, 2 personas tienen sangre AB y el resto tiene otro tipo. Con Guido en la escena del crimen hay \\(2/200\\) personas que podrían haber estado con él; sin Guido en la escena, pudieron haber estado cualquiera de las 119 personas de sangre 0 con cualquiera de las 2 personas de sangre AB.\n\n\n\nHijos de la probabilidad\nNos encontramos con alguien en la calle y nos dice que tiene dos hijos. Le preguntamos si alguno de ellos es mujer y nos responde que sí. ¿Cuál es la probabilidad de que ambos sean niñas? \nLos Reyes del Rock\nElvis Presley tenía un hermano varón que nació en el mismo parto pero que murió al poco tiempo. ¿Cuál es la probabilidad de que Elvis tuviera un gemelo? Alguna información adicional: en 1935, cuando Elvis nació, 1/3 de los hermanos del mismo parto eran gemelos y 2/3 mellizos; además, la probabilidad de que dos mellizos sean del mismo sexo biológico puede estimarse en 50%, mientras que dos gemelos son siempre del mismo sexo biológico. \n📌 ¿Alguien ordena las medias?\nDos cajones contienen medias. Uno de ellos tiene igual cantidad de medias blancas y negras. El otro contiene un número igual de medias rojas, verdes y azules. Se elige un cajón al azar, se sacan dos medias sin mirar y resultan ser las dos iguales. ¿Cuál es la probabilidad de que las medias sean blancas? Supóngase que sacar la primera media no altera las proporciones. \n\n\n\n\n\n\nAyuda\n\n\n\n\n\nSean los eventos\n\\(C\\): elegir el cajón BN (y no el RVA)\n\\(I\\): elegir un par de medias iguales\n\\(B\\): elegir un par de medias blancas\nMatemáticamente, buscamos \\(P(B \\mid I) = P(B, C \\mid I) + P(B, C' \\mid I)\\) (básicamente estamos marginalizando la variable “cajón”). El segundo término es 0 porque \\(B\\) y \\(C'\\) no pueden darse nunca (nunca voy a sacar un par blanco del cajón RVA). Entonces queda \\(P(B \\mid I) = P(B,C \\mid I) = P(B \\mid C, I) P(C \\mid I)\\), que son los dosvalores que encontramos arriba.\n¿De dónde sale que \\(P(B, C \\mid I) = P(B \\mid C, I) P(C \\mid I)\\)?\n\\(P(B, C \\mid I) = P(B,C,I)/P(I)\\) y el numerador, por regla de la cadena, es \\(P(A) P(C \\mid I) P(B \\mid C,I)\\).\n\n\n\n📌 La Falacia del Fiscal\nSally Clark era una abogada británica que fue erróneamente sentenciada a prisión perpetua en 1999 por la muerte de sus dos hijos bebés. Su hijo mayor, Christopher, murió con 11 semanas en diciembre de 1996 y su hijo más joven, Harry, con 8 semanas en enero de 1998. Durante el juicio, la defensa argumentó que las muertes se debieron al síndrome de muerte súbita del lactante (SIDS). Clark fue condenada a partir del testimonio del pediatra Sir Roy Meadow, quien argumentó en la corte lo siguiente:\n\nEn familias sanas, la chance de muerte por SIDS es de \\(\\frac{1}{8500}\\)\nLa probabilidad de dos muertes por SIDS en la misma familia es aproximadamente \\(\\frac{1}{8500^2} \\approx \\frac{1}{73000000}\\)\nEs, por ende, muy poco probable que Clark sea inocente\n\nLuego de pasar 3 años en prisión, Clark fue liberada en 2003 luego de que se determinara que el testimonio experto de Meadows era equivocado. Dos mujeres, a las cuales el testimonio de Meadows había enviado a prisión, también fueron liberadas.\n\nIdentifica una falla en la probabilidad de \\(\\frac{1}{73000000}\\) dada por Meadows.\nIncluso aceptando el número anterior como correcto, ¿cuál es el problema de interpretar esa probabilidad como la probabilidad de inocencia de Clark?\n\n\n\n\n\n\n\n\nAyuda\n\n\n\n\n\nAún asumiendo que \\(P(E \\mid I)\\) es lo que dice Meadows, esa no es la probabilidad de interés. La probabilidad que interesa es la de inocencia (o culpabilidad) dada la evidencia. Es decir, la probabilidad que nos interesa es \\(P(I \\mid E)\\) que la podemos escribir como \\(P(I \\mid E) = P(E \\mid I) P(I) / P(E)\\).\n\\(P(I \\mid E)\\) es solo similar a \\(P(E \\ mid I)\\) si \\(P(I)\\) es similar a \\(P(E)\\). Este no es el caso, ¿por qué?",
    "crumbs": [
      "Práctica",
      "Unidad 1"
    ]
  },
  {
    "objectID": "practica/practica_01.html#inferencia-bayesiana",
    "href": "practica/practica_01.html#inferencia-bayesiana",
    "title": "Práctica - Unidad 1",
    "section": "Inferencia Bayesiana",
    "text": "Inferencia Bayesiana\nEn esta parte de la práctica, se le otorga un significado a las cantidades que aparecen en la Regla de Bayes modificando conceptualmente el enfoque de las situaciones problemáticas. Ahora los problemas se tratan de realizar inferencias sobre posibles causas de ciertos datos observados. Se incrementa el rigor matemático, aparecen distribuciones de probabilidad y la necesidad de dejar ciertos cálculos en manos de la computadora.\n\n📌 El lenguaje de las probabilidades\nEscribir la expresión matemática para cada una de las siguientes descripciones verbales:\n\nProbabilidad de un parámetro dados los datos observados.\nLa distribución de probabilidad de los parámetros antes de ver los datos.\nLa verosimilitud de los datos para un valor dado de los parámetros.\nLa probabilidad de una observación nueva luego de observar los datos.\nLa probabilidad de una observación antes de ver los datos. \n\nQué datazo me tiraste, rey\nLos M&Ms azul fueron introducidos en el año 1995 (antes había dos tipos de marrón)\n\nAntes de 1995, la mezcla de colores en una bolsa de M&Ms era: 30% marron, 20% amarillo, 20% rojo, 10% verde, 10% naranja y 10% marrón bronceado.\nLuego de 1995, la mezcla pasó a ser: 24% azul, 20% verde, 16% naranja, 14% amarillo, 13% rojo y 13% marrón.\n\nUn amigo tiene dos bolsas de M&M y nos dice que una bolsa es de 1994 y la otra es de 1996, pero no nos dice cuál es cuál. Selecciona una primera bolsa y toma un M&M, luego toma la otra bolsa y saca un segundo M&M. El primero resulta amarillo y el otro, verde (ambos posiblemente estén vencidos). ¿Cuál es la probabilidad de que la primera bolsa sea la de 1994?\n💻 La Gran Estafa\nHay dos monedas en una caja. Una de ellas es una moneda común y la otra es una moneda que tiene dos caras.\n\nSe elige una moneda al azar, se arroja, y se obtiene cara. ¿Cuál es la probabilidad de que la moneda elegida sea la falsa?\nSe elige una moneda al azar y se arroja al aire tres veces, obteniéndose tres caras. ¿Cuál es la probabilidad de que la moneda elegida sea la falsa?\n\n📌 Vocabulario limitado\nSupongamos que existe un idioma con seis palabras:\n\\[\n\\text{\\{perro, parra, farra, carro, corro, tarro\\}}\n\\]\nUn análisis lingüístico exhaustivo de esta lengua ha descubierto que todas las palabras son igualmente probables, excepto por ‘perro’, que es \\(\\alpha\\) veces más probable que las otras.\nAdemás:\n\nCuando se tipean, un caracter se introduce erróneamente con probabilidad \\(\\theta\\).\nTodas las letras tienen la misma probabilidad de producir un error de tipeo.\nSi una letra se tipeó mal, la probabilidad de cometer un error en otro caracter no cambia.\nLos errores son independientes a lo largo de una palabras.\n\n\n¿Cuál es la probabilidad de escribir correctamente ‘tarro’?\n¿Cuál es la probabilidad de tipear ‘cerro’ o ‘curro’ al querer escribir ‘carro’?\nUtilizando la Regla de Bayes, desarrollar un corrector gramatical para esta lengua. Para las palabras tipeadas ‘farra’, ‘birra’ y ‘locos’, hallar la probabilidad de que cada palabra del diccionario sea la palabra que se había querido escribir. Utilizar las siguientes combinaciones de parámetros:\n\n\\(\\alpha = 2\\) y \\(\\theta = 0.1\\)\n\\(\\alpha = 50\\) y \\(\\theta = 0.1\\)\n\\(\\alpha = 2\\) y \\(\\theta = 0.9\\)\n\n\n📌 Que el árbol no tape el bosque\nSea \\(X_1 \\sim \\text{Bernoulli}(\\theta)\\) una variable que indica si una especie de árboles se halla en un determinado bosque y \\(\\theta \\in [0, 1]\\) representa la probabilidad a priori de que la especie se encuentre en el bosque. Una investigadora selecciona una muestra de \\(n\\) árboles del bosque y encuentra que \\(X_2\\) de ellas pertenecen a la especie de interés.\nEl modelo luego es\n\\[\n\\begin{array}{lc}\nX_2 \\mid X_1 = x_1 \\sim \\text{Binomial}(\\lambda x_1, n) & \\text{con} \\ \\lambda \\in [0, 1]\n\\end{array}\n\\]\n\\(\\lambda\\) representa la probabilidad de detectar la especie, dado que la especie se encuentra en el bosque.\nEncuntre expresiones matemáticas en término de \\(n\\), \\(\\theta\\) y \\(\\lambda\\) para las siguientes probabilidades:\n\n\\(P(X_1 = 0)\\).\n\\(P(X_1 = 0, X_2 = 0)\\).\n\\(P(X_2 = 0)\\).\n\\(P(X_1 = 0 \\mid X_2 = 0)\\).\n\\(P(X_2 = 0 \\mid X_1 = 0)\\).\n\\(P(X_1 = 0 \\mid X_2 = 1)\\).\n\\(P(X_2 = 0 \\mid X_1 = 1)\\).\nExplique de manera intuitiva cómo es que las probabilidades calculadas en (iv)-(vii) cambian según \\(n\\), \\(\\theta\\) y \\(\\lambda\\).\nAsuma \\(\\theta=0.5\\), \\(\\lambda=0.1\\) y \\(X_2 = 0\\) ¿Cuán grande debe ser \\(n\\) para que se puede concluir con 95% de confianza que la especie no se encuentra en el bosque? \n\n\n\n\n\n\nFoto de Sergei A en Unsplash\n\n\n\n\n💻 📌 ¡Ostras! ¡Estoy haciendo inferencia bayesiana!\nEn un estudio que utiliza métodos de la Estadística Bayesiana para predecir el número de especies que serán descubiertas en el futuro se reporta que la cantidad de especies marinas bivalvas2 descubiertas cada año entre 2010 y 2015 fue 64, 13, 33, 18, 30 y 20.\nSi se representa con \\(Y_t\\) a la cantidad de especies descubierta en el año \\(t\\), y asumiendo:\n\\[\n\\begin{aligned}\nY_t \\mid \\lambda &\\underset{iid}{\\sim} \\text{Poisson}(\\lambda) \\\\\n\\lambda       &\\sim \\text{Uniforme}(0, 100)\n\\end{aligned}\n\\]\nGraficar la distribución a posteriori de \\(\\lambda\\). \n💻 📌 Es negocio…\nSea \\(n\\) la cantidad desconocida de clientes que visitan una tienda en un dia cualquiera. El número de clientes que realizan una compra es \\(Y\\) y se cumple que\n\\[\nY \\mid \\theta, n \\sim \\text{Binomial}(\\theta, n)\n\\]\ndonde \\(\\theta\\) es la probabilidad de compra, dado que se produce la visita a la tienda. La distribución a priori de \\(n\\) es \\(n \\sim \\text{Poisson}(5)\\). Bajo el supuesto que \\(\\theta\\) es conocido y que \\(n\\) es desconocido, graficar la distribución a posteriori de \\(n\\) para todas las combinaciones de \\(Y \\in \\{0, 5, 10 \\}\\) y \\(\\theta \\in \\{0.2, 0.5\\}\\). Explique cual es del efecto de cambiar \\(Y\\) y \\(\\theta\\) sobre la distribución a posteriori.  \n📌 Con amigos así, quién necesita enemigos\nUn amigo arroja un dado y anota en secreto el número que sale (llamémoslo \\(T\\)). A continuación, nosotros, con los ojos vendados, arrojamos el dado varias veces. No podemos ver el número que sale pero nuestro amigo nos dice si el número que sacamos es mayor, menor o igual a \\(T\\).\nSupongamos que nos da la secuencia: \\(G,\\ G,\\ C,\\ I,\\ C,\\ C,\\ C, I,\\ G,\\ C\\) (siendo \\(G\\) más grande, \\(C\\) más chico e \\(I\\) igual). ¿Cuál es la distribución a posteriori de los valores de \\(T\\)? \n\n\n\n\n\n\nAyuda\n\n\n\n\n\nEstamos tratando de hacer inferencias sobre \\(T\\), es decir, la distribución a posteriori debe ser \\(P(T=1), P(T=2), \\dots P(T=6)\\).\nConviene analizar este problema en forma secuencial. Observamos \\(G\\), ¿qué significa eso? ¿cómo obtenemos la verosimilitud? \\(P(G \\mid T=1) = 5/6\\), \\(P(G \\mid T = 2) = 4 / 6\\), \\(P(G \\mid T=3) = 3/6\\)…\n\n\n\n💻 Orden en la sala\nEn las Jornadas Rosarinas de Ciencia de Datos, una expositora está dando una charla en un salón cuando el personal de seguridad la interrumpe porque cree que puede haber más de 1000 personas en la sala, superando el máximo permitido.\nLa expositora piensa que hay menos de 1000 personas y se ofrece a demostrarlo, aunque piensa que contarlas podría llevar mucho tiempo. Decide hacer un experimento:\n\nPregunta cuántas personas nacieron el 11 de mayo. Dos personas levantan la mano.\nPregunta cuántas personas nacieron el 23 de mayo. Una persona levanta la mano.\nPregunta cuántas personas nacieron el 1 de agosto. Nadie levanta la mano.\n\n¿Cuántas personas hay en la sala? O, mejor dicho, ¿cuál es la probabilidad de que haya más de 1000 personas en la sala? \n\n\n\n\n\n\nAyuda\n\n\n\n\n\nEstamos tratando de hacer inferencias sobre la cantidad de personas X (que puede ser un número entre, digamos, 1 y 3000).\nConviene analizar el problema en forma secuencial. Para la primera observación (dos personas cumpliendo años el mismo día), la verosimilitud es la probabilidad de que haya dos personas con el mismo cumpleaños si en la sala hay 1, 2, 3, … personas. Lo podemos pensar como una binomial\nSi \\(X=1\\), \\(P(Y=2 \\mid X = 1) = 0\\)\nSi \\(X=2\\), \\(P(Y=2 \\mid X=2) =\\) dbinom(2, size = 2, prob = 1/365)\nSi \\(X=100\\), \\(P(Y=2 \\mid X=100) =\\) dbinom(2, size = 100, prob = 1/365)\n\n\n\n💻 House of Cards\nHay 538 miembros en el Congreso de Estados Unidos. Supongamos que se auditan sus inversiones y se encuentra que 312 de ellos obtuvieron rendimientos por encima del mercado. Asumamos que un miembro honesto del Congreso tiene solo una probabilidad del 50% de tener rendimientos por encima del mercado, pero uno deshonesto que opera con información confidencial tiene una chance del 90% de hacerlo. ¿Cuántos miembros del Congreso son honestos? \n\n\n\n\n\nRobin Wright y Kevin Spacey, protagonistas de House of Cards\n\n\n\n\n💻 📌 Puede fallar…\nCansada de los experimentos de arrojar una moneda cientos de veces al aire, una estudiante diseña un sistema de reconocimiento de imágenes que determina si salió cara o ceca y registra el resultado.\nLógicamente, el sistema diseñado no es perfecto sino que presenta una tasa de error. En particular, la probabilidad de que clasificar mal es de 0.2 (20% de las veces que sale cara, el sistema dice ceca, y viceversa).\nSe arroja la moneda 250 veces y el sistema detecta 140 caras,\n\n¿Cuál es la distribución a posteriori de \\(\\theta\\), la probabilidad de obtener cara?\n¿Qué ocurre a medida que la probabilidad de clasificar mal varía? \n\n\n\n\n\n\n\nAyuda\n\n\n\n\n\nConsiderar:\n\n\\(\\theta\\) la probabilidad de obtener cara (es decir, que la moneda caiga en cara)\n\\(\\pi\\) es la probabilidad de observar cara\n\\(p\\) es la probabilidad de falla\n\n\\[\n\\begin{aligned}\n\\theta &\\sim \\text{Beta}(1, 1) \\\\\nY &=\n    \\begin{cases}\n    1 & \\text{Si la moneda cae en cara} \\\\\n    0 & \\text{Si la moneda cae en ceca}\n    \\end{cases} \\\\\nX &=\n    \\begin{cases}\n    1 & \\text{Si se clasifica bien} \\\\\n    0 & \\text{Si se clasifica mal}\n    \\end{cases} \\\\\nZ &=\n    \\begin{cases}\n    1 & \\text{Si se observa cara} \\\\\n    0 & \\text{Si se observa ceca}\n    \\end{cases}\n\\end{aligned}\n\\]\nObtener \\(\\pi\\) en base a la distribución conjunta de \\(X\\) e \\(Y\\) y luego realizar la inferencia con un modelo binomial para \\(T = \\sum_{i=1}^{250}{Z_i}\\) utilizando \\(\\pi\\) como probabilidad de éxito.\nEl parámetro \\(\\pi\\) quedará determinado en función de \\(\\theta\\), y luego la verosimilitud para \\(\\theta\\) se puede obtener a partir de la verosimilitud para el valor \\(\\pi = f(\\theta)\\) \\[\np_\\theta(T = t \\mid \\theta) = p_\\pi(T = t \\mid \\pi) = p_\\pi(T = t \\mid f(\\theta))\n\\]\ndonde \\(p_\\pi\\) es la función de masa de probabilidad de una binomial. Luego, \\[\n\\begin{aligned}\np_\\theta(\\theta \\mid T = t) &\\propto p_\\theta(T = t \\mid \\theta) p_\\theta(\\theta) \\\\\n&= p_\\theta(T = t \\mid \\theta)\n\\end{aligned}\n\\]\n\n\n\n¡Saludos a los cubos con puntos! (…) Serán dados\nDos dados de seis caras son arrojados. Se sabe que la suma de los dos puntajes obtenidos es 9. ¿Cuál es la distribución a posteriori de los puntajes de los dados? \n\n\n\n\n\n\nAyuda\n\n\n\n\n\nEstamos tratando de hacer inferencias sobre \\(D_1\\) y \\(D_2\\), los valores de cada uno de los datos. Definir la distribución a priori no es complicado (es una distribución sobre \\(D_1\\) y \\(D_2\\)).\nEl truco está en determinar la verosimilitud de cada par (\\(D_1, D_2\\)) luego de haber observado un 9.\n¿Cuál es la probabilidad de observar 9 en la suma de los dos dado que …?\n\\(D_1 = 1\\) y \\(D_2 = 1 -\\) la probabilidad de observar 9 es 0\n\\(D_1 = 1\\) y \\(D_2 = 2 -\\) la probabilidad de observar 9 es 0\n\\(D_1 = 4\\) y \\(D_2 = 5 -\\) la probabilidad de observar 9 es 1\n\\(D_1 = 5\\) y \\(D_2 = 4 -\\) la probabilidad de observar 9 es 1",
    "crumbs": [
      "Práctica",
      "Unidad 1"
    ]
  },
  {
    "objectID": "practica/practica_01.html#conceptuales",
    "href": "practica/practica_01.html#conceptuales",
    "title": "Práctica - Unidad 1",
    "section": "Conceptuales",
    "text": "Conceptuales\nEn esta sección, se nos invita a pensar sobre las características de la Estadística Bayesiana. En lugar de encontrar una respuesta única mediante cálculos matemáticos, se necesita comprender en profundidad tanto el enfoque frecuentista como el bayesiano para interpretar estas visiones en diferentes escenarios.\n\n📌 Voy a conseguir esa pasantía\nLa empresa de tecnología en la que todo el mundo quiere trabajar tiene varias vacantes para pasantes en ciencia de datos. Luego de leer la descripción de la búsqueda, te das cuenta que sos una persona calificada para el puesto: estos son tus datos. Tu objetivo es averiguar si te van a ofrecer el puesto: esta es tu hipótesis.\n\nDesde la perspectiva de una persona con un razonamiento frecuentista, ¿Qué es lo que se responde al evaluar la hipótesis de que te ofrecen el puesto?\nRepita el punto anterior considerando la perspectiva de una persona con un razonamiento Bayesiano.\n¿Qué pregunta tiene más sentido responder: la frecuentista o la Bayesiana? Justifica tu respuesta. \n\n📌 Beneficios de la Estadística Bayesiana\nUna amiga te cuenta que está interesada en aprender más sobre Estadística Bayesiana. Explícale lo siguiente:\n\n¿Por qué es útil el enfoque Bayesiano?\n¿Cuáles son las similitudes entre el enfoque frecuentista y el Bayesiano?",
    "crumbs": [
      "Práctica",
      "Unidad 1"
    ]
  },
  {
    "objectID": "practica/practica_01.html#footnotes",
    "href": "practica/practica_01.html#footnotes",
    "title": "Práctica - Unidad 1",
    "section": "Notas",
    "text": "Notas\n\n\nHay gente que dice que “cada dos por tres” te deja a pata. No nos vamos a pelear explicándoles que estan siendo demasiado exigentes, ya que un 25% también es un montón!↩︎\nUna clase de molusco. El mejillón, la ostra y la almeja son bivalvos↩︎",
    "crumbs": [
      "Práctica",
      "Unidad 1"
    ]
  },
  {
    "objectID": "practica/practica_05.html",
    "href": "practica/practica_05.html",
    "title": "Práctica - Unidad 5",
    "section": "",
    "text": "En esta última unidad de la práctica se presentan ejercicios que requieren el desarrollo de modelos de regresión con un nivdel de complejidad mayor. La distribución condicional de la respuesta ya no es necesariamente normal y la forma del predictor de la media incluye características que lo diferencian de un predictor lineal simple. Además, esta unidad presenta ejercicios con modelos jerárquicos.\n\n📌 Regresión Poisson\nConsidere el siguiente modelo para datos de conteo con un predictor \\(X\\) que toma valores entre -3 y 50:\n\\[\n\\begin{aligned}\nY_i   &\\sim \\mathrm{Poisson}(\\lambda_i) \\\\\n\\log(\\lambda_i) &= \\beta X \\\\\n\\end{aligned}\n\\]\n\nGenere 1000 valores de \\(X\\), asuma un valor conocido (y fijo) para \\(\\beta\\), simule los correspondientes valores de \\(\\lambda_i\\) y los de \\(Y_i\\). ¿Cómo es \\(\\lambda\\) en función de \\(X\\)? ¿Es lineal la relación entre \\(X\\) e \\(Y\\)? ¿Qué ocurre con la varianza de \\(Y\\) en función de \\(X\\)? ¿Cómo es la distribución marginal de \\(Y\\)?\nAhora añada incertidumbre al valor de \\(\\beta\\) (¿cómo se hace esto?) y simule nuevamente valores para \\(\\lambda_i\\) y \\(Y_i\\). Compare los resultados.\n\n📌 Regresión logística\nConsidere el siguiente modelo de clasificación con un predictor \\(X\\) que toma valores entre -30 y 10:\n\\[\n\\begin{aligned}\nY_i   &\\sim \\mathrm{Bernoulli}(\\theta_i) \\\\\n\\log\\left(\\frac{\\theta_i}{1 - \\theta_i}\\right) &= \\beta X \\\\\n\\end{aligned}\n\\]\n\nGenere 1000 valores de \\(X\\), asuma un valor conocido (y fijo) para \\(\\beta\\), simule los correspondientes valores de \\(\\theta_i\\) y los de \\(Y_i\\). ¿Cómo es \\(\\theta\\) en función de \\(X\\)? ¿Es lineal la relación entre \\(X\\) e \\(Y\\)?\nAhora añada incertidumbre al valor de \\(\\beta\\) (¿cómo se hace esto?) y simule nuevamente valores para \\(\\theta_i\\) y \\(Y_i\\). Compare los resultados.\n\n📌 Intención de voto\nEl conjunto de datos elecciones.csv contiene los resultados de un estudio piloto sobre intención de voto. Contiene las variables voto, edad y partido que indican respectivamente el candidato elegido, la edad y la afinidad partidaria del encuestado.\nUtilice un modelo de regresión logística para responder a las siguiente preguntas de investigación:\n\n¿Cómo se relaciona la edad de los encuestados con la intención de voto?\n¿Es esta relación diferente para las diferentes afinidades partidarias?\n\n📌 Días de ausencia\nUn organismo público de un Estado de los Estados Unidos está intereaso en estudiar el comportamiento de la asistencia de los estudiantes de secundaria. Para eso se cuenta con datos de 314 estudiantes tercer año en students.csv. Los predictores del número de días de ausencia incluyen el tipo de programa en el que está inscrito el estudiante y una prueba estandarizada de matemáticas.\nLas variables de interés en el conjunto de datos son:\n\ndaysabs: El número de días de ausencia. Es nuestra variable de respuesta.\nprogr: El tipo de programa. Puede ser uno de los siguientes: \"General\", \"Academic\" o \"Vocational\".\nmath: Puntuación en una prueba de matemáticas estandarizada.\n\nInteresa evaluar la asociación entre el tipo de programa y la puntuación en la prueba con los días de ausencia. También se desea ver ver si la asociación entre el puntaje en la prueba y los días de ausencia es diferente en cada tipo de programa.\nRealice un análisis exploratorio de los datos y elabore un modelo de regresión Poisson que permita explicar la asociación entre las variables predictoras y la cantidad de días que se ausentan los estudiantes.\n📌 Baseball\nEl béisbol es uno de los deportes donde se más intensivamente se utilizan herramientas estadísticas y analíticas. La cantidad de métricas que se calculan para los jugadores es muy elevada. Supongamos que estamos en un equipo de béisbol y nos gustaría cuantificar el rendimiento de los jugadores, siendo una de las métricas su promedio de bateo (definido por la cantidad de veces que un bateador golpea una pelota lanzada, dividido por el número de veces que se presenta al bate) ¿Cómo podríamos utilizar la estadística bayesiana para resolver este problema?\nLa tabla batting es una compilación de datos históricos de béisbol realizada por el Baseball Databank. Entre otras, contiene las siguientes columnas de interés:\n\nplayerID: Identificación del jugador\nAB: Cantidad de veces que el jugador se presenta al bate\nH: Cantidad de veces que el jugador golpea la pelota al batear\nbatting_avg: El cociente entre H y AB\n\nProponga un modelo de regresión logística para estimar la probabilidad de bateo para cada jugador. Incorpore la identificación del jugador en el modelo. Considere primero un modelo no jerárquico y luego un modelo jerárquico.\n📌 Privados del sueño\nEl conjunto de datos sleepstudy contiene el tiempo de reacción promedio en una serie de pruebas para un grupo de participantes en un estudio de privación del sueño. Los primeros dos días del estudio se consideran de adaptación y entrenamiento, el tercer día es una línea de base y la privación del sueño comienza después del día 3. Los sujetos de este grupo estaban restringidos a 3 horas de sueño por noche. El objetivo era analizar cómo la falta de sueño afectaba la capacidad de respuesta y la precisión en dicha tarea.\nLa variable de respuesta es \"Reaction\", que representa el promedio de las mediciones de tiempo de reacción de los participantes en un día determinado (en milisegundos). Las dos covariables son \"Days\", que indica el número de días de privación del sueño, y \"Subject\", que es el identificador del participante sobre el cual se realizó la medición.\nSe propone utilizar un modelo de regresión lineal de la forma:\n\\[\n\\begin{aligned}\n\\text{Reaction}_i & \\sim \\text{Normal}(\\mu_i, \\sigma) \\\\\n\\mu_i & = \\beta_0 + \\beta_1\\text{Days}_i \\\\\n\\end{aligned}\n\\]\nConsiderando a los sujetos como grupos, evalúe las siguientes alternativas para estimar el intercepto y la pendiente:\n\nComplete pooling\nNo pooling\nPartial pooling\n\n📌 Las 8 escuelas\nEl archivo escuelas.csv contiene la cantidad de horas semanales que dedican a estudiar los estudiantes de 8 escuelas. Obtener la distribución a posteriori para las medias poblacionales de los 8 colegios considerando el modelo normal jerárquico: \\[\n\\begin{aligned}\nY_{i} &\\sim \\text{Normal}(\\theta_{j[i]}, \\sigma^2) \\\\\n\\theta_j &\\sim \\text{Normal}(\\mu, \\tau^2) \\\\\n\\mu &\\sim \\text{Normal}(\\mu_0, \\gamma_0^2) \\\\\n1/\\tau^2   &\\sim \\text{Gamma}(\\eta_0 / 2, \\eta_0 \\tau_0^2 / 2) \\\\\n1/\\sigma^2 &\\sim \\text{Gamma}(\\nu_0 / 2, \\nu_0 \\sigma_0^2 / 2)\n\\end{aligned}\n\\]\ndonde \\(i \\in \\{1, 2, \\cdots, 180\\}\\) y \\(j \\in \\{1, 2, \\cdots, 8\\}\\) y los valores de los hiperparámetros fijos son:\n\\[\n\\mu_0 = 7,\\ \\gamma_0^2=5, \\  \\tau_0^2 = 10,\\ \\eta_0 = 2,\\ \\sigma_0^2 = 15,\\ \\nu_0 = 2.\n\\]\n\nUse {Stan} para obtener una muestra de la distribuición a posteriori. Para verificar la convergencia de las cadenas de Markov, considere la medida \\(R\\) de Rubin y un tamaño de muestra efectivo de al menos 1000.\nPresentar en una tabla la media, la mediana y el intervalo de credibilidad 95% para \\(\\sigma^2\\), \\(\\mu\\) y \\(\\tau^2\\).\nCompare las densidades a priori y a posteriori para cada uno de los parámetros. Interprete.\nDibuje las densidades a priori y a posteriori para \\(\\gamma=\\frac{\\tau^2}{\\sigma^2 + \\tau^2}\\) y compare.\nDetermine \\(P(\\theta_4 &lt; \\theta_3 \\mid \\boldsymbol{y}_1, \\cdots , \\boldsymbol{y}_8)\\)\nHacer un gráfico que relacione las medias muestrales \\(\\bar{y}_1, \\cdots, \\bar{y}_8\\) con las medias a posteriori de \\(\\theta_1, \\cdots,\\theta_8\\). Compare también la media a posteriori de \\(\\mu\\) con la media global de todas las observaciones.\n\n📌 Modelo lineal para elecciones en Estados Unidos\nUtilice el conjunto de datos de las elecciones presidenciales de Estados Unidos del año 2016 que se provee en Reich y Ghosh (2019) (rep_2012_2016). Elabore un modelo de regresión lineal bayesiano donde la variable respuesta es la diferencia porcentual entre el porcentaje de votos que obtuvo el candidato Republicano en el 2016 versus los que tuvo en el 2012 en cada condado y utilice todas las demás variables como predictoras.\n\nUtilice distribuciones a priori normales no informativas. Interprete las distribuciones a posteriori marginales de los coeficientes de regresión.\nCalcule los residuos \\(\\boldsymbol{r} = \\boldsymbol{y} - \\boldsymbol{X}\\hat{\\boldsymbol{\\beta}}\\) donde \\(\\hat{\\boldsymbol{\\beta}}\\) es la media a posteriori del vector de coeficientes de regresión ¿Puede concluir que los residuos siguen una distribución normal? ¿Qué condados presentan los residuos más grandes y más pequeños? ¿Qué puede indicar sobre estos condados? \n\nControl de armas\nUtilice el conjunto de datos sobre el control de armas en Estados Unidos. Estos datos provienen de un estudio transversal. Para el estado \\(i\\), sea \\(Y_i\\) el numero de homicidios y \\(N_i\\) el tamaño de la población.\n\nAjuste el modelo \\(Y_i \\mid \\boldsymbol{\\beta} \\sim \\text{Poisson}(N_i\\lambda_i)\\) donde \\(\\text{log}(\\lambda_i) = \\boldsymbol{X}_i\\boldsymbol{\\beta}\\). Use distribuciones a priori no informativas y \\(p = 7\\) de las covariables en \\(\\boldsymbol{X}_i\\): el intercepto, los cinco confounders \\(\\boldsymbol{Z}_i\\), y el número de leyes relacionadas a armas. Justifique que el sampler ha convergido y explorado suficientemente la distribución a posteriori y resuma la distribución a posteriori de \\(\\boldsymbol{\\beta}\\). \n\n🧩 ¿A cuántas Sofías conoces?\nDescargue el conjunto de datos babynames en R y calcule el log-odds de un bebé llamado “Sophia” en cada año luego de 1950.\n\nlibrary(babynames)\ndat &lt;- babynames\ndat &lt;- dat[dat$name == \"Sophia\" & dat$sex == \"F\" & dat$year &gt; 1950, ]\nyr &lt;- dat$year\np &lt;- dat$prop\nt &lt;- dat$year - 1950\nY &lt;- log(p / (1 - p))\n\nSea \\(Y_t\\) el log-odds muestral en el año \\(t + 1950\\). Ajuste el siguiente modelo auto-regresivo de orden 1:\n\\[\n\\begin{aligned}\nY_t   &= \\mu_t + \\rho(Y_{t - 1} + \\mu_{t - 1}) + \\varepsilon_t \\\\\n\\mu_t &= \\alpha + \\beta t \\\\\n\\varepsilon &\\underset{iid}{\\sim} \\text{Normal}(0, \\sigma^2) \\\\\n\\alpha, \\beta &\\sim \\text{Normal}(0, 100^2) \\\\\n\\rho &\\sim \\text{Uniforme}(-1, 1) \\\\\n\\sigma^2 &\\sim \\text{InvGamma}(0.1, 0.1)\n\\end{aligned}\n\\]\n\nInterprete los parámetros del modelo (\\(\\alpha\\), \\(\\beta\\), \\(\\rho\\) y \\(\\sigma^2\\))\nAjuste el modelo utilizando {RStan} para \\(t &gt; 1\\). Verifique la convergencia y reporte la media a posteriori e intervalos del 95% para los parámetros.\nGrafique la distribución predictiva a posteriori para \\(Y_t\\) en el año 2020. \n\n🧩 Meta-análisis\nEn este ejercicio se llevará a cabo un meta-análisis, es decir, un análisis que combina el resultado de varios estudios. Los datos provienen del paquete {rmeta} en R.\n\nlibrary(rmeta)\ndata(cochrane)\ncochrane\n\n          name ev.trt n.trt ev.ctrl n.ctrl\n1     Auckland     36   532      60    538\n2        Block      1    69       5     61\n3        Doran      4    81      11     63\n4        Gamsu     14   131      20    137\n5     Morrison      3    67       7     59\n6 Papageorgiou      1    71       7     75\n7      Tauesch      8    56      10     71\nLos datos provienen de siete ensayos aleatorizados que evalúan el efecto de la terapia con corticosteroides en la muerte neonatal. Para el ensayo \\(i \\in \\{1, \\dots, 7 \\}\\) \\(Y_{i0}\\) representa el número de eventos que ocurren en el grupo de control de tamaño \\(N_{i0}\\) y \\(Y_{i1}\\) representa el número de eventos que ocurren en el grupo tratado de tamaño \\(N_{i1}\\).\n\nAjuste el modelo \\(Y_{ij} \\mid \\theta_j \\underset{indep}{\\sim} \\text{Binomial}(N_{ij}, \\theta_j)\\) con \\(\\theta_0, \\theta_1 \\sim \\text{Uniforme}(0, 1)\\). ¿Se puede concluir que el tratamiento está asociado a una reducción de la tasa de muerte?\nAjuste el modelo \\(Y_{ij} \\mid \\theta_j \\underset{indep}{\\sim} \\text{Binomial}(N_{ij}, \\theta_j)\\) con\n\n\\(\\text{logit}(\\theta_{ij}) = \\alpha_{ij}\\)\n\\(\\boldsymbol{\\alpha}_i = (\\alpha_{i0}, \\alpha_{i1})^T \\underset{iid}{\\sim} \\text{Normal}(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma})\\)\n\\(\\boldsymbol{\\mu} \\sim \\text{Normal}(0, 10^2I_2)\\)\n\\(\\boldsymbol{\\Sigma} \\sim \\text{InvWishart}(3, I_2)\\)\n\nInterprete los resultados indicando si estos sugieren que el tratamiento está asociado a una reducción en la tasa de muerte.\nDibuje un DAG para ambos modelos.\nDiscuta las ventajas y desventajas de ambos modelos.\n¿Cuál modelo es el preferido para estos datos? \n\nComparando modelos normales\nUtilice el conjuto de datos airquality que viene con el paquete {datasets} que se carga automáticamente al crear una sesión de R.\n\nhead(airquality)\n\n  Ozone Solar.R Wind Temp Month Day\n1    41     190  7.4   67     5   1\n2    36     118  8.0   72     5   2\n3    12     149 12.6   74     5   3\n4    18     313 11.5   62     5   4\n5    NA      NA 14.3   56     5   5\n6    28      NA 14.9   66     5   6\nCompare los siguientes modelos utilizando 5-fold cross-validation:\n\\[\n\\begin{array}{l}\n\\mathcal{M}_1: \\text{Ozone}_i \\sim \\text{Normal}(\\beta_1 + \\beta_2 \\text{Solar.R}_i, \\sigma^2) \\\\\n\\mathcal{M}_2: \\text{Ozone}_i \\sim \\text{Normal}(\\beta_1 + \\beta_2 \\text{Solar.R}_i + \\beta_3 \\text{Temp}_i + \\beta_4 \\text{Wind}_i, \\sigma^2)\n\\end{array}\n\\]\nElija priors para los parámetros de ambos modelos explicando su elección. \n\n\n\n🧩 Curvas de crecimiento de tiranosáuridos\nSe analizan datos de 20 fósiles de tiranosáuridos para estimar las curvas de crecimiento de cuatro especies: Albertosaurio, Daspletosaurio, Gorgosaurio y Tiranosaurio. Los datos se toman de la Tabla 1 de Erickson et al. (2004) y se muestran en la Figura 1. El objetivo es determinar la curva de crecimiento, esto es, determinar el peso esperado por edad para todas las especies.\nEn el panel izquierdo de la Figura 1 se puede observar que hay una relación no lineal entre la edad y el peso. También se observan ciertos patrones comunes a las especies. Por ejemplo, la relación positiva entre las variables o el decrecimiento en la tasa de cambio conforme la edad es mayor.\n\n\n\n\n\n\n\n\nFigura 1: (Izquierda) Edad (años) vs Peso (kilogramos). (Derecha) Los mismos datos luego de aplicar la transformación logarítmica a ambas variables.\n\n\n\n\n\nSea \\(Y_{ij}\\) el peso y \\(X_{ij}\\) y la edad de la muestra \\(i\\) de la especie \\(j\\), con \\(j = 1, 2, 3, 4\\). Se propone el siguiente modelo:\n\\[\nY_{ij} = f_j(X_{ij}) \\epsilon_{ij}\n\\]\ndonde \\(f_j\\) es la verdadera curva de crecimiento para la especie \\(j\\) y \\(\\epsilon_{ij} &gt; 0\\) es un error multiplicativo.\n\n¿Por qué tiene sentido proponer un error multiplicativo?\n¿Cuál es un valor sensato para la media de la distribución del error?\nUtilice una distribución log-normal para el error, \\(\\log (\\epsilon_{ij}) \\sim \\text{Normal}\\). Proponga valores para la media y la varianza de forma tal que satisfagan la condición del punto anterior.\n\nEsto da lugar un al siguiente modelo log-normal para \\(Y_{ij}\\):\n\\[\n\\log (Y_{ij}) \\sim \\text{Normal}\n(\\log [f_j(X_{ij})] + \\mu_{\\log \\epsilon}, \\sigma^2_{\\log \\epsilon})\n\\]\ncon \\(\\mathbb{E}(Y_{ij}) = f_j(X_{ij})\\).\nA continuación se proponen cuatro modelos que varían según la relación funcional que se propone para \\(f_j\\) y la naturaleza de las distribuciones a priori que se utilizan.\nModelo 1\nObservando el panel derecho de la Figura 1 se puede concluir que luego de transformar ambas variables con la función logaritmo la relación se ve aproximadamente lineal. Por lo tanto, se propone el siguiente modelo log-lineal:\n\\[\n\\log [f_j(X)] = a_j + b_j \\log(X)\n\\]\ndonde \\(a_j\\) y \\(b_j\\) representan al intercepto y pendiente de la especie \\(j\\). La curva de crecimiento en la escala original resulta \\(f_j(X) = \\exp (a_j)X^{b_j}\\). Considere los siguientes priors:\n\\[\n\\begin{aligned}\na_j &\\sim \\text{Normal}(0, 10) \\\\\nb_j &\\sim \\text{Normal}(0, 10) \\\\\n\\sigma^2_j &\\sim \\text{InvGamma}(0.1, 0.1)\n\\end{aligned}\n\\]\n\nEscriba un programa en Stan que implemente el modelo y obtenga el posterior con {RStan}.\nAnalice los coeficientes del modelo y las curvas de crecimiento. Realice gráficos que permitan observar la curva ajustada y su incertidumbre para cada especie.\n\nModelo 2\nEste modelo es el mismo que el Modelo 1, excepto que las especies tienen la misma varianza, \\(\\sigma^2_j = \\sigma^2\\) y los coeficientes de regresión son modelados de manera jerárquica. Utilice los siguientes priors:\n\\[\n\\begin{aligned}\n\\mu_a    &\\sim \\text{Normal}(0, 10) \\\\\n\\sigma_a &\\sim \\text{InvGamma}(0.1, 0.1) \\\\\n\\mu_b    &\\sim \\text{Normal}(0, 10) \\\\\n\\sigma_b &\\sim \\text{InvGamma}(0.1, 0.1) \\\\\na_j      &\\sim \\text{Normal}(\\mu_a, \\sigma^2_a) \\\\\nb_j      &\\sim \\text{Normal}(\\mu_b, \\sigma^2_b) \\\\\n\\sigma^2 &\\sim \\text{InvGamma}(0.1, 0.1)\n\\end{aligned}\n\\]\n\nEscriba un programa en Stan que implemente el modelo y obtenga el posterior con {RStan}.\nAnalice los coeficientes del modelo y las curvas de crecimiento. Genere gráficos similares a los producidos en el punto anterior. Describa similitudes y diferencias respecto del modelo 1. Justifique su respuesta.\n¿Qué problemas detecta los modelos 1 y 2? Considere como evoluciona el peso conforme la edad según el modelo.\n\nModelo 3\nComo alternativa al componente log-lineal anterior, se propone la siguiente curva de crecimiento logístico:\n\\[\nf_j(X) = a_j + b_j \\frac{\\exp [d_j (\\log(X) - c_j)]}{1 + \\exp [d_j(\\log(X) - c_j)]}\n\\]\nEste modelo tiene cuatro parámetros:\n\n\\(a_j\\) es el peso esperado cuando la edad es 0;\n\\(b_j\\) es el peso máximo esperado (o la cota superior del peso);\n\\(\\log (c_j)\\) es la edad a la que la especie \\(j\\) alcanza la mitad del peso máximo;\n\\(d_j &gt; 0\\) determina la tasa de crecimiento del peso conforme aumenta la edad.\n\nPara que la curva sea positiva y creciente para todas las edades, se debe cumplir que \\(a_j &gt; 0\\), \\(b_j &gt; a_j\\) y \\(d_j &gt; 0\\). Se pueden satisfacer estas restricciones expresando los parámetros en función de parámetros cuyo dominio es \\(\\mathbb{R}\\):\n\n\\(a_j = \\exp (\\alpha_{j1})\\);\n\\(b_j = \\exp (\\alpha_{j2})\\);\n\\(c_j = \\alpha_{j3}\\);\n\\(d_j = \\exp (\\alpha_{j4})\\).\n\nConsidere las siguientes distribuciones a priori para los parámetros del modelo:\n\\[\n\\begin{aligned}\n\\alpha_{jk} &\\sim \\text{Normal}(0, 10) \\\\\n\\sigma^2_j  &\\sim \\text{InvGamma}(0.1, 0.1)\n\\end{aligned}\n\\]\n\nEscriba un programa en Stan que implemente el modelo y obtenga el posterior con {RStan}.\nAnalice los diagnósticos de la inferencia realizada.\nGrafique las curvas estimadas para cada especie junto a sus intervalos de credibilidad e interprete los resultados.\n\nModelo 4\nEste modelo es el mismo que el Modelo 3, excepto que las especies tienen la misma varianza, \\(\\sigma^2_j = \\sigma^2\\) y los coeficientes de regresión son modelados de manera jerárquica. Utilice los siguientes priors:\n\\[\n\\begin{aligned}\n\\mu_k             &\\sim \\text{Normal}(0, 10) \\\\\n\\sigma^2_k        &\\sim \\text{InvGamma}(0.1, 0.1) \\\\\n\\log(\\alpha_{jk}) &\\sim \\text{Normal}(\\mu_k, \\sigma^2_k) \\\\\n\\sigma^2          &\\sim \\text{InvGamma}(0.1, 0.1)\n\\end{aligned}\n\\]\n\nEscriba un programa en Stan que implemente el modelo y obtenga el posterior con {RStan}.\nAnalice los diagnósticos de la inferencia y compare con los resultados del modelo 3.\nGrafique las curvas estimadas para cada especie junto a sus intervalos de credibilidad e interprete los resultados. Compare con los resultados del modelo 3. ¿Qué diferencias observa? ¿Por qué se dan?\nEscriba una síntesis comparando todos los modelos desarrollados. Comente ventajas y desventajas de cada uno de ellos, explicando a que se deben en cada caso ¿Qué modelo resulta más conveniente para estimar la curva de crecimiento de los tiranosáuridos? Justifique su respuesta.",
    "crumbs": [
      "Práctica",
      "Unidad 5"
    ]
  },
  {
    "objectID": "practica/practica_05.html#modelos-de-regresión-avanzados",
    "href": "practica/practica_05.html#modelos-de-regresión-avanzados",
    "title": "Práctica - Unidad 5",
    "section": "",
    "text": "En esta última unidad de la práctica se presentan ejercicios que requieren el desarrollo de modelos de regresión con un nivdel de complejidad mayor. La distribución condicional de la respuesta ya no es necesariamente normal y la forma del predictor de la media incluye características que lo diferencian de un predictor lineal simple. Además, esta unidad presenta ejercicios con modelos jerárquicos.\n\n📌 Regresión Poisson\nConsidere el siguiente modelo para datos de conteo con un predictor \\(X\\) que toma valores entre -3 y 50:\n\\[\n\\begin{aligned}\nY_i   &\\sim \\mathrm{Poisson}(\\lambda_i) \\\\\n\\log(\\lambda_i) &= \\beta X \\\\\n\\end{aligned}\n\\]\n\nGenere 1000 valores de \\(X\\), asuma un valor conocido (y fijo) para \\(\\beta\\), simule los correspondientes valores de \\(\\lambda_i\\) y los de \\(Y_i\\). ¿Cómo es \\(\\lambda\\) en función de \\(X\\)? ¿Es lineal la relación entre \\(X\\) e \\(Y\\)? ¿Qué ocurre con la varianza de \\(Y\\) en función de \\(X\\)? ¿Cómo es la distribución marginal de \\(Y\\)?\nAhora añada incertidumbre al valor de \\(\\beta\\) (¿cómo se hace esto?) y simule nuevamente valores para \\(\\lambda_i\\) y \\(Y_i\\). Compare los resultados.\n\n📌 Regresión logística\nConsidere el siguiente modelo de clasificación con un predictor \\(X\\) que toma valores entre -30 y 10:\n\\[\n\\begin{aligned}\nY_i   &\\sim \\mathrm{Bernoulli}(\\theta_i) \\\\\n\\log\\left(\\frac{\\theta_i}{1 - \\theta_i}\\right) &= \\beta X \\\\\n\\end{aligned}\n\\]\n\nGenere 1000 valores de \\(X\\), asuma un valor conocido (y fijo) para \\(\\beta\\), simule los correspondientes valores de \\(\\theta_i\\) y los de \\(Y_i\\). ¿Cómo es \\(\\theta\\) en función de \\(X\\)? ¿Es lineal la relación entre \\(X\\) e \\(Y\\)?\nAhora añada incertidumbre al valor de \\(\\beta\\) (¿cómo se hace esto?) y simule nuevamente valores para \\(\\theta_i\\) y \\(Y_i\\). Compare los resultados.\n\n📌 Intención de voto\nEl conjunto de datos elecciones.csv contiene los resultados de un estudio piloto sobre intención de voto. Contiene las variables voto, edad y partido que indican respectivamente el candidato elegido, la edad y la afinidad partidaria del encuestado.\nUtilice un modelo de regresión logística para responder a las siguiente preguntas de investigación:\n\n¿Cómo se relaciona la edad de los encuestados con la intención de voto?\n¿Es esta relación diferente para las diferentes afinidades partidarias?\n\n📌 Días de ausencia\nUn organismo público de un Estado de los Estados Unidos está intereaso en estudiar el comportamiento de la asistencia de los estudiantes de secundaria. Para eso se cuenta con datos de 314 estudiantes tercer año en students.csv. Los predictores del número de días de ausencia incluyen el tipo de programa en el que está inscrito el estudiante y una prueba estandarizada de matemáticas.\nLas variables de interés en el conjunto de datos son:\n\ndaysabs: El número de días de ausencia. Es nuestra variable de respuesta.\nprogr: El tipo de programa. Puede ser uno de los siguientes: \"General\", \"Academic\" o \"Vocational\".\nmath: Puntuación en una prueba de matemáticas estandarizada.\n\nInteresa evaluar la asociación entre el tipo de programa y la puntuación en la prueba con los días de ausencia. También se desea ver ver si la asociación entre el puntaje en la prueba y los días de ausencia es diferente en cada tipo de programa.\nRealice un análisis exploratorio de los datos y elabore un modelo de regresión Poisson que permita explicar la asociación entre las variables predictoras y la cantidad de días que se ausentan los estudiantes.\n📌 Baseball\nEl béisbol es uno de los deportes donde se más intensivamente se utilizan herramientas estadísticas y analíticas. La cantidad de métricas que se calculan para los jugadores es muy elevada. Supongamos que estamos en un equipo de béisbol y nos gustaría cuantificar el rendimiento de los jugadores, siendo una de las métricas su promedio de bateo (definido por la cantidad de veces que un bateador golpea una pelota lanzada, dividido por el número de veces que se presenta al bate) ¿Cómo podríamos utilizar la estadística bayesiana para resolver este problema?\nLa tabla batting es una compilación de datos históricos de béisbol realizada por el Baseball Databank. Entre otras, contiene las siguientes columnas de interés:\n\nplayerID: Identificación del jugador\nAB: Cantidad de veces que el jugador se presenta al bate\nH: Cantidad de veces que el jugador golpea la pelota al batear\nbatting_avg: El cociente entre H y AB\n\nProponga un modelo de regresión logística para estimar la probabilidad de bateo para cada jugador. Incorpore la identificación del jugador en el modelo. Considere primero un modelo no jerárquico y luego un modelo jerárquico.\n📌 Privados del sueño\nEl conjunto de datos sleepstudy contiene el tiempo de reacción promedio en una serie de pruebas para un grupo de participantes en un estudio de privación del sueño. Los primeros dos días del estudio se consideran de adaptación y entrenamiento, el tercer día es una línea de base y la privación del sueño comienza después del día 3. Los sujetos de este grupo estaban restringidos a 3 horas de sueño por noche. El objetivo era analizar cómo la falta de sueño afectaba la capacidad de respuesta y la precisión en dicha tarea.\nLa variable de respuesta es \"Reaction\", que representa el promedio de las mediciones de tiempo de reacción de los participantes en un día determinado (en milisegundos). Las dos covariables son \"Days\", que indica el número de días de privación del sueño, y \"Subject\", que es el identificador del participante sobre el cual se realizó la medición.\nSe propone utilizar un modelo de regresión lineal de la forma:\n\\[\n\\begin{aligned}\n\\text{Reaction}_i & \\sim \\text{Normal}(\\mu_i, \\sigma) \\\\\n\\mu_i & = \\beta_0 + \\beta_1\\text{Days}_i \\\\\n\\end{aligned}\n\\]\nConsiderando a los sujetos como grupos, evalúe las siguientes alternativas para estimar el intercepto y la pendiente:\n\nComplete pooling\nNo pooling\nPartial pooling\n\n📌 Las 8 escuelas\nEl archivo escuelas.csv contiene la cantidad de horas semanales que dedican a estudiar los estudiantes de 8 escuelas. Obtener la distribución a posteriori para las medias poblacionales de los 8 colegios considerando el modelo normal jerárquico: \\[\n\\begin{aligned}\nY_{i} &\\sim \\text{Normal}(\\theta_{j[i]}, \\sigma^2) \\\\\n\\theta_j &\\sim \\text{Normal}(\\mu, \\tau^2) \\\\\n\\mu &\\sim \\text{Normal}(\\mu_0, \\gamma_0^2) \\\\\n1/\\tau^2   &\\sim \\text{Gamma}(\\eta_0 / 2, \\eta_0 \\tau_0^2 / 2) \\\\\n1/\\sigma^2 &\\sim \\text{Gamma}(\\nu_0 / 2, \\nu_0 \\sigma_0^2 / 2)\n\\end{aligned}\n\\]\ndonde \\(i \\in \\{1, 2, \\cdots, 180\\}\\) y \\(j \\in \\{1, 2, \\cdots, 8\\}\\) y los valores de los hiperparámetros fijos son:\n\\[\n\\mu_0 = 7,\\ \\gamma_0^2=5, \\  \\tau_0^2 = 10,\\ \\eta_0 = 2,\\ \\sigma_0^2 = 15,\\ \\nu_0 = 2.\n\\]\n\nUse {Stan} para obtener una muestra de la distribuición a posteriori. Para verificar la convergencia de las cadenas de Markov, considere la medida \\(R\\) de Rubin y un tamaño de muestra efectivo de al menos 1000.\nPresentar en una tabla la media, la mediana y el intervalo de credibilidad 95% para \\(\\sigma^2\\), \\(\\mu\\) y \\(\\tau^2\\).\nCompare las densidades a priori y a posteriori para cada uno de los parámetros. Interprete.\nDibuje las densidades a priori y a posteriori para \\(\\gamma=\\frac{\\tau^2}{\\sigma^2 + \\tau^2}\\) y compare.\nDetermine \\(P(\\theta_4 &lt; \\theta_3 \\mid \\boldsymbol{y}_1, \\cdots , \\boldsymbol{y}_8)\\)\nHacer un gráfico que relacione las medias muestrales \\(\\bar{y}_1, \\cdots, \\bar{y}_8\\) con las medias a posteriori de \\(\\theta_1, \\cdots,\\theta_8\\). Compare también la media a posteriori de \\(\\mu\\) con la media global de todas las observaciones.\n\n📌 Modelo lineal para elecciones en Estados Unidos\nUtilice el conjunto de datos de las elecciones presidenciales de Estados Unidos del año 2016 que se provee en Reich y Ghosh (2019) (rep_2012_2016). Elabore un modelo de regresión lineal bayesiano donde la variable respuesta es la diferencia porcentual entre el porcentaje de votos que obtuvo el candidato Republicano en el 2016 versus los que tuvo en el 2012 en cada condado y utilice todas las demás variables como predictoras.\n\nUtilice distribuciones a priori normales no informativas. Interprete las distribuciones a posteriori marginales de los coeficientes de regresión.\nCalcule los residuos \\(\\boldsymbol{r} = \\boldsymbol{y} - \\boldsymbol{X}\\hat{\\boldsymbol{\\beta}}\\) donde \\(\\hat{\\boldsymbol{\\beta}}\\) es la media a posteriori del vector de coeficientes de regresión ¿Puede concluir que los residuos siguen una distribución normal? ¿Qué condados presentan los residuos más grandes y más pequeños? ¿Qué puede indicar sobre estos condados? \n\nControl de armas\nUtilice el conjunto de datos sobre el control de armas en Estados Unidos. Estos datos provienen de un estudio transversal. Para el estado \\(i\\), sea \\(Y_i\\) el numero de homicidios y \\(N_i\\) el tamaño de la población.\n\nAjuste el modelo \\(Y_i \\mid \\boldsymbol{\\beta} \\sim \\text{Poisson}(N_i\\lambda_i)\\) donde \\(\\text{log}(\\lambda_i) = \\boldsymbol{X}_i\\boldsymbol{\\beta}\\). Use distribuciones a priori no informativas y \\(p = 7\\) de las covariables en \\(\\boldsymbol{X}_i\\): el intercepto, los cinco confounders \\(\\boldsymbol{Z}_i\\), y el número de leyes relacionadas a armas. Justifique que el sampler ha convergido y explorado suficientemente la distribución a posteriori y resuma la distribución a posteriori de \\(\\boldsymbol{\\beta}\\). \n\n🧩 ¿A cuántas Sofías conoces?\nDescargue el conjunto de datos babynames en R y calcule el log-odds de un bebé llamado “Sophia” en cada año luego de 1950.\n\nlibrary(babynames)\ndat &lt;- babynames\ndat &lt;- dat[dat$name == \"Sophia\" & dat$sex == \"F\" & dat$year &gt; 1950, ]\nyr &lt;- dat$year\np &lt;- dat$prop\nt &lt;- dat$year - 1950\nY &lt;- log(p / (1 - p))\n\nSea \\(Y_t\\) el log-odds muestral en el año \\(t + 1950\\). Ajuste el siguiente modelo auto-regresivo de orden 1:\n\\[\n\\begin{aligned}\nY_t   &= \\mu_t + \\rho(Y_{t - 1} + \\mu_{t - 1}) + \\varepsilon_t \\\\\n\\mu_t &= \\alpha + \\beta t \\\\\n\\varepsilon &\\underset{iid}{\\sim} \\text{Normal}(0, \\sigma^2) \\\\\n\\alpha, \\beta &\\sim \\text{Normal}(0, 100^2) \\\\\n\\rho &\\sim \\text{Uniforme}(-1, 1) \\\\\n\\sigma^2 &\\sim \\text{InvGamma}(0.1, 0.1)\n\\end{aligned}\n\\]\n\nInterprete los parámetros del modelo (\\(\\alpha\\), \\(\\beta\\), \\(\\rho\\) y \\(\\sigma^2\\))\nAjuste el modelo utilizando {RStan} para \\(t &gt; 1\\). Verifique la convergencia y reporte la media a posteriori e intervalos del 95% para los parámetros.\nGrafique la distribución predictiva a posteriori para \\(Y_t\\) en el año 2020. \n\n🧩 Meta-análisis\nEn este ejercicio se llevará a cabo un meta-análisis, es decir, un análisis que combina el resultado de varios estudios. Los datos provienen del paquete {rmeta} en R.\n\nlibrary(rmeta)\ndata(cochrane)\ncochrane\n\n          name ev.trt n.trt ev.ctrl n.ctrl\n1     Auckland     36   532      60    538\n2        Block      1    69       5     61\n3        Doran      4    81      11     63\n4        Gamsu     14   131      20    137\n5     Morrison      3    67       7     59\n6 Papageorgiou      1    71       7     75\n7      Tauesch      8    56      10     71\nLos datos provienen de siete ensayos aleatorizados que evalúan el efecto de la terapia con corticosteroides en la muerte neonatal. Para el ensayo \\(i \\in \\{1, \\dots, 7 \\}\\) \\(Y_{i0}\\) representa el número de eventos que ocurren en el grupo de control de tamaño \\(N_{i0}\\) y \\(Y_{i1}\\) representa el número de eventos que ocurren en el grupo tratado de tamaño \\(N_{i1}\\).\n\nAjuste el modelo \\(Y_{ij} \\mid \\theta_j \\underset{indep}{\\sim} \\text{Binomial}(N_{ij}, \\theta_j)\\) con \\(\\theta_0, \\theta_1 \\sim \\text{Uniforme}(0, 1)\\). ¿Se puede concluir que el tratamiento está asociado a una reducción de la tasa de muerte?\nAjuste el modelo \\(Y_{ij} \\mid \\theta_j \\underset{indep}{\\sim} \\text{Binomial}(N_{ij}, \\theta_j)\\) con\n\n\\(\\text{logit}(\\theta_{ij}) = \\alpha_{ij}\\)\n\\(\\boldsymbol{\\alpha}_i = (\\alpha_{i0}, \\alpha_{i1})^T \\underset{iid}{\\sim} \\text{Normal}(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma})\\)\n\\(\\boldsymbol{\\mu} \\sim \\text{Normal}(0, 10^2I_2)\\)\n\\(\\boldsymbol{\\Sigma} \\sim \\text{InvWishart}(3, I_2)\\)\n\nInterprete los resultados indicando si estos sugieren que el tratamiento está asociado a una reducción en la tasa de muerte.\nDibuje un DAG para ambos modelos.\nDiscuta las ventajas y desventajas de ambos modelos.\n¿Cuál modelo es el preferido para estos datos? \n\nComparando modelos normales\nUtilice el conjuto de datos airquality que viene con el paquete {datasets} que se carga automáticamente al crear una sesión de R.\n\nhead(airquality)\n\n  Ozone Solar.R Wind Temp Month Day\n1    41     190  7.4   67     5   1\n2    36     118  8.0   72     5   2\n3    12     149 12.6   74     5   3\n4    18     313 11.5   62     5   4\n5    NA      NA 14.3   56     5   5\n6    28      NA 14.9   66     5   6\nCompare los siguientes modelos utilizando 5-fold cross-validation:\n\\[\n\\begin{array}{l}\n\\mathcal{M}_1: \\text{Ozone}_i \\sim \\text{Normal}(\\beta_1 + \\beta_2 \\text{Solar.R}_i, \\sigma^2) \\\\\n\\mathcal{M}_2: \\text{Ozone}_i \\sim \\text{Normal}(\\beta_1 + \\beta_2 \\text{Solar.R}_i + \\beta_3 \\text{Temp}_i + \\beta_4 \\text{Wind}_i, \\sigma^2)\n\\end{array}\n\\]\nElija priors para los parámetros de ambos modelos explicando su elección. \n\n\n\n🧩 Curvas de crecimiento de tiranosáuridos\nSe analizan datos de 20 fósiles de tiranosáuridos para estimar las curvas de crecimiento de cuatro especies: Albertosaurio, Daspletosaurio, Gorgosaurio y Tiranosaurio. Los datos se toman de la Tabla 1 de Erickson et al. (2004) y se muestran en la Figura 1. El objetivo es determinar la curva de crecimiento, esto es, determinar el peso esperado por edad para todas las especies.\nEn el panel izquierdo de la Figura 1 se puede observar que hay una relación no lineal entre la edad y el peso. También se observan ciertos patrones comunes a las especies. Por ejemplo, la relación positiva entre las variables o el decrecimiento en la tasa de cambio conforme la edad es mayor.\n\n\n\n\n\n\n\n\nFigura 1: (Izquierda) Edad (años) vs Peso (kilogramos). (Derecha) Los mismos datos luego de aplicar la transformación logarítmica a ambas variables.\n\n\n\n\n\nSea \\(Y_{ij}\\) el peso y \\(X_{ij}\\) y la edad de la muestra \\(i\\) de la especie \\(j\\), con \\(j = 1, 2, 3, 4\\). Se propone el siguiente modelo:\n\\[\nY_{ij} = f_j(X_{ij}) \\epsilon_{ij}\n\\]\ndonde \\(f_j\\) es la verdadera curva de crecimiento para la especie \\(j\\) y \\(\\epsilon_{ij} &gt; 0\\) es un error multiplicativo.\n\n¿Por qué tiene sentido proponer un error multiplicativo?\n¿Cuál es un valor sensato para la media de la distribución del error?\nUtilice una distribución log-normal para el error, \\(\\log (\\epsilon_{ij}) \\sim \\text{Normal}\\). Proponga valores para la media y la varianza de forma tal que satisfagan la condición del punto anterior.\n\nEsto da lugar un al siguiente modelo log-normal para \\(Y_{ij}\\):\n\\[\n\\log (Y_{ij}) \\sim \\text{Normal}\n(\\log [f_j(X_{ij})] + \\mu_{\\log \\epsilon}, \\sigma^2_{\\log \\epsilon})\n\\]\ncon \\(\\mathbb{E}(Y_{ij}) = f_j(X_{ij})\\).\nA continuación se proponen cuatro modelos que varían según la relación funcional que se propone para \\(f_j\\) y la naturaleza de las distribuciones a priori que se utilizan.\nModelo 1\nObservando el panel derecho de la Figura 1 se puede concluir que luego de transformar ambas variables con la función logaritmo la relación se ve aproximadamente lineal. Por lo tanto, se propone el siguiente modelo log-lineal:\n\\[\n\\log [f_j(X)] = a_j + b_j \\log(X)\n\\]\ndonde \\(a_j\\) y \\(b_j\\) representan al intercepto y pendiente de la especie \\(j\\). La curva de crecimiento en la escala original resulta \\(f_j(X) = \\exp (a_j)X^{b_j}\\). Considere los siguientes priors:\n\\[\n\\begin{aligned}\na_j &\\sim \\text{Normal}(0, 10) \\\\\nb_j &\\sim \\text{Normal}(0, 10) \\\\\n\\sigma^2_j &\\sim \\text{InvGamma}(0.1, 0.1)\n\\end{aligned}\n\\]\n\nEscriba un programa en Stan que implemente el modelo y obtenga el posterior con {RStan}.\nAnalice los coeficientes del modelo y las curvas de crecimiento. Realice gráficos que permitan observar la curva ajustada y su incertidumbre para cada especie.\n\nModelo 2\nEste modelo es el mismo que el Modelo 1, excepto que las especies tienen la misma varianza, \\(\\sigma^2_j = \\sigma^2\\) y los coeficientes de regresión son modelados de manera jerárquica. Utilice los siguientes priors:\n\\[\n\\begin{aligned}\n\\mu_a    &\\sim \\text{Normal}(0, 10) \\\\\n\\sigma_a &\\sim \\text{InvGamma}(0.1, 0.1) \\\\\n\\mu_b    &\\sim \\text{Normal}(0, 10) \\\\\n\\sigma_b &\\sim \\text{InvGamma}(0.1, 0.1) \\\\\na_j      &\\sim \\text{Normal}(\\mu_a, \\sigma^2_a) \\\\\nb_j      &\\sim \\text{Normal}(\\mu_b, \\sigma^2_b) \\\\\n\\sigma^2 &\\sim \\text{InvGamma}(0.1, 0.1)\n\\end{aligned}\n\\]\n\nEscriba un programa en Stan que implemente el modelo y obtenga el posterior con {RStan}.\nAnalice los coeficientes del modelo y las curvas de crecimiento. Genere gráficos similares a los producidos en el punto anterior. Describa similitudes y diferencias respecto del modelo 1. Justifique su respuesta.\n¿Qué problemas detecta los modelos 1 y 2? Considere como evoluciona el peso conforme la edad según el modelo.\n\nModelo 3\nComo alternativa al componente log-lineal anterior, se propone la siguiente curva de crecimiento logístico:\n\\[\nf_j(X) = a_j + b_j \\frac{\\exp [d_j (\\log(X) - c_j)]}{1 + \\exp [d_j(\\log(X) - c_j)]}\n\\]\nEste modelo tiene cuatro parámetros:\n\n\\(a_j\\) es el peso esperado cuando la edad es 0;\n\\(b_j\\) es el peso máximo esperado (o la cota superior del peso);\n\\(\\log (c_j)\\) es la edad a la que la especie \\(j\\) alcanza la mitad del peso máximo;\n\\(d_j &gt; 0\\) determina la tasa de crecimiento del peso conforme aumenta la edad.\n\nPara que la curva sea positiva y creciente para todas las edades, se debe cumplir que \\(a_j &gt; 0\\), \\(b_j &gt; a_j\\) y \\(d_j &gt; 0\\). Se pueden satisfacer estas restricciones expresando los parámetros en función de parámetros cuyo dominio es \\(\\mathbb{R}\\):\n\n\\(a_j = \\exp (\\alpha_{j1})\\);\n\\(b_j = \\exp (\\alpha_{j2})\\);\n\\(c_j = \\alpha_{j3}\\);\n\\(d_j = \\exp (\\alpha_{j4})\\).\n\nConsidere las siguientes distribuciones a priori para los parámetros del modelo:\n\\[\n\\begin{aligned}\n\\alpha_{jk} &\\sim \\text{Normal}(0, 10) \\\\\n\\sigma^2_j  &\\sim \\text{InvGamma}(0.1, 0.1)\n\\end{aligned}\n\\]\n\nEscriba un programa en Stan que implemente el modelo y obtenga el posterior con {RStan}.\nAnalice los diagnósticos de la inferencia realizada.\nGrafique las curvas estimadas para cada especie junto a sus intervalos de credibilidad e interprete los resultados.\n\nModelo 4\nEste modelo es el mismo que el Modelo 3, excepto que las especies tienen la misma varianza, \\(\\sigma^2_j = \\sigma^2\\) y los coeficientes de regresión son modelados de manera jerárquica. Utilice los siguientes priors:\n\\[\n\\begin{aligned}\n\\mu_k             &\\sim \\text{Normal}(0, 10) \\\\\n\\sigma^2_k        &\\sim \\text{InvGamma}(0.1, 0.1) \\\\\n\\log(\\alpha_{jk}) &\\sim \\text{Normal}(\\mu_k, \\sigma^2_k) \\\\\n\\sigma^2          &\\sim \\text{InvGamma}(0.1, 0.1)\n\\end{aligned}\n\\]\n\nEscriba un programa en Stan que implemente el modelo y obtenga el posterior con {RStan}.\nAnalice los diagnósticos de la inferencia y compare con los resultados del modelo 3.\nGrafique las curvas estimadas para cada especie junto a sus intervalos de credibilidad e interprete los resultados. Compare con los resultados del modelo 3. ¿Qué diferencias observa? ¿Por qué se dan?\nEscriba una síntesis comparando todos los modelos desarrollados. Comente ventajas y desventajas de cada uno de ellos, explicando a que se deben en cada caso ¿Qué modelo resulta más conveniente para estimar la curva de crecimiento de los tiranosáuridos? Justifique su respuesta.",
    "crumbs": [
      "Práctica",
      "Unidad 5"
    ]
  },
  {
    "objectID": "presentaciones/presentacion_06.html#optimización",
    "href": "presentaciones/presentacion_06.html#optimización",
    "title": "Estadística Bayesiana",
    "section": "Optimización",
    "text": "Optimización\nPara modelizar la relación entre una variable dependiente \\(Y\\) y ciertos predictores \\(X_l\\) asumimos un modelo de la forma\n\\[Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\dots + \\beta_p X_p + \\eta\\]\nEn realidad tenemos \\(N\\) observaciones y por lo tanto para cada observación \\((y_i,\\mathbf{x}_i)\\) tenemos\n\\[y_i = \\beta_0 + \\beta_1 x_{1_i} + \\beta_2 x_{2_i} + \\dots + \\beta_p x_{p_i} + \\eta\\]\nO bien, matricialmente\n\\[y_i = \\boldsymbol{\\beta}^T \\mathbf{x}_i + \\eta\\]\nEl error \\(\\eta\\) es desconocido y, en principio, no es necesario asumir nada sobre este."
  },
  {
    "objectID": "presentaciones/presentacion_06.html#section",
    "href": "presentaciones/presentacion_06.html#section",
    "title": "Estadística Bayesiana",
    "section": "",
    "text": "Para predecir valores de \\(y_i\\) es necesario estimar \\(\\boldsymbol{\\beta}\\) por \\(\\hat{\\boldsymbol{\\beta}}\\) dando lugar al siguiente modelo predictivo:\n\\[\\hat{y}_i = \\hat\\beta_0 + \\hat\\beta_1 x_{1_i} + \\hat\\beta_2 x_{2_i} + \\dots + \\hat\\beta_p x_{p_i} = \\hat{\\boldsymbol{\\beta}}^T \\mathbf{x}_i\\]\nUna forma de estimar \\(\\boldsymbol{\\beta}\\) es minimizar alguna función del error de aproximar \\(y\\) por \\(\\hat{y}_i\\):\n\\[\\hat{\\boldsymbol{\\beta}} = \\underset{\\boldsymbol\\beta}{\\mathrm{arg\\,min}}\\left[ J(\\boldsymbol\\beta) \\right] = \\underset{\\boldsymbol\\beta}{\\mathrm{arg\\,min}}\\left[ \\sum_{i=1}^N \\left(y_i - \\boldsymbol\\beta^T \\mathbf{x}_i\\right)^2 \\right]\\]\nEl \\(\\boldsymbol{\\beta}\\) que minimiza el error cuadrático se conoce como estimador de mínimos cuadrados. Esto es lo que se conoce como enfoque de optimización."
  },
  {
    "objectID": "presentaciones/presentacion_06.html#estadística-clásica",
    "href": "presentaciones/presentacion_06.html#estadística-clásica",
    "title": "Estadística Bayesiana",
    "section": "Estadística clásica",
    "text": "Estadística clásica\nAsumiendo un modelo probabilístico para el error, \\(\\eta \\sim \\mathcal{N}(0,\\sigma^2)\\) se puede obtener el estimador de máxima verosimilitud de \\(\\boldsymbol\\beta\\).\nLa función de verosimilitud viene dada por el producto de las funciones de densidad normales:\n\\[\\ell(\\boldsymbol\\beta,\\sigma|\\mathbf{y}) = \\prod_{i=1}^N p(y_i|\\mathbf{x_i},\\boldsymbol\\beta^T,\\sigma^2) = \\prod_{i=1}^N \\frac{1}{\\sqrt{2\\pi\\sigma}} e^{-\\frac{(y_i - \\boldsymbol\\beta^T\\mathbf{x}_i)^2}{2\\sigma^2}}\\]"
  },
  {
    "objectID": "presentaciones/presentacion_06.html#section-2",
    "href": "presentaciones/presentacion_06.html#section-2",
    "title": "Estadística Bayesiana",
    "section": "",
    "text": "Maximizar la verosimilitud equivale a minimizar el opuesto de la log-verosimilitud \\[\\mathcal{L}(\\boldsymbol\\beta,\\sigma|\\mathbf{y}) = \\log(\\ell(\\boldsymbol\\beta,\\sigma|\\mathbf{y}))\\]\n\\[\\hat{\\boldsymbol{\\beta}}_{ML} = \\underset{\\boldsymbol\\beta}{\\mathrm{arg\\,min}}\\left[ - \\sum_{i=1}^N \\log \\left( \\left( \\frac{1}{2\\pi\\sigma^2} \\right)^{1/2} e^{-\\frac{(y_i - \\boldsymbol\\beta^T\\mathbf{x}_i)^2}{2\\sigma^2}} \\right) \\right]\\]\nLa expresión anterior puede minimizarse primero respecto de \\(\\boldsymbol\\beta\\) y luego respecto de \\(\\sigma\\). Resulta que maximizar la verosimilitud respecto de \\(\\boldsymbol\\beta\\) equivale a minimizar el error cuadrático."
  },
  {
    "objectID": "presentaciones/presentacion_06.html#estadística-bayesiana",
    "href": "presentaciones/presentacion_06.html#estadística-bayesiana",
    "title": "Estadística Bayesiana",
    "section": "Estadística bayesiana",
    "text": "Estadística bayesiana\nEn estadística bayesiana, consideramos a los parámetros como variables aleatorias y les asignamos una distribución a priori.\nAdemás, contamos con un modelo generativo (probabilístico) para las observaciones: ¿cómo obtendríamos observaciones si conociéramos los parámetros? Es una decisión de la modelización."
  },
  {
    "objectID": "presentaciones/presentacion_06.html#section-3",
    "href": "presentaciones/presentacion_06.html#section-3",
    "title": "Estadística Bayesiana",
    "section": "",
    "text": "Aquí asumimos:\n\\[Y_i \\mid \\boldsymbol\\beta,\\sigma \\sim \\mathcal{N}(\\boldsymbol\\beta^T \\mathbf{x}_i, \\sigma^2)\\]\no bien decimos\n\\[\n\\begin{aligned}\n    Y_i & \\sim  \\mathcal{N}(\\mu_i, \\sigma^2) \\\\\n    \\mu_i & = \\beta_0 + \\beta_1 x_{1_i} + \\beta_2 x_{2_i} + \\dots + \\beta_p x_{p_i}\n\\end{aligned}\n\\]\nY completamos el modelo especificando una distribución a priori \\(p(\\boldsymbol\\beta,\\sigma)\\)"
  },
  {
    "objectID": "presentaciones/presentacion_06.html#section-4",
    "href": "presentaciones/presentacion_06.html#section-4",
    "title": "Estadística Bayesiana",
    "section": "",
    "text": "La estimación se hace siempre de la misma manera\n\\(p(\\boldsymbol\\beta,\\sigma\\mid\\mathbf{y}) \\propto p(\\mathbf{y}|\\boldsymbol\\beta,\\sigma)p(\\boldsymbol\\beta,\\sigma)\\)"
  },
  {
    "objectID": "presentaciones/presentacion_06.html#section-5",
    "href": "presentaciones/presentacion_06.html#section-5",
    "title": "Estadística Bayesiana",
    "section": "",
    "text": "Observando un dato (¿se puede hacer inferencia con un solo punto?)…"
  },
  {
    "objectID": "presentaciones/presentacion_06.html#section-6",
    "href": "presentaciones/presentacion_06.html#section-6",
    "title": "Estadística Bayesiana",
    "section": "",
    "text": "Observando el dato que sigue…"
  },
  {
    "objectID": "presentaciones/presentacion_06.html#section-7",
    "href": "presentaciones/presentacion_06.html#section-7",
    "title": "Estadística Bayesiana",
    "section": "",
    "text": "Observando dos puntos juntos…"
  },
  {
    "objectID": "presentaciones/presentacion_06.html#section-8",
    "href": "presentaciones/presentacion_06.html#section-8",
    "title": "Estadística Bayesiana",
    "section": "",
    "text": "Comparemos con un prior más fuerte…"
  },
  {
    "objectID": "presentaciones/presentacion_06.html#section-9",
    "href": "presentaciones/presentacion_06.html#section-9",
    "title": "Estadística Bayesiana",
    "section": "",
    "text": "Observando un punto (¿se puede hacer inferencia con un solo punto?)"
  },
  {
    "objectID": "presentaciones/presentacion_06.html#section-10",
    "href": "presentaciones/presentacion_06.html#section-10",
    "title": "Estadística Bayesiana",
    "section": "",
    "text": "Observando diez puntos"
  },
  {
    "objectID": "presentaciones/presentacion_06.html#section-11",
    "href": "presentaciones/presentacion_06.html#section-11",
    "title": "Estadística Bayesiana",
    "section": "",
    "text": "\\(\\mu\\) depende de los parámetros (y por supuesto del valor de \\(x\\)), por lo que tiene una distribución de probabilidad asociada"
  },
  {
    "objectID": "presentaciones/presentacion_06.html#section-12",
    "href": "presentaciones/presentacion_06.html#section-12",
    "title": "Estadística Bayesiana",
    "section": "",
    "text": "Por supuesto, las predicciones para \\(y\\) (\\(\\tilde{y}\\)) también son probabilísticas: distribución predictiva a posteriori"
  },
  {
    "objectID": "presentaciones/presentacion_06.html#resumen",
    "href": "presentaciones/presentacion_06.html#resumen",
    "title": "Estadística Bayesiana",
    "section": "Resumen",
    "text": "Resumen\n\nTenemos una distribución de probabilidad para los parámetros. Es decir, tenemos incertidumbre en los valores de los parámetros\nTenemos que trabajar con todo el posterior (a través de muestras) y no con estimaciones puntuales\nNo confundir predicción de la media (también llamado predictor lineal) con distribución predictiva (para las observaciones)\nA medida que aumenta el tamaño de muestra, los coeficientes de la regresión se estiman cada vez con mayor precisión y la incertidumbre del predictor lineal desaparece (incertidumbre epistémica). No obstante, la incertidumbre en la distribución predictiva no desaparece (siempre quedará \\(\\sigma\\): incertidumbre aleatoria)."
  },
  {
    "objectID": "presentaciones/presentacion_06.html#validación-interna",
    "href": "presentaciones/presentacion_06.html#validación-interna",
    "title": "Estadística Bayesiana",
    "section": "Validación interna",
    "text": "Validación interna\n\nEl modo fundamental de validar el ajuste de un modelo bayesiano es generar réplicas del conjunto de datos (utilizando el modelo ajustado) y compararlas con los datos reales. Esto es lo que se conoce como validación interna.\nPara cada muestra de parámetros del posterior podemos generar un dataset \\[\n\\left[\n\\begin{array}{l|l|l|l}\nY_1^{(1)} & Y_2^{(1)} & \\cdots & Y_{N}^{(1)} \\\\\nY_1^{(2)} & Y_2^{(2)} & \\cdots & Y_{N}^{(2)} \\\\\n\\vdots & \\vdots &  & \\vdots \\\\\nY_1^{(S)} & Y_2^{(S)} & \\cdots & Y_{N}^{(S)} \\\\\n\\end{array}\n\\right]\n\\]\nEsta práctica da lugar a los posterior predictive checks (PPC)"
  },
  {
    "objectID": "presentaciones/presentacion_06.html#validación-externa",
    "href": "presentaciones/presentacion_06.html#validación-externa",
    "title": "Estadística Bayesiana",
    "section": "Validación externa",
    "text": "Validación externa\n\nIdealmente quisiéramos ver si nuestro modelo tiene capacidad predictiva para datos nuevos (no usados para ajustarlo)\nAntes de preocuparnos por los datos nuevos, pensemos en las predicciones… Las predicciones son probabilísticas\nNo podemos simplemente comparar \\(y_i\\) con \\(\\hat{y}_i\\)\nDebemos utilizar toda la distribución a posteriori para evaluar el ajuste (y la capacidad predictiva) del modelo"
  },
  {
    "objectID": "presentaciones/presentacion_06.html#section-15",
    "href": "presentaciones/presentacion_06.html#section-15",
    "title": "Estadística Bayesiana",
    "section": "",
    "text": "Un posible score predictivo para un determinado valor \\(y_i\\) es la probabilidad que el modelo le asocia (también llamada densidad predictiva), \\[\\int p\\left(y_i\\mid\\theta\\right) p(\\theta\\mid y) d\\theta \\approx \\frac{1}{S}\\sum_{s=1}^S p(y_i\\mid \\theta^{(s)})\\]"
  },
  {
    "objectID": "presentaciones/presentacion_06.html#section-16",
    "href": "presentaciones/presentacion_06.html#section-16",
    "title": "Estadística Bayesiana",
    "section": "",
    "text": "El score predictivo total (para todas las observaciones) es la log-posterior pointwise predictive density. A mayor \\(\\mathrm{lppd}\\), mejor es el ajuste del modelo.\n\\[\n\\mathrm{lppd} = \\color{#1f7a8c}{\\sum_{i=1}^{N}} \\color{#EB8A90}{\\log} \\left( \\color{#683257}{\\int p\\left(y_i\\mid\\theta\\right) \\underline{p(\\theta\\mid y)}d\\theta} \\right)\n\\]\n\nQue podemos estimar a través de muestras del posterior\n\\[\n\\mathrm{lppd} = \\color{#1f7a8c}{\\sum_{i=1}^{N}} \\color{#EB8A90}{\\log} \\left( \\color{#683257}{\\frac{1}{S} \\sum_{s=1}^{S} p\\left(y_i\\mid\\theta^{(s)}\\right)} \\right)\n\\]"
  },
  {
    "objectID": "presentaciones/presentacion_06.html#section-17",
    "href": "presentaciones/presentacion_06.html#section-17",
    "title": "Estadística Bayesiana",
    "section": "",
    "text": "La deviance de un modelo es\n\\[D = -2\\ \\mathrm{lppd}\\]\n\nLa deviance (o la \\(\\mathrm{lppd}\\)) evalúa las predicciones de un modelo (el ajuste), no nos dice qué tan correcto es…\nSon medidas que siempre mejoran con más parámetros\nEn realidad nos importa cómo se desempeña el modelo con datos nuevos."
  },
  {
    "objectID": "presentaciones/presentacion_06.html#section-18",
    "href": "presentaciones/presentacion_06.html#section-18",
    "title": "Estadística Bayesiana",
    "section": "",
    "text": "La \\(\\mathrm{lppd}\\) predice el \\(i\\)-ésimo valor con un posterior que usa todos los datos, incluido el \\(i\\). Más que la \\(\\mathrm{lppd}\\) nos interesa su valor esperado en datos nuevos (\\(\\mathrm{elppd}\\)). Por supuesto, no conocemos datos nuevos.\n\nPodemos aproximar o estimar \\(\\mathrm{elppd}\\) haciendo cross-validation (CV) o, en particular, leave-one-out cross-validation (LOO-CV).\n\\[\\mathrm{elppd} \\approx \\mathrm{lppd}_{LOO} = \\sum_{i=1}^{N} \\log \\left( \\frac{1}{S} \\sum_{s=1}^{S} p\\left(y_i\\mid\\theta_{-i}^{(s)}\\right) \\right)\\]\ndonde los \\(\\theta_{-i}^{(s)}\\) son muestras del posterior de \\(\\theta\\) obtenido sin considerar la \\(i\\)-ésima observación.\nEl problema de hacer LOO-CV es que, si tenemos 1000 observaciones, hay que calcular 1000 distribuciones a posteriori."
  },
  {
    "objectID": "presentaciones/presentacion_06.html#section-19",
    "href": "presentaciones/presentacion_06.html#section-19",
    "title": "Estadística Bayesiana",
    "section": "",
    "text": "No contamos con muestras de \\(p(\\theta\\mid \\mathbf{y}_{-i})\\) sino simplemente de \\(p(\\theta\\mid \\mathbf{y})\\). No sabemos la distribución a posteriori de \\(\\theta\\) sin considerar la observación \\(i\\). Hay formas de aproximar el desempeño en LOO-CV sin necesidad de reajustar el modelo. Una forma de hacerlo es usar la “importancia” de cada observación en el posterior. Esto da lugar a una técnica que se conoce como Pareto-smoothed importance sampling cross-validation (PSIS)"
  },
  {
    "objectID": "presentaciones/presentacion_06.html#section-20",
    "href": "presentaciones/presentacion_06.html#section-20",
    "title": "Estadística Bayesiana",
    "section": "",
    "text": "Históricamente se han desarrollado los llamados criterios de información que penalizan la verosimilitud con un término adicional para compensar la capacidad de sobreajuste de un modelo que tiene más parámetros."
  },
  {
    "objectID": "presentaciones/presentacion_06.html#section-21",
    "href": "presentaciones/presentacion_06.html#section-21",
    "title": "Estadística Bayesiana",
    "section": "",
    "text": "El AIC (Akaike information criterion) es \\[AIC = D + 2p = -2\\ lppd + 2p\\] donde \\(p\\) es el número de parámetros del modelo y \\(D=-2\\ \\mathrm{lppd}\\) se conoce como deviance. Penalizamos el \\(\\mathrm{lppd}\\) con la tendencia (o capacidad) del modelo de sobreajustar."
  },
  {
    "objectID": "presentaciones/presentacion_06.html#section-22",
    "href": "presentaciones/presentacion_06.html#section-22",
    "title": "Estadística Bayesiana",
    "section": "",
    "text": "El WAIC (widely applicable information criterion) es un criterio más general que el AIC (y un poquitito más difícil de calcular):\n\\[WAIC = - 2\\left(\\mathrm{lppd} - \\sum_{i=1}^N \\mathbb{V}_\\theta\\left[\\log\\left(p(y_i\\mid\\theta\\right)\\right]\\right)\\]\n\\(\\sum_{i=1}^N \\mathrm{V}_\\theta\\left[\\log\\left(p(y_i\\mid\\theta\\right)\\right]\\) es un término de penalización que se suele llamar “número efectivo de parámetros”. Es la suma de las varianzas en la log-probabilidad de cada observación \\(i\\) (o sea, la varianza total). Si, para un determinado dato \\(i\\), las diferentes muestras del posterior \\(\\theta_{(s)}\\) dan como resultado predicciones muy diferentes, es porque el modelo tiene mucha incertidumbre (y es posiblemente muy flexible).\n\n\n\n\n\nEstadística Bayesiana – 2025"
  },
  {
    "objectID": "presentaciones/presentacion_03.html#gamma-poisson",
    "href": "presentaciones/presentacion_03.html#gamma-poisson",
    "title": "Estadística Bayesiana",
    "section": "Gamma-Poisson",
    "text": "Gamma-Poisson\nSea una muestra \\(\\mathbf{y} = (y_1,y_2,\\dots,y_n)\\) obtenida de un modelo Poisson, es decir:\n\\[Y_i \\sim \\mathrm{Poisson}(\\lambda)\\]\nInteresa realizar una inferencia sobre el valor de \\(\\lambda\\)\n\n¿Cómo asignamos una credibilidad a priori para \\(\\lambda\\)?"
  },
  {
    "objectID": "presentaciones/presentacion_03.html#section",
    "href": "presentaciones/presentacion_03.html#section",
    "title": "Estadística Bayesiana",
    "section": "",
    "text": "\\[\n\\lambda \\sim \\mathrm{Gamma}(s, r)\n\\]\n\\[\np(\\lambda \\mid s, r) = p(\\lambda) = \\frac{r^s}{\\Gamma(s)} \\lambda^{s-1}e^{-r\\lambda}\n\\]\n\n\n\n\n\n\nCuidado\n\n\n\\(\\mathrm{Gamma}(s, r)\\) en R es dgamma(x, shape = s, scale = 1/r)"
  },
  {
    "objectID": "presentaciones/presentacion_03.html#section-1",
    "href": "presentaciones/presentacion_03.html#section-1",
    "title": "Estadística Bayesiana",
    "section": "",
    "text": "El modelo propuesto es \\[\n\\begin{aligned}\n  Y_i \\mid \\lambda & \\sim  Po(\\lambda)\\\\\n  \\lambda & \\sim  \\mathrm{Gamma}(s, r)\n\\end{aligned}\n\\]\nEl likelihood es Poisson: \\[p(y_i\\mid \\lambda) = \\frac{\\lambda^{y_i}e^{-\\lambda}}{y_i!} \\rightarrow p(\\mathbf{y}\\mid \\lambda) = \\prod_i \\frac{\\lambda^{y_i}e^{-\\lambda}}{y_i!} = \\frac{\\lambda^{\\sum_i y_i}e^{-n\\lambda}}{\\prod_{i}y_i!}\\]\nEl prior es Gamma: \\[p(\\lambda) = \\frac{r^s}{\\Gamma(s)} \\lambda^{s-1}e^{-r\\lambda}\\]\nInteresa hallar \\(p(\\lambda\\mid \\mathbf{y})\\)"
  },
  {
    "objectID": "presentaciones/presentacion_03.html#section-2",
    "href": "presentaciones/presentacion_03.html#section-2",
    "title": "Estadística Bayesiana",
    "section": "",
    "text": "\\[p(\\lambda\\mid \\mathbf{y}) \\propto p(\\mathbf{y}\\mid\\lambda) p(\\lambda)\\]\n\n\\[\np(\\lambda \\mid \\mathbf{y}) \\propto \\frac{\\lambda^{\\sum_i y_i}e^{-n\\lambda}}{\\prod_{i}y_i!} \\frac{r^s}{\\Gamma(s)} \\lambda^{s-1}e^{-r\\lambda}\n\\]\n\\[\np(\\lambda \\mid \\mathbf{y}) \\propto \\frac{r^s}{\\Gamma(s)\\prod_i y_i!} \\lambda^{\\sum_iy_i+s-1} e^{-n\\lambda - r \\lambda}\n\\]\n\n\n\\[p(\\lambda \\mid \\mathbf{y}) = K C \\lambda^{\\sum_iy_i+s-1} e^{-n\\lambda - r \\lambda}\\]\n\n\n\\[p(\\lambda \\mid \\mathbf{y}) = K^* \\lambda^{\\sum_iy_i+s-1} e^{-(n + r)\\lambda}\\]"
  },
  {
    "objectID": "presentaciones/presentacion_03.html#section-3",
    "href": "presentaciones/presentacion_03.html#section-3",
    "title": "Estadística Bayesiana",
    "section": "",
    "text": "Para que \\(\\int_0^\\infty p(\\lambda\\mid \\mathbf{y})d \\lambda = 1\\), debe ser\n\n\\[K^* = \\frac{(n + r )^{\\sum_i y_i + s}}{\\Gamma(\\sum_i y_i + s)}\\]\n\n\nPor lo tanto, resulta que la distribución a posteriori es Gamma de parámetros \\(\\sum_i y_i + s\\) y \\(n+r\\)\n\n\n\\[\np(\\lambda\\mid \\mathbf{y}) = \\frac{(n + r )^{\\sum_i y_i + s}}{\\Gamma(\\sum_i y_i + s)} \\lambda^{\\sum_iy_i+s-1} e^{-(n + r)\\lambda}\n\\]\n\\[\n\\lambda\\mid \\mathbf{y} \\sim  \\mathrm{Gamma}(\\sum_i y_i + s, n+r)\n\\]"
  },
  {
    "objectID": "presentaciones/presentacion_03.html#normal-normal",
    "href": "presentaciones/presentacion_03.html#normal-normal",
    "title": "Estadística Bayesiana",
    "section": "Normal-normal",
    "text": "Normal-normal\nSea una muestra \\(\\mathbf{y} = (y_1,y_2,\\dots,y_n)\\) obtenida de un modelo normal con varianza conocida \\(\\sigma^2\\), es decir:\n\\[Y_i \\sim \\mathcal{N}(\\mu,\\sigma^2)\\]\nInteresa realizar una inferencia sobre el valor de \\(\\mu\\)\n\n¿Cómo asignamos una credibilidad a priori para \\(\\mu\\)?"
  },
  {
    "objectID": "presentaciones/presentacion_03.html#section-4",
    "href": "presentaciones/presentacion_03.html#section-4",
    "title": "Estadística Bayesiana",
    "section": "",
    "text": "El modelo propuesto es: \\[\n\\small{\n  \\begin{aligned}\n      y_i \\mid \\mu & \\sim  \\mathcal{N}(\\mu,\\sigma^2)\\\\\n      \\mu & \\sim \\mathcal{N}(\\theta,\\tau^2)\n  \\end{aligned}\n}\n\\]\n\nEl likelihood es normal: \\[\n\\small{\n  \\begin{aligned}\n  p(y_i\\mid \\mu) &= \\frac{1}{\\sqrt{2\\pi\\sigma}} e^{-\\frac{(y_i-\\mu)^2}{2\\sigma^2}} \\rightarrow \\\\\n  p(\\mathbf{y}\\mid \\mu) &= \\left(\\frac{1}{2\\pi\\sigma}\\right)^{n/2}  e^{-\\frac{\\sum_i(y_i-\\mu)^2}{2\\sigma^2}} \\propto e^{-\\frac{(\\bar{y}-\\mu)^2}{2\\sigma^2/n}}\n  \\end{aligned}\n}\n\\]\n\n\nEl prior es normal: \\[\n\\small{\n  p(\\mu) = \\frac{1}{\\sqrt{2\\pi\\tau}} e^{-\\frac{(\\mu-\\theta)^2}{2\\tau^2}}\n}\n\\]\n\n\nInteresa hallar \\(p(\\mu \\mid \\mathbf{y})\\)"
  },
  {
    "objectID": "presentaciones/presentacion_03.html#section-5",
    "href": "presentaciones/presentacion_03.html#section-5",
    "title": "Estadística Bayesiana",
    "section": "",
    "text": "\\[p(\\mu\\mid \\mathbf{y}) \\propto p(\\mathbf{y}\\mid\\mu) p(\\mu)\\] \\[p(\\mu \\mid \\mathbf{y}) \\propto   e^{-\\frac{(\\bar{y}-\\mu)^2}{2\\sigma^2/n}} \\frac{1}{\\sqrt{2\\pi\\tau}} e^{-\\frac{(\\mu-\\theta)^2}{2\\tau^2}}\\]\n\\[\n\\begin{aligned}\np(\\mu \\mid \\mathbf{y}) & \\propto  e^{-\\frac{(\\bar{y}-\\mu)^2}{2\\sigma^2/n}} e^{-\\frac{(\\mu-\\theta)^2}{2\\tau^2}} \\\\\n&  \\propto  e^{-\\left[\\frac{(\\bar{y}-\\mu)^2}{2\\sigma^2/n}+\\frac{(\\mu-\\theta)^2}{2\\tau^2}\\right]} \\\\\n& \\propto e^{-\\left[ \\frac{\\bar{y}^2 - 2\\bar{y} \\mu + \\mu^2}{2\\sigma^2/n} + \\frac{\\mu^2 - 2\\mu\\theta^2 + \\theta^2}{2\\tau^2} \\right]} \\\\\n& \\propto e^{\\left[ \\frac{ 2\\bar{y} \\mu - \\mu^2}{2\\sigma^2/n} + \\frac{-\\mu^2 + 2\\mu\\theta^2}{2\\tau^2} \\right]} \\\\\n& \\propto e^{\\left[ \\frac{(2\\bar{y} \\mu - \\mu^2)n\\tau^2 + (-\\mu^2 + 2\\mu\\theta^2)\\sigma^2}{2\\sigma^2\\tau^2} \\right]}\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "presentaciones/presentacion_03.html#section-6",
    "href": "presentaciones/presentacion_03.html#section-6",
    "title": "Estadística Bayesiana",
    "section": "",
    "text": "\\[\n    \\begin{aligned}\nP(\\mu \\mid \\mathbf{y}) & \\propto e^{\\frac{2\\mu(\\theta\\sigma^2+ \\bar{y}n\\tau^2)-\\mu^2(n\\tau^2+\\sigma^2)}{2\\tau^2\\sigma^2}} \\\\\n& \\propto e^{\\frac{-\\mu^2 + 2\\mu \\left( \\frac{\\theta\\sigma^2 + \\bar{y}n\\tau^2}{n\\tau^2 + \\sigma^2} \\right)}{2\\tau^2\\sigma^2/(n\\tau^2 + \\sigma^2)}} e^{-\\left(\\frac{\\theta\\sigma^2 + \\bar{y}n\\tau^2}{n\\tau^2 + \\sigma^2}\\right)^2} \\\\\n& \\propto e^{-\\frac{\\left(\\mu -  \\frac{\\theta\\sigma^2 + \\bar{y}n\\tau^2}{n\\tau^2 + \\sigma^2} \\right)^2}{2\\tau^2\\sigma^2/(n\\tau^2+\\sigma^2)}}\n    \\end{aligned}\n\\] \\[p(\\mu\\mid\\mathbf{y}) = K^* e^{-\\frac{\\left(\\mu -  \\frac{\\theta\\sigma^2 + \\bar{y}n\\tau^2}{n\\tau^2 + \\sigma^2} \\right)^2}{2\\tau^2\\sigma^2/(n\\tau^2+\\sigma^2)}}\\]"
  },
  {
    "objectID": "presentaciones/presentacion_03.html#section-7",
    "href": "presentaciones/presentacion_03.html#section-7",
    "title": "Estadística Bayesiana",
    "section": "",
    "text": "Por lo tanto, resulta que la distribución a posteriori es normal de parámetros \\(\\theta_n\\) y \\(\\tau_n^2\\)\n\n\\[\n\\begin{aligned}\n\\mu\\mid \\mathbf{y} & \\sim  \\mathcal{N}\\left( \\frac{\\theta\\sigma^2 + \\bar{y}n\\tau^2}{n\\tau^2 + \\sigma^2},\\frac{\\tau^2\\sigma^2}{n\\tau^2+\\sigma^2} \\right) \\\\\n& \\sim \\mathcal{N}\\left( \\theta_n,\\tau_n^2 \\right)\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "presentaciones/presentacion_03.html#section-8",
    "href": "presentaciones/presentacion_03.html#section-8",
    "title": "Estadística Bayesiana",
    "section": "",
    "text": "Reflexionemos… \\[\n\\begin{aligned}\n    y_i\\mid\\mu & \\sim  \\mathcal{N}(\\mu,\\sigma^2) \\\\\n    \\mu & \\sim  \\mathcal{N}(\\theta,\\tau^2) \\\\\n    \\mu \\mid \\mathbf{y} & \\sim \\mathcal{N}(\\theta_n,\\tau_n^2)\n\\end{aligned}\n\\]\n¿Parámetros desconocidos en la verosimilitud?\n\n¿Dimensión y característica del espacio de parámetros?\n\n\n¿Constantes de ajuste del prior?\n\n\n¿Forma del posterior?\n\n\n¿Qué son \\(\\theta_n\\) y \\(\\tau_n^2\\)?"
  },
  {
    "objectID": "presentaciones/presentacion_03.html#section-9",
    "href": "presentaciones/presentacion_03.html#section-9",
    "title": "Estadística Bayesiana",
    "section": "",
    "text": "¿Parámetros de la verosimilitud?"
  },
  {
    "objectID": "presentaciones/presentacion_03.html#section-10",
    "href": "presentaciones/presentacion_03.html#section-10",
    "title": "Estadística Bayesiana",
    "section": "",
    "text": "Otro modo de verlo"
  },
  {
    "objectID": "presentaciones/presentacion_03.html#section-11",
    "href": "presentaciones/presentacion_03.html#section-11",
    "title": "Estadística Bayesiana",
    "section": "",
    "text": "¿Puedo representar los datos en el gráfico de la izquierda?\n\nNo, es el mundo de los parámetros\n\n\n¿Qué representan los valores marcados con \\(\\mathbf{\\times}\\)?\n\nPosibles valores de \\(\\mu\\) que podrían esperarse a priori.\n\n\n\n¿Media y varianza de la normal de la izquierda?\n\n\\(\\theta\\) y \\(\\tau^2\\)\n\n\n\n¿Media y varianza de las normales de la derecha?\n\n\\(\\mu\\) y \\(\\sigma^2\\)"
  },
  {
    "objectID": "presentaciones/presentacion_03.html#section-12",
    "href": "presentaciones/presentacion_03.html#section-12",
    "title": "Estadística Bayesiana",
    "section": "",
    "text": "¿Qué estamos viendo?"
  },
  {
    "objectID": "presentaciones/presentacion_03.html#section-13",
    "href": "presentaciones/presentacion_03.html#section-13",
    "title": "Estadística Bayesiana",
    "section": "",
    "text": "A posteriori (luego de observar los datos)… ¿qué ocurre con la plausibilidad de los valores de \\(\\mu\\)?"
  },
  {
    "objectID": "presentaciones/presentacion_03.html#section-14",
    "href": "presentaciones/presentacion_03.html#section-14",
    "title": "Estadística Bayesiana",
    "section": "",
    "text": "¿Media y varianza de la normal de la izquierda?\n\n\\(\\theta_n\\) y \\(\\tau_n^2\\)\n\n¿Media y varianza de las normales de la derecha?\n\n\\(\\mu\\) y \\(\\sigma^2\\)"
  },
  {
    "objectID": "presentaciones/presentacion_03.html#compromiso",
    "href": "presentaciones/presentacion_03.html#compromiso",
    "title": "Estadística Bayesiana",
    "section": "Compromiso",
    "text": "Compromiso\n\\[\n\\small{\n  \\mathbb{E}[p(\\mu\\mid \\mathbf{y})] = \\theta_n = \\frac{\\theta\\sigma^2 + \\bar{y}n\\tau^2}{n\\tau^2 + \\sigma^2}\n}\n\\]\n\\[\n\\small{\n  \\mathbb{E}[p(\\mu\\mid \\mathbf{y})] =  \\theta\\frac{\\sigma^2}{n\\tau^2 + \\sigma^2} + \\bar{y}\\frac{n\\tau^2}{n\\tau^2 + \\sigma^2}\n}\n\\]\n\nRepresenta un balance (promedio ponderado o combinación convexa) entre la media muestral y la media esperada a priori."
  },
  {
    "objectID": "presentaciones/presentacion_03.html#compromiso-1",
    "href": "presentaciones/presentacion_03.html#compromiso-1",
    "title": "Estadística Bayesiana",
    "section": "Compromiso",
    "text": "Compromiso\n\\[\n\\small{\n  \\mathbb{V}[p(\\mu\\mid \\mathbf{y})] = \\tau_n^2 = \\frac{\\tau^2\\sigma^2}{n\\tau^2+\\sigma^2}\n}\n\\]\n\\[\n\\small{\n  \\mathbb{V}[p(\\mu\\mid \\mathbf{y})] = \\frac{1}{\\frac{n}{\\sigma^2}+\\frac{1}{\\tau^2}}\n}\n\\]\n\\[\n\\small{\n  \\frac{1}{\\mathbb{V}[p(\\mu\\mid \\mathbf{y})]} =  \\frac{n}{\\sigma^2} + \\frac{1}{\\tau^2}\n}\n\\]\n\nLa precisión a posteriori es la suma de las precisiones del prior y la muestra."
  },
  {
    "objectID": "presentaciones/presentacion_03.html#section-15",
    "href": "presentaciones/presentacion_03.html#section-15",
    "title": "Estadística Bayesiana",
    "section": "",
    "text": "Distribución predictiva a posteriori\n\\[p(\\tilde{y}\\mid \\mathbf{y}) = \\int p(\\tilde{y}\\mid \\mu) p(\\mu\\mid \\mathbf{y})d\\mu\\] El integrando es el producto de dos normales: una normal bivariada. Por lo tanto toda la integral es una distribución marginal de una normal: otra normal."
  },
  {
    "objectID": "presentaciones/presentacion_03.html#section-16",
    "href": "presentaciones/presentacion_03.html#section-16",
    "title": "Estadística Bayesiana",
    "section": "",
    "text": "Demostración poco formal…\nA posteriori vale \\[\n\\begin{array}{ccc}\ny & = & (y-\\mu) + \\mu \\\\\ny-\\mu \\mid \\mu & \\sim & \\mathcal{N}(0,\\sigma^2) \\\\\n\\mu \\mid \\mathbf{y} & \\sim & \\mathcal{N}(\\theta_n,\\tau_n^2)\n\\end{array}\n\\]\nResulta\n\\[p(\\tilde{y}\\mid \\mathbf{y}) = \\mathcal{N}(\\mu_n,\\sigma^2 + \\tau_n^2)\\]"
  },
  {
    "objectID": "presentaciones/presentacion_03.html#section-17",
    "href": "presentaciones/presentacion_03.html#section-17",
    "title": "Estadística Bayesiana",
    "section": "",
    "text": "La varianza predictiva \\(\\sigma^2 + \\tau_n^2\\) es una medida de la incertidumbre a posteriori respecto a una observación nueva \\(\\tilde{y}\\).\n\nLa incertidumbre en \\(\\tilde{y}\\) proviene de la variabilidad debida al azar (\\(\\sigma\\)) y de la variabilidad debida al desconocimiento de \\(\\mu\\) (\\(\\tau_n\\))\n\n\nEn otras palabras, si supiéramos que \\(\\mu = 2\\), toda la variabilidad provendría de \\(\\sigma\\), ¡pero no sabemos cuánto vale \\(\\mu\\)! Puede ser \\(2\\) o \\(1.98\\) o \\(1.43\\)… Por lo que hay una componente adicional de varianza."
  },
  {
    "objectID": "presentaciones/presentacion_03.html#section-18",
    "href": "presentaciones/presentacion_03.html#section-18",
    "title": "Estadística Bayesiana",
    "section": "",
    "text": "No se entendió nada. Simular para creer.\n\n¿Cómo obtenemos una observación nueva si sabemos que \\(\\mu = 2\\) (sabiendo que \\(\\sigma = 1.2\\))?\n\n\nDirectamente tomamos una muestra \\(\\tilde{y}\\) de \\(\\mathcal{N}\\left(\\mu=2,\\sigma^2= 1.2^2\\right)\\)\n\ny_new &lt;- rnorm(1, mean = 2, sd = 1.2)\n\n\n\nPero en estadística bayesiana \\(\\mu\\) tiene una distribución de probabilidad (por ejemplo \\(\\mathcal{N}\\left(\\theta_n=2,\\tau_n^2=1.8^2\\right)\\)), ¿cómo hacemos la simulación?\n\n\n\nTomamos una muestra \\(\\mu^{(s)}\\) de la distribución de \\(\\mu\\)\nObtenemos \\(\\tilde{y}\\) a partir de \\(\\mathcal{N}(\\mu=\\mu^{(s)},\\sigma^2=1.2^2)\\)\n\n\n\n\nmu_s &lt;- rnorm(1, mean = 2, sd = 1.8)\ny_new &lt;- rnorm(1, mean = mu_s, sd = 1.2)\n\n\n\n¿Qué va a pasar en cada caso si construimos la distribución de \\(\\tilde{y}\\)?"
  },
  {
    "objectID": "presentaciones/presentacion_03.html#section-19",
    "href": "presentaciones/presentacion_03.html#section-19",
    "title": "Estadística Bayesiana",
    "section": "",
    "text": "La distribución predictiva contiene la variabilidad inherente al fenómeno en estudio (\\(\\sigma\\)) y la incertidumbre en el parámetro \\(\\mu\\)."
  },
  {
    "objectID": "presentaciones/presentacion_03.html#normal-normal-gamma-inversa",
    "href": "presentaciones/presentacion_03.html#normal-normal-gamma-inversa",
    "title": "Estadística Bayesiana",
    "section": "Normal – normal-gamma-inversa",
    "text": "Normal – normal-gamma-inversa\nSea una muestra \\(\\mathbf{y} = (y_1,y_2,\\dots,y_n)\\) obtenida de un modelo normal con varianza desconocida \\(\\sigma^2\\), es decir:\n\\[Y_i \\sim \\mathcal{N}(\\mu,\\sigma^2)\\]\nE interesa realizar una inferencia sobre el valor de \\(\\mu\\) y el valor de \\(\\sigma\\)\n\n¿Cómo asignamos una credibilidad a priori para \\(\\mu\\) y \\(\\sigma\\)? ¡Con una distribución en dos dimensiones!"
  },
  {
    "objectID": "presentaciones/presentacion_03.html#section-20",
    "href": "presentaciones/presentacion_03.html#section-20",
    "title": "Estadística Bayesiana",
    "section": "",
    "text": "El modelo es\n\\[\n\\begin{aligned}\n    Y_i\\mid\\mu,\\sigma^2 & \\sim  \\mathcal{N}(\\mu,\\sigma^2) \\\\\n    \\mu,\\sigma^2 & \\sim  \\mathcal{N}GI(\\theta,\\tau,\\alpha,\\beta)\n\\end{aligned}\n\\]\n\\(\\mu\\) y \\(\\sigma^2\\) tienen distribución conjunta normal-gamma-inversa:\n\\[\np(\\mu,\\sigma^2 \\mid \\theta, \\tau, \\alpha, \\beta ) =\n  \\frac{\\sqrt{\\tau}}{\\sqrt{2\\pi\\sigma^2}}\n  \\frac{\\beta^\\alpha}{\\Gamma(\\alpha)}\n  \\left( \\frac{1}{\\sigma^2} \\right)^{\\alpha+1}\n  e^{-\\frac{2\\beta + \\tau(\\mu-\\theta)^2}{2\\sigma^2}}\n\\]\nSi anticipamos que la normal-gamma-inversa es conjugada de la normal (para los parámetros \\(\\mu\\) y \\(\\sigma^2\\)), ¿qué podemos decir de la distribución a posteriori (conjunta) de \\(\\mu\\) y \\(\\sigma^2\\)"
  },
  {
    "objectID": "presentaciones/presentacion_03.html#section-21",
    "href": "presentaciones/presentacion_03.html#section-21",
    "title": "Estadística Bayesiana",
    "section": "",
    "text": "Efectivamente, se puede probar que:\n\\[\n\\begin{aligned}\n\\mu,\\sigma^2 \\mid \\mathbf{y} & \\sim  \\mathcal{N}GI(\\theta_n,\\tau_n,\\alpha_n,\\beta_n)\n\\end{aligned}\n\\] con\n\\[\n\\begin{cases}\n\\theta_n = \\frac{\\tau\\theta + n \\bar{y}}{\\tau+n}\\\\\n\\tau_n = \\tau + n\\\\\n\\alpha_n = \\alpha + \\frac{n}{2}\\\\\n\\beta_n = \\beta + \\frac{1}{2} \\sum_i (y_i - \\bar{y})^2 + \\frac{n\\tau}{\\tau + n} \\frac{(\\bar{y}-\\theta)^2}{2}\n\\end{cases}\n\\]\n¿Parámetros desconocidos en la verosimilitud? ¿Dimensión del espacio de parámetros? ¿Constantes de ajuste del prior? ¿Forma del posterior?"
  },
  {
    "objectID": "presentaciones/presentacion_03.html#section-22",
    "href": "presentaciones/presentacion_03.html#section-22",
    "title": "Estadística Bayesiana",
    "section": "",
    "text": "Reflexionemos…\n\\[\n\\begin{aligned}\n    Y_i \\mid\\mu & \\sim  \\mathcal{N}(\\mu,\\sigma^2) \\\\\n    \\mu & \\sim  \\mathcal{N}(\\theta,\\tau^2) \\\\\n    \\mu \\mid \\mathbf{y} & \\sim \\mathcal{N}(\\theta_n,\\tau_n^2)\n\\end{aligned}\n\\]\n¿Parámetros desconocidos en la verosimilitud? ¿Dimensión y característica del espacio de parámetros? ¿Constantes de ajuste del prior? ¿Forma del posterior? ¿Qué son \\(\\theta_n\\) y \\(\\tau_n^2\\)?"
  },
  {
    "objectID": "presentaciones/presentacion_03.html#section-24",
    "href": "presentaciones/presentacion_03.html#section-24",
    "title": "Estadística Bayesiana",
    "section": "",
    "text": "¿Puedo representar los datos en el gráfico de la izquierda?\n\nNo, es el mundo de los parámetros\n\n\n¿Qué representan los valores marcados con \\(\\mathbf{\\times}\\)?\n\nPosibles valores de \\(\\mu\\) y \\(\\sigma^2\\) que podrían esperarse a priori.\n\n\n\n¿Qué le da forma a la distribución de la izquierda?\n\n\\(\\theta\\), \\(\\tau\\), \\(\\alpha\\) y \\(\\beta\\)\n\n\n\n¿Media y varianza de las normales de la derecha?\n\n\\(\\mu\\) y \\(\\sigma^2\\)"
  },
  {
    "objectID": "presentaciones/presentacion_03.html#section-25",
    "href": "presentaciones/presentacion_03.html#section-25",
    "title": "Estadística Bayesiana",
    "section": "",
    "text": "¿Qué estamos viendo?"
  },
  {
    "objectID": "presentaciones/presentacion_03.html#section-26",
    "href": "presentaciones/presentacion_03.html#section-26",
    "title": "Estadística Bayesiana",
    "section": "",
    "text": "A posteriori (luego de observar los datos)… ¿qué ocurre con la plausibilidad de los valores de \\(\\mu\\) y \\(\\sigma^2\\)?"
  },
  {
    "objectID": "presentaciones/presentacion_03.html#section-27",
    "href": "presentaciones/presentacion_03.html#section-27",
    "title": "Estadística Bayesiana",
    "section": "",
    "text": "¿Parámetros de la distribución de la izquierda?\n\n\\(\\theta_n\\), \\(\\tau_n\\), \\(\\alpha_n\\) y \\(\\beta_n\\)\n\n\n¿Media y varianza de las normales de la derecha?\n\n\\(\\mu\\) y \\(\\sigma^2\\)\n\n\n\n\n\nEstadística Bayesiana – 2025"
  },
  {
    "objectID": "presentaciones/presentacion_01.html#lógica-deductiva",
    "href": "presentaciones/presentacion_01.html#lógica-deductiva",
    "title": "Estadística Bayesiana",
    "section": "Lógica deductiva",
    "text": "Lógica deductiva\n\\[A \\Rightarrow B\\] \\(A\\) es verdadero, por lo tanto \\(B\\) es verdadero\n\\(B\\) es falso, por lo tanto \\(A\\) falso\n\n\\(A\\): Tom es un gato\n\\(B\\): Tom es un animal\n\n\\(B\\) es verdadero, por lo tanto…"
  },
  {
    "objectID": "presentaciones/presentacion_01.html#section",
    "href": "presentaciones/presentacion_01.html#section",
    "title": "Estadística Bayesiana",
    "section": "",
    "text": "Pero este no es el tipo de razonamiento que utilizamos en la vida cotidiana:\n\n\\(A\\): va a llover a las 10 de la mañana\n\\(B\\): se nubla antes de las 10 de la mañana\n\n\\(B\\) es verdadero, por lo tanto \\(A\\) se vuelve más plausible"
  },
  {
    "objectID": "presentaciones/presentacion_01.html#section-1",
    "href": "presentaciones/presentacion_01.html#section-1",
    "title": "Estadística Bayesiana",
    "section": "",
    "text": "En una noche oscura, un policía camina por una calle aparentemente desierta. De repente, se escucha la alarma de un local. Se da vuelta y ve, en la vereda de enfrente, una joyería con la vidriera rota. Un hombre con una máscara sale agachado a través del vidrio roto, con una bolsa llena de joyas caras. El policía no duda en concluir que el hombre no tiene buenas intenciones.\n\nEl razonamiento del policía no fue una deducción lógica, ya que podría existir una explicación alternativa para lo ocurrido.\n\nDada la evidencia, no podemos decir con seguridad que las intenciones del hombre no son buenas, pero sí que es extremadamente plausible que no lo sean."
  },
  {
    "objectID": "presentaciones/presentacion_01.html#razonamiento-plausible",
    "href": "presentaciones/presentacion_01.html#razonamiento-plausible",
    "title": "Estadística Bayesiana",
    "section": "Razonamiento plausible",
    "text": "Razonamiento plausible\nEl cerebro humano permanentemente determina si algo se vuelve más o menos plausible. Más aún, de alguna manera, evalúa el grado de plausibilidad de una proposición.\n\n\nLa plausibilidad de que llueva a las 10 de la mañana depende fuertemente de la oscuridad de las nubes a las 9:45.\n\n\n\nEste razonamiento hace uso de nuestra experiencia previa. Combina información a priori con evidencia disponible. Esto da lugar a un proceso secuencial."
  },
  {
    "objectID": "presentaciones/presentacion_01.html#apuestas",
    "href": "presentaciones/presentacion_01.html#apuestas",
    "title": "Estadística Bayesiana",
    "section": "Apuestas",
    "text": "Apuestas\n\n\n\nPáguese $1000 al portador de esta tarjeta si en este grupo hay alguien que tiene un loro como mascota\n\n\n\n\n\n\nPáguese $1000 al portador de esta tarjeta si en este grupo nadie tiene un loro como mascota\n\n\n\n\nTienen a su disposición estas tarjetas. Podemos comprarlas o venderlas. Al final de la clase develamos el misterio y, quien tenga la tarjeta, cobra.\n\n\n¿Por cuál pagarían más? ¿Cuánto estarían dispuestos a pagar como máximo?\n\n\nNotar que el precio máximo que estarían dispuestos a pagar para comprarla es el precio mínimo por el que estarían dispuestos a venderla."
  },
  {
    "objectID": "presentaciones/presentacion_01.html#section-2",
    "href": "presentaciones/presentacion_01.html#section-2",
    "title": "Estadística Bayesiana",
    "section": "",
    "text": "Todos pagaríamos \\(p\\cdot\\$ 1000\\) con \\(0 \\leq p \\leq 1\\).\n\nDecidimos cuánto apostar en función de nuestra incertidumbre en la ocurrencia de un evento (de lo plausible que lo consideremos). Decidimos apostar \\(p\\cdot\\$ 1000\\) en favor de un evento, porque le asignamos una plausibilidad o credibilidad de grado \\(p\\)."
  },
  {
    "objectID": "presentaciones/presentacion_01.html#section-3",
    "href": "presentaciones/presentacion_01.html#section-3",
    "title": "Estadística Bayesiana",
    "section": "",
    "text": "Páguese $1000 al portador de esta tarjeta si el profe tiene una remera negra\n\n\n\n¿Cuánto están dispuestos a pagar para tener esta tarjeta? ¿Por cuánto venderían la tarjeta si la tuvieran?"
  },
  {
    "objectID": "presentaciones/presentacion_01.html#section-4",
    "href": "presentaciones/presentacion_01.html#section-4",
    "title": "Estadística Bayesiana",
    "section": "",
    "text": "Páguese $1000 al portador de esta tarjeta si esta materia es la mejor del cuatrimestre\n\n\n\n\n\n\nPáguese $1000 al portador de esta tarjeta si esta materia no es la mejor del cuatrimestre\n\n\n\n\nPor la primera pagarían como máximo \\(p\\cdot\\$ 1000\\) y por la segunda, \\(q\\cdot\\$ 1000\\). Es necesario que \\(p+q=1\\). ¿Por qué?"
  },
  {
    "objectID": "presentaciones/presentacion_01.html#dutch-book",
    "href": "presentaciones/presentacion_01.html#dutch-book",
    "title": "Estadística Bayesiana",
    "section": "Dutch book",
    "text": "Dutch book\nSupongamos que \\(p=0.7\\) y \\(q=0.5\\). Eso significa que:\n\nSi no tienen las tarjetas, estarían dispuestos a comprar ambas por \\(\\$1200\\).\n\nSupongamos que \\(p=0.3\\) y \\(q=0.2\\). Eso significa que:\n\nSi tienen las tarjetas, estarían dispuestos a vender ambas por \\(\\$500\\).\n\n\nSabemos que a fin de cuatrimestre, quien tenga las dos tarjetas ganará \\(\\$1000\\)…\n\nThe canonical way to measure degrees of belief appeals to the notion of fair odds.\nThe degree of belief that a given epistemic agent—let’s say it’s you—has in this proposition A can be determined by what you deem to be the fair price of this lottery. Here the “fair price” is the price at which you are willing to either buy or sell the lottery ticket."
  },
  {
    "objectID": "presentaciones/presentacion_01.html#dutch-book-1",
    "href": "presentaciones/presentacion_01.html#dutch-book-1",
    "title": "Estadística Bayesiana",
    "section": "Dutch book",
    "text": "Dutch book\n\n\n\nDutch book\n\n\nUn Dutch book es un conjunto de apuestas que aseguran una pérdida. El argumento del Dutch book dice que una persona que tiene creencias inconsistentes actúa irracionalmente y puede ser llevado a una pérdida segura en un juego de apuestas\n\n\n\n\nLos grados de plausibilidad o grados de creencia que una persona le asigna a un conjunto de eventos deben respetar los axiomas de probabilidad.\n\n\nSe puede asignar un valor de probabilidad a cualquier proposición."
  },
  {
    "objectID": "presentaciones/presentacion_01.html#section-5",
    "href": "presentaciones/presentacion_01.html#section-5",
    "title": "Estadística Bayesiana",
    "section": "",
    "text": "Las probabilidades son la mejor herramienta disponible para cuantificar la incertidumbre y las leyes de la probabilidad, la mejor herramienta para operar con ella."
  },
  {
    "objectID": "presentaciones/presentacion_01.html#probabilidad-1",
    "href": "presentaciones/presentacion_01.html#probabilidad-1",
    "title": "Estadística Bayesiana",
    "section": "Probabilidad",
    "text": "Probabilidad\nTres ideas de probabilidad\n\nClásica: si \\(n\\) eventos son equiprobables, la probabilidad de uno de ellos es \\(1/n\\). Además, la probabilidad de un evento se puede calcular como el número de casos favorables dividido el número de casos posibles.\nFrecuentista: la probabilidad de un evento se puede estimar observando su frecuencia relativa sobre un gran número de realizaciones o ensayos.\nSubjetiva: las probabilidades reflejan el grado de creencia o plausibilidad que una persona le asigna a un evento."
  },
  {
    "objectID": "presentaciones/presentacion_01.html#probabilidad-subjetiva",
    "href": "presentaciones/presentacion_01.html#probabilidad-subjetiva",
    "title": "Estadística Bayesiana",
    "section": "Probabilidad subjetiva",
    "text": "Probabilidad subjetiva\n\nEs la forma más general de interpretar la probabilidad (eventos no equiprobables y eventos que no pueden repetirse)\nSe utiliza para cuantificar la incertidumbre o ignorancia (o certidumbre o conocimiento) acerca de un evento o proposición\nEs personal\nDepende del estado actual de conocimiento del mundo\n\n\nTodos los métodos estadísticos son subjetivos en el sentido que se basan en idealizaciones matemáticas de la realidad (modelos)."
  },
  {
    "objectID": "presentaciones/presentacion_01.html#incertidumbre",
    "href": "presentaciones/presentacion_01.html#incertidumbre",
    "title": "Estadística Bayesiana",
    "section": "Incertidumbre",
    "text": "Incertidumbre\nDistinguimos dos tipos de incertidumbre:\n\n\nIncertidumbre epistémica\nIncertidumbre aleatoria\n\n\n\nLo retomaremos a lo largo del curso."
  },
  {
    "objectID": "presentaciones/presentacion_01.html#elicitación-de-probabilidades",
    "href": "presentaciones/presentacion_01.html#elicitación-de-probabilidades",
    "title": "Estadística Bayesiana",
    "section": "Elicitación de probabilidades",
    "text": "Elicitación de probabilidades\nConsideremos la siguiente proposición:\n\nVoy a aprobar todas las materias de este cuatrimestre (\\(W\\))\n\n\nUna caja con 5 bolas azules y 5 bolas rojas. Se extrae una bola al azar. \\(A\\) es el evento extraer una bola azul\n\n\n\n\\(A_1\\): $1000 si \\(W\\)\n\\(A_2\\): $1000 si \\(A\\)\n\n\n\nSi prefieren \\(A_1\\) entonces… 8 bolas azules y 2 bolas rojas. Se extrae una bola al azar. \\(A\\) es el evento extraer una bola azul.\n\n\n\n\\(A_3\\): $1000 si \\(W\\)\n\\(A_4\\): $1000 si \\(A\\)"
  },
  {
    "objectID": "presentaciones/presentacion_01.html#section-6",
    "href": "presentaciones/presentacion_01.html#section-6",
    "title": "Estadística Bayesiana",
    "section": "",
    "text": "Interludio…\n\n¿Qué es más probable?\n\nQue el PSG le gane al Lyon\nQue el PSG le gane al Lyon y Messi haga un gol"
  },
  {
    "objectID": "presentaciones/presentacion_01.html#sesgos",
    "href": "presentaciones/presentacion_01.html#sesgos",
    "title": "Estadística Bayesiana",
    "section": "Sesgos",
    "text": "Sesgos\nLos seres humanos no estamos optimizados para operar con probabilidades (al menos no intuitivamente)."
  },
  {
    "objectID": "presentaciones/presentacion_01.html#section-7",
    "href": "presentaciones/presentacion_01.html#section-7",
    "title": "Estadística Bayesiana",
    "section": "",
    "text": "Probabilidad de un evento \\[\\mathrm{Pr}(A)\\] \\[\\mathrm{Pr}(\\bar{A}) = 1-\\mathrm{Pr}(A)\\]"
  },
  {
    "objectID": "presentaciones/presentacion_01.html#section-8",
    "href": "presentaciones/presentacion_01.html#section-8",
    "title": "Estadística Bayesiana",
    "section": "",
    "text": "Probabilidad de la conjunción:\n\\[\\mathrm{Pr}(A\\wedge B) = \\mathrm{Pr}(A,B)\\] Si \\(A\\) y \\(B\\) son independientes, entonces\n\\[\\mathrm{Pr}(A\\wedge B) = \\mathrm{Pr}(A)\\mathrm{Pr}(B)\\] Probabilidad de la unión: \\[\\mathrm{Pr}(A \\vee B) = \\mathrm{Pr}(A) + \\mathrm{Pr}(B) - \\mathrm{Pr}(A,B)\\] Donde, si \\(A\\) y \\(B\\) son mutuamente excluyentes,\n\\[\\mathrm{Pr}(A \\vee B) = \\mathrm{Pr}(A) + \\mathrm{Pr}(B)\\]"
  },
  {
    "objectID": "presentaciones/presentacion_01.html#section-9",
    "href": "presentaciones/presentacion_01.html#section-9",
    "title": "Estadística Bayesiana",
    "section": "",
    "text": "\\[\\mathrm{Pr}(B\\mid A) = \\frac{\\mathrm{Pr}(A,B)}{\\mathrm{Pr}(A)} \\] siempre que \\(\\mathrm{Pr}(A)&gt;0\\) (no se puede condicionar a eventos imposibles)"
  },
  {
    "objectID": "presentaciones/presentacion_01.html#variables-aleatorias",
    "href": "presentaciones/presentacion_01.html#variables-aleatorias",
    "title": "Estadística Bayesiana",
    "section": "Variables aleatorias",
    "text": "Variables aleatorias\nUna variable aleatoria (univariada) \\(X\\) es una función que mapea elementos del espacio muestral \\(\\mathcal{X}\\) a la recta real \\(\\mathbb{R}\\)\n\nSi \\(\\mathcal{X}\\) es finito o infinito numerable entonces \\(X\\) es una variable aleatoria discreta\nSi \\(\\mathcal{X}\\) es cualquier valor en \\(\\mathbb{R}\\) entonces \\(X\\) es una variable aleatoria continua"
  },
  {
    "objectID": "presentaciones/presentacion_01.html#section-10",
    "href": "presentaciones/presentacion_01.html#section-10",
    "title": "Estadística Bayesiana",
    "section": "",
    "text": "Para el caso discreto: \\[p(x) = \\mathrm{Pr}(X=x) \\quad \\text{(pmf)}\\]\nPara el caso continuo: \\[P(x) = \\mathrm{Pr}(X\\leq x) \\quad \\text{(cdf)}\\] \\[ p(x) = \\frac{d}{dx}P(x) \\quad \\text{(pdf)}\\]\n\\[\\mathrm{Pr}(x\\leq X\\leq x+dx) = p(x)dx\\]"
  },
  {
    "objectID": "presentaciones/presentacion_01.html#distribuciones-conjuntas",
    "href": "presentaciones/presentacion_01.html#distribuciones-conjuntas",
    "title": "Estadística Bayesiana",
    "section": "Distribuciones conjuntas",
    "text": "Distribuciones conjuntas\nCaso discreto\n\\[\n\\begin{array}{c|cc}\np(X,Y) & Y=0 & Y=1 \\\\\n\\hline\nX=0 & 0.2 & 0.3 \\\\\nX=1 & 0.3 & 0.2 \\\\\n\\end{array}\n\\]"
  },
  {
    "objectID": "presentaciones/presentacion_01.html#section-11",
    "href": "presentaciones/presentacion_01.html#section-11",
    "title": "Estadística Bayesiana",
    "section": "",
    "text": "Distribución marginal\n\\[p(x)=\\sum_y p(x,y)\\]\n\\[p(y)=\\sum_x p(x,y)\\]\nSe conoce como marginalizar (en inglés también integrate out)"
  },
  {
    "objectID": "presentaciones/presentacion_01.html#section-12",
    "href": "presentaciones/presentacion_01.html#section-12",
    "title": "Estadística Bayesiana",
    "section": "",
    "text": "Caso continuo\n\\[p(x,y)\\]"
  },
  {
    "objectID": "presentaciones/presentacion_01.html#section-13",
    "href": "presentaciones/presentacion_01.html#section-13",
    "title": "Estadística Bayesiana",
    "section": "",
    "text": "Distribución marginal\n\\[p(x)=\\int p(x,y) dy\\]\n\\[p(y)=\\int p(x,y) dx\\]"
  },
  {
    "objectID": "presentaciones/presentacion_01.html#distribución-condicional",
    "href": "presentaciones/presentacion_01.html#distribución-condicional",
    "title": "Estadística Bayesiana",
    "section": "Distribución condicional",
    "text": "Distribución condicional\n\\[p(x \\mid y) = \\frac{p(x,y)}{p(y)}\\]\n\\[p(y \\mid x) = \\frac{p(x,y)}{p(x)}\\]\n\\(p(x)\\) normaliza a \\(p(x,y)\\) (una función de \\(y\\) ya que \\(x\\) tomó un valor fijo).\n\nimagine a circular dart board, split into 20 equal sections, labelled from 1 to 20. Randy, a dart thrower, hits any one of the 20 sections uniformly at random. Hence the probability that a dart thrown by Randy occurs in any one of the 20 regions is p(region i) = 1=20. A friend of Randy tells him that he hasn’t hit the 20 region. What is the probability that Randy has hit the 5 region?"
  },
  {
    "objectID": "presentaciones/presentacion_01.html#regla-del-producto",
    "href": "presentaciones/presentacion_01.html#regla-del-producto",
    "title": "Estadística Bayesiana",
    "section": "Regla del producto",
    "text": "Regla del producto\nTambién conocida como regla de la cadena. Recobramos la distribución conjunta haciendo\n\\[p(x,y) = p(x\\mid y) p(y)\\]\n\\[p(x,y) = p(y\\mid x) p(x)\\]\n\\[p(x,y,z) = p(z) p(y\\mid z) p(x\\mid y,z)\\]"
  },
  {
    "objectID": "presentaciones/presentacion_01.html#regla-de-la-probabilidad-total",
    "href": "presentaciones/presentacion_01.html#regla-de-la-probabilidad-total",
    "title": "Estadística Bayesiana",
    "section": "Regla de la probabilidad total",
    "text": "Regla de la probabilidad total\n\\[p(x) = \\int p(x\\mid y) p(y) dy\\] \\[p(y) = \\int p(y\\mid x) p(x) dy\\]\n\nLa probabilidad marginal de \\(x\\) (una función de \\(x\\)) se obtiene ponderando todos los posibles \\(p(x\\mid y)\\) (una función de \\(x\\) para cada valor de \\(y\\)) según la probabilidad de \\(p(y)\\). Y viceversa."
  },
  {
    "objectID": "presentaciones/presentacion_01.html#regla-de-bayes",
    "href": "presentaciones/presentacion_01.html#regla-de-bayes",
    "title": "Estadística Bayesiana",
    "section": "Regla de Bayes",
    "text": "Regla de Bayes\n\\[p(x\\mid y) = \\frac{p(y\\mid x) p(x)}{p(y)}\\]\n\nAsí expresada no nos dice mucho.\n\n\nRecordemos que utilizamos las probabilidades para expresar nuestra incertidumbre. La mejor forma de actualizar nuestro grado de creencia sobre alguna hipótesis \\(\\mathcal{H}\\) frente a nueva información \\(E\\) es utilizar la Regla de Bayes.\n\\[p(\\mathcal{H}\\mid E) = \\frac{p(E\\mid\\mathcal{H}) p(\\mathcal{H})}{p(E)}\\]"
  },
  {
    "objectID": "presentaciones/presentacion_01.html#orígenes-de-la-regla-de-bayes",
    "href": "presentaciones/presentacion_01.html#orígenes-de-la-regla-de-bayes",
    "title": "Estadística Bayesiana",
    "section": "Orígenes de la Regla de Bayes",
    "text": "Orígenes de la Regla de Bayes\n\n\n\nAlrededor de 1740, Thomas Bayes propone una versión de la regla pero no la publica (¿su descubrimiento era inútil? ¿era muy modesto?). Propuso el experimento imaginario de un juego con bolitas. Asignó iguales probabilidades a priori\n\n\n\nRichard Price publicó el resultado del Teorema de la Probabilidad Inversa de Bayes en An Essay Towards Solving a Problem in the Doctrine of Chances (1763)\n\n\n\nPierre-Simon Laplace llegó al mismo resultado que Bayes (algo que llamó la probabilidad de las causas) y lo publicó en Memoire sur la Probabilité des Causes par les Évenements (1774). Se asemeja más a lo que hoy conocemos. Reconoció que Bayes había descubierto algo similar."
  },
  {
    "objectID": "presentaciones/presentacion_01.html#section-14",
    "href": "presentaciones/presentacion_01.html#section-14",
    "title": "Estadística Bayesiana",
    "section": "",
    "text": "Bayes’s rule is a mistake, perhaps the only mistake to which the mathematical world has so deeply committed itself (Fisher, ~1920)\n\n\n\nBayes’s theorem is to the theory of probability what Pythagoras’s theorem is to geometry (Savage, ~1950)"
  },
  {
    "objectID": "presentaciones/presentacion_01.html#ejemplos",
    "href": "presentaciones/presentacion_01.html#ejemplos",
    "title": "Estadística Bayesiana",
    "section": "Ejemplos",
    "text": "Ejemplos\nVamos a trabajar con un conjunto de ejemplos que consisten en la aplicación de la regla de Bayes, acercándonos de a poco a forma en la que se usa en la estadística bayesiana."
  },
  {
    "objectID": "presentaciones/presentacion_01.html#ejemplo-1",
    "href": "presentaciones/presentacion_01.html#ejemplo-1",
    "title": "Estadística Bayesiana",
    "section": "Ejemplo 1",
    "text": "Ejemplo 1\nNos encontramos con alguien en la calle y nos dice que tiene dos hijos. Le preguntamos si alguno de ellos es mujer y nos responde que sí. ¿Cuál es la probabilidad de que tenga dos niñas?"
  },
  {
    "objectID": "presentaciones/presentacion_01.html#section-15",
    "href": "presentaciones/presentacion_01.html#section-15",
    "title": "Estadística Bayesiana",
    "section": "",
    "text": "¿Cómo lo escribimos con símbolos?"
  },
  {
    "objectID": "presentaciones/presentacion_01.html#ejemplo-2",
    "href": "presentaciones/presentacion_01.html#ejemplo-2",
    "title": "Estadística Bayesiana",
    "section": "Ejemplo 2",
    "text": "Ejemplo 2\nUn taxi se vio involucrado en una accidente nocturno y se dio a la fuga. En la ciudad hay dos empresas de taxis, la Verde y la Azul. Sobre el accidente se tienen los siguientes datos:\n\n85% de los taxis de la ciudad son de la empresa Verde y 15% de la Azul\nUn testigo identificó el taxi como azul. La corte evaluó la confiabilidad del testigo en las circunstancias del accidente y concluyó que es capaz de identificar correctamente el color en un 80% de los casos.\n\n\n¿Cuál es la probabilidad de que el taxi haya sido azul, de acuerdo a la declaración del testigo?"
  },
  {
    "objectID": "presentaciones/presentacion_01.html#section-16",
    "href": "presentaciones/presentacion_01.html#section-16",
    "title": "Estadística Bayesiana",
    "section": "",
    "text": "\\[p(A\\mid T_A) = \\frac{p(T_A\\mid A) p(A)}{p(T_A)}\\]\n\n\\[p(A\\mid T_A) = \\frac{p(T_A\\mid A) p(A)}{p(T_A\\mid A)p(A) + p(T_A\\mid V)p(V)}\\]\n\n\n\\[P(A\\mid T_A) = \\frac{0.80\\cdot 0.15}{0.80\\cdot 0.15 + 0.2\\cdot 0.85}\\]\n\\[p(A\\mid T_A) = 0.414\\]"
  },
  {
    "objectID": "presentaciones/presentacion_01.html#ejemplo-3",
    "href": "presentaciones/presentacion_01.html#ejemplo-3",
    "title": "Estadística Bayesiana",
    "section": "Ejemplo 3",
    "text": "Ejemplo 3\nSe realiza un test de hipótesis que tiene una potencia \\(1-\\beta = 80\\%\\). Se fija el nivel de significación en \\(\\alpha = 5\\%\\). Se testea \\(H_0\\) versus una hipótesis alternativa \\(H_1:\\text{ no }H_0\\).\n\nSi se supone que la probabilidad de que \\(H_0\\) sea cierta es de \\(50\\%\\), ¿cuál es la probabilidad de que \\(H_1\\) sea cierta luego de observar un resultado estadísticamente significativo?\nSi la hipótesis alternativa es muy rara (digamos \\(10\\%\\)), ¿cuál es la probabilidad de que \\(H_1\\) sea cierta luego de observar un resultado estadísticamente significativo?"
  },
  {
    "objectID": "presentaciones/presentacion_01.html#section-18",
    "href": "presentaciones/presentacion_01.html#section-18",
    "title": "Estadística Bayesiana",
    "section": "",
    "text": "\\[p(H_1 \\mid \\text{rechazo }H_0) = \\frac{p(\\text{rechazo }H_0 \\mid H_1)p(H_1)}{p(\\text{rechazo }H_0)}\\]\n\nSi en el denominador enumeramos exhaustivamente las formas de rechazar \\(H_0\\):\n\n\n\\[p(H_1 \\mid \\text{rechazo }H_0) = \\frac{(1-\\beta)p(H_1)}{\\alpha p(H_0) + (1-\\beta) p(H_1)}\\]\nPara el primer caso: \\(p(H_1 \\mid \\text{rechazo }H_0) = \\frac{{0.80}\\ {0.50}}{{0.05}\\ {0.50} + {0.80}\\ {0.50}} = {0.94}\\)\nPara el segundo caso: \\(p(H_1 \\mid \\text{rechazo }H_0) = \\frac{{0.80}\\ {0.10}}{{0.05}\\ {0.90} + {0.80}\\ {0.10}} = {0.64}\\)"
  },
  {
    "objectID": "presentaciones/presentacion_01.html#el-problema-de-las-urnas",
    "href": "presentaciones/presentacion_01.html#el-problema-de-las-urnas",
    "title": "Estadística Bayesiana",
    "section": "El problema de las urnas",
    "text": "El problema de las urnas\n\nSe cuenta con 11 urnas etiquetadas según \\(u = 0,1,\\dots,10\\), que contienen diez bolas cada una. La urna \\(u\\) contiene \\(u\\) bolas azules y \\(10-u\\) bolas blancas. Fede elige una urna \\(u\\) al azar y extrae con reposición \\(N\\) bolas, obteniendo \\(n_A\\) azules y \\(N-n_A\\) blancas. Nico, el amigo de Fede, observa atentamente. Si después de \\(N=10\\) extracciones resulta \\(n_A = 3\\), ¿cuál es la probabilidad de que la urna que Fede está usando sea la \\(u\\)?"
  },
  {
    "objectID": "presentaciones/presentacion_01.html#section-19",
    "href": "presentaciones/presentacion_01.html#section-19",
    "title": "Estadística Bayesiana",
    "section": "",
    "text": "La teoría de las probabilidades permite predecir una distribución sobre posibles valores de un resultado dado cierto conocimiento (o estado) del universo: probabilidad hacia adelante\n\nPor el contrario, muchas veces estamos interesados en realizar inferencias sobre el estado del universo a partir de observaciones: probabilidad inversa.\n\n\n\\[p(\\mathcal{H}\\mid E) = \\frac{p(E\\mid\\mathcal{H}) p(\\mathcal{H})}{p(E)}\\]\n\\[p(\\mathcal{H}\\mid E) \\propto p(E\\mid\\mathcal{H}) p(\\mathcal{H})\\]\n\n\n\n\nEstadística Bayesiana – 2025"
  },
  {
    "objectID": "presentaciones/stan.html#qué-es-stan",
    "href": "presentaciones/stan.html#qué-es-stan",
    "title": "Estadística Bayesiana",
    "section": "¿Qué es Stan?",
    "text": "¿Qué es Stan?\n\nStan es un lenguaje escrito en C++ para realizar Inferencia Bayesiana en utilizando probabilísticos.\nEl programa es de código abierto, todo lo relacionado a Stan se puede encontrar en http://mc-stan.org/ junto con un manual de usuario instrucciones de uso.\nLa primera versión, Stan 1.0, se lanzó en 2012 y actualmente el programa ya está en la versión XXX\nSe puede llamar a Stan desde R usando el paquete {rstan}, o desde Python usando el paquete pystan. Stan cuenta también cuenta con interfaces a otros lenguajes."
  },
  {
    "objectID": "presentaciones/stan.html#instalación-de-stan-y-rstan",
    "href": "presentaciones/stan.html#instalación-de-stan-y-rstan",
    "title": "Estadística Bayesiana",
    "section": "Instalación de Stan y RStan",
    "text": "Instalación de Stan y RStan\n\nEn Windows (o Mac), escriba en la consola R: install.packages(”rstan”, repositorios = c(”https://mc-stan.org/r-packages/”, getOption(”repos”)))\nEn Linux recomiendo instalar vía terminal sudo add-apt-repository ppa:c2d4u.team/c2d4u4.0+     sudo apt install –no-install-recomienda r-cran-rstan\nCualquier problema de instalación verifique: https://github.com/stan-dev/rstan/wiki/RStan-Getting-Started"
  },
  {
    "objectID": "presentaciones/stan.html#elementos-de-un-programa-de-stan",
    "href": "presentaciones/stan.html#elementos-de-un-programa-de-stan",
    "title": "Estadística Bayesiana",
    "section": "Elementos de un programa de Stan",
    "text": "Elementos de un programa de Stan\nEn Stan definimos un modelo probabilístico utilizando el Idioma Stan.\n\nEl modelo se define intuitivamente pero debe seguir algunas normas.\nUn programa en Stan se define en bloques, son:\n\nDatos.\nParámetros.\nParámetros transformados.\nModelo."
  },
  {
    "objectID": "presentaciones/stan.html#datos",
    "href": "presentaciones/stan.html#datos",
    "title": "Estadística Bayesiana",
    "section": "Datos",
    "text": "Datos\ndata {\n    int&lt;lower=1&gt; N; // Cantidad de observaciones\n    int&lt;lower=1&gt; K; // Cantidad de variables\n    matrix[N, K] X; // Matriz de diseño\n    vector[N] y;    // Vector de respuestas\n}\n\nEn este bloque definimos qué datos, y de qué tipo, serán utilizado en el modelo.\nTenemos que declarar el tipo y dimensión de los datos.\nAdemás de los datos, también puede haber otras constantes, tamaño de la muestra y número de predictores.\nLos tipos más comunes son: int, real, matriz, vector"
  },
  {
    "objectID": "presentaciones/stan.html#parámetros",
    "href": "presentaciones/stan.html#parámetros",
    "title": "Estadística Bayesiana",
    "section": "Parámetros",
    "text": "Parámetros\nparameters {\n    vector[K] beta;\n    real&lt;lower=0&gt; sigma;\n}\n\nLos parámetros del modelo a estimar se declaran en el bloque de parámetros.\nDebemos imponer las restricciones necesarias a los parámetros.\nLos parámetros generalmente se definen como vectores (números reales) o uno real."
  },
  {
    "objectID": "presentaciones/stan.html#modelo",
    "href": "presentaciones/stan.html#modelo",
    "title": "Estadística Bayesiana",
    "section": "Modelo",
    "text": "Modelo\nmodel {\n    vector[N] mu;\n    mu = X * beta;\n    \n    // priors\n    beta ~ normal(0, 10);\n    sigma ~ cauchy(0, 5);\n\n    // likelihood\n    y ~ normal(mu, sigma);\n}\n\nEl bloque modelo es donde definimos las distribuciones previas y la verosimilitud del modelo.\nTambién podemos declarar algunas variables que no son de nuestro interés pero facilita la escritura del modelo."
  },
  {
    "objectID": "presentaciones/stan.html#section",
    "href": "presentaciones/stan.html#section",
    "title": "Estadística Bayesiana",
    "section": "",
    "text": "Stan tiene muchas distribuciones de probabilidad ya definidas. Pero también existe la posibilidad de que el usuario defina su propia distribución.\nEs muy recomendable escribir el modelo de forma matricial.\n\nTODO agregar ejemplos, parametrizaciones, y enlaces a la documentación"
  },
  {
    "objectID": "presentaciones/stan.html#datos-de-tipo-escalar",
    "href": "presentaciones/stan.html#datos-de-tipo-escalar",
    "title": "Estadística Bayesiana",
    "section": "Datos de tipo escalar",
    "text": "Datos de tipo escalar\nEnteros y reales sin restricciones:\nint N;\nreal theta;\nEnteros y reales con restricciones:\nint&lt;lower = 1&gt; N;\nreal&lt;lower = 0&gt; sigma;\nreal&lt;lower =-1, upper = 1&gt; rho;"
  },
  {
    "objectID": "presentaciones/stan.html#vectores-y-matrices",
    "href": "presentaciones/stan.html#vectores-y-matrices",
    "title": "Estadística Bayesiana",
    "section": "Vectores y matrices",
    "text": "Vectores y matrices\nLos vectores en Stan son de tipo columna y se declaran junto con la dimensión:\nvector[3] u;\nTambien pueden tener restricciones\nvector&lt;lower = 0&gt;[3] u;\nLas matrices se declaran junto a su número de filas y columnas\nmatrix[3, 3] A;\nmatrix&lt;upper=0&gt;[3, 4] B;"
  },
  {
    "objectID": "presentaciones/stan.html#tipos-especiales-de-matrices-y-vectores",
    "href": "presentaciones/stan.html#tipos-especiales-de-matrices-y-vectores",
    "title": "Estadística Bayesiana",
    "section": "Tipos especiales de matrices y vectores",
    "text": "Tipos especiales de matrices y vectores\nVector unitario simplex (suma de elementos es igual a 1)\nsimplex[5] theta;\nVector ordenado o positivo y ordenado\nordered[5] c;\npositive_ordered[5] d;\nVector fila\nrow_vector[3] u;\nMatriz de covarianza\ncov_matrix[K] Omega;\nMatriz de correlación\ncorr_matrix[3] Sigma;"
  },
  {
    "objectID": "presentaciones/stan.html#ejemplos",
    "href": "presentaciones/stan.html#ejemplos",
    "title": "Estadística Bayesiana",
    "section": "Ejemplos",
    "text": "Ejemplos\n\nModelo Bernoulli\nModleo binomial\nModelo Poisson\nModelo con prior uniforme\nModelo de regresión lineal\nModelo de regresión logistica\n\n\n\n\n\nEstadística Bayesiana – 2025"
  },
  {
    "objectID": "presentaciones/presentacion_04.html#section",
    "href": "presentaciones/presentacion_04.html#section",
    "title": "Estadística Bayesiana",
    "section": "",
    "text": "Un meteorólogo estima, con un 95% de confianza, que la probabilidad de que el huracán no llegue a la ciudad está entre 99% y 100%. Muy feliz con su precisión y su modelo, aconseja que la evacuación de la ciudad no es necesaria. Desafortunadamente, el huracán llega a la ciudad produciendo una grave inundación.\n\n\n“I would rather be vaguely right than very wrong.”"
  },
  {
    "objectID": "presentaciones/presentacion_04.html#section-1",
    "href": "presentaciones/presentacion_04.html#section-1",
    "title": "Estadística Bayesiana",
    "section": "",
    "text": "Con el clima, las personas tienden a notar un error más que otro. Cuando llueve sin estar anunciado, se tiende a insultar al servicio meteorológico mientras que la ausencia de lluvia a pesar del pronóstico se toma con buena cara.\n\n\n\nEl Weather Channel exagera ligeramente la probabilidad de lluvia cuando es poco probable que ocurra: dicen que es de 20% cuando en realidad es de 5% o 10%"
  },
  {
    "objectID": "presentaciones/presentacion_04.html#section-2",
    "href": "presentaciones/presentacion_04.html#section-2",
    "title": "Estadística Bayesiana",
    "section": "",
    "text": "En la estadística bayesiana, la distribución a posteriori es la base de todas inferencia: combina el conocimiento a priori con la información provista por los datos. Contiene todo lo que se sabe y no se sabe sobre un parámetro desconocido.\n\nLa respuesta a los problemas es toda la distribución a posteriori de los parámetros (y de otras cantidades de interés)."
  },
  {
    "objectID": "presentaciones/presentacion_04.html#section-3",
    "href": "presentaciones/presentacion_04.html#section-3",
    "title": "Estadística Bayesiana",
    "section": "",
    "text": "No obstante, puede ser de utilidad (o incluso necesario) tomar decisiones concretas o resumir la distribución a posteriori.\n\n\\(a\\) es la acción que tomamos (intervenir o no intervenir quirúrgicamente a una persona) o la respuesta que damos (ganancia de una campaña de marketing).\n\n\nPuede ser una estimación puntual \\(\\hat{\\theta}\\): dada una inferencia sobre la ganancia de una campaña de marketing, es necesario informar un valor puntual (quizás con un intervalo)"
  },
  {
    "objectID": "presentaciones/presentacion_04.html#section-4",
    "href": "presentaciones/presentacion_04.html#section-4",
    "title": "Estadística Bayesiana",
    "section": "",
    "text": "Tratamos a los parámetros sobre los que realizamos inferencias como variables aleatorias. Una muestra de la distribución a posteriori es una posible realización del verdadero valor del parámetro.\n\nAl dar una respuesta (o resumir la información a posteriori), podemos incurrir en un error (grande o chico) según se den los eventos posibles.\n\n\n¿Qué es un error? ¿Cómo definimos si el error es grande o chico? ¿Cómo definimos si el error es relevante o no?"
  },
  {
    "objectID": "presentaciones/presentacion_04.html#funciones-de-pérdida",
    "href": "presentaciones/presentacion_04.html#funciones-de-pérdida",
    "title": "Estadística Bayesiana",
    "section": "Funciones de pérdida",
    "text": "Funciones de pérdida\n\\[L(\\theta,\\hat{\\theta}) = f(\\theta,\\hat{\\theta})\\] es una función de pérdida, qué tanto pierdo por usar \\(\\hat{\\theta}\\) para estimar \\(\\theta\\)."
  },
  {
    "objectID": "presentaciones/presentacion_04.html#section-5",
    "href": "presentaciones/presentacion_04.html#section-5",
    "title": "Estadística Bayesiana",
    "section": "",
    "text": "Por ejemplo:\n\\[L_2 = (\\theta - \\hat{\\theta})^2\\]\n\\[L_1 = |\\theta - \\hat{\\theta}|\\]\n\\[\nL_{0/1} =\n\\begin{cases}\n0 \\text{ si } \\hat{\\theta} = \\theta  \\\\\n1 \\text{ si } \\hat{\\theta} \\neq \\theta\n\\end{cases}\n\\]\n\\[\nL( \\theta, \\hat{\\theta} ) =\n  \\begin{cases}\n    (\\theta -  \\hat{\\theta} )^2 & \\hat{\\theta} &lt; \\theta \\\\ \\\\\n    c( \\theta -  \\hat{\\theta} )^2 & \\hat{\\theta} \\ge \\theta, \\quad 0 &lt; c &lt; 1\n  \\end{cases}\n\\]"
  },
  {
    "objectID": "presentaciones/presentacion_04.html#section-6",
    "href": "presentaciones/presentacion_04.html#section-6",
    "title": "Estadística Bayesiana",
    "section": "",
    "text": "Buscamos elegir \\(\\hat{\\theta}\\) de manera tal que minimice \\(L\\). El problema es que no conocemos \\(\\theta\\) y por lo tanto no podemos calcular \\(L(\\theta,\\hat{\\theta})\\).\n\n¿Sabemos algo sobre \\(\\theta\\) que nos pueda ayudar? Conocemos su distribución a posteriori\n\n\nPodemos promediar \\(L\\) para los valores posibles de \\(\\theta\\) (ponderando según la distribución a posteriori)"
  },
  {
    "objectID": "presentaciones/presentacion_04.html#section-7",
    "href": "presentaciones/presentacion_04.html#section-7",
    "title": "Estadística Bayesiana",
    "section": "",
    "text": "El riesgo a posteriori (posterior risk o posterior expected loss) es la pérdida esperada ponderada por los valores de \\(\\theta\\) (y su distribución a posteriori).\n\n\\[R(\\hat{\\theta}) = \\mathbb{E}_{\\theta\\mid y}[L(\\theta,\\hat{\\theta})] = \\int L(\\theta,\\hat{\\theta}) p(\\theta\\mid y) d\\theta\\]\n\n\nEs una función de los posibles valores que puede tomar \\(\\hat{\\theta}\\).\n\n\nPodemos obtener el \\(\\hat{\\theta}\\) que minimice \\(R(\\hat{\\theta})\\). Es decir, buscamos un valor (un estimador) que minimice la pérdida esperada al usarlo para resumir \\(p(\\theta\\mid y)\\): \\[\\hat{\\theta} = \\underset{\\hat{\\theta}}{\\mathrm{arg\\,min}}\\left[ R(\\hat{\\theta}) \\right]\\]"
  },
  {
    "objectID": "presentaciones/presentacion_04.html#section-8",
    "href": "presentaciones/presentacion_04.html#section-8",
    "title": "Estadística Bayesiana",
    "section": "",
    "text": "Simulemos…\n\nSupongamos que \\(\\theta\\mid y \\sim \\mathrm{Beta}(2,9)\\)\n\n\n\n\\(R\\) es una función de \\(\\hat{\\theta}\\) (los distintos valores que podemos usar para resumir \\(p(\\theta\\mid y)\\))\nPara distintos valores de \\(\\hat{\\theta}\\) voy a tomar muestras de \\(p(\\theta\\mid y)\\) y calcular la pérdida \\(L\\)\nPara cada valor de \\(\\hat{\\theta}\\) voy a calcular la pérdida promedio (ya va a estar ponderada por la probabilidad a posteriori de \\(\\theta\\))"
  },
  {
    "objectID": "presentaciones/presentacion_04.html#section-9",
    "href": "presentaciones/presentacion_04.html#section-9",
    "title": "Estadística Bayesiana",
    "section": "",
    "text": "L_2 &lt;- function(theta,theta_hat) (theta-theta_hat)^2\n\nloss &lt;- data.frame(theta_hat = double(),\n                   theta = double(),\n                   L = double())\n\nfor(theta_hat in seq(0,1,0.008)){\n  theta &lt;- rbeta(2000, shape1 = 2, shape2 = 9)\n  L &lt;- L_2(theta,theta_hat)\n  loss &lt;- bind_rows(loss,data.frame(theta_hat,theta,L))\n}\n\nexpected.loss &lt;- loss |&gt;\n  group_by(theta_hat) |&gt;\n  summarise(loss.mean = mean(L))"
  },
  {
    "objectID": "presentaciones/presentacion_04.html#section-11",
    "href": "presentaciones/presentacion_04.html#section-11",
    "title": "Estadística Bayesiana",
    "section": "",
    "text": "Si deseamos resumir la distribución a posteriori con un único valor (¡perdiendo información!), puede usarse:\n\n\nLa media: minimiza la pérdida cuadrática esperada a posteriori\nLa mediana: minimiza la pérdida absoluta esperada a posteriori\nLa moda (también llamado MAP por maximum a posteriori o estimador generalizado de máxima verosimilitud): minimiza la pérdida \\(0/1\\) esperada a posteriori"
  },
  {
    "objectID": "presentaciones/presentacion_04.html#section-12",
    "href": "presentaciones/presentacion_04.html#section-12",
    "title": "Estadística Bayesiana",
    "section": "",
    "text": "Una prueba más formal para el caso de la media…\n\nSea la pérdida cuadrática \\(L(\\theta, \\hat{\\theta}) = (\\theta - \\hat{\\theta}) ^ 2\\), el riesgo (posterior expected loss) es:\n\n\n\\[\n\\mathbb{E}_{\\theta \\mid y}[L(\\theta, \\hat{\\theta})]\n  = \\mathbb{E}_{\\theta \\mid y}[\\theta^2] - 2 \\hat{\\theta} \\mathbb{E}_{\\theta\\mid y}[\\theta] + {\\hat{\\theta}}^2\n\\]\n\n\nderivando respecto a \\(\\hat{\\theta}\\) e igualando a cero se obtiene que \\(\\underset{\\hat{\\theta}}{\\text{arg min}}\\left[ R(\\hat{\\theta}) \\right] = \\mathbb{E}_{\\theta \\mid y}[\\theta] = \\mathbb{E}[p(\\theta \\mid y)]\\)"
  },
  {
    "objectID": "presentaciones/presentacion_04.html#section-13",
    "href": "presentaciones/presentacion_04.html#section-13",
    "title": "Estadística Bayesiana",
    "section": "",
    "text": "Una prueba más formal para el caso de la mediana…\n\nSea la pérdida absoluta \\(L(\\theta,\\hat{\\theta})=\\lvert \\theta-\\hat{\\theta} \\rvert\\), el riesgo (posterior expected loss) es:\n\n\n\\[\n\\mathbb{E}_{\\theta \\mid y}[L(\\theta, \\hat{\\theta})] = \\int_{-\\infty}^\\infty \\lvert \\theta-\\hat{\\theta} \\rvert  p(\\theta \\mid y) d\\theta\n\\]\n\n\n\\[\n\\int_{-\\infty}^{\\hat{\\theta}} (\\hat{\\theta} - \\theta) p(\\theta\\mid y)d\\theta + \\int_{\\hat{\\theta}}^\\infty (\\theta-\\hat{\\theta}) p(\\theta\\mid y)d\\theta\n\\]\n\n\nPara derivar, se utiliza la regla integral de Leibniz:\n\n\n\\[\n\\frac{d}{d\\hat{\\theta}} \\int_{-\\infty}^{\\hat{\\theta}} g(\\hat{\\theta}, \\theta)d\\theta\n  = g(\\hat{\\theta},\\hat{\\theta})\n    + \\int_{-\\infty}^{\\hat{\\theta}} \\frac{\\partial}{\\partial\\hat{\\theta}}g(\\hat{\\theta}, \\theta)d\\theta\n\\]\n\n\nSe puede probar que \\(\\int_{-\\infty}^{\\hat{\\theta}} p(\\theta\\mid y)d\\theta = \\frac{1}{2}\\), por lo que el \\(\\hat{\\theta}\\) que minimiza la expresión es la mediana."
  },
  {
    "objectID": "presentaciones/presentacion_04.html#intervalos-de-credibilidad",
    "href": "presentaciones/presentacion_04.html#intervalos-de-credibilidad",
    "title": "Estadística Bayesiana",
    "section": "Intervalos de Credibilidad",
    "text": "Intervalos de Credibilidad\nTambién llamados: intervalos de probabilidad, intervalo de confianza bayesiano, región de credibilidad. Es una región del dominio del parámetro que tiene alta probabilidad de contenerlo. Se utiliza para resumir el posterior.\n\nUn intervalo de credibilidad es una región \\(C\\) tal que la probabilidad de que contenga al parámetro sea al menos \\(1 - \\alpha\\):\n\n\n\\[p(\\theta \\in C \\mid y) = \\int_C p(\\theta\\mid y) d\\theta = 1-\\alpha\\] en el caso discreto es (\\(\\geq 1-\\alpha\\))\n\n\nDecimos: la probabilidad de que \\(\\theta\\) esté contenido en \\(C\\), dados los datos (y el modelo) es de \\(1-\\alpha\\)"
  },
  {
    "objectID": "presentaciones/presentacion_04.html#section-15",
    "href": "presentaciones/presentacion_04.html#section-15",
    "title": "Estadística Bayesiana",
    "section": "",
    "text": "En el análisis de datos bayesiano, es habitual resumir los hallazgos reportando:\n\n\nUn gráfico de la distribución a posteriori\nAlgún medida de centralidad de la distribución a posteriori\nPercentiles relevantes de la distribución a posteriori\nProbabilidades a posteriori de interés \\(p(\\theta&gt;c\\mid y)\\) para algún \\(c\\) interesante, por ejemplo \\(c=0\\)"
  },
  {
    "objectID": "presentaciones/presentacion_04.html#simulaciones",
    "href": "presentaciones/presentacion_04.html#simulaciones",
    "title": "Estadística Bayesiana",
    "section": "Simulaciones",
    "text": "Simulaciones\nPara interpretar los resultados de la inferencia bayesiana podemos simplemente realizar simulaciones a partir del posterior y estimar probabilidades contando.\n\n\nlos parámetros\nfunciones de los parámetros\nla variable respuesta (predicciones)"
  },
  {
    "objectID": "presentaciones/presentacion_04.html#section-16",
    "href": "presentaciones/presentacion_04.html#section-16",
    "title": "Estadística Bayesiana",
    "section": "",
    "text": "Un nuevo ejemplo del modelo beta–Binomial: partimos de \\(\\text{Beta}(2,2)\\), observamos \\(4\\) caras en \\(6\\) tiradas y nuestra creencia a posteriori pasa a ser \\(\\text{Beta}(6,4)\\)."
  },
  {
    "objectID": "presentaciones/presentacion_04.html#section-17",
    "href": "presentaciones/presentacion_04.html#section-17",
    "title": "Estadística Bayesiana",
    "section": "",
    "text": "Parámetros\n¿Cuál es la probabilidad a posteriori de que \\(\\pi\\) sea mayor a \\(0.50\\)? ¿y de que sea mayor a \\(0.80\\)?\n\nmuestras_pi &lt;- rbeta(2000,6,4)\nmean(muestras_pi &gt; 0.5)\nmean(muestras_pi &gt; 0.8)"
  },
  {
    "objectID": "presentaciones/presentacion_04.html#section-18",
    "href": "presentaciones/presentacion_04.html#section-18",
    "title": "Estadística Bayesiana",
    "section": "",
    "text": "Funciones de los parámetros\n¿Cuál es la distribución a posteriori de la chance de obtener cara \\(\\frac{\\pi}{1-\\pi}\\)?\n\nmuestras_pi &lt;- rbeta(2000,6,4)\nmuestras_odds &lt;- muestras_pi/(1-muestras_pi)"
  },
  {
    "objectID": "presentaciones/presentacion_04.html#section-19",
    "href": "presentaciones/presentacion_04.html#section-19",
    "title": "Estadística Bayesiana",
    "section": "",
    "text": "Predicciones\nSi se arroja la moneda 11 veces más ¿cuál es la distribución de probabilidad de la cantidad de caras? (es la distribución predictiva a posteriori)\n\nmuestras_pi &lt;- rbeta(2000,6,4)\ny_new &lt;- rbinom(2000,11,muestras_pi)"
  },
  {
    "objectID": "presentaciones/presentacion_04.html#section-20",
    "href": "presentaciones/presentacion_04.html#section-20",
    "title": "Estadística Bayesiana",
    "section": "",
    "text": "Toda cantidad que dependa de los parámetros tiene una distribución a posteriori: una incertidumbre asociada.\n\n\n\n\nEstadística Bayesiana – 2025"
  },
  {
    "objectID": "trabajos_practicos/descripcion.html",
    "href": "trabajos_practicos/descripcion.html",
    "title": "Generalidades",
    "section": "",
    "text": "Para aprobar la materia es necesario completar tres trabajos prácticos cortos. La denominación cortos hace referencia a que los trabajos son guiados y las tareas a realizar están delimitadas.\nLos trabajos prácticos tienen como objetivo repasar y afianzar los conocimientos adquiridos durante las clases, adquirir práctica en la aplicación de conceptos trabajados, mejorar las habilidades de programación y el uso de R, e incorporar algunos conceptos complementarios.\nCada trabajo práctico será presentado y discutido en clase. Se destinará una fracción de la clase a comenzar a pensar algunas de las actividades.\nLa fecha de entrega de cada trabajo práctico será de dos semanas luego de la fecha de presentación. Se podrá entregar el trabajo práctico una semana después de la fecha de entrega con una penalización del 25% de la nota final.\nPara cada trabajo práctico, cada grupo deberá entregar un informe en formato pdf donde se resuelvan las actividades propuestas. El informe debe estar obligatoriamente elaborado utilizando \\(\\mathrm{\\LaTeX{}}\\) (a través de Quarto, RMarkdown o alguna otra variante). Tener en cuenta que los apartados presentados en el enunciado del trabajo práctico constituyen una guía de actividades a resolver y no deben responderse uno a uno como si se tratara de un cuestionario. El informe deberá permitir una lectura fluida de los resultados y análisis presentados. Cuando la resolución de una problemática consista en una función o porción de código en R, el código deberá mostrarse en el informe.\nSe evaluarán los siguientes aspectos del informe: presentación, redacción (claridad, coherencia y cohesión), estética, resultados obtenidos, profundidad del análisis.",
    "crumbs": [
      "Trabajos Prácticos",
      "Generalidades"
    ]
  },
  {
    "objectID": "trabajos_practicos/02_tp2.html",
    "href": "trabajos_practicos/02_tp2.html",
    "title": "El Dibu de la vida",
    "section": "",
    "text": "El ozono (O₃) es un gas que se encuentra tanto en la atmósfera como en la superficie terrestre y cuya molécula está formada por tres átomos de oxígeno. Seguramente, lo han escuchado nombrar por su papel en la capa de ozono, una región de la estratófera que se encuentra entre unos 15 y 30 kilómetros sobre la superficie terrestre, donde se concentra este gas y filtra los rayos ultravioletas del sol.\nLa capa de ozono permite que exista la vida en la Tierra tal como la conocemos y que una escapada de verano a la isla no termine siendo solo un viaje de ida. En otras palabras, la capa de ozono es como un Dibu Martínez de la vida: evita que el sol nos clave un golazo de radiación y nos deje, literalmente, fritos.\nPero no todo lo que brilla es oro. A nivel del suelo, el ozono se convierte en un contaminante atmosférico dañino tanto para la salud humana como para el medio ambiente. Puede deteriorar la función pulmonar, desencadenar ataques de asma, causar irritación en los ojos, la nariz y la garganta, e incluso dañar la vegetación. Entonces, ¿todo mal con el ozono?\nQuien se planteó esa pregunta fue Brian Tarkington, del California Primate Research Center de la Universidad de California en Davis. Preocupado por la presencia de ozono en el smog californiano y las posibles consecuencias en la salud humana, decidió llevar a cabo un estudio para evaluar el efecto del ozono en el crecimiento de ratas. Tomó un grupo de 46 ratas jóvenes y de la misma edad, las pesó, las dividió aleatoriamente en 2 grupos de igual tamaño y las dejó en 2 ambientes distintos, el primero rico en ozono y el segundo libre del mismo. Luego de 7 días las pesó nuevamente y registró la diferencia entre los pesos.\nEl archivo tarkington.csv contiene, para cada una de las ratas, el diferencial de peso en gramos y el grupo al que pertenece. Interesa conocer si la exposición prolongada a un ambiente contaminado con ozono se asocia a un deterioro en el desarrollo de las ratas, es decir, a un menor incremento de peso.",
    "crumbs": [
      "Trabajos Prácticos",
      "TP 2: El Dibu de la vida"
    ]
  },
  {
    "objectID": "trabajos_practicos/02_tp2.html#introducción",
    "href": "trabajos_practicos/02_tp2.html#introducción",
    "title": "El Dibu de la vida",
    "section": "",
    "text": "El ozono (O₃) es un gas que se encuentra tanto en la atmósfera como en la superficie terrestre y cuya molécula está formada por tres átomos de oxígeno. Seguramente, lo han escuchado nombrar por su papel en la capa de ozono, una región de la estratófera que se encuentra entre unos 15 y 30 kilómetros sobre la superficie terrestre, donde se concentra este gas y filtra los rayos ultravioletas del sol.\nLa capa de ozono permite que exista la vida en la Tierra tal como la conocemos y que una escapada de verano a la isla no termine siendo solo un viaje de ida. En otras palabras, la capa de ozono es como un Dibu Martínez de la vida: evita que el sol nos clave un golazo de radiación y nos deje, literalmente, fritos.\nPero no todo lo que brilla es oro. A nivel del suelo, el ozono se convierte en un contaminante atmosférico dañino tanto para la salud humana como para el medio ambiente. Puede deteriorar la función pulmonar, desencadenar ataques de asma, causar irritación en los ojos, la nariz y la garganta, e incluso dañar la vegetación. Entonces, ¿todo mal con el ozono?\nQuien se planteó esa pregunta fue Brian Tarkington, del California Primate Research Center de la Universidad de California en Davis. Preocupado por la presencia de ozono en el smog californiano y las posibles consecuencias en la salud humana, decidió llevar a cabo un estudio para evaluar el efecto del ozono en el crecimiento de ratas. Tomó un grupo de 46 ratas jóvenes y de la misma edad, las pesó, las dividió aleatoriamente en 2 grupos de igual tamaño y las dejó en 2 ambientes distintos, el primero rico en ozono y el segundo libre del mismo. Luego de 7 días las pesó nuevamente y registró la diferencia entre los pesos.\nEl archivo tarkington.csv contiene, para cada una de las ratas, el diferencial de peso en gramos y el grupo al que pertenece. Interesa conocer si la exposición prolongada a un ambiente contaminado con ozono se asocia a un deterioro en el desarrollo de las ratas, es decir, a un menor incremento de peso.",
    "crumbs": [
      "Trabajos Prácticos",
      "TP 2: El Dibu de la vida"
    ]
  },
  {
    "objectID": "trabajos_practicos/02_tp2.html#modelización-estadística",
    "href": "trabajos_practicos/02_tp2.html#modelización-estadística",
    "title": "El Dibu de la vida",
    "section": "Modelización estadística",
    "text": "Modelización estadística\nLa Estadística ofrece una amplia variedad de técnicas para dar respuesta a la inquietud de Tarkington. En el contexto de esta materia, naturalmente, abordaremos el problema mediante el uso modelos bayesianos.\nY dado que el entusiasmo en esta materia es lo que sobra, aprovecharemos la oportunidad para utilizar no uno, sino dos modelos, sutilmente distintos. Esto nos permitirá no solo evaluar el efecto del ozono en el crecimiento de las ratas, sino también profundizar en esos pequeños detalles que hacen que la inferencia estadística sea tan interesante.\n\nModelo normal\nEl primer modelo que proponemos es el clásico caballito de batalla de innumerables análisis estadísticos: el viejo y confiable modelo normal. Aquí, el cambio en el peso de las ratas en cada grupo se modeliza mediante una distribución normal, permitiendo medias y varianzas específicas para cada grupo.\n\\[\n\\begin{aligned}\nY_{O, i} &\\sim \\text{Normal}(\\mu_O, \\sigma_O^2) & i = 1, \\dots, N_O \\\\\nY_{C, j} &\\sim \\text{Normal}(\\mu_C, \\sigma_C^2) & j = 1, \\dots, N_C \\\\\n\\mu_O, \\mu_C &\\sim \\text{Normal}(0, 5^2) \\\\\n\\sigma_O, \\sigma_C & \\sim \\text{Gamma}(\\alpha=6, \\beta=2)\n\\end{aligned}\n\\] donde \\(Y_{O, i}\\) representa el cambio en el peso de la \\(i\\)-ésima rata en el grupo expuesto al ozono e \\(Y_{C, j}\\) representa el cambio en el peso de la \\(j\\)-ésima rata en el grupo control.\n\n\nModelo T de Student\nEl segundo modelo se parece demasiado al primero. La única —y, en principio, sutil— diferencia es que, en lugar de modelizar la variable respuesta en cada grupo con una distribución normal, utilizamos una T de Student con 3 grados de libertad.\n\\[\n\\begin{aligned}\nY_{O, i} &\\sim \\text{Student-T}(\\mu_O, \\sigma_O^2, \\nu=3) & i = 1, \\dots, N_O \\\\\nY_{C, i} &\\sim \\text{Student-T}(\\mu_C, \\sigma_C^2, \\nu=3) & j = 1, \\dots, N_C \\\\\n\\mu_O, \\mu_C &\\sim \\text{Normal}(0, 5^2) \\\\\n\\sigma_O, \\sigma_C & \\sim \\text{Gamma}(\\alpha=6, \\beta=2)\n\\end{aligned}\n\\]",
    "crumbs": [
      "Trabajos Prácticos",
      "TP 2: El Dibu de la vida"
    ]
  },
  {
    "objectID": "trabajos_practicos/02_tp2.html#un-deep-dive-bien-profundo-en-metropolis-hastings",
    "href": "trabajos_practicos/02_tp2.html#un-deep-dive-bien-profundo-en-metropolis-hastings",
    "title": "El Dibu de la vida",
    "section": "Un deep dive bien profundo en Metropolis-Hastings",
    "text": "Un deep dive bien profundo en Metropolis-Hastings\nAdemás de brindarle una respuesta al pobre de Brian Tarkington, que hace décadas que espera que alguien le ayude a resolver su problema, el objetivo principal de este trabajo práctico es profundizar en el uso de Metropolis-Hastings (MH) como algoritmo de inferencia bayesiana.\nEl algoritmo de Metropolis-Hastings permite generar muestras (pseudo-)aleatorias a partir de una distribución de probabilidad \\(P\\) que no necesariamente pertence a una familia de distribuciones conocida. El único requisito es que se pueda evaluar la función de densidad (o de masa de probabilidad) \\(p^*(\\boldsymbol{\\theta})\\) en cualquier valor de \\(\\boldsymbol{\\theta}\\), incluso cuando \\(p^*(\\boldsymbol{\\theta})\\) sea impropia (es decir, incluso aunque sea desconocida la constante de normalización que hace que la integral en el soporte de la función sea igual a uno).\n\n\n\n\n\n\nAlgoritmo de Metropolis-Hastings\n\n\n\nSe desea generar una muestra de valores \\(\\{y^{(1)}, y^{(2)}, \\cdots, y^{(n)} \\}\\) a partir de una distribución de probabilidad \\(P\\) con función de densidad \\(p\\).\n\nSeleccionar un punto inicial \\(y^{(1)}\\).\nPara cada \\(t\\in \\{1, \\cdots, n\\}\\), repetir:\n\n\nRealizar propuesta\nObtener un valor aleatorio \\(y'\\) de una variable \\(Y'\\) cuya distribución está dada por la distribución de propuesta \\(Q\\), centrada en el valor de la última muestra obtenida: \\[\n    Y' \\sim Q(y^{(t)})\n    \\]\nCalcular la probabilidad de aceptación\nCalcular el cociente entre la función de densidad en el punto propuesto y en el punto actual, ponderando por la densidad de la distribución de propuesta. La probabilidad de aceptación es igual a este cociente si es menor a 1, caso contrario es igual a 1. \\[\n    \\alpha = \\min \\left\\{ 1, \\frac{p(y')q(y^{(t)} \\mid y')}{p(y^{(t)})q(y' \\mid y^{(t)})} \\right\\}\n    \\]\nSeleccionar el nuevo valor\nGenerar un valor aleatorio \\(u\\) de una distribución \\(\\mathcal{U}(0, 1)\\) y determinar \\(y^{(t + 1)}\\) de la siguiente manera: \\[\n    y^{(t + 1)} = \\left\\{\n    \\begin{array}{ll}\n    y' & \\text{si} \\quad u \\le \\alpha \\\\\n    y^{(t)} & \\text{si} \\quad u &gt; \\alpha\n    \\end{array}\\right.\n    \\]\n\n\n\n\nMetropolis-Hastings en espacios paramétricos acotados\nLa distribución normal suele ser una elección conveniente a la hora de proponer saltos, pero presenta desventajas cuando el espacio paramétrico es acotado: inevitablemente, algunas propuestas caerán fuera del dominio y rechazaremos más que de costumbre.\nPara abordar este problema, existen dos estrategias principales. Una opción es utilizar distribuciones cuyo soporte coincida con el espacio objetivo. Sin embargo, encontrar parametrizaciones basadas en la media para todas ellas no es trivial.\nLa otra alternativa, más general, consiste en aplicar una transformación de variables para trabajar en un espacio no acotado, permitiendo así seguir utilizando una distribución de propuesta normal. Esta estrategia requiere conocer la densidad objetivo en el nuevo espacio, lo que implica calcular el determinante del jacobiano de la transformación. A su favor, muchas de las transformaciones más utilizadas permiten un cálculo sencillo, y este proceso puede automatizarse con herramientas de diferenciación automática existentes.\n\n\n\n\n\n\nTransformación de variables aleatorias\n\n\n\nSean \\(X\\) e \\(Y = g(X)\\) variables aleatorias continuas, donde \\(g\\) es una función uno a uno y \\(X\\) tiene función de densidad \\(f_X(x)\\) conocida. Luego, la función de densidad de \\(Y\\) es: \\[\nf_Y(y) = f_X\\left(g^{-1}(y)\\right) \\left\\lvert \\frac{d}{dy}g^{-1}(y) \\right\\rvert\n\\]\ndonde \\(\\left\\lvert \\frac{d}{dy}g^{-1}(y) \\right\\rvert\\) es el determinante del jacobiano de la transformación. Por ejemplo, en el caso de la función \\(g(u) = \\text{logit}(u)\\), se tiene: \\[\n\\begin{aligned}\ng : (0, 1) \\to \\mathbb{R} & \\quad g(u)= \\text{logit}(u) = \\log\\left(\\frac{u}{1 - u}\\right) \\\\\ng^{-1} : \\mathbb{R} \\to (0, 1) & \\quad g(v)= \\text{expit}(v) =  \\frac{1}{1 + \\exp(-v)}\n\\end{aligned}\n\\]\ny luego, utilizando \\(X\\) e \\(Y\\) definidas al principio: \\[\nf_Y(y) = f_X(\\text{expit}(y)) \\cdot \\text{expit}(y) \\cdot ( 1 - \\text{expit}(y))\n\\]\n\n\n\n\nMetropolis-Hastings en escala logarítmica\nSin importar cuán moderna y potente sea nuestra computadora, siempre tendremos que lidiar con el talón de Aquiles del cálculo computacional: los problemas de subdesbordamiento y sobredesbordamiento (conocidos como underflow y overflow en inglés). Por ejemplo, si intentamos multiplicar 100 números del orden de 0.0001 en R, la computadora podría interpretar el resultado como 0, aunque matemáticamente esto no sea cierto.\nEl underflow es un problema frecuente en el cómputo estadístico. Un caso típico es el que se da al evaluar funciones de verosimilitud, donde se multiplican densidades correspondientes a cada observación. Dado que estas densidades suelen ser valores muy pequeños, es común enfrentarse a un problema de subdesbordamiento. Este riesgo aumenta con la cantidad de observaciones y el número de dimensiones de la distribución objetivo.\nEl problema se agrava al utilizar algoritmos como Metropolis-Hastings, ya que la densidad objetivo se evalúa miles de veces en diferentes puntos del espacio paramétrico. Además, en inferencia bayesiana, esta función suele involucrar una función de verosimilitud, que puede implicar una multiplicación de gran cantidad de números pequeños, lo que incrementa aún más las chances de que el cómputo resulte en un underflow.\nSi nuestro algoritmo de muestreo encuentra un problema de underflow, pueden ocurrir dos situaciones:\n\nSe genera un error que detiene la ejecución del programa.\nSe ignora la propuesta problemática y se realiza una nueva, pero nuestro programa ya no sigue la cadena de Markov deseada.\n\nEntonces, ¿cómo solucionamos este problema? La respuesta es sencilla: realizar todos los cálculos en escala logarítmica. Las multiplicaciones se vuelven sumas y el cómputo se estabiliza.\n\n\n\nListado 1: Implementación del algoritmo de Metropolis-Hastings en escala logarítmica.\n\n\nmetropolis_hastings_log &lt;- function(logp, x, n, sigma = NULL) {\n  # Algortimo de Metropolis Hastings en escala logarítmica\n  #\n  # Parámetros\n  #  ------------------------------------------------------------------------\n  # | logp     | Función de densidad objetivo, normalizada o sin normalizar, |\n  # |          | en escala logarítmica.                                      |\n  # | x        | Posición inicial del algoritmo.                             |\n  # | n        | Cantidad de muestras a obtener.                             |\n  # | sigma    | Matriz de varianzas y covarianzas para la distribución de   |\n  # |          | propuesta. Por defecto, es NULL y usa la matriz identidad.  |\n  #  ------------------------------------------------------------------------\n  #\n  # Salida\n  #  ------------------------------------------------------------------------\n  # | muestras | Matriz con las muestras obtenidas.                          |\n  #  ------------------------------------------------------------------------\n\n  # Obtener dimensionalidad de la distribución objetivo a partir del punto inicial\n  p &lt;- length(x)\n\n  # Inicializar matriz donde se guardan las muestras\n  muestras &lt;- matrix(NA, nrow = n, ncol = p)\n\n  # Usar matriz diagonal unitaria para la distribución de propuesta cuando no se especifica\n  if (is.null(sigma)) {\n    sigma &lt;- diag(p)\n  }\n\n  # Almacenar el punto inicial en la matriz de muestras\n  muestras[1, ] &lt;- x\n\n  for (i in 1:(n - 1)) {\n    # Obtener el valor de la muestra actual\n    muestra_actual &lt;- muestras[i, ]\n\n    # Proponer un nuevo valor\n    muestra_propuesta &lt;- mvtnorm::rmvnorm(1, mean = muestra_actual, sigma = sigma)\n\n    # Evaluar la log densidad en el valor actual y en el propuesto\n    logp_propuesta &lt;- logp(muestra_propuesta)\n    logp_actual &lt;- logp(muestra_actual)\n\n    # Log densidad al pasar de muestra_propuesta a muestra_actual y viceversa\n1    logq_actual &lt;- mvtnorm::dmvnorm(\n      muestra_actual, mean = muestra_propuesta, sigma = sigma, log = TRUE\n    )\n    logq_propuesta &lt;- mvtnorm::dmvnorm(\n      muestra_propuesta, mean = muestra_actual, sigma = sigma, log = TRUE\n    )\n\n    # Calcular log probabilidad de aceptación\n2    log_alpha &lt;- ...\n\n    # Simular aceptación o rechazo\n    log_u &lt;- log(runif(1))\n\n    aceptar &lt;- log_u &lt; log_alpha\n\n    # Determinar siguiente paso en base al criterio de selección\n    if (aceptar) {\n      muestras[i + 1, ] &lt;- muestra_propuesta\n    } else {\n      muestras[i + 1, ] &lt;- muestra_actual\n    }\n  }\n  # Convertir 'muestras' a vector si se trata de una distrbución univariada\n  if (p == 1) {\n    muestras &lt;- as.vector(muestras)\n  }\n\n  return(muestras)\n}\n\n1\n\n¿Es necesario este paso?\n\n2\n\n¿Cómo se calcula la log probabilidad de aceptación?",
    "crumbs": [
      "Trabajos Prácticos",
      "TP 2: El Dibu de la vida"
    ]
  },
  {
    "objectID": "trabajos_practicos/02_tp2.html#ahora-sí-cerebro-a-la-obra",
    "href": "trabajos_practicos/02_tp2.html#ahora-sí-cerebro-a-la-obra",
    "title": "El Dibu de la vida",
    "section": "Ahora sí, cerebro a la obra",
    "text": "Ahora sí, cerebro a la obra\nLa función metropolis_hastings_log() es una herramienta poderosa que, en teoría, permite obtener muestras de cualquier distribución a posteriori que se pueda implementar en R en escala logarítmica. Pero, ¿de qué sirve tener una herramienta tan potente si no está nada claro cómo dominarla?\nA esta altura, intentar usar metropolis_hastings_log() para muestrear del posterior en los modelos propuestos inicialmente resultaría muy dificultoso. Sería como pasar de manejar un Golcito por las calles del pueblo a subirse a un Fórmula 1 en el medio de un Gran Premio.\nComo bien dice el refrán, la práctica hace al maestro. Y quienes conocen de primera mano la verdad de estas palabras son, casualmente, los pilotos de Fórmula 1. No solo acumulan experiencia tras años de competencia en categorías menores, desde el karting hasta la Fórmula 2, sino que también, antes de cada Gran Premio, dan entre 400 y 500 vueltas al circuito… ¡pero en un simulador!\nDe este modo, en preparación para la aplicación final en este trabajo —nuestro propio Gran Premio— comenzaremos utilizando el algoritmo de Metropolis-Hastings en escenarios simples y controlados, aumentando gradualmente la complejidad. Esto nos permitirá practicar su uso con transformación de variables, en escala logarítmica y en posteriors de múltiples dimensiones. Y al final, como en toda buena carrera, llegará el Gran Premio.\n\n1. En el simulador: ensayos con la distribución Gamma\nEl objetivo de esta sección es familiarizarnos con el uso de transformaciones para obtener muestras de distribuciones con soporte acotado. Inicialmente, utilizaremos la función metropolis_hastings() desarrollada en la práctica de la materia, que opera en escala original. Luego, utilizaremos la función metropolis_hastings_log() incluida más arriba, que trabaja en escala logarítmica.\nSea \\(X \\sim \\text{Gamma}(\\alpha = 3, \\beta = 2)\\).\n\nObtenga y grafique la función de densidad de \\(Y = \\log(X)\\).\nUtilice Metropolis-Hastings para obtener 10000 muestras de \\(X\\). Para ello, obtenga muestras de \\(Y\\) utilizando una propuesta normal con \\(\\sigma = 0.2\\) y realice la transformación correspondiente para obtener las muestras de \\(X\\). Para evaluar la bondad del método, calcule el tamaño de muestra efectivo, visualice la trayectoria de la cadena con un traceplot y compare la distribución empírica con la función de densidad teórica utilizando un histograma.\nImplemente la función de densidad de \\(Y\\) en escala logarítmica y utilice metropolis_hastings_log() para obtener muestras de \\(X\\). Calcule el tamaño efectivo de muestra efectivo y compárelo con el obtenido anteriormente. Finalmente, obtenga un histograma de las muestras y compárelo con la densidad teórica y con el histograma obtenido en el punto anterior.\n\n\n\n2. Free practice: múltiples dimensiones\nAl cómputo en escala logarítmica y el uso de transformaciones de variables, en esta etapa se le agrega que la distribución objetivo tiene múltiples dimensiones y debe ser obtenida mediante el uso de la regla de Bayes.\nDado el siguiente modelo: \\[\n\\begin{aligned}\nY_i & \\sim \\text{Normal}(\\mu, \\sigma^2) \\\\\n\\mu & \\sim \\text{Normal}(0, 1^2) \\\\\n\\sigma & \\sim \\text{Gamma}(\\alpha=2, \\beta=2) \\\\\n\\end{aligned}\n\\]\ncon \\(i = 1, \\dots, N\\).\n\nImplemente en R una función que permita calcular la densidad a posteriori en escala logarítmica, sin incluir la constante de normalización. Esta función tendrá 2 parámetros de entrada, uno para el vector de parámetros y otro para el vector de valores observados.\nUtilice los valores de \\(y_i\\) en el archivo precalentamiento-mh.txt y la función metropolis_hastings_log() para obtener muestras del posterior de \\(\\boldsymbol{\\theta} = \\{\\mu, \\sigma\\}\\). Corra dos cadenas de Markov. Calcule el tamaño efectivo de muestra y evalúe mezcla y convergencia de manera gráfica.\n\n\n\n\n\n\n\nEvaluación parcial de funciones\n\n\n\nEn las líneas 41 y 42 del Listado 1 se puede ver que la función logp que se pasa a metropolis_hastings_log() debe funcionar correctamente al ser llamada con un único argumento, que es el vector de parámetros. Pero la función que implementamos en el punto 4 tiene 2 argumentos, uno para el vector de parámetros y otro para el vector de datos… ¿cómo hacemos?\nLa función partial(), de la librería purrr, permite pre-llenar argumentos de una función. Toma como entrada una función, los argumentos con sus valores a pre-llenar, y devuelve una nueva función que hace lo mismo que la función original, pero con valores de argumentos ya especificados.\nEn nuestro caso, podríamos hacer:\nlibrary(purrr)\nlogp &lt;- partial(log_posterior, y = datos)\nLuego, la función logp() puede ser llamada solamente pasando el vector de parámetros. El valor de y será el que se pasó originalmente a la función partial().\n\n\n\n\n3. El Gran Prix: ¿qué le decimos a Brian?\nA esta altura del partido (o de la carrera mejor dicho) ya estamos ‘retequecontra’ preparados para trabajar con los modelos presentados inicialmente y dar respuesta a la pregunta sobre el efecto del ozono en desarrollo de las ratas de Brian Tarkington. Llegó la hora de realizar el análisis estadístico.\n\nRealice un análisis exploratorio de los datos y describa sus hallazgos.\nPara una media y una varianza dada, ¿en qué se diferencia la distribución normal de la distribución T de Student con 3 grados de libertad?\nTanto para el modelo normal como para el basado en la T de Student:\n\nEscriba la función de densidad a posteriori en escala logarítmica, sin considerar la constante de normalización. De ser necesario, realice las transformaciones de variables que crea conveniente.\nImplemente la función anterior en R.\nUse metropolis_hastings_log() para obtener muestras del posterior. Utilice 4 cadenas de 5000 muestras cada una. Evalúe mezcla y convergencia mediante medidas numéricas y gráficas. Visualice las distribuciones a posteriori marginales.\nConcluya en términos del problema.\n\n¿Llega a la misma conclusión con ambos modelos? ¿Por qué?\n\n\n\n\n\n\n\nT de Student no centrada y escalada\n\n\n\nLa función dt() de R permite evaluar la función de densidad de una distribución T de Student estándar, con media 0 y desvío 1. Para evaluar la función de densidad con media y desvío arbitrario se puede utilizar la siguiente función de R:\nscaled_t_density &lt;- function(x, df, mean = 0, sd = 1) {\n  dt((x - mean) / sd, df) / sd\n}",
    "crumbs": [
      "Trabajos Prácticos",
      "TP 2: El Dibu de la vida"
    ]
  },
  {
    "objectID": "teoria/u3_teoria_05.html",
    "href": "teoria/u3_teoria_05.html",
    "title": "Unidad 3 - Métodos Computacionales",
    "section": "",
    "text": "Descargar presentación en PDF",
    "crumbs": [
      "Teoría",
      "U3 - Métodos Computacionales"
    ]
  },
  {
    "objectID": "teoria/u1_teoria_01.html",
    "href": "teoria/u1_teoria_01.html",
    "title": "Unidad 1 - Introducción",
    "section": "",
    "text": "Descargar presentación en PDF",
    "crumbs": [
      "Teoría",
      "U1 - Introducción"
    ]
  },
  {
    "objectID": "teoria/u5_teoria_07.html",
    "href": "teoria/u5_teoria_07.html",
    "title": "Unidad 5 - Modelos Avanzados",
    "section": "",
    "text": "Descargar presentación en PDF",
    "crumbs": [
      "Teoría",
      "U5 - Modelos Avanzados"
    ]
  },
  {
    "objectID": "teoria/u4_teoria_06.html",
    "href": "teoria/u4_teoria_06.html",
    "title": "Unidad 4 - Modelos Lineales",
    "section": "",
    "text": "Descargar presentación en PDF",
    "crumbs": [
      "Teoría",
      "U4 - Modelos Lineales"
    ]
  }
]