[
  {
    "objectID": "info/aprobacion.html",
    "href": "info/aprobacion.html",
    "title": "Condiciones de aprobaci√≥n",
    "section": "",
    "text": "Instancias de evaluaci√≥n\n\nüìù Parcial ‚Äì escrito e individual, se aprueba con 6, hay una instancia de recuperaci√≥n;\nüíªüíªüíª Trabajos pr√°cticos cortos ‚Äì grupales (hasta tres personas), se hacen por fuera del horario de clase, se entrega informe;\nüìà Trabajo pr√°ctico final ‚Äì realizaci√≥n grupal, presentaci√≥n oral individual y defensa.\n\n\n\nCondiciones de aprobaci√≥n\n\nPromoci√≥n\n\nLas y los estudiantes que hayan aprobado el parcial o su recuperatorio (con nota \\(P\\)), los tres trabajos pr√°cticos cortos (con promedio simple \\(T\\)) y hayan entregado el trabajo pr√°ctico final, acceder√°n a una instancia de evaluaci√≥n oral donde se discutir√° el trabajo pr√°ctico final y se evaluar√°n de manera general todos los contenidos conceptuales de la asignatura; esta instancia puede incluir la realizaci√≥n de algunas actividades en computadora (el trabajo pr√°ctico final y la instancia oral tendr√°n nota \\(O\\)).\nLa nota final se obtendr√° seg√∫n \\(0.3\\ T + 0.4\\ P + 0.3\\ O\\)\n\nRegularidad\n\nQuienes no alcancen la condici√≥n de promoci√≥n podr√°n quedar en condici√≥n de estudiante regular si aprueban el parcial o su recuperatorio y aprueban al menos dos de los trabajos pr√°cticos cortos. Para alcanzar la aprobaci√≥n de la asignatura, los y las estudiantes en condici√≥n de regulares deber√°n presentar el trabajo final en una mesa de examen y aprobar una instancia oral de defensa del trabajo y de evaluaci√≥n integral de los contenidos de la materia. Esta instancia puede incluir la realizaci√≥n de algunas actividades en computadora.\n\nLibres\n\nAquellas personas que no alcancen la promoci√≥n de la asignatura ni la condici√≥n de estudiante regular quedar√°n en condici√≥n de estudiante libre. Los y las estudiantes en condici√≥n de libre deber√°n presentar un trabajo pr√°ctico y rendir un examen te√≥rico-pr√°ctico sobre la totalidad de los temas de la asignatura.",
    "crumbs": [
      "Informaci√≥n",
      "Condiciones de aprobaci√≥n"
    ]
  },
  {
    "objectID": "info/programa.html",
    "href": "info/programa.html",
    "title": "Programa",
    "section": "",
    "text": "Fundamentaci√≥n\nLa Estad√≠stica Bayesiana es un enfoque de la inferencia estad√≠stica que se basa en utilizar probabilidades para representar el conocimiento disponible sobre el conjunto de par√°metros de un modelo y actualizar esa informaci√≥n utilizando la Regla de Bayes a partir de la observaci√≥n de un conjunto de datos. El conocimiento inicial se representa con una distribuci√≥n de probabilidad a priori, la informaci√≥n contenida en los datos observados se modeliza con una funci√≥n de verosimilitud y ambas fuentes de informaci√≥n se combinan para obtener una distribuci√≥n de probabilidad a posteriori. La informaci√≥n a posteriori puede ser utilizada para extraer conclusiones sobre el fen√≥meno en estudio y realizar predicciones sobre datos no observados o eventos futuros.\nLos m√©todos bayesianos requieren, salvo en casos muy simples, una complejidad computacional que resultaba inalcanzable hace algunos a√±os. Gracias a desarrollos revolucionarios en el √°mbito de la computaci√≥n, la principal barrera para la implementaci√≥n de los modelos bayesianos desapareci√≥ y la utilizaci√≥n de estos se ha incrementado masivamente en m√∫ltiples campos cient√≠ficos. Esta creciente popularidad se debe a que la inferencia bayesiana brinda un marco te√≥rico consistente que permite la incorporaci√≥n de informaci√≥n a priori, el desarrollo de un aprendizaje secuencial, la obtenci√≥n de inferencias y predicciones en forma de distribuciones de probabilidad, el tratamiento de datos faltantes, los an√°lisis con pocos datos, entre otras ventajas.\n\n\nObjetivos\nQue quienes cursen la materia logren:\n\nentender las caracter√≠sticas y los conceptos fundamentales de la Estad√≠stica Bayesiana;\ndescribir las caracter√≠sticas principales de la Estad√≠stica Bayesiana;\ncomprender la complejidad anal√≠tica de la inferencia bayesiana y la necesidad de la utilizaci√≥n de un enfoque computacional para superar estas dificultades;\nser capaces de aplicar m√©todos bayesianos a problemas reales utilizando software espec√≠fico; e\ninterpretar los resultados del proceso de an√°lisis bayesiano de datos.\n\n\n\nContenidos\n\nUnidad 1: Introducci√≥n y Fundamentos de la Estad√≠stica Bayesiana\n\nProbabilidad para cuantificar la incertidumbre. Modelos de probabilidad. Regla de Bayes. Inferencia bayesiana. Distribuci√≥n a priori, funci√≥n de verosimilitud, distribuci√≥n a posteriori.\n\nUnidad 2: Inferencia Bayesiana\n\nModelos de distribuciones conjugadas. Modelos de un par√°metro. Modelo beta-binomial. Enfoque intuitivo. Distribuci√≥n a posteriori como compromiso entre la verosimilitud y la distribuci√≥n a priori. Razonamiento secuencial. Modelo normal-normal. Modelo gamma‚ÄìPoisson. Modelos de varios par√°metros. Modelo normal ‚Äì normal-gamma-inversa. Modelo Dirichlet‚Äìmultinomial.\nElecci√≥n de distribuciones a priori: no informativas (impropias, de Jeffrey) y d√©bilmente informativas. Medidas de resumen de la distribuci√≥n a posteriori. Intervalos de credibilidad. Distribuci√≥n predictiva a posteriori. Nociones de teor√≠a de la decisi√≥n bayesiana. Riesgo bayesiano. Estimador de Bayes.\n\nUnidad 3: M√©todos Computacionales\n\nLimitaciones del enfoque anal√≠tico: c√°lculo de probabilidades y determinaci√≥n de la distribuci√≥n a posteriori. Soluciones: an√°lisis de datos simulados y aproximaci√≥n de grilla. Introducci√≥n al c√≥mputo bayesiano. Nociones b√°sicas de m√©todos de cadenas de Markov ‚Äì Montecarlo (MCMC). Algoritmo de Metropolis‚ÄìHastings. Montecarlo Hamiltoniano. Diagn√≥stico de m√©todos MCMC.\nProgramaci√≥n probabil√≠stica. Alternativas. Sintaxis de modelos. Ejemplos. Diagn√≥stico. Medidas de resumen a partir de las cadenas obtenidas. Visualizaciones.\n\nUnidad 4: Modelos Lineales\n\nModelos lineales. Elecci√≥n de distribuciones a priori. Regularizaci√≥n. Diagn√≥stico de modelos. Predicciones basadas en distribuciones de probabilidad. Pruebas predictivas a priori y a posteriori. Densidad predictiva a posteriori logar√≠tmica evaluada punto a punto (lppd). Deviance. Criterios de informaci√≥n: AIC, BIC, WAIC. Validacion cruzada. Sobreajuste y subajuste. Validaci√≥n cruzada utilizando muestreo por importancia mediante suavizado Pareto (PSIS-CV).\n\nUnidad 5: Modelos Avanzados\n\nRegresi√≥n log√≠stica. Regresi√≥n Poisson. Comparaci√≥n de grupos. Modelos de variable latente. Formulaci√≥n gr√°fica. An√°lisis de sensibilidad.\nEl enfoque multinivel: modelos jer√°rquicos. Modelo beta-binomial jer√°rquico. Shrinkage de par√°metros. Variaci√≥n en el intercepto. Variaci√≥n en la pendiente. Pooling de estimaciones. Problemas de estimaci√≥n.",
    "crumbs": [
      "Informaci√≥n",
      "Programa"
    ]
  },
  {
    "objectID": "info/bibliografia.html",
    "href": "info/bibliografia.html",
    "title": "Bibliograf√≠a",
    "section": "",
    "text": "Bibliograf√≠a principal\n\nJohnson, Ott, y Dogucu (2022) McElreath (2020) Gelman y Hill (2006) Kruschke (2014) Reich y Ghosh (2019)\n\n\n\nGelman, Andrew, y Jennifer Hill. 2006. Data Analysis Using Regression and Multilevel-Hierarchical Models. 1st edition. Cambridge University Press.\n\n\nJohnson, Alicia A., Miles Q. Ott, y Mine Dogucu. 2022. Bayes Rules! An Introduction to Bayesian Modeling. 1st edition. Chapman; Hall/CRC. https://www.bayesrulesbook.com/.\n\n\nKruschke, John. 2014. Doing Bayesian Data Analysis: A Tutorial with R, JAGS, and Stan. 2nd edition. Academic Press.\n\n\nMcElreath, Richard. 2020. Statistical Rethinking: A Bayesian Course with Examples in R and Stan. 2nd edition. Chapman; Hall/CRC.\n\n\nReich, Brian J., y Sujit K. Ghosh. 2019. Bayesian Statistical Methods. 1st edition. Chapman; Hall/CRC.\n\n\n\n\nBibliograf√≠a complementaria\n\nGelman et¬†al. (2013), Gelman, Hill, y Vehtari (2021), Downey (2021), Lee y Wagenmakers (2014), Davidson-Pilon (2015), Nicenboim, Schad, y Vasishth (2022), Barr (2021), Carlin y Louis (2008), Hoff (2009), MacKay (2003), Lambert (2018), Murphy (2022), Murphy (2023), Bishop (2006), Martin, Kumar, y Lao (2021), Theoridis (2020), Clyde et¬†al. (2022), Ma, Kording, y Goldreich (2022)\n\n\n\n\n\n\n\n\nBarr, Dale J. 2021. Learning statistical models through simulation in R: An interactive textbook. 1st edition. https://psyteachr.github.io/stat-models-v1/.\n\n\nBishop, Christopher M. 2006. Pattern Recognition and Machine Learning. 1st edition. Springer.\n\n\nCarlin, Bradley P., y Thomas A. Louis. 2008. Bayesian Methods for Data Analysis. 3rd edition. Chapman; Hall/CRC.\n\n\nClyde, Merlise, Mine √áetinkaya-Rundel, Colin Rundel, David Banks, Christine Chai, y Lizzy Huang. 2022. An Introduction to Bayesian Thinking. 1st edition. https://statswithr.github.io/book/.\n\n\nDavidson-Pilon, Cameron. 2015. Bayesian Methods for Hackers: Probabilistic Programming and Bayesian Inference. 1st edition. Addison-Wesley Data; Analytics Series.\n\n\nDowney, Allen B. 2021. Think Bayes: Bayesian Statistics in Python. 2nd edition. O‚ÄôReilly Media. http://allendowney.github.io/ThinkBayes2/.\n\n\nGelman, Andrew, John B. Carlin, Hal S. Stern, David B. Dunson, Aki Vehtari, y Donald B. Rubin. 2013. Bayesian Data Analysis. 3rd edition. Chapman; Hall/CRC.\n\n\nGelman, Andrew, Jennifer Hill, y Aki Vehtari. 2021. Regression and Other Stories. 1st edition. Cambridge University Press. https://users.aalto.fi/~ave/ROS.pdf.\n\n\nHoff, Peter D. 2009. A First Course in Bayesian Statistical Methods. 1st edition. Springer.\n\n\nLambert, Ben. 2018. A Student‚Äôs Guide to Bayesian Statistics. 1st edition. SAGE Publications Ltd.\n\n\nLee, Michael D., y Eric-Jan Wagenmakers. 2014. Bayesian Cognitive Modeling: A Practical Course. 1st edition. Cambridge University Press.\n\n\nMa, Wei Ji, Konrad P. Kording, y Daniel Goldreich. 2022. Bayesian Models of Perception and Action: An Introduction. 3rd edition. http://www.cns.nyu.edu/malab/bayesianbook.html.\n\n\nMacKay, David J. C. 2003. Information Theory, Inference and Learning Algorithms. 1st edition. Cambridge University Press.\n\n\nMartin, Osvaldo A., Ravin Kumar, y Junpeng Lao. 2021. Bayesian Modeling and Computation in Python. 1st edition. Chapman; Hall/CRC.\n\n\nMurphy, Kevin P. 2022. Probabilistic Machine Learning: An Introduction. 1st edition. The MIT Press. https://probml.ai/.\n\n\nMurphy, Kevin P. 2023. Probabilistic Machine Learning: Advanced Topics. 1st edition. The MIT Press. https://probml.ai/.\n\n\nNicenboim, Bruno, Daniel Schad, y Shravan Vasishth. 2022. An Introduction to Bayesian Data Analysis for Cognitive Science. https://vasishth.github.io/bayescogsci/book/.\n\n\nTheoridis, Sergios. 2020. Machine Learning: A Bayesian and Optimization Perspective. 2nd edition. Academic Press.",
    "crumbs": [
      "Informaci√≥n",
      "Bibliografia"
    ]
  },
  {
    "objectID": "presentaciones/presentacion_06.html#optimizaci√≥n",
    "href": "presentaciones/presentacion_06.html#optimizaci√≥n",
    "title": "Estad√≠stica Bayesiana",
    "section": "Optimizaci√≥n",
    "text": "Optimizaci√≥n\nPara modelizar la relaci√≥n entre una variable dependiente \\(Y\\) y ciertos predictores \\(X_l\\) asumimos un modelo de la forma\n\\[Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\dots + \\beta_p X_p + \\eta\\]\nEn realidad tenemos \\(N\\) observaciones y por lo tanto para cada observaci√≥n \\((y_i,\\mathbf{x}_i)\\) tenemos\n\\[y_i = \\beta_0 + \\beta_1 x_{1_i} + \\beta_2 x_{2_i} + \\dots + \\beta_p x_{p_i} + \\eta\\]\nO bien, matricialmente\n\\[y_i = \\boldsymbol{\\beta}^T \\mathbf{x}_i + \\eta\\]\nEl error \\(\\eta\\) es desconocido y, en principio, no es necesario asumir nada sobre este."
  },
  {
    "objectID": "presentaciones/presentacion_06.html#section",
    "href": "presentaciones/presentacion_06.html#section",
    "title": "Estad√≠stica Bayesiana",
    "section": "",
    "text": "Para predecir valores de \\(y_i\\) es necesario estimar \\(\\boldsymbol{\\beta}\\) por \\(\\hat{\\boldsymbol{\\beta}}\\) dando lugar al siguiente modelo predictivo:\n\\[\\hat{y}_i = \\hat\\beta_0 + \\hat\\beta_1 x_{1_i} + \\hat\\beta_2 x_{2_i} + \\dots + \\hat\\beta_p x_{p_i} = \\hat{\\boldsymbol{\\beta}}^T \\mathbf{x}_i\\]\nUna forma de estimar \\(\\boldsymbol{\\beta}\\) es minimizar alguna funci√≥n del error de aproximar \\(y\\) por \\(\\hat{y}_i\\):\n\\[\\hat{\\boldsymbol{\\beta}} = \\underset{\\boldsymbol\\beta}{\\mathrm{arg\\,min}}\\left[ J(\\boldsymbol\\beta) \\right] = \\underset{\\boldsymbol\\beta}{\\mathrm{arg\\,min}}\\left[ \\sum_{i=1}^N \\left(y_i - \\boldsymbol\\beta^T \\mathbf{x}_i\\right)^2 \\right]\\]\nEl \\(\\boldsymbol{\\beta}\\) que minimiza el error cuadr√°tico se conoce como estimador de m√≠nimos cuadrados. Esto es lo que se conoce como enfoque de optimizaci√≥n."
  },
  {
    "objectID": "presentaciones/presentacion_06.html#estad√≠stica-cl√°sica",
    "href": "presentaciones/presentacion_06.html#estad√≠stica-cl√°sica",
    "title": "Estad√≠stica Bayesiana",
    "section": "Estad√≠stica cl√°sica",
    "text": "Estad√≠stica cl√°sica\nAsumiendo un modelo probabil√≠stico para el error, \\(\\eta \\sim \\mathcal{N}(0,\\sigma^2)\\) se puede obtener el estimador de m√°xima verosimilitud de \\(\\boldsymbol\\beta\\).\nLa funci√≥n de verosimilitud viene dada por el producto de las funciones de densidad normales:\n\\[\\ell(\\boldsymbol\\beta,\\sigma|\\mathbf{y}) = \\prod_{i=1}^N p(y_i|\\mathbf{x_i},\\boldsymbol\\beta^T,\\sigma^2) = \\prod_{i=1}^N \\frac{1}{\\sqrt{2\\pi\\sigma}} e^{-\\frac{(y_i - \\boldsymbol\\beta^T\\mathbf{x}_i)^2}{2\\sigma^2}}\\]"
  },
  {
    "objectID": "presentaciones/presentacion_06.html#section-2",
    "href": "presentaciones/presentacion_06.html#section-2",
    "title": "Estad√≠stica Bayesiana",
    "section": "",
    "text": "Maximizar la verosimilitud equivale a minimizar el opuesto de la log-verosimilitud \\[\\mathcal{L}(\\boldsymbol\\beta,\\sigma|\\mathbf{y}) = \\log(\\ell(\\boldsymbol\\beta,\\sigma|\\mathbf{y}))\\]\n\\[\\hat{\\boldsymbol{\\beta}}_{ML} = \\underset{\\boldsymbol\\beta}{\\mathrm{arg\\,min}}\\left[ - \\sum_{i=1}^N \\log \\left( \\left( \\frac{1}{2\\pi\\sigma^2} \\right)^{1/2} e^{-\\frac{(y_i - \\boldsymbol\\beta^T\\mathbf{x}_i)^2}{2\\sigma^2}} \\right) \\right]\\]\nLa expresi√≥n anterior puede minimizarse primero respecto de \\(\\boldsymbol\\beta\\) y luego respecto de \\(\\sigma\\). Resulta que maximizar la verosimilitud respecto de \\(\\boldsymbol\\beta\\) equivale a minimizar el error cuadr√°tico."
  },
  {
    "objectID": "presentaciones/presentacion_06.html#estad√≠stica-bayesiana",
    "href": "presentaciones/presentacion_06.html#estad√≠stica-bayesiana",
    "title": "Estad√≠stica Bayesiana",
    "section": "Estad√≠stica bayesiana",
    "text": "Estad√≠stica bayesiana\nEn estad√≠stica bayesiana, consideramos a los par√°metros como variables aleatorias y les asignamos una distribuci√≥n a priori.\nAdem√°s, contamos con un modelo generativo (probabil√≠stico) para las observaciones: ¬øc√≥mo obtendr√≠amos observaciones si conoci√©ramos los par√°metros? Es una decisi√≥n de la modelizaci√≥n."
  },
  {
    "objectID": "presentaciones/presentacion_06.html#section-3",
    "href": "presentaciones/presentacion_06.html#section-3",
    "title": "Estad√≠stica Bayesiana",
    "section": "",
    "text": "Aqu√≠ asumimos:\n\\[Y_i \\mid \\boldsymbol\\beta,\\sigma \\sim \\mathcal{N}(\\boldsymbol\\beta^T \\mathbf{x}_i, \\sigma^2)\\]\no bien decimos\n\\[\n\\begin{align*}\n    Y_i & \\sim  \\mathcal{N}(\\mu_i, \\sigma^2) \\\\\n    \\mu_i & = \\beta_0 + \\beta_1 x_{1_i} + \\beta_2 x_{2_i} + \\dots + \\beta_p x_{p_i}\n\\end{align*}\n\\]\nY completamos el modelo especificando una distribuci√≥n a priori \\(p(\\boldsymbol\\beta,\\sigma)\\)"
  },
  {
    "objectID": "presentaciones/presentacion_06.html#section-4",
    "href": "presentaciones/presentacion_06.html#section-4",
    "title": "Estad√≠stica Bayesiana",
    "section": "",
    "text": "La estimaci√≥n se hace siempre de la misma manera\n\\(p(\\boldsymbol\\beta,\\sigma\\mid\\mathbf{y}) \\propto p(\\mathbf{y}|\\boldsymbol\\beta,\\sigma)p(\\boldsymbol\\beta,\\sigma)\\)"
  },
  {
    "objectID": "presentaciones/presentacion_06.html#section-5",
    "href": "presentaciones/presentacion_06.html#section-5",
    "title": "Estad√≠stica Bayesiana",
    "section": "",
    "text": "Observando un dato (¬øse puede hacer inferencia con un solo punto?)‚Ä¶"
  },
  {
    "objectID": "presentaciones/presentacion_06.html#section-6",
    "href": "presentaciones/presentacion_06.html#section-6",
    "title": "Estad√≠stica Bayesiana",
    "section": "",
    "text": "Observando el dato que sigue‚Ä¶"
  },
  {
    "objectID": "presentaciones/presentacion_06.html#section-7",
    "href": "presentaciones/presentacion_06.html#section-7",
    "title": "Estad√≠stica Bayesiana",
    "section": "",
    "text": "Observando dos puntos juntos‚Ä¶"
  },
  {
    "objectID": "presentaciones/presentacion_06.html#section-8",
    "href": "presentaciones/presentacion_06.html#section-8",
    "title": "Estad√≠stica Bayesiana",
    "section": "",
    "text": "Comparemos con un prior m√°s fuerte‚Ä¶"
  },
  {
    "objectID": "presentaciones/presentacion_06.html#section-9",
    "href": "presentaciones/presentacion_06.html#section-9",
    "title": "Estad√≠stica Bayesiana",
    "section": "",
    "text": "Observando un punto (¬øse puede hacer inferencia con un solo punto?)"
  },
  {
    "objectID": "presentaciones/presentacion_06.html#section-10",
    "href": "presentaciones/presentacion_06.html#section-10",
    "title": "Estad√≠stica Bayesiana",
    "section": "",
    "text": "Observando diez puntos"
  },
  {
    "objectID": "presentaciones/presentacion_06.html#section-11",
    "href": "presentaciones/presentacion_06.html#section-11",
    "title": "Estad√≠stica Bayesiana",
    "section": "",
    "text": "\\(\\mu\\) depende de los par√°metros (y por supuesto del valor de \\(x\\)), por lo que tiene una distribuci√≥n de probabilidad asociada"
  },
  {
    "objectID": "presentaciones/presentacion_06.html#section-12",
    "href": "presentaciones/presentacion_06.html#section-12",
    "title": "Estad√≠stica Bayesiana",
    "section": "",
    "text": "Por supuesto, las predicciones para \\(y\\) (\\(\\tilde{y}\\)) tambi√©n son probabil√≠sticas: distribuci√≥n predictiva a posteriori"
  },
  {
    "objectID": "presentaciones/presentacion_06.html#resumen",
    "href": "presentaciones/presentacion_06.html#resumen",
    "title": "Estad√≠stica Bayesiana",
    "section": "Resumen",
    "text": "Resumen\n\nTenemos una distribuci√≥n de probabilidad para los par√°metros. Es decir, tenemos incertidumbre en los valores de los par√°metros\nTenemos que trabajar con todo el posterior (a trav√©s de muestras) y no con estimaciones puntuales\nNo confundir predicci√≥n de la media (tambi√©n llamado predictor lineal) con distribuci√≥n predictiva (para las observaciones)\nA medida que aumenta el tama√±o de muestra, los coeficientes de la regresi√≥n se estiman cada vez con mayor precisi√≥n y la incertidumbre del predictor lineal desaparece (incertidumbre epist√©mica). No obstante, la incertidumbre en la distribuci√≥n predictiva no desaparece (siempre quedar√° \\(\\sigma\\): incertidumbre aleatoria)."
  },
  {
    "objectID": "presentaciones/presentacion_06.html#validaci√≥n-interna",
    "href": "presentaciones/presentacion_06.html#validaci√≥n-interna",
    "title": "Estad√≠stica Bayesiana",
    "section": "Validaci√≥n interna",
    "text": "Validaci√≥n interna\n\nEl modo fundamental de validar el ajuste de un modelo bayesiano es generar r√©plicas del conjunto de datos (utilizando el modelo ajustado) y compararlas con los datos reales. Esto es lo que se conoce como validaci√≥n interna.\nPara cada muestra de par√°metros del posterior podemos generar un dataset \\[\n\\left[\n\\begin{array}{l|l|l|l}\nY_1^{(1)} & Y_2^{(1)} & \\cdots & Y_{N}^{(1)} \\\\\nY_1^{(2)} & Y_2^{(2)} & \\cdots & Y_{N}^{(2)} \\\\\n\\vdots & \\vdots &  & \\vdots \\\\\nY_1^{(S)} & Y_2^{(S)} & \\cdots & Y_{N}^{(S)} \\\\\n\\end{array}\n\\right]\n\\]\nEsta pr√°ctica da lugar a los posterior predictive checks (PPC)"
  },
  {
    "objectID": "presentaciones/presentacion_06.html#validaci√≥n-externa",
    "href": "presentaciones/presentacion_06.html#validaci√≥n-externa",
    "title": "Estad√≠stica Bayesiana",
    "section": "Validaci√≥n externa",
    "text": "Validaci√≥n externa\n\nIdealmente quisi√©ramos ver si nuestro modelo tiene capacidad predictiva para datos nuevos (no usados para ajustarlo)\nAntes de preocuparnos por los datos nuevos, pensemos en las predicciones‚Ä¶ Las predicciones son probabil√≠sticas\nNo podemos simplemente comparar \\(y_i\\) con \\(\\hat{y}_i\\)\nDebemos utilizar toda la distribuci√≥n a posteriori para evaluar el ajuste (y la capacidad predictiva) del modelo"
  },
  {
    "objectID": "presentaciones/presentacion_06.html#section-15",
    "href": "presentaciones/presentacion_06.html#section-15",
    "title": "Estad√≠stica Bayesiana",
    "section": "",
    "text": "Un posible score predictivo para un determinado valor \\(y_i\\) es la probabilidad que el modelo le asocia (tambi√©n llamada densidad predictiva), \\[\\int p\\left(y_i\\mid\\theta\\right) p(\\theta\\mid y) d\\theta \\approx \\frac{1}{S}\\sum_{s=1}^S p(y_i\\mid \\theta^{(s)})\\]"
  },
  {
    "objectID": "presentaciones/presentacion_06.html#section-16",
    "href": "presentaciones/presentacion_06.html#section-16",
    "title": "Estad√≠stica Bayesiana",
    "section": "",
    "text": "El score predictivo total (para todas las observaciones) es la log-posterior pointwise predictive density. A mayor \\(\\mathrm{lppd}\\), mejor es el ajuste del modelo.\n\\[\n\\mathrm{lppd} =  \\color{#1f7a8c}{\\sum_{i=1}^{N}} \\color{#EB8A90}{\\log} \\left( \\color{#683257}{\\int p\\left(y_i\\mid\\theta\\right) \\underline{p(\\theta\\mid y)}d\\theta} \\right)\n\\]\n\nQue podemos estimar a trav√©s de muestras del posterior\n\\[\n\\mathrm{lppd} = \\color{#1f7a8c}{\\sum_{i=1}^{N}} \\color{#EB8A90}{\\log} \\left( \\color{#683257}{\\frac{1}{S} \\sum_{s=1}^{S} p\\left(y_i\\mid\\theta^{(s)}\\right)} \\right)\n\\]"
  },
  {
    "objectID": "presentaciones/presentacion_06.html#section-17",
    "href": "presentaciones/presentacion_06.html#section-17",
    "title": "Estad√≠stica Bayesiana",
    "section": "",
    "text": "La deviance de un modelo es\n\\[D = -2\\ \\mathrm{lppd}\\]\n\nLa deviance (o la \\(\\mathrm{lppd}\\)) eval√∫a las predicciones de un modelo (el ajuste), no nos dice qu√© tan correcto es‚Ä¶\nSon medidas que siempre mejoran con m√°s par√°metros\nEn realidad nos importa c√≥mo se desempe√±a el modelo con datos nuevos."
  },
  {
    "objectID": "presentaciones/presentacion_06.html#section-18",
    "href": "presentaciones/presentacion_06.html#section-18",
    "title": "Estad√≠stica Bayesiana",
    "section": "",
    "text": "La \\(\\mathrm{lppd}\\) predice el \\(i\\)-√©simo valor con un posterior que usa todos los datos, incluido el \\(i\\). M√°s que la \\(\\mathrm{lppd}\\) nos interesa su valor esperado en datos nuevos (\\(\\mathrm{elppd}\\)). Por supuesto, no conocemos datos nuevos.\n\nPodemos aproximar o estimar \\(\\mathrm{elppd}\\) haciendo cross-validation (CV) o, en particular, leave-one-out cross-validation (LOO-CV).\n\\[\\mathrm{elppd} \\approx \\mathrm{lppd}_{LOO} = \\sum_{i=1}^{N} \\log \\left( \\frac{1}{S} \\sum_{s=1}^{S} p\\left(y_i\\mid\\theta_{-i}^{(s)}\\right) \\right)\\]\ndonde los \\(\\theta_{-i}^{(s)}\\) son muestras del posterior de \\(\\theta\\) obtenido sin considerar la \\(i\\)-√©sima observaci√≥n.\nEl problema de hacer LOO-CV es que, si tenemos 1000 observaciones, hay que calcular 1000 distribuciones a posteriori."
  },
  {
    "objectID": "presentaciones/presentacion_06.html#section-19",
    "href": "presentaciones/presentacion_06.html#section-19",
    "title": "Estad√≠stica Bayesiana",
    "section": "",
    "text": "No contamos con muestras de \\(p(\\theta\\mid \\mathbf{y}_{-i})\\) sino simplemente de \\(p(\\theta\\mid \\mathbf{y})\\). No sabemos la distribuci√≥n a posteriori de \\(\\theta\\) sin considerar la observaci√≥n \\(i\\). Hay formas de aproximar el desempe√±o en LOO-CV sin necesidad de reajustar el modelo. Una forma de hacerlo es usar la ‚Äúimportancia‚Äù de cada observaci√≥n en el posterior. Esto da lugar a una t√©cnica que se conoce como Pareto-smoothed importance sampling cross-validation (PSIS)"
  },
  {
    "objectID": "presentaciones/presentacion_06.html#section-20",
    "href": "presentaciones/presentacion_06.html#section-20",
    "title": "Estad√≠stica Bayesiana",
    "section": "",
    "text": "Hist√≥ricamente se han desarrollado los llamados criterios de informaci√≥n que penalizan la verosimilitud con un t√©rmino adicional para compensar la capacidad de sobreajuste de un modelo que tiene m√°s par√°metros."
  },
  {
    "objectID": "presentaciones/presentacion_06.html#section-21",
    "href": "presentaciones/presentacion_06.html#section-21",
    "title": "Estad√≠stica Bayesiana",
    "section": "",
    "text": "El AIC (Akaike information criterion) es \\[AIC = D + 2p = -2\\ lppd + 2p\\] donde \\(p\\) es el n√∫mero de par√°metros del modelo y \\(D=-2\\ \\mathrm{lppd}\\) se conoce como deviance. Penalizamos el \\(\\mathrm{lppd}\\) con la tendencia (o capacidad) del modelo de sobreajustar."
  },
  {
    "objectID": "presentaciones/presentacion_06.html#section-22",
    "href": "presentaciones/presentacion_06.html#section-22",
    "title": "Estad√≠stica Bayesiana",
    "section": "",
    "text": "El WAIC (widely applicable information criterion) es un criterio m√°s general que el AIC (y un poquitito m√°s dif√≠cil de calcular):\n\\[WAIC = - 2\\left(\\mathrm{lppd} - \\sum_{i=1}^N \\mathbb{V}_\\theta\\left[\\log\\left(p(y_i\\mid\\theta\\right)\\right]\\right)\\]\n\\(\\sum_{i=1}^N \\mathrm{V}_\\theta\\left[\\log\\left(p(y_i\\mid\\theta\\right)\\right]\\) es un t√©rmino de penalizaci√≥n que se suele llamar ‚Äún√∫mero efectivo de par√°metros‚Äù. Es la suma de las varianzas en la log-probabilidad de cada observaci√≥n \\(i\\) (o sea, la varianza total). Si, para un determinado dato \\(i\\), las diferentes muestras del posterior \\(\\theta_{(s)}\\) dan como resultado predicciones muy diferentes, es porque el modelo tiene mucha incertidumbre (y es posiblemente muy flexible).\n\n\n\n\n\nEstad√≠stica Bayesiana ‚Äì 2025"
  },
  {
    "objectID": "presentaciones/presentacion_08.html#introducci√≥n",
    "href": "presentaciones/presentacion_08.html#introducci√≥n",
    "title": "Estad√≠stica Bayesiana",
    "section": "Introducci√≥n",
    "text": "Introducci√≥n\nConsideremos el siguiente modelo: \\[\n\\begin{align*}\n    y\\mid\\pi & \\sim  \\mathrm{Bi}(N_i,\\pi)\\\\\n    \\pi & \\sim  Beta(a,b)\n\\end{align*}\n\\]"
  },
  {
    "objectID": "presentaciones/presentacion_08.html#section",
    "href": "presentaciones/presentacion_08.html#section",
    "title": "Estad√≠stica Bayesiana",
    "section": "",
    "text": "Sabemos (gracias al TP2) que la funci√≥n de densidad de la distribuci√≥n beta se puede expresar en t√©rminos de su moda \\(\\omega\\) y su concentraci√≥n \\(\\kappa\\)\n\\[\n\\begin{align*}\n    y\\mid\\pi & \\sim  \\mathrm{Bi}(N,\\pi)\\\\\n    \\pi & \\sim  Beta(\\omega(\\kappa-2)+1,\\ (1-\\omega)(\\kappa-2)+1)\n\\end{align*}\n\\]\nEl valor de \\(\\pi\\) depende del valor de \\(\\omega\\). Lo sab√≠amos, despu√©s de todo, \\(\\omega\\) y \\(\\kappa\\) son las constantes de ajuste del prior o hiperpar√°metros. \\(\\kappa\\) refleja el grado de credibilidad a priori sobre los valores de \\(\\pi\\) (alrededor de \\(\\omega\\))."
  },
  {
    "objectID": "presentaciones/presentacion_08.html#section-1",
    "href": "presentaciones/presentacion_08.html#section-1",
    "title": "Estad√≠stica Bayesiana",
    "section": "",
    "text": "¬øQu√© pasa si \\(\\omega\\) no es fijo sino otro par√°metro a estimar?\n\nEn el contexto de una moneda: \\(\\pi\\) es la probabilidad de cara de la moneda y \\(\\omega\\) es el valor de probabilidad de cara al que el fabricante de monedas le apunta en la construcci√≥n. \\(\\kappa\\) (fijo y conocido) es una medida de la dispersi√≥n que tiene el proceso de fabricaci√≥n (de lo consistente que es este proceso) o, en otros t√©rminos, mide el grado de asociaci√≥n entre \\(\\pi\\) y \\(\\omega\\).\n\n\n¬øQu√© necesitamos para \\(\\omega\\)? Digamos que, a priori, \\(\\omega \\sim \\mathrm{Beta}(A_\\omega,B_\\omega)\\)\n\n\n\\[\n\\begin{align*}\n    y\\mid\\pi & \\sim  \\mathrm{Bi}(N,\\pi)\\\\\n    \\pi \\mid \\omega & \\sim  \\mathrm{Beta}(\\omega(\\kappa-2)+1,\\ (1-\\omega)(\\kappa-2)+1) \\\\\n    \\omega & \\sim \\mathrm{Beta}(A_\\omega,B_\\omega)\n\\end{align*}\n\\]\n¬øCu√°ntos par√°metros tiene este modelo?"
  },
  {
    "objectID": "presentaciones/presentacion_08.html#section-2",
    "href": "presentaciones/presentacion_08.html#section-2",
    "title": "Estad√≠stica Bayesiana",
    "section": "",
    "text": "Es un modelo de dos par√°metros (hay una distribuci√≥n conjunta a priori y una distribuci√≥n conjunta a posteriori) pero no como el \\(\\mu\\) y el \\(\\sigma\\) de una distribuci√≥n normal o el \\(\\beta_0\\) y \\(\\beta_1\\) de un modelo de regresi√≥n lineal‚Ä¶\n¬øC√≥mo funciona el modelo hacia adelante? \\(\\omega \\rightarrow \\pi \\rightarrow y\\) (\\(\\omega\\) influye en el valor de \\(y\\) solo a trav√©s de \\(\\pi\\))\n¬øY el razonamiento inverso? De \\(N\\) tiradas podemos hacer una inferencia sobre \\(\\pi\\), lo que nos permitir√° hacer una inferencia sobre \\(\\omega\\)"
  },
  {
    "objectID": "presentaciones/presentacion_08.html#section-3",
    "href": "presentaciones/presentacion_08.html#section-3",
    "title": "Estad√≠stica Bayesiana",
    "section": "",
    "text": "¬øQu√© distribuci√≥n a posteriori buscamos? \\(p(\\pi,\\omega\\mid y)\\)\n¬øY la Regla de Bayes? ¬øVale? ¬øC√≥mo la escribimos?\n\\[p(\\pi,\\omega\\mid y) = \\frac{p(y\\mid\\pi,\\omega)p(\\pi,\\omega)}{p(y)} = \\frac{p(y\\mid\\pi)p(\\pi\\mid\\omega)p(\\omega)}{p(y)}\\]\n¬øTenemos forma de expresar \\(p(y\\mid\\pi)\\), \\(p(\\pi\\mid\\omega)\\), y \\(p(\\omega)\\)?"
  },
  {
    "objectID": "presentaciones/presentacion_08.html#section-4",
    "href": "presentaciones/presentacion_08.html#section-4",
    "title": "Estad√≠stica Bayesiana",
    "section": "",
    "text": "Estamos haciendo inferencia bayesiana sobre una distribuci√≥n conjunta (de \\(\\pi\\) y \\(\\omega\\)). Pero la relaci√≥n entre los par√°metros (y la funci√≥n de verosimilitud) es jer√°rquica. La jerarqu√≠a tiene una interpretaci√≥n para el modelo."
  },
  {
    "objectID": "presentaciones/presentacion_08.html#section-5",
    "href": "presentaciones/presentacion_08.html#section-5",
    "title": "Estad√≠stica Bayesiana",
    "section": "",
    "text": "pi &lt;- seq(0,1,length.out=101)\nomega &lt;- seq(0,1,length.out=101)\nA_omega &lt;- 2\nB_omega &lt;- 2\nkappa &lt;- 5\nN &lt;- 9\ny &lt;- 3"
  },
  {
    "objectID": "presentaciones/presentacion_08.html#section-6",
    "href": "presentaciones/presentacion_08.html#section-6",
    "title": "Estad√≠stica Bayesiana",
    "section": "",
    "text": "prior &lt;- expand.grid(pi = pi, omega = omega) |&gt;\n  mutate(p_omega = dbeta(omega, A_omega, B_omega),\n         p_pi_given_omega = dbeta(pi, omega*(kappa-2)+1, (1-omega)*(kappa-2)+1),\n         prior = p_pi_given_omega * p_omega)\n\nplot_prior &lt;- ggplot(prior) +\n  geom_raster(aes(x=pi, y=omega, fill=prior)) +\n  geom_hline(yintercept = 0.6) +\n  geom_hline(yintercept = 0.9) +\n  scale_x_continuous(expression(pi), expand = c(0,0)) +\n  scale_y_continuous(expression(omega), expand = c(0,0)) +\n  viridis::scale_fill_viridis()\n\nplot_prior_omega &lt;- ggplot(prior) +\n  geom_line(aes(x=omega, y=p_omega))\n\nplot_prior_pi_omega1 &lt;- ggplot(prior |&gt; filter(omega == 0.6)) +\n  geom_line(aes(x=pi, y=prior)) +\n  xlab(expression(pi))\n\nplot_prior_pi_omega2 &lt;- ggplot(prior |&gt; filter(omega == 0.9)) +\n  geom_line(aes(x=pi, y=prior)) +\n  xlab(expression(pi))"
  },
  {
    "objectID": "presentaciones/presentacion_08.html#section-8",
    "href": "presentaciones/presentacion_08.html#section-8",
    "title": "Estad√≠stica Bayesiana",
    "section": "",
    "text": "likelihood &lt;- expand.grid(pi = pi, omega = omega) |&gt;\n  mutate(likelihood = dbinom(y, size = N, prob = pi))\n\nplot_likelihood &lt;- ggplot(likelihood) +\n  geom_raster(aes(x=pi, y=omega, fill=likelihood)) +\n  scale_x_continuous(expression(pi), expand = c(0,0)) +\n  scale_y_continuous(expression(omega), expand = c(0,0)) +\n  viridis::scale_fill_viridis()\n\nposterior &lt;- inner_join(prior,likelihood) |&gt;\n  mutate(posterior = prior * likelihood)\n\nplot_posterior &lt;- ggplot(posterior) +\n  geom_raster(aes(x=pi, y=omega, fill=posterior)) +\n  scale_x_continuous(expression(pi), expand = c(0,0)) +\n  scale_y_continuous(expression(omega), expand = c(0,0)) +\n  viridis::scale_fill_viridis()"
  },
  {
    "objectID": "presentaciones/presentacion_08.html#section-10",
    "href": "presentaciones/presentacion_08.html#section-10",
    "title": "Estad√≠stica Bayesiana",
    "section": "",
    "text": "pi &lt;- seq(0,1,length.out=101)\nomega &lt;- seq(0,1,length.out=101)\nA_omega &lt;- 20\nB_omega &lt;- 20\nkappa &lt;- 10\nN &lt;- 9\ny &lt;- 3"
  },
  {
    "objectID": "presentaciones/presentacion_08.html#section-13",
    "href": "presentaciones/presentacion_08.html#section-13",
    "title": "Estad√≠stica Bayesiana",
    "section": "",
    "text": "La inferencia bayesiana en un modelo jer√°rquico es inferencia en el espacio de la distribuci√≥n conjunta de los par√°metros pero reformulando la distribuci√≥n conjunta en t√©rminos jer√°rquicos: se refactoriza \\(p(\\pi,\\omega)\\) como \\(p(\\pi\\mid\\omega)p(\\omega)\\)"
  },
  {
    "objectID": "presentaciones/presentacion_08.html#extensi√≥n",
    "href": "presentaciones/presentacion_08.html#extensi√≥n",
    "title": "Estad√≠stica Bayesiana",
    "section": "Extensi√≥n",
    "text": "Extensi√≥n\n¬øQu√© pasar√≠a si contamos con m√°s de una moneda creada por la misma f√°brica? Cada moneda tiene un valor de \\(\\pi_s\\) que es propio y que a su vez tienen algo en com√∫n: provienen de la f√°brica que tiene par√°metro \\(\\omega\\).\nCon \\(y_1\\) caras en \\(N_1\\) tiradas de la moneda 1 estimamos \\(\\pi_1\\), con \\(y_2\\) caras en \\(N_2\\) tiradas de la moneda 1 estimamos \\(\\pi_2\\)‚Ä¶ y luego, con todas las tiradas, podemos estimar \\(\\omega\\)."
  },
  {
    "objectID": "presentaciones/presentacion_08.html#section-14",
    "href": "presentaciones/presentacion_08.html#section-14",
    "title": "Estad√≠stica Bayesiana",
    "section": "",
    "text": "Consideremos un caso real. \\(S\\) personas reciben una droga y son sometidos a un test de memoria. La probabilidad de que el sujeto \\(s\\) recuerde un √≠tem que se le muestra es \\(\\pi_s\\). El sujeto \\(s\\) recuerda \\(y_s\\) √≠tems de \\(N_s\\) que se le presentan. Asumimos que la droga induce un efecto en los sujetos alrededor de una tendencia central \\(\\omega\\).\nConsideremos por simplicidad que se tienen dos sujetos‚Ä¶\n¬øCu√°ntos par√°metros tiene el modelo? ¬øC√≥mo podemos representar la relaci√≥n entre los \\(\\theta_s\\)?"
  },
  {
    "objectID": "presentaciones/presentacion_08.html#section-15",
    "href": "presentaciones/presentacion_08.html#section-15",
    "title": "Estad√≠stica Bayesiana",
    "section": "",
    "text": "\\[\n\\begin{align*}\n    y_s\\mid\\pi_s & \\sim  \\mathrm{Bi}(N_s,\\pi_s)\\\\\n    \\pi_s \\mid \\omega & \\sim  \\mathrm{Beta}(\\omega(\\kappa-2)+1,\\ (1-\\omega)(\\kappa-2)+1) \\\\\n    \\omega & \\sim \\mathrm{Beta}(A_\\omega,B_\\omega)\n\\end{align*}\n\\]\nSi fijo \\(\\omega\\), los valores de los \\(\\pi_s\\) son independientes: \\(\\pi_1\\) y \\(\\pi_2\\) son independientes dado \\(\\omega\\).\nLa probabilidad a priori que es \\(p(\\pi_1,\\pi_2,\\omega)\\) ahora puede factorizarse como \\(p(\\pi_1,\\pi_2\\mid \\omega) p(\\omega) = p(\\pi_1\\mid \\omega) p(\\pi_2\\mid \\omega) p(\\omega)\\)"
  },
  {
    "objectID": "presentaciones/presentacion_08.html#section-16",
    "href": "presentaciones/presentacion_08.html#section-16",
    "title": "Estad√≠stica Bayesiana",
    "section": "",
    "text": "pi_1 &lt;- seq(0,1,length.out=101)\npi_2 &lt;- seq(0,1,length.out=101)\nomega &lt;- seq(0,1,length.out=101)\nA_omega &lt;- 2\nB_omega &lt;- 2\nkappa &lt;- 5\nN_1 &lt;- 20\ny_1 &lt;- 5\nN_2 &lt;- 8\ny_2 &lt;- 4"
  },
  {
    "objectID": "presentaciones/presentacion_08.html#section-17",
    "href": "presentaciones/presentacion_08.html#section-17",
    "title": "Estad√≠stica Bayesiana",
    "section": "",
    "text": "prior &lt;- expand.grid(pi_1 = pi_1, pi_2 = pi_2, omega = omega) |&gt;\n  mutate(\n    p_omega = dbeta(omega, A_omega, B_omega),\n    p_pi1_given_omega = dbeta(pi_1, \n                              omega*(kappa-2)+1, \n                              (1-omega)*(kappa-2)+1),\n    p_pi2_given_omega = dbeta(pi_2, \n                              omega*(kappa-2)+1, \n                              (1-omega)*(kappa-2)+1),\n    prior = p_pi1_given_omega *  p_pi2_given_omega * p_omega)\n\nprior_pi1 &lt;- ggplot(prior |&gt; \n                      group_by(pi_1,omega) |&gt; \n                      summarise(prior = sum(prior))) +\n  geom_raster(aes(x=pi_1, y=omega, fill=prior)) +\n  scale_x_continuous(expression(pi[1]), expand = c(0,0)) +\n  scale_y_continuous(expression(omega), expand = c(0,0)) +\n  viridis::scale_fill_viridis()\n\nprior_pi2 &lt;- ggplot(prior |&gt; \n                      group_by(pi_2,omega) |&gt; \n                      summarise(prior = sum(prior))) +\n  geom_raster(aes(x=pi_2, y=omega, fill=prior)) +\n  scale_x_continuous(expression(pi[2]), expand = c(0,0)) +\n  scale_y_continuous(expression(omega), expand = c(0,0)) +\n  viridis::scale_fill_viridis()"
  },
  {
    "objectID": "presentaciones/presentacion_08.html#section-19",
    "href": "presentaciones/presentacion_08.html#section-19",
    "title": "Estad√≠stica Bayesiana",
    "section": "",
    "text": "likelihood &lt;- expand.grid(pi_1 = pi_1, pi_2 = pi_2, omega = omega) |&gt;\n  mutate(likelihood_pi1 = dbinom(y_1, size = N_1, prob = pi_1),\n         likelihood_pi2 = dbinom(y_2, size = N_2, prob = pi_2),\n         likelihood = likelihood_pi1 * likelihood_pi2)\n\nlikelihood1 &lt;- ggplot(likelihood |&gt; \n                        group_by(pi_1,omega) |&gt; \n                        summarise(likelihood = sum(likelihood))) +\n  geom_raster(aes(x=pi_1, y=omega, fill=likelihood)) +\n  scale_x_continuous(expression(pi[1]), expand = c(0,0)) +\n  scale_y_continuous(expression(omega), expand = c(0,0)) +\n  viridis::scale_fill_viridis()\n\nlikelihood2 &lt;- ggplot(likelihood |&gt; \n                        group_by(pi_2,omega) |&gt; \n                        summarise(likelihood = sum(likelihood))) +\n  geom_raster(aes(x=pi_2, y=omega, fill=likelihood)) +\n  scale_x_continuous(expression(pi[2]), expand = c(0,0)) +\n  scale_y_continuous(expression(omega), expand = c(0,0)) +\n  viridis::scale_fill_viridis()"
  },
  {
    "objectID": "presentaciones/presentacion_08.html#section-21",
    "href": "presentaciones/presentacion_08.html#section-21",
    "title": "Estad√≠stica Bayesiana",
    "section": "",
    "text": "posterior &lt;- inner_join(prior, likelihood) |&gt;\n  mutate(posterior = prior * likelihood)\n\nposterior_pi1 &lt;- ggplot(posterior |&gt; \n                          group_by(pi_1,omega) |&gt; \n                          summarise(posterior = sum(posterior))) +\n  geom_raster(aes(x=pi_1, y=omega, fill=posterior)) +\n  scale_x_continuous(expression(pi[1]), expand = c(0,0)) +\n  scale_y_continuous(expression(omega), expand = c(0,0)) +\n  viridis::scale_fill_viridis()\n\nposterior_pi2 &lt;- ggplot(posterior |&gt; \n                          group_by(pi_2,omega) |&gt; \n                          summarise(posterior = sum(posterior))) +\n  geom_raster(aes(x=pi_2, y=omega, fill=posterior)) +\n  scale_x_continuous(expression(pi[2]), expand = c(0,0)) +\n  scale_y_continuous(expression(omega), expand = c(0,0)) +\n  viridis::scale_fill_viridis()\n\nposterior_omega &lt;- ggplot(posterior |&gt; \n                            group_by(omega) |&gt; \n                            summarise(posterior = sum(posterior))) +\n  geom_line(aes(x=omega, y=posterior)) +\n  xlab(expression(omega))\n\nposterior_pi1_marg &lt;- ggplot(posterior |&gt; \n                               group_by(pi_1) |&gt; \n                               summarise(posterior = sum(posterior))) +\n  geom_line(aes(x=pi_1, y=posterior)) +\n  xlab(expression(pi[1]))\n\nposterior_pi2_marg &lt;- ggplot(posterior |&gt; \n                               group_by(pi_2) |&gt; \n                               summarise(posterior = sum(posterior))) +\n  geom_line(aes(x=pi_2, y=posterior)) +\n  xlab(expression(pi[2]))"
  },
  {
    "objectID": "presentaciones/presentacion_08.html#section-24",
    "href": "presentaciones/presentacion_08.html#section-24",
    "title": "Estad√≠stica Bayesiana",
    "section": "",
    "text": "La funci√≥n de verosimilitud no depende de \\(\\omega\\)\nLa funci√≥n de verosimilitud es m√°s estrecha para el sujeto 1 que para el sujeto 2\nEl posterior marginal de \\(\\pi_1\\) est√° cerca de la proporci√≥n muestral\nEl posterior marginal de \\(\\pi_2\\) est√° cerca de la proporci√≥n muestral\nEl posterior marginal de \\(\\pi_1\\) tiene menos incertidumbre que el de \\(\\pi_2\\)\n\n\n¬øQu√© ocurre si se cambia el valor de \\(\\kappa\\)?"
  },
  {
    "objectID": "presentaciones/presentacion_08.html#section-25",
    "href": "presentaciones/presentacion_08.html#section-25",
    "title": "Estad√≠stica Bayesiana",
    "section": "",
    "text": "pi_1 &lt;- seq(0,1,length.out=101)\npi_2 &lt;- seq(0,1,length.out=101)\nomega &lt;- seq(0,1,length.out=101)\nA_omega &lt;- 2\nB_omega &lt;- 2\nkappa &lt;- 100\nN_1 &lt;- 20\ny_1 &lt;- 5\nN_2 &lt;- 8\ny_2 &lt;- 4"
  },
  {
    "objectID": "presentaciones/presentacion_08.html#section-30",
    "href": "presentaciones/presentacion_08.html#section-30",
    "title": "Estad√≠stica Bayesiana",
    "section": "",
    "text": "El posterior marginal de \\(\\pi_2\\) se alej√≥ de la proporci√≥n muestral. El sujeto 1 ten√≠a un tama√±o de muestra mayor (m√°s evidencia) y por lo tanto influy√≥ m√°s en la estimaci√≥n de \\(\\omega\\), lo que a la vez influye en la estimaci√≥n de \\(\\pi_2\\)."
  },
  {
    "objectID": "presentaciones/presentacion_08.html#shrinkage",
    "href": "presentaciones/presentacion_08.html#shrinkage",
    "title": "Estad√≠stica Bayesiana",
    "section": "Shrinkage",
    "text": "Shrinkage\nLa estructura jer√°rquica de los modelos hace que las estimaciones de los par√°metros de los niveles m√°s bajos se acerquen m√°s de lo que lo har√≠an si no hubiera una distribuci√≥n en un nivel superior. Esto es lo que se conoce como shrinkage de las estimaciones.\nLas estimaciones de los par√°metros de los niveles m√°s bajos son tiradas (pulled) o se estrechan o tienden a concentrarse hacia la moda de la distribuci√≥n superior."
  },
  {
    "objectID": "presentaciones/presentacion_08.html#section-31",
    "href": "presentaciones/presentacion_08.html#section-31",
    "title": "Estad√≠stica Bayesiana",
    "section": "",
    "text": "El shrinkage ocurre porque los par√°metros de los niveles bajos (los \\(\\pi_s\\)) son influenciados por:\n\n\nEl conjunto de datos que dependen directamente de ese par√°metro\nLos par√°metros de niveles m√°s altos de los cuales dependen los par√°metros de niveles m√°s bajos (¬°y que son afectados por todos los datos!)\n\n\n\nPor ejemplo, sobre \\(\\pi_1\\) influyen \\(y_1\\) y \\(N_1\\) pero tambi√©n \\(\\omega\\) (cuya estimaci√≥n depende de \\(\\pi_2\\) y \\(N_2\\)).\n\n\nNota: el shrinkage es consecuencia exclusivamente de la estructura jer√°rquica (y no de la inferencia bayesiana). Existe en la teor√≠a cl√°sica de estimaci√≥n (ver estimador de James-Stein)"
  },
  {
    "objectID": "presentaciones/presentacion_08.html#otro-ejemplo",
    "href": "presentaciones/presentacion_08.html#otro-ejemplo",
    "title": "Estad√≠stica Bayesiana",
    "section": "Otro ejemplo",
    "text": "Otro ejemplo\nEl rad√≥n es un gas radioactivo y cancer√≠geno. Los productos de la desintegraci√≥n del rad√≥n son tambi√©n radioactivos y en altas concentraciones se sabe que producen c√°ncer de pulm√≥n. Trabajaremos con datos de mediciones de rad√≥n en el estado de Minnesota. Se cuenta con mediciones en hogares de diferentes condados dentro del estado."
  },
  {
    "objectID": "presentaciones/presentacion_08.html#section-33",
    "href": "presentaciones/presentacion_08.html#section-33",
    "title": "Estad√≠stica Bayesiana",
    "section": "",
    "text": "\\(i\\) es el √≠ndice de los hogares\n\\(Y_i\\) es el nivel de rad√≥n (log rad√≥n) del hogar \\(i\\)\n\\(j\\) (entre \\(1\\) y \\(J\\)) es el √≠ndice de los condados\n\\(j[i] = \\mathrm{county}[i]\\) es el condado al que pertenece el hogar \\(i\\)"
  },
  {
    "objectID": "presentaciones/presentacion_08.html#section-35",
    "href": "presentaciones/presentacion_08.html#section-35",
    "title": "Estad√≠stica Bayesiana",
    "section": "",
    "text": "Complete pooling\n\\[\n\\begin{align*}\n    Y_i\\mid\\mu_i,\\sigma & \\sim  \\mathcal{N}(\\mu_i,\\sigma^2) \\\\\n    \\mu_i & = \\alpha  \\\\\n    \\alpha & \\sim  P(\\alpha) \\\\\n    \\sigma & \\sim  P(\\sigma)\n\\end{align*}\n\\]\nHay una √∫nica media \\(\\alpha\\) com√∫n para todos los \\(i\\), independientemente del grupo \\(j\\) al que pertenezcan"
  },
  {
    "objectID": "presentaciones/presentacion_08.html#section-36",
    "href": "presentaciones/presentacion_08.html#section-36",
    "title": "Estad√≠stica Bayesiana",
    "section": "",
    "text": "No pooling\n\\[\n\\begin{align*}\n    Y_i\\mid\\mu_i,\\sigma & \\sim  \\mathcal{N}(\\mu_i,\\sigma^2)\\\\\n    \\mu_i & = \\alpha_{j[i]}  \\\\\n    \\alpha_j & \\sim  P(\\alpha_j) \\\\\n    \\sigma & \\sim  P(\\sigma)\n\\end{align*}\n\\]\nDecimos que los \\(Y_i\\) tienen distribuci√≥n de media \\(\\alpha_{j[i]}\\), sin imponer ninguna restricci√≥n sobre los \\(\\alpha_j\\). \\(P(\\alpha_j)\\) es una distribuci√≥n no informativa (muy ancha y chata). Todos los \\(\\alpha_j\\) son independientes. Coincide con la estimaci√≥n cl√°sica que incluye una variable dummy para cada grupo."
  },
  {
    "objectID": "presentaciones/presentacion_08.html#section-37",
    "href": "presentaciones/presentacion_08.html#section-37",
    "title": "Estad√≠stica Bayesiana",
    "section": "",
    "text": "Podemos mejorar el modelo anterior incorporando un prior que regularice los \\(\\alpha_j\\)\n\\[\\begin{align*}\n    Y_i\\mid\\mu_i,\\sigma & \\sim  \\mathcal{N}(\\mu_i,\\sigma^2) \\\\\n    \\mu_i & = \\alpha_{j[i]}  \\\\\n    \\alpha_j & \\sim  \\mathcal{N}(0,10) \\\\\n    \\sigma & \\sim  P(\\sigma)\n\\end{align*}\\]\n0 y 10 son valores arbitrarios para la media y la varianza de la distribuci√≥n a priori de los \\(\\alpha_j\\). Los \\(\\alpha_j\\) dejan de poder ser estimados libremente. Hay regularizaci√≥n y tendemos a evitar el overfitting. Hay un partial pooling. Si en lugar de 10 se elige un valor m√°s grande, tendemos a no pooling; si se elige un valor m√°s chico, tendemos a pooling completo"
  },
  {
    "objectID": "presentaciones/presentacion_08.html#section-38",
    "href": "presentaciones/presentacion_08.html#section-38",
    "title": "Estad√≠stica Bayesiana",
    "section": "",
    "text": "Mejor a√∫n, podemos estimar el grado de regularizaci√≥n partir de los datos. ¬øCu√°nto pooling es necesario? Se estima a partir de los datos‚Ä¶\n\\[\\begin{align*}\n    Y_i\\mid\\mu_i,\\sigma & \\sim  \\mathcal{N}(\\mu_i,\\sigma^2) \\\\\n    \\mu_i & = \\alpha_{j[i]}  \\\\\n    \\alpha_j & \\sim  \\mathcal{N}(\\mu_\\alpha,\\sigma_\\alpha^2) \\\\\n    \\sigma & \\sim  P(\\sigma) \\\\\n    \\mu_\\alpha & \\sim  P(\\mu_\\alpha) \\\\\n    \\sigma_\\alpha & \\sim  P(\\sigma_\\alpha) \\\\\n\\end{align*}\\]"
  },
  {
    "objectID": "presentaciones/presentacion_08.html#section-39",
    "href": "presentaciones/presentacion_08.html#section-39",
    "title": "Estad√≠stica Bayesiana",
    "section": "",
    "text": "\\(\\mu_\\alpha\\) y \\(\\sigma_\\alpha\\) son hiperpar√°metros (par√°metros de la distribuci√≥n de a priori de los par√°metros) y por lo tanto tienen hiperpriors\nEl chiste es que todos los datos se usan para estimar \\(\\mu_\\alpha\\) y \\(\\sigma_\\alpha\\) y por lo tanto en la estimaci√≥n de cada \\(\\alpha_j\\) hay informaci√≥n de todos los datos. La regularizaci√≥n es adaptativa (se aprende de los datos)."
  },
  {
    "objectID": "presentaciones/presentacion_08.html#shrinkage-1",
    "href": "presentaciones/presentacion_08.html#shrinkage-1",
    "title": "Estad√≠stica Bayesiana",
    "section": "Shrinkage",
    "text": "Shrinkage\n\nSiempre que hay regularizaci√≥n, hay shrinkage de par√°metros.\nLos datos de un grupo ayudan en la estimaci√≥n de los par√°metros de los otros grupos (partial pooling: pr√©stamo de informaci√≥n).\nAs√≠, los grupos que tienen menor tama√±o de muestra toman m√°s informaci√≥n del resto de los grupos y el shrinkage es m√°s intenso."
  },
  {
    "objectID": "presentaciones/presentacion_08.html#section-40",
    "href": "presentaciones/presentacion_08.html#section-40",
    "title": "Estad√≠stica Bayesiana",
    "section": "",
    "text": "Pooling completo: hay una √∫nica media para todos los individuos, independientemente del grupo. La variaci√≥n entre los grupos es cero. Underfitting.\nNo pooling: cada grupo tiene una media independiente de la de los dem√°s. La variaci√≥n entre los grupos es infinita. No se comparte informaci√≥n entre los grupos, lo que se sabe de un grupo no ayuda a inferir sobre los dem√°s. Overfitting.\nPartial pooling: cada grupo tiene una media pero todas las medias est√°n conectadas. Es una soluci√≥n de compromiso, un punto medio entre pooling completo y no pooling."
  },
  {
    "objectID": "presentaciones/presentacion_08.html#section-41",
    "href": "presentaciones/presentacion_08.html#section-41",
    "title": "Estad√≠stica Bayesiana",
    "section": "",
    "text": "Para algunos condados: a la izquierda est√° la estimaci√≥n de la media no pooling de la media, a la derecha la estimaci√≥n del modelo multinivel (pooling parcial). En l√≠nea de trazos el pooling completo."
  },
  {
    "objectID": "presentaciones/presentacion_08.html#otro-ejemplo-1",
    "href": "presentaciones/presentacion_08.html#otro-ejemplo-1",
    "title": "Estad√≠stica Bayesiana",
    "section": "Otro ejemplo",
    "text": "Otro ejemplo\nCorredores que han participado varias veces de una famosa marat√≥n en Washington. Se registraron los tiempos de los participantes."
  },
  {
    "objectID": "presentaciones/presentacion_08.html#section-45",
    "href": "presentaciones/presentacion_08.html#section-45",
    "title": "Estad√≠stica Bayesiana",
    "section": "",
    "text": "La informaci√≥n de la estimaci√≥n de la pendiente de un grupo es √∫til para estimar las otras pendientes\nLa informaci√≥n de la estimaci√≥n de las ordenadas al origen de un grupo es √∫til para estimar las otras ordenadas al origen\nLas pendientes y las ordenadas al origen trabajan de forma conjunta para describir a un corredor, covar√≠an"
  },
  {
    "objectID": "presentaciones/presentacion_08.html#section-46",
    "href": "presentaciones/presentacion_08.html#section-46",
    "title": "Estad√≠stica Bayesiana",
    "section": "",
    "text": "\\[\\begin{align*}\n    Y_i  \\mid \\mu_i,\\sigma & \\sim  \\mathcal{N}(\\mu_i,\\sigma^2) \\\\\n    \\mu_i & = \\alpha_{j[i]} + \\beta_{j[i]} x_i \\\\\n    \\left[\\begin{array}{c}\\alpha_j\\\\\\beta_j\\end{array}\\right] & \\sim \\mathcal{N}\\left(\\left[\\begin{array}{c}\\mu_\\alpha\\\\\\mu_\\beta\\end{array}\\right],\\Sigma\\right)\n\\end{align*}\\]"
  },
  {
    "objectID": "presentaciones/presentacion_08.html#section-47",
    "href": "presentaciones/presentacion_08.html#section-47",
    "title": "Estad√≠stica Bayesiana",
    "section": "",
    "text": "\\(\\left[\\begin{array}{c}\\alpha_j\\\\\\beta_j\\end{array}\\right]\\) tienen una distribuci√≥n conjunta, normal multivariada de hiperpar√°metros \\(\\left[\\begin{array}{c}\\mu_\\alpha\\\\\\mu_\\beta\\end{array}\\right]\\) y \\(\\Sigma\\). ¬°Necesitan !\n\n\\(\\Sigma\\) puede factorizarse seg√∫n: \n\\[\n\\Sigma = \\left( \\begin{array}{cc} \\sigma_\\alpha^2 & \\sigma_\\alpha\\sigma_\\beta\\rho \\\\ \\sigma_\\alpha\\sigma_\\beta\\rho & \\sigma_\\beta^2 \\end{array}\\right) =\n\\left( \\begin{array}{cc} \\sigma_\\alpha & 0 \\\\ 0 & \\sigma_\\beta \\end{array}\\right) \\left( \\begin{array}{cc} 1 & \\rho \\\\ \\rho & 1 \\end{array}\\right)\n\\left( \\begin{array}{cc} \\sigma_\\alpha & 0 \\\\ 0 & \\sigma_\\beta \\end{array}\\right)\n\\]\nLlamando \\[R = \\left( \\begin{array}{cc} 1 & \\rho \\\\ \\rho & 1 \\end{array}\\right)\\] solo habr√≠a que definir una distribuci√≥n para \\(\\sigma_\\alpha\\), \\(\\sigma_\\beta\\) y \\(R\\) (o \\(\\rho\\))"
  },
  {
    "objectID": "presentaciones/presentacion_08.html#section-48",
    "href": "presentaciones/presentacion_08.html#section-48",
    "title": "Estad√≠stica Bayesiana",
    "section": "",
    "text": "No es solo para modelos lineales‚Ä¶ Se tienen 60 tanques con ranitas de la especie Hyperolius viridiflavus. Cada tanque \\(i\\) de ellos contiene una cantidad inicial de renacuajos \\(N_i\\). Al cabo de unas semanas se observa el n√∫mero \\(S_i\\) de renacuajos que sobrevivieron en el tanque \\(i\\)."
  },
  {
    "objectID": "presentaciones/presentacion_08.html#section-49",
    "href": "presentaciones/presentacion_08.html#section-49",
    "title": "Estad√≠stica Bayesiana",
    "section": "",
    "text": "Se modeliza la probabilidad de supervivencia de cada tanque con una regresi√≥n log√≠stica:\n\\[\\begin{align*}\n    S_i  \\mid p_i & \\sim  Binomial(N_i,p_i) \\\\\n   \\log\\left( \\frac{p_i}{1-p_i} \\right) & = \\alpha_{i} \\\\\n   \\alpha_i & \\sim \\mathcal{N}(\\mu_\\alpha,\\sigma_\\alpha^2)\n\\end{align*}\\]\nObservar que no hay grupos (no hay √≠ndice \\(j\\)), simplemente hacemos de los individuos."
  },
  {
    "objectID": "presentaciones/presentacion_08.html#section-50",
    "href": "presentaciones/presentacion_08.html#section-50",
    "title": "Estad√≠stica Bayesiana",
    "section": "",
    "text": "Comparamos la estimaci√≥n de \\(p_i = \\frac{e^{\\alpha_i}}{1+e^{\\alpha_i}}\\) con la obtenida por m√°xima verosimilitud en cada tanque: \\(p_{i,ML} = \\frac{S_i}{N_i}\\)"
  },
  {
    "objectID": "presentaciones/presentacion_08.html#intercambiabilidad",
    "href": "presentaciones/presentacion_08.html#intercambiabilidad",
    "title": "Estad√≠stica Bayesiana",
    "section": "Intercambiabilidad",
    "text": "Intercambiabilidad\nSi no existe otra informaci√≥n m√°s que los datos observados \\(y_i\\) para distinguir a los individuos \\(i\\) y estos no pueden ordenarse ninguna manera entonces se puede asumir una simetr√≠a de los par√°metros. Se dice que los par√°metros \\(\\theta_i\\) son intercambiables (exchangeable).\nSi las observaciones pueden agruparse y los grupos son indistinguibles (con caracter√≠sticas propias desconocidas) con propiedades/particularidades ignoradas entonces los grupos son intercambiables y los individuos, parcialmente o condicionalmente intercambiables."
  },
  {
    "objectID": "presentaciones/presentacion_08.html#distribuciones-predictivas",
    "href": "presentaciones/presentacion_08.html#distribuciones-predictivas",
    "title": "Estad√≠stica Bayesiana",
    "section": "Distribuciones predictivas",
    "text": "Distribuciones predictivas\nEn los modelos jer√°rquicos hay dos tipos de distribuciones predictivas a posteriori:\n\nPredicciones para individuos que pertenecen a grupos ya existentes (tiradas de la moneda con la que se realizaron las inferencias, otra tarea de memoria para un individuo que ya particip√≥ del experimento)\nPredicciones para individuos pertenecientes a grupos nuevos (tiradas de una nueva moneda de la f√°brica, c√≥mo afectar√≠a la droga a un individuo nuevo)"
  },
  {
    "objectID": "presentaciones/presentacion_08.html#resumen",
    "href": "presentaciones/presentacion_08.html#resumen",
    "title": "Estad√≠stica Bayesiana",
    "section": "Resumen",
    "text": "Resumen\n\nLos modelos jer√°rquicos resultan atractivos para problemas en los cuales los par√°metros se pueden considerar vinculados de cierta forma, por ejemplo en grupos.\nLos modelos jer√°rquicos o multinivel son extensiones de los modelos lineales (y de los modelos lineales generalizados) para datos que tienen alg√∫n grado de agrupamiento y en los cuales se permite que los par√°metros var√≠en por grupo\nLos modelos multinivel permiten mejorar las inferencias en contextos donde la muestra es peque√±a. Si un individuo que tiene pocas observaciones pertenece a un determinado grupo, se supone que compartir√° caracter√≠sticas con otros individuos de ese grupo y por lo tanto la estimaci√≥n de sus par√°metros podr√° ser informada por la de sus pares.\n\n\n\n\n\nEstad√≠stica Bayesiana ‚Äì 2025"
  },
  {
    "objectID": "presentaciones/stan.html#qu√©-es-stan",
    "href": "presentaciones/stan.html#qu√©-es-stan",
    "title": "Estad√≠stica Bayesiana",
    "section": "¬øQu√© es Stan?",
    "text": "¬øQu√© es Stan?\n\nStan es un lenguaje escrito en C++ para realizar Inferencia Bayesiana en utilizando probabil√≠sticos.\nEl programa es de c√≥digo abierto, todo lo relacionado a Stan se puede encontrar en http://mc-stan.org/ junto con un manual de usuario instrucciones de uso.\nLa primera versi√≥n, Stan 1.0, se lanz√≥ en 2012 y actualmente el programa ya est√° en la versi√≥n XXX\nSe puede llamar a Stan desde R usando el paquete {rstan}, o desde Python usando el paquete pystan. Stan cuenta tambi√©n cuenta con interfaces a otros lenguajes."
  },
  {
    "objectID": "presentaciones/stan.html#instalaci√≥n-de-stan-y-rstan",
    "href": "presentaciones/stan.html#instalaci√≥n-de-stan-y-rstan",
    "title": "Estad√≠stica Bayesiana",
    "section": "Instalaci√≥n de Stan y RStan",
    "text": "Instalaci√≥n de Stan y RStan\n\nEn Windows (o Mac), escriba en la consola R: install.packages(‚Äùrstan‚Äù, repositorios = c(‚Äùhttps://mc-stan.org/r-packages/‚Äù, getOption(‚Äùrepos‚Äù)))\nEn Linux recomiendo instalar v√≠a terminal sudo add-apt-repository ppa:c2d4u.team/c2d4u4.0+     sudo apt install ‚Äìno-install-recomienda r-cran-rstan\nCualquier problema de instalaci√≥n verifique: https://github.com/stan-dev/rstan/wiki/RStan-Getting-Started"
  },
  {
    "objectID": "presentaciones/stan.html#elementos-de-un-programa-de-stan",
    "href": "presentaciones/stan.html#elementos-de-un-programa-de-stan",
    "title": "Estad√≠stica Bayesiana",
    "section": "Elementos de un programa de Stan",
    "text": "Elementos de un programa de Stan\nEn Stan definimos un modelo probabil√≠stico utilizando el Idioma Stan.\n\nEl modelo se define intuitivamente pero debe seguir algunas normas.\nUn programa en Stan se define en bloques, son:\n\nDatos.\nPar√°metros.\nPar√°metros transformados.\nModelo."
  },
  {
    "objectID": "presentaciones/stan.html#datos",
    "href": "presentaciones/stan.html#datos",
    "title": "Estad√≠stica Bayesiana",
    "section": "Datos",
    "text": "Datos\ndata {\n    int&lt;lower=1&gt; N; // Cantidad de observaciones\n    int&lt;lower=1&gt; K; // Cantidad de variables\n    matrix[N, K] X; // Matriz de dise√±o\n    vector[N] y;    // Vector de respuestas\n}\n\nEn este bloque definimos qu√© datos, y de qu√© tipo, ser√°n utilizado en el modelo.\nTenemos que declarar el tipo y dimensi√≥n de los datos.\nAdem√°s de los datos, tambi√©n puede haber otras constantes, tama√±o de la muestra y n√∫mero de predictores.\nLos tipos m√°s comunes son: int, real, matriz, vector"
  },
  {
    "objectID": "presentaciones/stan.html#par√°metros",
    "href": "presentaciones/stan.html#par√°metros",
    "title": "Estad√≠stica Bayesiana",
    "section": "Par√°metros",
    "text": "Par√°metros\nparameters {\n    vector[K] beta;\n    real&lt;lower=0&gt; sigma;\n}\n\nLos par√°metros del modelo a estimar se declaran en el bloque de par√°metros.\nDebemos imponer las restricciones necesarias a los par√°metros.\nLos par√°metros generalmente se definen como vectores (n√∫meros reales) o uno real."
  },
  {
    "objectID": "presentaciones/stan.html#modelo",
    "href": "presentaciones/stan.html#modelo",
    "title": "Estad√≠stica Bayesiana",
    "section": "Modelo",
    "text": "Modelo\nmodel {\n    vector[N] mu;\n    mu = X * beta;\n    \n    // priors\n    beta ~ normal(0, 10);\n    sigma ~ cauchy(0, 5);\n\n    // likelihood\n    y ~ normal(mu, sigma);\n}\n\nEl bloque modelo es donde definimos las distribuciones previas y la verosimilitud del modelo.\nTambi√©n podemos declarar algunas variables que no son de nuestro inter√©s pero facilita la escritura del modelo."
  },
  {
    "objectID": "presentaciones/stan.html#section",
    "href": "presentaciones/stan.html#section",
    "title": "Estad√≠stica Bayesiana",
    "section": "",
    "text": "Stan tiene muchas distribuciones de probabilidad ya definidas. Pero tambi√©n existe la posibilidad de que el usuario defina su propia distribuci√≥n.\nEs muy recomendable escribir el modelo de forma matricial.\n\nTODO agregar ejemplos, parametrizaciones, y enlaces a la documentaci√≥n"
  },
  {
    "objectID": "presentaciones/stan.html#datos-de-tipo-escalar",
    "href": "presentaciones/stan.html#datos-de-tipo-escalar",
    "title": "Estad√≠stica Bayesiana",
    "section": "Datos de tipo escalar",
    "text": "Datos de tipo escalar\nEnteros y reales sin restricciones:\nint N;\nreal theta;\nEnteros y reales con restricciones:\nint&lt;lower = 1&gt; N;\nreal&lt;lower = 0&gt; sigma;\nreal&lt;lower =-1, upper = 1&gt; rho;"
  },
  {
    "objectID": "presentaciones/stan.html#vectores-y-matrices",
    "href": "presentaciones/stan.html#vectores-y-matrices",
    "title": "Estad√≠stica Bayesiana",
    "section": "Vectores y matrices",
    "text": "Vectores y matrices\nLos vectores en Stan son de tipo columna y se declaran junto con la dimensi√≥n:\nvector[3] u;\nTambien pueden tener restricciones\nvector&lt;lower = 0&gt;[3] u;\nLas matrices se declaran junto a su n√∫mero de filas y columnas\nmatrix[3, 3] A;\nmatrix&lt;upper=0&gt;[3, 4] B;"
  },
  {
    "objectID": "presentaciones/stan.html#tipos-especiales-de-matrices-y-vectores",
    "href": "presentaciones/stan.html#tipos-especiales-de-matrices-y-vectores",
    "title": "Estad√≠stica Bayesiana",
    "section": "Tipos especiales de matrices y vectores",
    "text": "Tipos especiales de matrices y vectores\nVector unitario simplex (suma de elementos es igual a 1)\nsimplex[5] theta;\nVector ordenado o positivo y ordenado\nordered[5] c;\npositive_ordered[5] d;\nVector fila\nrow_vector[3] u;\nMatriz de covarianza\ncov_matrix[K] Omega;\nMatriz de correlaci√≥n\ncorr_matrix[3] Sigma;"
  },
  {
    "objectID": "presentaciones/stan.html#ejemplos",
    "href": "presentaciones/stan.html#ejemplos",
    "title": "Estad√≠stica Bayesiana",
    "section": "Ejemplos",
    "text": "Ejemplos\n\nModelo Bernoulli\nModleo binomial\nModelo Poisson\nModelo con prior uniforme\nModelo de regresi√≥n lineal\nModelo de regresi√≥n logistica\n\n\n\n\n\nEstad√≠stica Bayesiana ‚Äì 2025"
  },
  {
    "objectID": "presentaciones/presentacion_05.html#el-problema",
    "href": "presentaciones/presentacion_05.html#el-problema",
    "title": "Estad√≠stica Bayesiana",
    "section": "El Problema",
    "text": "El Problema\nT√≠picamente interesa resolver los siguientes problemas:\n\n\nCalcular integrales de la forma \\(\\mathbb{E}[\\phi(x)] = \\int \\phi(x) p(x) d x\\) (law of the unconscious statistician)\nGenerar \\(S\\) muestras independientes \\(x^{(s)}\\) de una distribuci√≥n de probabilidad \\(p(x)\\)\n\n\n\nEn la estad√≠stica bayesiana, \\(x\\) es \\(\\theta\\), el par√°metro desconocido de alguna distribuci√≥n de probabilidad y \\(p(x)\\) es el posterior."
  },
  {
    "objectID": "presentaciones/presentacion_05.html#m√©todos-de-montecarlo",
    "href": "presentaciones/presentacion_05.html#m√©todos-de-montecarlo",
    "title": "Estad√≠stica Bayesiana",
    "section": "M√©todos de Montecarlo",
    "text": "M√©todos de Montecarlo"
  },
  {
    "objectID": "presentaciones/presentacion_05.html#section",
    "href": "presentaciones/presentacion_05.html#section",
    "title": "Estad√≠stica Bayesiana",
    "section": "",
    "text": "Para el primer problema, sabemos que si \\(X_i \\sim p(x)\\), bajo ciertas condiciones podemos aproximar\n\n\\[\\mathbb{E}[X] \\approx \\frac{1}{N} \\sum_{i=1}^N x_i\\]\n\n\nSi \\(X\\) es una variable aleatoria, entonces para funciones continuas \\(\\phi\\) tenemos que \\(\\phi(X)\\) tambi√©n es una variable aleatoria y por lo tanto\n\n\n\\[\\mathbb{E}[\\phi(X)] = \\int\\phi(x)p(x)dx \\approx \\frac{1}{N} \\sum_{i=1}^N \\phi(x_i)\\]\n\n\nEs decir, si los \\(x_i\\) son muestras de \\(p(x)\\), entonces la integral \\(\\int\\phi(x)p(x)dx\\) puede aproximarse por \\(\\frac{1}{N} \\sum_{i=1}^N \\phi(x_i)\\)."
  },
  {
    "objectID": "presentaciones/presentacion_05.html#section-1",
    "href": "presentaciones/presentacion_05.html#section-1",
    "title": "Estad√≠stica Bayesiana",
    "section": "",
    "text": "Esto ya lo hemos hecho\n\n\nLa distribuci√≥n predictiva a posteriori es \\(\\int p(\\tilde{y} \\mid \\theta) p(\\theta\\mid y) d\\theta\\) y puede aproximarse por \\(\\frac{1}{N} \\sum_{i=1}^N p(\\tilde{y}\\mid \\theta_i)\\)\nEl riesgo bayesiano es \\(\\int L(\\theta,\\hat\\theta) p(\\theta\\mid y) d\\theta\\) y puede aproximarse por \\(\\frac{1}{N} \\sum_{i=1}^N L(\\theta_i,\\hat\\theta)\\)\nSi consideramos la integral \\(\\int \\mathbb{I}_{\\theta \\in A} p(\\theta\\mid y) d\\theta = \\int_A p(\\theta\\mid y)d\\theta\\) es la probabilidad de que \\(\\theta\\) est√© en \\(A\\) y puede aproximarse por \\(\\frac{1}{N} \\sum_{i=1}^N \\mathbb{I}_{\\theta_i \\in A}\\)"
  },
  {
    "objectID": "presentaciones/presentacion_05.html#section-2",
    "href": "presentaciones/presentacion_05.html#section-2",
    "title": "Estad√≠stica Bayesiana",
    "section": "",
    "text": "Teniendo muestras de \\(p(x)\\) es f√°cil estimar las integrales \\(\\mathbb{E}[\\phi(x)] = \\int \\phi(x) p(x) d x\\) por lo que nos centraremos en el problema de c√≥mo obtener muestras de \\(p(x)\\).\n\nPara algunas distribuciones de probabilidad es f√°cil obtener muestras. Pero no siempre existe una funci√≥n rbinom, rbeta, rnorm, rpoiss, etc."
  },
  {
    "objectID": "presentaciones/presentacion_05.html#section-3",
    "href": "presentaciones/presentacion_05.html#section-3",
    "title": "Estad√≠stica Bayesiana",
    "section": "",
    "text": "Tomar muestras de una distribuci√≥n de probabilidad \\(p(x)\\) implica obtener valores que provienen, con mayor frecuencia, de regiones donde \\(p(x)\\) es grande.\n¬øPor qu√© es dif√≠cil tomar muestras de una distribuci√≥n de probabilidad?\n\nEn estad√≠stica bayesiana tenemos \\(p(\\theta \\mid y ) \\propto p(y\\mid\\theta) p(\\theta)\\) por lo que en general llegamos a \\(p^*(\\theta \\mid y) = \\frac{1}{Z} p(\\theta\\mid y)\\)\n\n\n\nLa determinaci√≥n de \\(Z\\) implica resolver una integral (potencialmente multivariada) que puede no tener soluci√≥n anal√≠tica (intractability of the integral)\nA√∫n conociendo \\(Z\\), no hay una manera determinada de obtener muestras de \\(p(\\theta\\mid y)\\)\nTomar muestras de distribuciones discretas es m√°s f√°cil que hacerlo de distribuciones continuas"
  },
  {
    "objectID": "presentaciones/presentacion_05.html#section-4",
    "href": "presentaciones/presentacion_05.html#section-4",
    "title": "Estad√≠stica Bayesiana",
    "section": "",
    "text": "¬øC√≥mo tomamos muestras de una distribuci√≥n discreta?"
  },
  {
    "objectID": "presentaciones/presentacion_05.html#grid-approximation",
    "href": "presentaciones/presentacion_05.html#grid-approximation",
    "title": "Estad√≠stica Bayesiana",
    "section": "Grid approximation",
    "text": "Grid approximation\nUna soluci√≥n puede ser discretizar la variable. Esta soluci√≥n vale incluso si no conocemos \\(Z\\). Conocemos \\(p^*(x) = \\frac{1}{Z} p(x)\\) (izquierda) y pasamos a una discreta \\(\\tilde{p}^*(x) = \\frac{1}{\\tilde{Z}} \\tilde{p}(x)\\) (centro).\n\nEvaluando \\(\\tilde{p}\\) en todos los posibles \\(x_i\\) de la grilla podemos calcular \\(Z=\\sum_{i} \\tilde{p}^*(x_i)\\). Luego tomamos muestras de \\(\\tilde{p}(x)\\) (derecha)."
  },
  {
    "objectID": "presentaciones/presentacion_05.html#section-5",
    "href": "presentaciones/presentacion_05.html#section-5",
    "title": "Estad√≠stica Bayesiana",
    "section": "",
    "text": "En c√≥digo:\n\nprob &lt;- function(x) return(exp(0.4*(x-0.4)^2 - 0.08*x^4)) # sabemos evaluar p\nx &lt;- seq(-4.5, 4.5, 0.5)\np_ &lt;- prob(x) # ~p*\nZ &lt;- sum(p_)\np_rulito &lt;- p_/Z # ~p\nsample(x, replace = TRUE, prob = p_rulito)"
  },
  {
    "objectID": "presentaciones/presentacion_05.html#section-6",
    "href": "presentaciones/presentacion_05.html#section-6",
    "title": "Estad√≠stica Bayesiana",
    "section": "",
    "text": "¬øC√≥mo se aplica esto en estad√≠stica bayesiana?\n\nEl posterior es \\(\\frac{1}{Z} p(y\\mid \\theta) p(\\theta)\\). Sabemos calcular el valor del posterior (sin normalizar) para cualquier valor de \\(\\theta\\): haciendo el producto del prior por el likelihood.\n\n\nPodemos considerar una grilla de valores del par√°metro (o los par√°metros), computar el posterior sin normalizar para cada valor de la grilla, normalizarlo y tomar muestras de √©l.\n\n\nPero, escala muy mal con el n√∫mero de par√°metros‚Ä¶"
  },
  {
    "objectID": "presentaciones/presentacion_05.html#section-7",
    "href": "presentaciones/presentacion_05.html#section-7",
    "title": "Estad√≠stica Bayesiana",
    "section": "",
    "text": "Ejemplo\n\n\n\n\n\n\nQueremos realizar inferencias sobre la media y la varianza de una normal. Para eso proponemos el siguiente modelo: \\[    \n\\begin{align*}\n    y_i\\mid\\mu,\\sigma^2 & \\sim  \\mathcal{N}(\\mu,\\sigma^2) \\\\\n    \\mu,\\sigma^2 & \\sim  \\frac{1}{K} \\frac{e^{-\\sigma^2}}{\\eta} e^{-\\frac{(\\mu - \\xi)^2}{2\\psi^2}}\n\\end{align*}\n\\] (¬øCu√°les son las constantes que ajustan el prior?)"
  },
  {
    "objectID": "presentaciones/presentacion_05.html#section-8",
    "href": "presentaciones/presentacion_05.html#section-8",
    "title": "Estad√≠stica Bayesiana",
    "section": "",
    "text": "Deber√≠amos tomar valores de \\(\\mu\\) en el intervalo \\((-4,4)\\) y valores de \\(\\sigma\\) en el intervalo \\((0,3)\\) y construir una grilla de valores.\nPara cada valor de la grilla podr√≠amos calular el posterior sin normalizar haciendo el producto del prior por el likelihood (necesitamos la muestra)."
  },
  {
    "objectID": "presentaciones/presentacion_05.html#rejection-sampling",
    "href": "presentaciones/presentacion_05.html#rejection-sampling",
    "title": "Estad√≠stica Bayesiana",
    "section": "Rejection sampling",
    "text": "Rejection sampling\nSe basa en buscar una distribuci√≥n de probabilidad candidata \\(q(x)\\) tal que \\(Cq(x)\\geq p^*(x)\\). Se toma una muestra de \\(q(x)\\). Luego se toma una muestra \\(u\\) de \\(\\mathrm{Unif}(0,Cq(x))\\). La muestra de \\(q(x)\\) se retiene si \\(u&lt;p^*(x)\\)."
  },
  {
    "objectID": "presentaciones/presentacion_05.html#section-9",
    "href": "presentaciones/presentacion_05.html#section-9",
    "title": "Estad√≠stica Bayesiana",
    "section": "",
    "text": "Necesitamos elegir con cuidado \\(q(x)\\)"
  },
  {
    "objectID": "presentaciones/presentacion_05.html#markov-chain-monte-carlo",
    "href": "presentaciones/presentacion_05.html#markov-chain-monte-carlo",
    "title": "Estad√≠stica Bayesiana",
    "section": "Markov chain Monte-Carlo",
    "text": "Markov chain Monte-Carlo\nQueremos obtener muestras de \\(p(x)\\). Vamos a hacer un viaje por los distintos valores de \\(x\\) tratando de pasar m√°s tiempo (m√°s iteraciones) en los puntos donde \\(p(x)\\) es grande.\n\nIdea general:\n\nVisitar los distintos valores posibles de \\(x\\)\nGenerar una secuencia de iteraciones: \\(\\{x^{(1)},x^{(2)},\\dots,x^{(S)}\\}\\)\nEn general, para obtener \\(x^{(i+1)}\\) usamos \\(x^{(i)}\\)\n\n\nEn nuestro caso tenemos \\(p(\\theta) \\propto p(y\\mid\\theta)p(\\theta) = p^*(\\theta\\mid y)\\) (unnormalized posterior)\n¬øQu√© necesitamos? Poder evaluar el prior y poder evaluar el likelihood para cualquier valor de \\(\\theta\\)"
  },
  {
    "objectID": "presentaciones/presentacion_05.html#metropolis-hastings-mh",
    "href": "presentaciones/presentacion_05.html#metropolis-hastings-mh",
    "title": "Estad√≠stica Bayesiana",
    "section": "Metropolis-Hastings (MH)",
    "text": "Metropolis-Hastings (MH)\nEl algoritmo de Metropolis‚ÄìHastings (1953)\n\nEn la iteraci√≥n \\(i\\) estamos en el valor del par√°metro \\(\\theta^{(i)}\\)\nEn funci√≥n del valor de par√°metro actual \\(\\theta^{(i)}=\\theta\\), proponemos un nuevo valor \\(\\theta'\\) en funci√≥n de \\(q(\\theta'\\mid\\theta)\\)\nDecidimos si vamos a la nueva ubicaci√≥n \\(\\theta^{(i+1)} = \\theta'\\) o si nos quedamos \\(\\theta^{(i+1)} = \\theta\\):\n\nCalcular la probabilidad de salto: \\[\\alpha_{\\theta \\rightarrow \\theta'} = \\min\\left\\{ 1,\\frac{f(\\theta')}{f(\\theta)} \\right\\}\\]\nPasar a \\(\\theta'\\) con probabilidad \\(\\alpha_{\\theta \\rightarrow \\theta'}\\): \\[\\theta^{(i+1)} =\n\\begin{cases}\n\\theta' \\text{ con probabilidad } \\alpha_{\\theta \\rightarrow \\theta'} \\\\\n\\theta \\text{ con probabilidad } (1-\\alpha_{\\theta \\rightarrow \\theta'})\n\\end{cases}\\]"
  },
  {
    "objectID": "presentaciones/presentacion_05.html#section-10",
    "href": "presentaciones/presentacion_05.html#section-10",
    "title": "Estad√≠stica Bayesiana",
    "section": "",
    "text": "\\(q(\\theta'\\mid\\theta)\\) se llama distribuci√≥n de proposici√≥n o de salto propuesto. Todo lo que necesitamos saber es d√≥nde estamos \\(f(\\theta)\\) y hacia donde queremos ir \\(f(\\theta')\\).\n\nPuede probarse que para cualquier \\(q(\\theta'\\mid\\theta)\\), cuando \\(s\\to \\infty\\) la distribuci√≥n de probabilidad de la secuencia \\(\\left\\{\\theta^{(s)} \\right\\}_{s=1}^S\\) tiende a \\(p(\\theta)\\). No sabemos nada sobre la rapidez con la que lo hace.\n\n\nEn infinitos pasos, cualquier cadena dar√° muestras de la distribuci√≥n \\(p(\\theta)\\), en la pr√°ctica hay que tener algunos cuidados."
  },
  {
    "objectID": "presentaciones/presentacion_05.html#section-12",
    "href": "presentaciones/presentacion_05.html#section-12",
    "title": "Estad√≠stica Bayesiana",
    "section": "",
    "text": "Necesitamos muestras de \\(p(\\theta)\\)\nTomamos un punto inicial\nElegimos una distribuci√≥n de saltos posibles \\(q(\\theta'\\mid\\theta)\\)\nProponemos un salto\n¬øSaltamos?"
  },
  {
    "objectID": "presentaciones/presentacion_05.html#section-13",
    "href": "presentaciones/presentacion_05.html#section-13",
    "title": "Estad√≠stica Bayesiana",
    "section": "",
    "text": "Notar que la probabilidad de transicionar de \\(\\theta\\) a \\(\\theta'\\) es \\(t(\\theta'\\mid\\theta) = q(\\theta'\\mid\\theta) \\alpha_{\\theta \\rightarrow \\theta'}\\) (la probabilidad de proponer el salto a ese \\(\\theta'\\) y de aceptarla)\n\nEn realidad, una de las condiciones necesarias para que la secuencia de muestras tienda a la distribuci√≥n buscada \\(f(\\theta)\\) es que el salto sea reversible: es decir, que la probabilidad de estar en \\(\\theta\\) y transicionar a \\(\\theta'\\) tiene que ser igual que la de estar en \\(\\theta'\\) y transicionar a \\(\\theta\\): \\(f(\\theta)t(\\theta'\\mid\\theta) = f(\\theta')t(\\theta\\mid\\theta')\\).\n\n\nSi la distribuci√≥n propuesta \\(q\\) es sim√©trica, esto est√° resuelto. ¬øSe puede elegir una \\(q\\) que no sea sim√©trica? ¬øQu√© es lo que nos define la probabilidad de pasar de \\(\\theta\\) a \\(\\theta'\\) (y viceversa)?\n\n\nTenemos que ajustar la probabilidad de transici√≥n:\n\\[\\alpha = \\min\\left\\{ 1,\\frac{f(\\theta')}{f(\\theta)} \\frac{q(\\theta \\mid \\theta')}{q(\\theta' \\mid \\theta)} \\right\\}\\]"
  },
  {
    "objectID": "presentaciones/presentacion_05.html#section-14",
    "href": "presentaciones/presentacion_05.html#section-14",
    "title": "Estad√≠stica Bayesiana",
    "section": "",
    "text": "Para el caso de \\(q\\) sim√©trica:\n\ntheta &lt;- double()\ntheta[1] &lt;- -1    \ni &lt;- 1\n\npropuesta &lt;- rnorm(1, mean = theta[i], sd = 0.8)\n\nf_actual &lt;- fx(theta[i])\nf_propuesta &lt;- fx(propuesta)\n\nalpha &lt;- min(c(1,f_propuesta/f_actual))\n\nquehacemos &lt;- sample(c(\"salto\",\"no salto\"), \n                    size = 1, \n                    prob = c(alpha,1-alpha))\n\nif(quehacemos==\"salto\") {\n  theta[i+1] &lt;- propuesta \n} else {\n  theta[i+1] &lt;- theta[i]\n  }\n\nDebe repetirse el proceso en un for"
  },
  {
    "objectID": "presentaciones/presentacion_05.html#section-15",
    "href": "presentaciones/presentacion_05.html#section-15",
    "title": "Estad√≠stica Bayesiana",
    "section": "",
    "text": "\\[\\sigma = 0.8\\]\n\n\n \\[\\sigma = 0.1\\]\n\n\n \\[\\sigma = 0.6\\]\n\n\n \\[\\sigma = 4.8\\]"
  },
  {
    "objectID": "presentaciones/presentacion_05.html#section-16",
    "href": "presentaciones/presentacion_05.html#section-16",
    "title": "Estad√≠stica Bayesiana",
    "section": "",
    "text": "¬øQu√© esperamos de nuestra cadena?\n\nRepresentatividad: haber explorado el rango completo de la distribuci√≥n a posteriori, independientemente de las condiciones iniciales\nPrecisi√≥n y estabilidad: a lo largo de diferentes cadenas (distintas condiciones iniciales)\nEficiencia: esperamos requerir la menor cantidad posible de muestras\n\nNing√∫n objetivo se alcanza absolutamente, existen chequeos gr√°ficos y num√©ricos para saber si las cadenas de MCMC est√°n sanas."
  },
  {
    "objectID": "presentaciones/presentacion_05.html#trace-plots",
    "href": "presentaciones/presentacion_05.html#trace-plots",
    "title": "Estad√≠stica Bayesiana",
    "section": "Trace Plots",
    "text": "Trace Plots\nGraficar los valores que toma el algoritmo como funci√≥n del tiempo (lo que t√≠picamente llamamos la cadena). Se tiene que ver como un fuzzy caterpillar (buen mixing). Para los impresionables: ruido blanco sin ning√∫n patr√≥n particular."
  },
  {
    "objectID": "presentaciones/presentacion_05.html#autocorrelaci√≥n",
    "href": "presentaciones/presentacion_05.html#autocorrelaci√≥n",
    "title": "Estad√≠stica Bayesiana",
    "section": "Autocorrelaci√≥n",
    "text": "Autocorrelaci√≥n\nLas muestras tienen que ser independientes. La dependencia de valores anteriores tiene que desaparecer r√°pido . Podemos medirlo con la autocorrelaci√≥n.\nPara cada valor de lag \\(k\\) se calcula la correlaci√≥n de la serie consigo misma originando la funci√≥n de autocorrelaci√≥n (\\(ACF(k)\\))"
  },
  {
    "objectID": "presentaciones/presentacion_05.html#n√∫mero-efectivo-de-muestras",
    "href": "presentaciones/presentacion_05.html#n√∫mero-efectivo-de-muestras",
    "title": "Estad√≠stica Bayesiana",
    "section": "N√∫mero efectivo de muestras",
    "text": "N√∫mero efectivo de muestras\nLas muestras no son independientes. ¬øA cu√°ntas muestras independientes equivalen nuestras \\(S\\) muestras? \\(N_{eff}\\) es el n√∫mero de muestras independientes que tienen el mismo poder de estimaci√≥n que \\(S\\) muestras correlacionadas (el error de estimaci√≥n es proporcional a \\(\\frac{1}{\\sqrt{N_{eff}}}\\))\n\\[N_{eff} = \\frac{S}{1 + 2 \\sum_{k=1}^\\infty ACF(k)}\\]"
  },
  {
    "objectID": "presentaciones/presentacion_05.html#hatr",
    "href": "presentaciones/presentacion_05.html#hatr",
    "title": "Estad√≠stica Bayesiana",
    "section": "\\(\\hat{R}\\)",
    "text": "\\(\\hat{R}\\)\nEl estad√≠stico de Rubin‚ÄìGelman \\(\\hat{R}\\) es un indicador de convergencia. Si m√∫ltiples cadenas se establizaron en un muestreo representativo del posterior, la diferencia promedio entre cadenas debe ser similar a la diferencia promedio en la cadena.\n\\[\\hat{R} = \\sqrt{\\frac{\\frac{S-1}{S} W  +  \\frac{1}{S}  B}{W}}\\]\nEl valor 1 indica convergencia. Si una cadena se perdi√≥/divergi√≥, el \\(\\hat{R}\\) ser√° mucho mayor a 1."
  },
  {
    "objectID": "presentaciones/presentacion_05.html#section-18",
    "href": "presentaciones/presentacion_05.html#section-18",
    "title": "Estad√≠stica Bayesiana",
    "section": "",
    "text": "Si tenemos \\(M\\) cadenas, \\(\\theta_m\\), cada una de las cuales tiene \\(S\\) muestras \\(\\theta_m^{(s)}\\). La varianza entre cadenas (\\(B\\)) es:\n\\[B = \\frac{S}{M-1} \\sum_{m=1}^M (\\bar{\\theta}^{(\\bullet)}_{m} - \\bar{\\theta}^{(\\bullet)}_{\\bullet})^2\\]\n\\[\\bar{\\theta}_m^{(\\bullet)} = \\frac{1}{S} \\sum_{s = 1}^S \\theta_m^{(s)}\\]\n\\[\\bar{\\theta}^{(\\bullet)}_{\\bullet} = \\frac{1}{M} \\, \\sum_{m=1}^M \\bar{\\theta}_m^{(\\bullet)}\\]\nLa varianza intra cadena (\\(W\\)) es:\n\\[W = \\frac{1}{M} \\, \\sum_{m=1}^M s_m^2\\]\n\\[ s_m^2 = \\frac{1}{S-1} \\, \\sum_{s=1}^S (\\theta^{(s)}_m - \\bar{\\theta}^{(\\bullet)}_m)^2\\]"
  },
  {
    "objectID": "presentaciones/presentacion_05.html#section-19",
    "href": "presentaciones/presentacion_05.html#section-19",
    "title": "Estad√≠stica Bayesiana",
    "section": "",
    "text": "El estimador de la varianza total\n\\[\\widehat{\\mbox{var}}^{+}\\!(\\theta|y) = \\frac{N-1}{N}\\, W \\, + \\, \\frac{1}{N} \\, B\\]\n\\[\\hat{R} \\, = \\, \\sqrt{\\frac{\\widehat{\\mbox{var}}^{+}\\!(\\theta|y)}{W}}\\]"
  },
  {
    "objectID": "presentaciones/presentacion_05.html#hamiltonian-montecarlo",
    "href": "presentaciones/presentacion_05.html#hamiltonian-montecarlo",
    "title": "Estad√≠stica Bayesiana",
    "section": "Hamiltonian Montecarlo",
    "text": "Hamiltonian Montecarlo\n\nMetropolis-Hastings (MH) es una exploraci√≥n a ciegas del espacio de par√°metros\nLa distribuci√≥n de propuesta de salto es fija\nEn las colas de la distribuci√≥n, se proponen tanto saltos que se acercan al grueso (bulk) de la distribuci√≥n como saltos que se alejan. Se rechazan muchos saltos propuestos.\nHamiltonian-Montecarlo (HMC) es una variante m√°s eficiente de MCMC. Para lograr la eficiencia, los saltos propuestos se adaptan a la forma del posterior.\nLa forma del posterior est√° en su gradiente"
  },
  {
    "objectID": "presentaciones/presentacion_05.html#section-21",
    "href": "presentaciones/presentacion_05.html#section-21",
    "title": "Estad√≠stica Bayesiana",
    "section": "",
    "text": "HMC trata de aprovechar la geometr√≠a local del posterior para decidir d√≥nde ir en la pr√≥xima iteraci√≥n.\nSi bien MH no ignora por completo la forma del posterior, HMC utiliza m√°s informaci√≥n (el gradiente)\nPara entender conceptualmente HMC se necesita un poco de imaginaci√≥n y entender algo de F√≠sica"
  },
  {
    "objectID": "presentaciones/presentacion_05.html#section-22",
    "href": "presentaciones/presentacion_05.html#section-22",
    "title": "Estad√≠stica Bayesiana",
    "section": "",
    "text": "\\(p^*(\\theta\\mid y)\\) es el posterior sin normalizar. Consideraremos \\(-\\log[p^*(\\theta\\mid y)]\\).\nLos puntos de alta densidad de probabilidad (m√°ximos locales de \\(p^*(\\theta\\mid y)\\)) se convierten en m√≠nimos locales de \\(-\\log[p^*(\\theta\\mid y)]\\)\nLa l√≥gica es la misma que en MH (despu√©s de todo, se trata de un algoritmo de MCMC): estamos en alg√∫n punto del espacio de par√°metros y decidimos movernos a otro‚Ä¶ Aqu√≠ cambia c√≥mo proponemos un salto.\nPara ello, imaginamos un trineo (o culipat√≠n, o bolita) que puede deslizarse por la superficie determinada por \\(-\\log[p^*(\\theta\\mid y)]\\)"
  },
  {
    "objectID": "presentaciones/presentacion_05.html#section-23",
    "href": "presentaciones/presentacion_05.html#section-23",
    "title": "Estad√≠stica Bayesiana",
    "section": "",
    "text": "Si soltamos el trineo en alg√∫n punto de la superficie, tender√° a deslizar hacia abajo de \\(-\\log[p^*(\\theta\\mid y)]\\) por efecto de la gravedad. E ir√° cada vez m√°s r√°pido.\nEst√° bueno que el trineo deslice hacia los m√≠nimos de \\(-\\log[p^*(\\theta\\mid y)]\\) pues son zonas de alta densidad de probabilidad\nQuisi√©ramos que nuestro trineo explore otras zonas del posterior, para eso en lugar de soltar el trineo le damos un impulso inicial (velocidad inicial o momento).\nEste impulso inicial ser√° aleatorio"
  },
  {
    "objectID": "presentaciones/presentacion_05.html#section-24",
    "href": "presentaciones/presentacion_05.html#section-24",
    "title": "Estad√≠stica Bayesiana",
    "section": "",
    "text": "Conociendo la posici√≥n inicial del trineo y el impulso que se le da (la velocidad inicial), la F√≠sica permite calcular cu√°l ser√° su trayectoria (y por ende su posici√≥n despu√©s de un tiempo)\nLa posici√≥n final despu√©s de un tiempo ser√° el nuevo \\(\\theta\\) propuesto. Es decir: mientras que en MH propon√≠amos un salto con la distribuci√≥n \\(q(\\theta'\\mid\\theta)\\), aqu√≠ lo hacemos con un momento inicial y estudiando la posici√≥n del trineo.\nLuego se acepta o se rechaza el salto propuesto"
  },
  {
    "objectID": "presentaciones/presentacion_05.html#section-27",
    "href": "presentaciones/presentacion_05.html#section-27",
    "title": "Estad√≠stica Bayesiana",
    "section": "",
    "text": "HMC propone nuevos saltos de manera m√°s sofisticada que MH\nBusca que los saltos propuestos sean hacia valores del par√°metro m√°s prometedores"
  },
  {
    "objectID": "presentaciones/presentacion_05.html#section-28",
    "href": "presentaciones/presentacion_05.html#section-28",
    "title": "Estad√≠stica Bayesiana",
    "section": "",
    "text": "C√≥mo calcular la trayectoria del trineo es una de las cuestiones claves del algoritmo. Planteamos la conservaci√≥n de la energ√≠a:\n\\[\\mathcal{H}(\\theta,v) = U(\\theta) + K(v)\\]\n\\(\\mathcal{H}\\) se conoce como hamiltoniano y representa la energ√≠a total del sistema que es la suma de la energ√≠a potencial \\(U(\\theta)\\) (funci√≥n de la posici√≥n \\(\\theta\\)) y la energ√≠a cin√©tica \\(K(v)\\) (funci√≥n de la velocidad \\(v\\)).\nSe toma \\(U(\\theta) = -\\log[p^*(\\theta\\mid y)]\\) y \\(K(v) = \\frac{1}{2} m v^2\\)"
  },
  {
    "objectID": "presentaciones/presentacion_05.html#section-29",
    "href": "presentaciones/presentacion_05.html#section-29",
    "title": "Estad√≠stica Bayesiana",
    "section": "",
    "text": "Las ecuaciones de Hamilton describen el cambio de \\(\\theta\\) y de \\(v\\) en funci√≥n del tiempo\n\\[\\frac{d\\theta}{dt} = \\frac{\\partial \\mathcal{H}}{\\partial v}\\] \\[\\frac{dv}{dt} = -\\frac{\\partial \\mathcal{H}}{\\partial \\theta}\\]"
  },
  {
    "objectID": "presentaciones/presentacion_05.html#section-30",
    "href": "presentaciones/presentacion_05.html#section-30",
    "title": "Estad√≠stica Bayesiana",
    "section": "",
    "text": "Es necesario resolver estas ecuaciones‚Ä¶ Queremos hallar la posici√≥n (\\(\\theta\\)) del trineo tras un tiempo. No se pueden resolver anal√≠ticamente. Discretizamos el tiempo estudiando \\(L\\) peque√±os intervalitos de duraci√≥n \\(\\varepsilon\\)"
  },
  {
    "objectID": "presentaciones/presentacion_05.html#section-31",
    "href": "presentaciones/presentacion_05.html#section-31",
    "title": "Estad√≠stica Bayesiana",
    "section": "",
    "text": "Se tiene: \\[\n\\small{\n  \\begin{aligned}\n  \\frac{dv}{dt} &\\approx \\frac{v_{t+\\varepsilon} - v_{t}}{\\varepsilon} = \\frac{v_{t_2} - v_{t_1}}{\\varepsilon} \\\\\n  \\frac{d\\theta}{dt} &\\approx \\frac{\\theta_{t+\\varepsilon} - \\theta_{t}}{\\varepsilon} = \\frac{\\theta_{t_2} - \\theta_{t_1}}{\\varepsilon}\n  \\end{aligned}\n}\n\\]\ncon lo cual: \\[\n\\small{\n  \\begin{aligned}\n  v_{t_2} &= v_{t_1} + \\varepsilon \\frac{dv}{dt} = v_{t_1}-\\varepsilon \\frac{\\partial \\mathcal{H}}{\\partial \\theta} \\\\\n  \\theta_{t_2} &= \\theta_{t_1} + \\varepsilon \\frac{d\\theta}{dt} = \\theta_{t_1} + \\varepsilon \\frac{\\partial \\mathcal{H}}{\\partial v}\n  \\end{aligned}\n}\n\\]\nEstas aproximaciones no son buenas‚Ä¶"
  },
  {
    "objectID": "presentaciones/presentacion_05.html#leapfrog-integrator",
    "href": "presentaciones/presentacion_05.html#leapfrog-integrator",
    "title": "Estad√≠stica Bayesiana",
    "section": "Leapfrog integrator",
    "text": "Leapfrog integrator\nSe parte de \\(t\\) y se busca \\(v\\) en \\(t+\\frac{\\varepsilon}{2}\\). Luego se busca \\(\\theta\\) en \\(t+\\varepsilon\\) usando el resultado anterior \\(v\\) en \\(\\frac{\\varepsilon}{2}\\).\n\\[v(t+\\frac{\\varepsilon}{2}) = v(t) - \\frac{\\varepsilon}{2} \\frac{\\partial \\mathcal{H}}{\\partial \\theta}\\rvert_t\\]\n\\[\\theta(t+\\varepsilon) = \\theta(t) + \\varepsilon \\frac{\\partial \\mathcal{H}}{\\partial v}\\rvert_{t+\\frac{\\varepsilon}{2}}\\]\n\\[v(t+\\varepsilon) = v(t+\\frac{\\varepsilon}{2}) - \\frac{\\varepsilon}{2} \\frac{\\partial \\mathcal{H}}{\\partial \\theta}\\rvert_{t+\\varepsilon}\\]"
  },
  {
    "objectID": "presentaciones/presentacion_05.html#section-33",
    "href": "presentaciones/presentacion_05.html#section-33",
    "title": "Estad√≠stica Bayesiana",
    "section": "",
    "text": "La elecci√≥n de \\(\\varepsilon\\) es clave para el algoritmo. Si \\(L\\cdot\\varepsilon\\) es peque√±o, tomar√° mucho tiempo explorar el posterior. Con un valor muy grande, ocurrir√°n giros en U."
  },
  {
    "objectID": "presentaciones/presentacion_05.html#section-35",
    "href": "presentaciones/presentacion_05.html#section-35",
    "title": "Estad√≠stica Bayesiana",
    "section": "",
    "text": "Propuesta: A partir de \\(\\theta^{(i)}\\) disparar una bolita en alguna direcci√≥n aleatoria, con una velocidad (momento lineal) aleatoria\nLeapfrog integration: Calcular una serie de \\(L\\) pasos (leapfrog steps) de duraci√≥n fija \\(\\varepsilon\\) (step size): instantes d√≥nde vamos a sacar una foto de la posici√≥n de la part√≠cula\nAceptaci√≥n: Obtener la posici√≥n final \\(\\theta^{(i+1)}\\) como la posici√≥n final luego de \\(L\\) steps siempre y cuando la aproximaci√≥n haya sido buena (la energ√≠a se haya conservado)\n\n\nUn \\(\\varepsilon\\) peque√±o da m√°s resoluci√≥n sobre la trayectoria, permitiendo que la bolita gire √°ngulos pronunciados (¬øpero?).\n\n\nUn \\(\\varepsilon\\) grande har√° que los saltos sean largos y podemos saltear el punto donde la part√≠cula iba a girar (divergent transition)."
  },
  {
    "objectID": "presentaciones/presentacion_05.html#section-36",
    "href": "presentaciones/presentacion_05.html#section-36",
    "title": "Estad√≠stica Bayesiana",
    "section": "",
    "text": "Estad√≠stica Bayesiana ‚Äì 2025"
  },
  {
    "objectID": "presentaciones/presentacion_01.html#l√≥gica-deductiva",
    "href": "presentaciones/presentacion_01.html#l√≥gica-deductiva",
    "title": "Estad√≠stica Bayesiana",
    "section": "L√≥gica deductiva",
    "text": "L√≥gica deductiva\n\\[A \\Rightarrow B\\] \\(A\\) es verdadero, por lo tanto \\(B\\) es verdadero\n\\(B\\) es falso, por lo tanto \\(A\\) falso\n\n\\(A\\): Tom es un gato\n\\(B\\): Tom es un animal\n\n\\(B\\) es verdadero, por lo tanto‚Ä¶"
  },
  {
    "objectID": "presentaciones/presentacion_01.html#section",
    "href": "presentaciones/presentacion_01.html#section",
    "title": "Estad√≠stica Bayesiana",
    "section": "",
    "text": "Pero este no es el tipo de razonamiento que utilizamos en la vida cotidiana:\n\n\\(A\\): va a llover a las 10 de la ma√±ana\n\\(B\\): se nubla antes de las 10 de la ma√±ana\n\n\\(B\\) es verdadero, por lo tanto \\(A\\) se vuelve m√°s plausible"
  },
  {
    "objectID": "presentaciones/presentacion_01.html#section-1",
    "href": "presentaciones/presentacion_01.html#section-1",
    "title": "Estad√≠stica Bayesiana",
    "section": "",
    "text": "En una noche oscura, un polic√≠a camina por una calle aparentemente desierta. De repente, se escucha la alarma de un local. Se da vuelta y ve, en la vereda de enfrente, una joyer√≠a con la vidriera rota. Un hombre con una m√°scara sale agachado a trav√©s del vidrio roto, con una bolsa llena de joyas caras. El polic√≠a no duda en concluir que el hombre no tiene buenas intenciones.\n\nEl razonamiento del polic√≠a no fue una deducci√≥n l√≥gica, ya que podr√≠a existir una explicaci√≥n alternativa para lo ocurrido.\n\nDada la evidencia, no podemos decir con seguridad que las intenciones del hombre no son buenas, pero s√≠ que es extremadamente plausible que no lo sean."
  },
  {
    "objectID": "presentaciones/presentacion_01.html#razonamiento-plausible",
    "href": "presentaciones/presentacion_01.html#razonamiento-plausible",
    "title": "Estad√≠stica Bayesiana",
    "section": "Razonamiento plausible",
    "text": "Razonamiento plausible\nEl cerebro humano permanentemente determina si algo se vuelve m√°s o menos plausible. M√°s a√∫n, de alguna manera, eval√∫a el grado de plausibilidad de una proposici√≥n.\n\n\nLa plausibilidad de que llueva a las 10 de la ma√±ana depende fuertemente de la oscuridad de las nubes a las 9:45.\n\n\n\nEste razonamiento hace uso de nuestra experiencia previa. Combina informaci√≥n a priori con evidencia disponible. Esto da lugar a un proceso secuencial."
  },
  {
    "objectID": "presentaciones/presentacion_01.html#apuestas",
    "href": "presentaciones/presentacion_01.html#apuestas",
    "title": "Estad√≠stica Bayesiana",
    "section": "Apuestas",
    "text": "Apuestas\n\n\n\nP√°guese $1000 al portador de esta tarjeta si en este grupo hay alguien que tiene un loro como mascota\n\n\n\n\n\n\nP√°guese $1000 al portador de esta tarjeta si en este grupo nadie tiene un loro como mascota\n\n\n\n\nTienen a su disposici√≥n estas tarjetas. Podemos comprarlas o venderlas. Al final de la clase develamos el misterio y, quien tenga la tarjeta, cobra.\n\n\n¬øPor cu√°l pagar√≠an m√°s? ¬øCu√°nto estar√≠an dispuestos a pagar como m√°ximo?\n\n\nNotar que el precio m√°ximo que estar√≠an dispuestos a pagar para comprarla es el precio m√≠nimo por el que estar√≠an dispuestos a venderla."
  },
  {
    "objectID": "presentaciones/presentacion_01.html#section-2",
    "href": "presentaciones/presentacion_01.html#section-2",
    "title": "Estad√≠stica Bayesiana",
    "section": "",
    "text": "Todos pagar√≠amos \\(p\\cdot\\$ 1000\\) con \\(0 \\leq p \\leq 1\\).\n\nDecidimos cu√°nto apostar en funci√≥n de nuestra incertidumbre en la ocurrencia de un evento (de lo plausible que lo consideremos). Decidimos apostar \\(p\\cdot\\$ 1000\\) en favor de un evento, porque le asignamos una plausibilidad o credibilidad de grado \\(p\\)."
  },
  {
    "objectID": "presentaciones/presentacion_01.html#section-3",
    "href": "presentaciones/presentacion_01.html#section-3",
    "title": "Estad√≠stica Bayesiana",
    "section": "",
    "text": "P√°guese $1000 al portador de esta tarjeta si el profe tiene una remera negra\n\n\n\n¬øCu√°nto est√°n dispuestos a pagar para tener esta tarjeta? ¬øPor cu√°nto vender√≠an la tarjeta si la tuvieran?"
  },
  {
    "objectID": "presentaciones/presentacion_01.html#section-4",
    "href": "presentaciones/presentacion_01.html#section-4",
    "title": "Estad√≠stica Bayesiana",
    "section": "",
    "text": "P√°guese $1000 al portador de esta tarjeta si esta materia es la mejor del cuatrimestre\n\n\n\n\n\n\nP√°guese $1000 al portador de esta tarjeta si esta materia no es la mejor del cuatrimestre\n\n\n\n\nPor la primera pagar√≠an como m√°ximo \\(p\\cdot\\$ 1000\\) y por la segunda, \\(q\\cdot\\$ 1000\\). Es necesario que \\(p+q=1\\). ¬øPor qu√©?"
  },
  {
    "objectID": "presentaciones/presentacion_01.html#dutch-book",
    "href": "presentaciones/presentacion_01.html#dutch-book",
    "title": "Estad√≠stica Bayesiana",
    "section": "Dutch book",
    "text": "Dutch book\nSupongamos que \\(p=0.7\\) y \\(q=0.5\\). Eso significa que:\n\nSi no tienen las tarjetas, estar√≠an dispuestos a comprar ambas por \\(\\$1200\\).\n\nSupongamos que \\(p=0.3\\) y \\(q=0.2\\). Eso significa que:\n\nSi tienen las tarjetas, estar√≠an dispuestos a vender ambas por \\(\\$500\\).\n\n\nSabemos que a fin de cuatrimestre, quien tenga las dos tarjetas ganar√° \\(\\$1000\\)‚Ä¶\n\nThe canonical way to measure degrees of belief appeals to the notion of fair odds.\nThe degree of belief that a given epistemic agent‚Äîlet‚Äôs say it‚Äôs you‚Äîhas in this proposition A can be determined by what you deem to be the fair price of this lottery. Here the ‚Äúfair price‚Äù is the price at which you are willing to either buy or sell the lottery ticket."
  },
  {
    "objectID": "presentaciones/presentacion_01.html#dutch-book-1",
    "href": "presentaciones/presentacion_01.html#dutch-book-1",
    "title": "Estad√≠stica Bayesiana",
    "section": "Dutch book",
    "text": "Dutch book\n\n\n\nDutch book\n\n\nUn Dutch book es un conjunto de apuestas que aseguran una p√©rdida. El argumento del Dutch book dice que una persona que tiene creencias inconsistentes act√∫a irracionalmente y puede ser llevado a una p√©rdida segura en un juego de apuestas\n\n\n\n\nLos grados de plausibilidad o grados de creencia que una persona le asigna a un conjunto de eventos deben respetar los axiomas de probabilidad.\n\n\nSe puede asignar un valor de probabilidad a cualquier proposici√≥n."
  },
  {
    "objectID": "presentaciones/presentacion_01.html#section-5",
    "href": "presentaciones/presentacion_01.html#section-5",
    "title": "Estad√≠stica Bayesiana",
    "section": "",
    "text": "Las probabilidades son la mejor herramienta disponible para cuantificar la incertidumbre y las leyes de la probabilidad, la mejor herramienta para operar con ella."
  },
  {
    "objectID": "presentaciones/presentacion_01.html#probabilidad-1",
    "href": "presentaciones/presentacion_01.html#probabilidad-1",
    "title": "Estad√≠stica Bayesiana",
    "section": "Probabilidad",
    "text": "Probabilidad\nTres ideas de probabilidad\n\nCl√°sica: si \\(n\\) eventos son equiprobables, la probabilidad de uno de ellos es \\(1/n\\). Adem√°s, la probabilidad de un evento se puede calcular como el n√∫mero de casos favorables dividido el n√∫mero de casos posibles.\nFrecuentista: la probabilidad de un evento se puede estimar observando su frecuencia relativa sobre un gran n√∫mero de realizaciones o ensayos.\nSubjetiva: las probabilidades reflejan el grado de creencia o plausibilidad que una persona le asigna a un evento."
  },
  {
    "objectID": "presentaciones/presentacion_01.html#probabilidad-subjetiva",
    "href": "presentaciones/presentacion_01.html#probabilidad-subjetiva",
    "title": "Estad√≠stica Bayesiana",
    "section": "Probabilidad subjetiva",
    "text": "Probabilidad subjetiva\n\nEs la forma m√°s general de interpretar la probabilidad (eventos no equiprobables y eventos que no pueden repetirse)\nSe utiliza para cuantificar la incertidumbre o ignorancia (o certidumbre o conocimiento) acerca de un evento o proposici√≥n\nEs personal\nDepende del estado actual de conocimiento del mundo\n\n\nTodos los m√©todos estad√≠sticos son subjetivos en el sentido que se basan en idealizaciones matem√°ticas de la realidad (modelos)."
  },
  {
    "objectID": "presentaciones/presentacion_01.html#incertidumbre",
    "href": "presentaciones/presentacion_01.html#incertidumbre",
    "title": "Estad√≠stica Bayesiana",
    "section": "Incertidumbre",
    "text": "Incertidumbre\nDistinguimos dos tipos de incertidumbre:\n\n\nIncertidumbre epist√©mica\nIncertidumbre aleatoria\n\n\n\nLo retomaremos a lo largo del curso."
  },
  {
    "objectID": "presentaciones/presentacion_01.html#elicitaci√≥n-de-probabilidades",
    "href": "presentaciones/presentacion_01.html#elicitaci√≥n-de-probabilidades",
    "title": "Estad√≠stica Bayesiana",
    "section": "Elicitaci√≥n de probabilidades",
    "text": "Elicitaci√≥n de probabilidades\nConsideremos la siguiente proposici√≥n:\n\nVoy a aprobar todas las materias de este cuatrimestre (\\(W\\))\n\n\nUna caja con 5 bolas azules y 5 bolas rojas. Se extrae una bola al azar. \\(A\\) es el evento extraer una bola azul\n\n\n\n\\(A_1\\): $1000 si \\(W\\)\n\\(A_2\\): $1000 si \\(A\\)\n\n\n\nSi prefieren \\(A_1\\) entonces‚Ä¶ 8 bolas azules y 2 bolas rojas. Se extrae una bola al azar. \\(A\\) es el evento extraer una bola azul.\n\n\n\n\\(A_3\\): $1000 si \\(W\\)\n\\(A_4\\): $1000 si \\(A\\)"
  },
  {
    "objectID": "presentaciones/presentacion_01.html#section-6",
    "href": "presentaciones/presentacion_01.html#section-6",
    "title": "Estad√≠stica Bayesiana",
    "section": "",
    "text": "Interludio‚Ä¶\n\n¬øQu√© es m√°s probable?\n\nQue el PSG le gane al Lyon\nQue el PSG le gane al Lyon y Messi haga un gol"
  },
  {
    "objectID": "presentaciones/presentacion_01.html#sesgos",
    "href": "presentaciones/presentacion_01.html#sesgos",
    "title": "Estad√≠stica Bayesiana",
    "section": "Sesgos",
    "text": "Sesgos\nLos seres humanos no estamos optimizados para operar con probabilidades (al menos no intuitivamente)."
  },
  {
    "objectID": "presentaciones/presentacion_01.html#section-7",
    "href": "presentaciones/presentacion_01.html#section-7",
    "title": "Estad√≠stica Bayesiana",
    "section": "",
    "text": "Probabilidad de un evento \\[\\mathrm{Pr}(A)\\] \\[\\mathrm{Pr}(\\bar{A}) = 1-\\mathrm{Pr}(A)\\]"
  },
  {
    "objectID": "presentaciones/presentacion_01.html#section-8",
    "href": "presentaciones/presentacion_01.html#section-8",
    "title": "Estad√≠stica Bayesiana",
    "section": "",
    "text": "Probabilidad de la conjunci√≥n:\n\\[\\mathrm{Pr}(A\\wedge B) = \\mathrm{Pr}(A,B)\\] Si \\(A\\) y \\(B\\) son independientes, entonces\n\\[\\mathrm{Pr}(A\\wedge B) = \\mathrm{Pr}(A)\\mathrm{Pr}(B)\\] Probabilidad de la uni√≥n: \\[\\mathrm{Pr}(A \\vee B) = \\mathrm{Pr}(A) + \\mathrm{Pr}(B) - \\mathrm{Pr}(A,B)\\] Donde, si \\(A\\) y \\(B\\) son mutuamente excluyentes,\n\\[\\mathrm{Pr}(A \\vee B) = \\mathrm{Pr}(A) + \\mathrm{Pr}(B)\\]"
  },
  {
    "objectID": "presentaciones/presentacion_01.html#section-9",
    "href": "presentaciones/presentacion_01.html#section-9",
    "title": "Estad√≠stica Bayesiana",
    "section": "",
    "text": "\\[\\mathrm{Pr}(B\\mid A) = \\frac{\\mathrm{Pr}(A,B)}{\\mathrm{Pr}(A)} \\] siempre que \\(\\mathrm{Pr}(A)&gt;0\\) (no se puede condicionar a eventos imposibles)"
  },
  {
    "objectID": "presentaciones/presentacion_01.html#variables-aleatorias",
    "href": "presentaciones/presentacion_01.html#variables-aleatorias",
    "title": "Estad√≠stica Bayesiana",
    "section": "Variables aleatorias",
    "text": "Variables aleatorias\nUna variable aleatoria (univariada) \\(X\\) es una funci√≥n que mapea elementos del espacio muestral \\(\\mathcal{X}\\) a la recta real \\(\\mathbb{R}\\)\n\nSi \\(\\mathcal{X}\\) es finito o infinito numerable entonces \\(X\\) es una variable aleatoria discreta\nSi \\(\\mathcal{X}\\) es cualquier valor en \\(\\mathbb{R}\\) entonces \\(X\\) es una variable aleatoria continua"
  },
  {
    "objectID": "presentaciones/presentacion_01.html#section-10",
    "href": "presentaciones/presentacion_01.html#section-10",
    "title": "Estad√≠stica Bayesiana",
    "section": "",
    "text": "Para el caso discreto: \\[p(x) = \\mathrm{Pr}(X=x) \\quad \\text{(pmf)}\\]\nPara el caso continuo: \\[P(x) = \\mathrm{Pr}(X\\leq x) \\quad \\text{(cdf)}\\] \\[ p(x) = \\frac{d}{dx}P(x) \\quad \\text{(pdf)}\\]\n\\[\\mathrm{Pr}(x\\leq X\\leq x+dx) = p(x)dx\\]"
  },
  {
    "objectID": "presentaciones/presentacion_01.html#distribuciones-conjuntas",
    "href": "presentaciones/presentacion_01.html#distribuciones-conjuntas",
    "title": "Estad√≠stica Bayesiana",
    "section": "Distribuciones conjuntas",
    "text": "Distribuciones conjuntas\nCaso discreto\n\\[\n\\begin{array}{c|cc}\np(X,Y) & Y=0 & Y=1 \\\\\n\\hline\nX=0 & 0.2 & 0.3 \\\\\nX=1 & 0.3 & 0.2 \\\\\n\\end{array}\n\\]"
  },
  {
    "objectID": "presentaciones/presentacion_01.html#section-11",
    "href": "presentaciones/presentacion_01.html#section-11",
    "title": "Estad√≠stica Bayesiana",
    "section": "",
    "text": "Distribuci√≥n marginal\n\\[p(x)=\\sum_y p(x,y)\\]\n\\[p(y)=\\sum_x p(x,y)\\]\nSe conoce como marginalizar (en ingl√©s tambi√©n integrate out)"
  },
  {
    "objectID": "presentaciones/presentacion_01.html#section-12",
    "href": "presentaciones/presentacion_01.html#section-12",
    "title": "Estad√≠stica Bayesiana",
    "section": "",
    "text": "Caso continuo\n\\[p(x,y)\\]"
  },
  {
    "objectID": "presentaciones/presentacion_01.html#section-13",
    "href": "presentaciones/presentacion_01.html#section-13",
    "title": "Estad√≠stica Bayesiana",
    "section": "",
    "text": "Distribuci√≥n marginal\n\\[p(x)=\\int p(x,y) dy\\]\n\\[p(y)=\\int p(x,y) dx\\]"
  },
  {
    "objectID": "presentaciones/presentacion_01.html#distribuci√≥n-condicional",
    "href": "presentaciones/presentacion_01.html#distribuci√≥n-condicional",
    "title": "Estad√≠stica Bayesiana",
    "section": "Distribuci√≥n condicional",
    "text": "Distribuci√≥n condicional\n\\[p(x \\mid y) = \\frac{p(x,y)}{p(y)}\\]\n\\[p(y \\mid x) = \\frac{p(x,y)}{p(x)}\\]\n\\(p(x)\\) normaliza a \\(p(x,y)\\) (una funci√≥n de \\(y\\) ya que \\(x\\) tom√≥ un valor fijo).\n\nimagine a circular dart board, split into 20 equal sections, labelled from 1 to 20. Randy, a dart thrower, hits any one of the 20 sections uniformly at random. Hence the probability that a dart thrown by Randy occurs in any one of the 20 regions is p(region i) = 1=20. A friend of Randy tells him that he hasn‚Äôt hit the 20 region. What is the probability that Randy has hit the 5 region?"
  },
  {
    "objectID": "presentaciones/presentacion_01.html#regla-del-producto",
    "href": "presentaciones/presentacion_01.html#regla-del-producto",
    "title": "Estad√≠stica Bayesiana",
    "section": "Regla del producto",
    "text": "Regla del producto\nTambi√©n conocida como regla de la cadena. Recobramos la distribuci√≥n conjunta haciendo\n\\[p(x,y) = p(x\\mid y) p(y)\\]\n\\[p(x,y) = p(y\\mid x) p(x)\\]\n\\[p(x,y,z) = p(z) p(y\\mid z) p(x\\mid y,z)\\]"
  },
  {
    "objectID": "presentaciones/presentacion_01.html#regla-de-la-probabilidad-total",
    "href": "presentaciones/presentacion_01.html#regla-de-la-probabilidad-total",
    "title": "Estad√≠stica Bayesiana",
    "section": "Regla de la probabilidad total",
    "text": "Regla de la probabilidad total\n\\[p(x) = \\int p(x\\mid y) p(y) dy\\] \\[p(y) = \\int p(y\\mid x) p(x) dy\\]\n\nLa probabilidad marginal de \\(x\\) (una funci√≥n de \\(x\\)) se obtiene ponderando todos los posibles \\(p(x\\mid y)\\) (una funci√≥n de \\(x\\) para cada valor de \\(y\\)) seg√∫n la probabilidad de \\(p(y)\\). Y viceversa."
  },
  {
    "objectID": "presentaciones/presentacion_01.html#regla-de-bayes",
    "href": "presentaciones/presentacion_01.html#regla-de-bayes",
    "title": "Estad√≠stica Bayesiana",
    "section": "Regla de Bayes",
    "text": "Regla de Bayes\n\\[p(x\\mid y) = \\frac{p(y\\mid x) p(x)}{p(y)}\\]\n\nAs√≠ expresada no nos dice mucho.\n\n\nRecordemos que utilizamos las probabilidades para expresar nuestra incertidumbre. La mejor forma de actualizar nuestro grado de creencia sobre alguna hip√≥tesis \\(\\mathcal{H}\\) frente a nueva informaci√≥n \\(E\\) es utilizar la Regla de Bayes.\n\\[p(\\mathcal{H}\\mid E) = \\frac{p(E\\mid\\mathcal{H}) p(\\mathcal{H})}{p(E)}\\]"
  },
  {
    "objectID": "presentaciones/presentacion_01.html#or√≠genes-de-la-regla-de-bayes",
    "href": "presentaciones/presentacion_01.html#or√≠genes-de-la-regla-de-bayes",
    "title": "Estad√≠stica Bayesiana",
    "section": "Or√≠genes de la Regla de Bayes",
    "text": "Or√≠genes de la Regla de Bayes\n\n\n\nAlrededor de 1740, Thomas Bayes propone una versi√≥n de la regla pero no la publica (¬øsu descubrimiento era in√∫til? ¬øera muy modesto?). Propuso el experimento imaginario de un juego con bolitas. Asign√≥ iguales probabilidades a priori\n\n\n\nRichard Price public√≥ el resultado del Teorema de la Probabilidad Inversa de Bayes en An Essay Towards Solving a Problem in the Doctrine of Chances (1763)\n\n\n\nPierre-Simon Laplace lleg√≥ al mismo resultado que Bayes (algo que llam√≥ la probabilidad de las causas) y lo public√≥ en Memoire sur la Probabilit√© des Causes par les √âvenements (1774). Se asemeja m√°s a lo que hoy conocemos. Reconoci√≥ que Bayes hab√≠a descubierto algo similar."
  },
  {
    "objectID": "presentaciones/presentacion_01.html#section-14",
    "href": "presentaciones/presentacion_01.html#section-14",
    "title": "Estad√≠stica Bayesiana",
    "section": "",
    "text": "Bayes‚Äôs rule is a mistake, perhaps the only mistake to which the mathematical world has so deeply committed itself (Fisher, ~1920)\n\n\n\nBayes‚Äôs theorem is to the theory of probability what Pythagoras‚Äôs theorem is to geometry (Savage, ~1950)"
  },
  {
    "objectID": "presentaciones/presentacion_01.html#ejemplos",
    "href": "presentaciones/presentacion_01.html#ejemplos",
    "title": "Estad√≠stica Bayesiana",
    "section": "Ejemplos",
    "text": "Ejemplos\nVamos a trabajar con un conjunto de ejemplos que consisten en la aplicaci√≥n de la regla de Bayes, acerc√°ndonos de a poco a forma en la que se usa en la estad√≠stica bayesiana."
  },
  {
    "objectID": "presentaciones/presentacion_01.html#ejemplo-1",
    "href": "presentaciones/presentacion_01.html#ejemplo-1",
    "title": "Estad√≠stica Bayesiana",
    "section": "Ejemplo 1",
    "text": "Ejemplo 1\nNos encontramos con alguien en la calle y nos dice que tiene dos hijos. Le preguntamos si alguno de ellos es mujer y nos responde que s√≠. ¬øCu√°l es la probabilidad de que tenga dos ni√±as?"
  },
  {
    "objectID": "presentaciones/presentacion_01.html#section-15",
    "href": "presentaciones/presentacion_01.html#section-15",
    "title": "Estad√≠stica Bayesiana",
    "section": "",
    "text": "¬øC√≥mo lo escribimos con s√≠mbolos?"
  },
  {
    "objectID": "presentaciones/presentacion_01.html#ejemplo-2",
    "href": "presentaciones/presentacion_01.html#ejemplo-2",
    "title": "Estad√≠stica Bayesiana",
    "section": "Ejemplo 2",
    "text": "Ejemplo 2\nUn taxi se vio involucrado en una accidente nocturno y se dio a la fuga. En la ciudad hay dos empresas de taxis, la Verde y la Azul. Sobre el accidente se tienen los siguientes datos:\n\n85% de los taxis de la ciudad son de la empresa Verde y 15% de la Azul\nUn testigo identific√≥ el taxi como azul. La corte evalu√≥ la confiabilidad del testigo en las circunstancias del accidente y concluy√≥ que es capaz de identificar correctamente el color en un 80% de los casos.\n\n\n¬øCu√°l es la probabilidad de que el taxi haya sido azul, de acuerdo a la declaraci√≥n del testigo?"
  },
  {
    "objectID": "presentaciones/presentacion_01.html#section-16",
    "href": "presentaciones/presentacion_01.html#section-16",
    "title": "Estad√≠stica Bayesiana",
    "section": "",
    "text": "\\[p(A\\mid T_A) = \\frac{p(T_A\\mid A) p(A)}{p(T_A)}\\]\n\n\\[p(A\\mid T_A) = \\frac{p(T_A\\mid A) p(A)}{p(T_A\\mid A)p(A) + p(T_A\\mid V)p(V)}\\]\n\n\n\\[P(A\\mid T_A) = \\frac{0.80\\cdot 0.15}{0.80\\cdot 0.15 + 0.2\\cdot 0.85}\\]\n\\[p(A\\mid T_A) = 0.414\\]"
  },
  {
    "objectID": "presentaciones/presentacion_01.html#ejemplo-3",
    "href": "presentaciones/presentacion_01.html#ejemplo-3",
    "title": "Estad√≠stica Bayesiana",
    "section": "Ejemplo 3",
    "text": "Ejemplo 3\nSe realiza un test de hip√≥tesis que tiene una potencia \\(1-\\beta = 80\\%\\). Se fija el nivel de significaci√≥n en \\(\\alpha = 5\\%\\). Se testea \\(H_0\\) versus una hip√≥tesis alternativa \\(H_1:\\text{ no }H_0\\).\n\nSi se supone que la probabilidad de que \\(H_0\\) sea cierta es de \\(50\\%\\), ¬øcu√°l es la probabilidad de que \\(H_1\\) sea cierta luego de observar un resultado estad√≠sticamente significativo?\nSi la hip√≥tesis alternativa es muy rara (digamos \\(10\\%\\)), ¬øcu√°l es la probabilidad de que \\(H_1\\) sea cierta luego de observar un resultado estad√≠sticamente significativo?"
  },
  {
    "objectID": "presentaciones/presentacion_01.html#section-18",
    "href": "presentaciones/presentacion_01.html#section-18",
    "title": "Estad√≠stica Bayesiana",
    "section": "",
    "text": "\\[p(H_1 \\mid \\text{rechazo }H_0) = \\frac{p(\\text{rechazo }H_0 \\mid H_1)p(H_1)}{p(\\text{rechazo }H_0)}\\]\n\nSi en el denominador enumeramos exhaustivamente las formas de rechazar \\(H_0\\):\n\n\n\\[p(H_1 \\mid \\text{rechazo }H_0) = \\frac{(1-\\beta)p(H_1)}{\\alpha p(H_0) + (1-\\beta) p(H_1)}\\]\nPara el primer caso: \\(p(H_1 \\mid \\text{rechazo }H_0) = \\frac{{0.80}\\ {0.50}}{{0.05}\\ {0.50} + {0.80}\\ {0.50}} = {0.94}\\)\nPara el segundo caso: \\(p(H_1 \\mid \\text{rechazo }H_0) = \\frac{{0.80}\\ {0.10}}{{0.05}\\ {0.90} + {0.80}\\ {0.10}} = {0.64}\\)"
  },
  {
    "objectID": "presentaciones/presentacion_01.html#el-problema-de-las-urnas",
    "href": "presentaciones/presentacion_01.html#el-problema-de-las-urnas",
    "title": "Estad√≠stica Bayesiana",
    "section": "El problema de las urnas",
    "text": "El problema de las urnas\n\nSe cuenta con 11 urnas etiquetadas seg√∫n \\(u = 0,1,\\dots,10\\), que contienen diez bolas cada una. La urna \\(u\\) contiene \\(u\\) bolas azules y \\(10-u\\) bolas blancas. Fede elige una urna \\(u\\) al azar y extrae con reposici√≥n \\(N\\) bolas, obteniendo \\(n_A\\) azules y \\(N-n_A\\) blancas. Nico, el amigo de Fede, observa atentamente. Si despu√©s de \\(N=10\\) extracciones resulta \\(n_A = 3\\), ¬øcu√°l es la probabilidad de que la urna que Fede est√° usando sea la \\(u\\)?"
  },
  {
    "objectID": "presentaciones/presentacion_01.html#section-19",
    "href": "presentaciones/presentacion_01.html#section-19",
    "title": "Estad√≠stica Bayesiana",
    "section": "",
    "text": "La teor√≠a de las probabilidades permite predecir una distribuci√≥n sobre posibles valores de un resultado dado cierto conocimiento (o estado) del universo: probabilidad hacia adelante\n\nPor el contrario, muchas veces estamos interesados en realizar inferencias sobre el estado del universo a partir de observaciones: probabilidad inversa.\n\n\n\\[p(\\mathcal{H}\\mid E) = \\frac{p(E\\mid\\mathcal{H}) p(\\mathcal{H})}{p(E)}\\]\n\\[p(\\mathcal{H}\\mid E) \\propto p(E\\mid\\mathcal{H}) p(\\mathcal{H})\\]\n\n\n\n\nEstad√≠stica Bayesiana ‚Äì 2025"
  },
  {
    "objectID": "recursos/software/01_instalar_r.html",
    "href": "recursos/software/01_instalar_r.html",
    "title": "Instalar R",
    "section": "",
    "text": "El primer paso es ir al sitio https://r-project.org/ y hacer clic en donde dice CRAN.\n\n\n\n\n\n\n\n\n\nUna vez all√≠, aparecen mirrors (r√©plicas o copias) de la red CRAN donde se almacenan m√∫ltiples versiones de R, paquetes y otras dependencias. Se recomienda seleccionar la r√©plica Cloud o Argentina.\n\n\n\n\n\n\n\n\n\nAqu√≠ se incluyen diferentes enlaces que proveen el instalador de R para diferentes sistemas operativos. En el caso de encontrarse en Windows, hay que cliquear en Download R for Windows.\n\n\n\n\n\n\n\n\n\nLa siguiente p√°gina muestra diferentes subdirectorios. Para instalar R base hay que seleccionar el que dice base. Notar que tambi√©n aparece RTools, un programa utilitario que ser√° necesario m√°s adelante.\n\n\n\n\n\n\n\n\n\nEn esta secci√≥n, aparece por defecto el enlace para descargar la √∫ltima versi√≥n de R. Al momento de realizar esta gu√≠a la versi√≥n R 4.4.0 se encuentra reci√©n lanzada, por lo que se recomienda utilizar la √∫ltima versi√≥n de la serie 4.3. Para encontrar el enlace de descarga de versiones anteriores hay que hacer clic en Previous releases.\n\n\n\n\n\n\n\n\n\nAll√≠ se encuentran enlaces para descargar versiones anteriores de R, junto a su fecha de publicaci√≥n. Se recomienda seleccionar la version 4.3.3.\n\n\n\n\n\n\n\n\n\nLuego se presenta un listado de archivos disponibles en el directorio. Hay que seleccionar el instalador, que es el archivo que termina en .exe, y esto comienza la descarga del mismo.\n\n\n\n\n\n\n\n\n\nAl finalizar la descarga, se tiene que encontrar un archivo como el que se muestra debajo. Simplemente hay que hacer doble clic para ejecutar el instalador.\n\n\n\n\n\n\n\n\n\nLa instalaci√≥n es sencilla y solamente hay que hacer seleccionar Next hasta finalizar. Las capturas a continuaci√≥n se incluyen para mostrar las opciones seleccionadas en los diferentes pasos de la instalaci√≥n.",
    "crumbs": [
      "Recursos",
      "Instalaci√≥n de software",
      "Instalar R"
    ]
  },
  {
    "objectID": "recursos/software/03_instalar_rstudio.html",
    "href": "recursos/software/03_instalar_rstudio.html",
    "title": "Instalar RStudio",
    "section": "",
    "text": "La descarga e instalaci√≥n de RStudio en Windows es sencilla. Para descargar RStudio hay que ir a la p√°gina de descargas de RStudio de Posit https://posit.co/download/rstudio-desktop/ y seleccionar el bot√≥n que descarga RStudio para Windows.\n\n\n\n\n\n\n\n\n\nUna vez que se tiene el instalador descargado, hay que ejecutarlo haciendo doble clic y seleccionar Next e Install hasta finalizar.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLa primera vez que se abre, RStudio requiere que se especifique la instalaci√≥n de R a utilizar. Al menos que se cuente con m√∫ltiples instalaciones y se quiera usar una en particular, es recomendable utilizar la opci√≥n seleccionada por defecto.",
    "crumbs": [
      "Recursos",
      "Instalaci√≥n de software",
      "Instalar RStudio"
    ]
  },
  {
    "objectID": "recursos/software/02_instalar_rtools.html",
    "href": "recursos/software/02_instalar_rtools.html",
    "title": "Instalar RTools",
    "section": "",
    "text": "RTools es un conjunto esencial de herramientas que incluye compiladores, paquetes y utilidades necesarias para compilar tanto paquetes de R como c√≥digo fuente en diversos lenguajes de bajo nivel, como C, C++ y Fortran, en computadoras con el sistema operativo Windows.\nEn el contexto de la materia, RTools adquiere una relevancia significativa al posibilitar el uso de Stan en Windows, ya que este requiere la compilaci√≥n de programas en C++ para obtener muestras del posterior de un modelo.\nEn la misma p√°gina donde se seleccion√≥ la opci√≥n base para descargar R base, se encuentra el enlace para continuar con la descarga de RTools.\n\n\n\n\n\n\n\n\n\nAl igual que R, RTools tambi√©n cuenta con diferentes versiones. Es muy importante tener presente que la versi√≥n de RTools que se utilice debe corresponderse con la versi√≥n de R que se tenga instalada. Si se instal√≥ R 4.3.x, de acuerdo a la gu√≠a Instalar R, se debe seleccionar RTools 4.3.\n\n\n\n\n\n\n\n\n\nA continuaci√≥n, se muestra una p√°gina con abundante informaci√≥n sobre RTools. El enlace para realizar la descarga se encuentra en Rtools43 installer.\n\n\n\n\n\n\n\n\n\nCuando finaliza la descarga se tiene que encontrar un archivo como el que se muestra debajo. Nuevamente, es solo cuesti√≥n de hacer doble clic sobre el mismo y seleccionar Next e Install hasta finalizar.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPara comprobar si RTools fue instalado en una ruta identificada por R, se puede correr Sys.which(\"make\") en el int√©rprete de R y esto tiene que devolver una cadena con la ruta donde se encuentra el programa make (que en Windows es provista por RTools).",
    "crumbs": [
      "Recursos",
      "Instalaci√≥n de software",
      "Instalar RTools"
    ]
  },
  {
    "objectID": "recursos/codigo/01_froot_loops_grilla.html",
    "href": "recursos/codigo/01_froot_loops_grilla.html",
    "title": "01 - Liberen al Tuc√°n Sam",
    "section": "",
    "text": "En este recurso se muestra como usar R para resolver el ejercicio de los Froot Loops hecho durante la primera clase.\n\nlibrary(ggplot2)\nlibrary(patchwork)\n\ncolores &lt;- c(\"#f08533\", \"#3b78b0\", \"#d1352c\")\n\nEn base al prior elicitado grupalmente en clase:\n\n# Determinar grilla de puntos\npi_grid &lt;- seq(0, 1, length.out = 11)\nprint(pi_grid)\n\n [1] 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0\n\n# Especificar prior\n# Tenemos $1000 y los dividimos en los diferentes valores de \"pi\"\nprior_ &lt;- c(0, 50, 150, 600, 150, 50, 0, 0, 0, 0, 0)\nprior &lt;- prior_ / sum(prior_)\n\n# Recolectar datos\ncantidad_de_cereales &lt;- 29 # n\ncantidad_de_cereales_amarillos &lt;- 7 # y\n\n# Calcular verosimilitud para cada valor de \"pi\" en la grilla\nlikelihood &lt;- dbinom(\n  cantidad_de_cereales_amarillos,\n  cantidad_de_cereales,\n  pi_grid\n)\n\n# Obtener posterior\nposterior_ &lt;- prior * likelihood\nposterior &lt;- posterior_ / sum(posterior_) # normalizaci√≥n\n\n# Graficar prior\nplt_prior &lt;- data.frame(x = pi_grid, y = prior) |&gt;\n  ggplot() +\n  geom_segment(aes(x = x, xend = x, y = 0, yend = y), color = colores[1], linewidth = 0.8) +\n  geom_point(aes(x = x, y = y), , color = colores[1], size = 2.4) +\n  scale_x_continuous(breaks = pi_grid) +\n  labs(\n    x = expression(pi),\n    y = expression(p ~ \"(\" ~ pi ~ \")\"),\n    title = \"Distribuci√≥n a priori\"\n  ) +\n  theme_bw() +\n  theme(panel.grid.minor = element_blank())\n\n# Graficar verosimilitud\nplt_likelihood &lt;- data.frame(x = pi_grid, y = likelihood) |&gt;\n  ggplot() +\n  geom_segment(aes(x = x, xend = x, y = 0, yend = y), color = colores[2], linewidth = 0.8) +\n  geom_point(aes(x = x, y = y), , color = colores[2], size = 2.4) +\n  scale_x_continuous(breaks = pi_grid) +\n  labs(\n    x = expression(pi),\n    y = expression(p ~ \"(y | \" ~ pi ~ \")\"),\n    title = \"Funci√≥n de verosimilitud\"\n  ) +\n  theme_bw() +\n  theme(panel.grid.minor = element_blank())\n\n# Graficar posterior\nplt_posterior &lt;- data.frame(x = pi_grid, y = posterior) |&gt;\n  ggplot() +\n  geom_segment(aes(x = x, xend = x, y = 0, yend = y), color = colores[3], linewidth = 0.8) +\n  geom_point(aes(x = x, y = y), , color = colores[3], size = 2.4) +\n  scale_x_continuous(breaks = pi_grid) +\n  labs(\n    x = expression(pi),\n    y = expression(p ~ \"(\" ~ pi ~ \" | y)\"),\n    title = \"Distribuci√≥n a posteriori\"\n  ) +\n  theme_bw() +\n  theme(panel.grid.minor = element_blank())\n\n# Concatenar graficos\nplt_prior | plt_likelihood | plt_posterior\n\n\n\n\n\n\n\n\nEn base a un prior beta:\n\n# Determinar grilla de puntos\npi_grid &lt;- seq(0, 1, length.out = 200)\n\n# Obtener prior\nprior_ &lt;- dbeta(pi_grid, 3, 7)\nprior &lt;- prior_ / sum(prior_)\n\n# Calcular verosimilitud para cada valor de \"pi\"\nlikelihood &lt;- dbinom(\n  cantidad_de_cereales_amarillos,\n  cantidad_de_cereales,\n  pi_grid\n)\n\n# Obtener posterior\nposterior_ &lt;- prior * likelihood\nposterior &lt;- posterior_ / sum(posterior_)\n\n# Graficar prior\nplt_prior &lt;- data.frame(x = pi_grid, y = prior) |&gt;\n  ggplot() +\n  geom_line(aes(x = x, y = y), color = colores[1], linewidth = 1) +\n  labs(\n    x = expression(pi),\n    y = expression(p ~ \"(\" ~ pi ~ \")\"),\n    title = \"Distribuci√≥n a priori\"\n  ) +\n  theme_bw() +\n  theme(panel.grid.minor = element_blank())\n\n# Graficar verosimilitud\nplt_likelihood &lt;- data.frame(x = pi_grid, y = likelihood) |&gt;\n  ggplot() +\n  geom_line(aes(x = x, y = y), color = colores[2], linewidth = 1) +\n  labs(\n    x = expression(pi),\n    y = expression(p ~ \"(y | \" ~ pi ~ \")\"),\n    title = \"Funci√≥n de verosimilitud\"\n  ) +\n  theme_bw() +\n  theme(panel.grid.minor = element_blank())\n\n# Graficar posterior\nplt_posterior &lt;- data.frame(x = pi_grid, y = posterior) |&gt;\n  ggplot() +\n  geom_line(aes(x = x, y = y), color = colores[3], linewidth = 1) +\n  labs(\n    x = expression(pi),\n    y = expression(p ~ \"(\" ~ pi ~ \" | y)\"), \n    title = \"Distribuci√≥n a posteriori\"\n  ) +\n  theme_bw() +\n  theme(panel.grid.minor = element_blank())\n\n# Concatenar graficos\nplt_prior | plt_likelihood | plt_posterior\n\n\n\n\n\n\n\n\nPreguntas\n\n¬øQu√© pasa con el prior si incrementamos el \\(n\\)?\n¬øQu√© pasa con el posterior si incrementamos el \\(n\\)?",
    "crumbs": [
      "Recursos",
      "C√≥digo",
      "01 - Liberen al Tuc√°n Sam"
    ]
  },
  {
    "objectID": "recursos/codigo/index.html",
    "href": "recursos/codigo/index.html",
    "title": "Instalaci√≥n de Software",
    "section": "",
    "text": "Esta secci√≥n contiene scripts de R que son de utilidad para el curso.\n01 - Liberen al Tuc√°n Sam02 - ¬°Ostras! ¬°Estoy haciendo inferencia bayesiana!03 - ¬øQui√©n domina el posterior?04 - Diferentes observaciones, diferentes posteriors",
    "crumbs": [
      "Recursos",
      "C√≥digo"
    ]
  },
  {
    "objectID": "trabajos_practicos/02_tp2.html",
    "href": "trabajos_practicos/02_tp2.html",
    "title": "El Dibu de la vida",
    "section": "",
    "text": "El ozono (O‚ÇÉ) es un gas cuya mol√©cula est√° formada por tres √°tomos de ox√≠geno y se encuentra tanto en la atm√≥sfera como en la superficie terrestre. Seguramente, lo han escuchado nombrar por su papel en la capa de ozono, una regi√≥n de la estrat√≥fera que se encuentra entre unos 15 y 30 kil√≥metros sobre la superficie terrestre, donde se concentra este gas y filtra los rayos ultravioletas del sol.\nLa capa de ozono permite que exista la vida en la Tierra tal como la conocemos y que una escapada de verano a la isla no termine siendo solo un viaje de ida. En otras palabras, la capa de ozono es como un Dibu Mart√≠nez de la vida: evita que el sol nos clave un golazo de radiaci√≥n y nos deje, literalmente, fritos.\nPero no todo lo que brilla es oro. A nivel del suelo, el ozono se convierte en un contaminante atmosf√©rico da√±ino tanto para la salud humana como para el medio ambiente. Puede deteriorar la funci√≥n pulmonar, desencadenar ataques de asma, causar irritaci√≥n en los ojos, la nariz y la garganta, e incluso da√±ar la vegetaci√≥n. Entonces, ¬øtodo mal con el ozono?\nQuien se plante√≥ esa pregunta fue Brian Tarkington, del California Primate Research Center de la Universidad de California en Davis. Preocupado por la presencia de ozono en el smog californiano y las posibles consecuencias en la salud humana, decidi√≥ llevar a cabo un estudio para evaluar el efecto del ozono en el crecimiento de ratas. Tom√≥ un grupo de 46 ratas j√≥venes y de la misma edad, las pes√≥, las dividi√≥ aleatoriamente en 2 grupos de igual tama√±o y las dej√≥ en 2 ambientes distintos, el primero rico en ozono y el segundo libre del mismo. Luego de 7 d√≠as las pes√≥ nuevamente y registr√≥ la diferencia entre los pesos.\nEl archivo tarkington.csv contiene, para cada una de las ratas, el diferencial de peso en gramos y el grupo al que pertenece. Interesa conocer si la exposici√≥n prolongada a un ambiente contaminado con ozono se asocia a un deterioro en el desarrollo de las ratas, es decir, a un menor incremento de peso.",
    "crumbs": [
      "Trabajos Pr√°cticos",
      "TP 2: El Dibu de la vida"
    ]
  },
  {
    "objectID": "trabajos_practicos/02_tp2.html#introducci√≥n",
    "href": "trabajos_practicos/02_tp2.html#introducci√≥n",
    "title": "El Dibu de la vida",
    "section": "",
    "text": "El ozono (O‚ÇÉ) es un gas cuya mol√©cula est√° formada por tres √°tomos de ox√≠geno y se encuentra tanto en la atm√≥sfera como en la superficie terrestre. Seguramente, lo han escuchado nombrar por su papel en la capa de ozono, una regi√≥n de la estrat√≥fera que se encuentra entre unos 15 y 30 kil√≥metros sobre la superficie terrestre, donde se concentra este gas y filtra los rayos ultravioletas del sol.\nLa capa de ozono permite que exista la vida en la Tierra tal como la conocemos y que una escapada de verano a la isla no termine siendo solo un viaje de ida. En otras palabras, la capa de ozono es como un Dibu Mart√≠nez de la vida: evita que el sol nos clave un golazo de radiaci√≥n y nos deje, literalmente, fritos.\nPero no todo lo que brilla es oro. A nivel del suelo, el ozono se convierte en un contaminante atmosf√©rico da√±ino tanto para la salud humana como para el medio ambiente. Puede deteriorar la funci√≥n pulmonar, desencadenar ataques de asma, causar irritaci√≥n en los ojos, la nariz y la garganta, e incluso da√±ar la vegetaci√≥n. Entonces, ¬øtodo mal con el ozono?\nQuien se plante√≥ esa pregunta fue Brian Tarkington, del California Primate Research Center de la Universidad de California en Davis. Preocupado por la presencia de ozono en el smog californiano y las posibles consecuencias en la salud humana, decidi√≥ llevar a cabo un estudio para evaluar el efecto del ozono en el crecimiento de ratas. Tom√≥ un grupo de 46 ratas j√≥venes y de la misma edad, las pes√≥, las dividi√≥ aleatoriamente en 2 grupos de igual tama√±o y las dej√≥ en 2 ambientes distintos, el primero rico en ozono y el segundo libre del mismo. Luego de 7 d√≠as las pes√≥ nuevamente y registr√≥ la diferencia entre los pesos.\nEl archivo tarkington.csv contiene, para cada una de las ratas, el diferencial de peso en gramos y el grupo al que pertenece. Interesa conocer si la exposici√≥n prolongada a un ambiente contaminado con ozono se asocia a un deterioro en el desarrollo de las ratas, es decir, a un menor incremento de peso.",
    "crumbs": [
      "Trabajos Pr√°cticos",
      "TP 2: El Dibu de la vida"
    ]
  },
  {
    "objectID": "trabajos_practicos/02_tp2.html#modelizaci√≥n-estad√≠stica",
    "href": "trabajos_practicos/02_tp2.html#modelizaci√≥n-estad√≠stica",
    "title": "El Dibu de la vida",
    "section": "Modelizaci√≥n estad√≠stica",
    "text": "Modelizaci√≥n estad√≠stica\nLa estad√≠stica ofrece una amplia variedad de t√©cnicas para dar respuesta a la inquietud de Tarkington. En el contexto de esta materia, naturalmente, abordaremos el problema mediante el uso modelos bayesianos.\nY dado que el entusiasmo en esta materia es lo que sobra, aprovecharemos la oportunidad para utilizar no uno, sino dos modelos, sutilmente distintos. Esto nos permitir√° no solo evaluar el efecto del ozono en el crecimiento de las ratas, sino tambi√©n profundizar en esos peque√±os detalles que hacen que la inferencia estad√≠stica sea tan interesante.\n\nModelo normal\nEl primer modelo que proponemos es el cl√°sico caballito de batalla de innumerables an√°lisis estad√≠sticos: el viejo y confiable modelo normal. Aqu√≠, el cambio en el peso de las ratas en cada grupo se modela mediante una distribuci√≥n normal, permitiendo medias y varianzas espec√≠ficas para cada grupo.\n\\[\n\\begin{aligned}\nY_{O, i} &\\sim \\text{Normal}(\\mu_O, \\sigma_O^2) & i = 1, \\dots, N_O \\\\\nY_{C, j} &\\sim \\text{Normal}(\\mu_C, \\sigma_C^2) & j = 1, \\dots, N_C \\\\\n\\mu_O, \\mu_C &\\sim \\text{Normal}(0, 5^2) \\\\\n\\sigma_O, \\sigma_C & \\sim \\text{Gamma}(\\alpha=6, \\beta=2)\n\\end{aligned}\n\\] donde \\(Y_{O, i}\\) representa el cambio en el peso de la \\(i\\)-√©sima rata en el grupo expuesto al ozono e \\(Y_{C, j}\\) representa el cambio en el peso de la \\(j\\)-√©sima rata en el grupo control.\n\n\nModelo T de Student\nEl segundo modelo se parece demasiado al primero. La √∫nica ‚Äîy, en principio, sutil‚Äî diferencia es que, en lugar de modelar la variable respuesta en cada grupo con una distribuci√≥n normal, utilizamos una T de Student con 3 grados de libertad.\n\\[\n\\begin{aligned}\nY_{O, i} &\\sim \\text{Student-T}(\\mu_O, \\sigma_O^2, \\nu=3) & i = 1, \\dots, N_O \\\\\nY_{C, i} &\\sim \\text{Student-T}(\\mu_C, \\sigma_C^2, \\nu=3) & j = 1, \\dots, N_C \\\\\n\\mu_O, \\mu_C &\\sim \\text{Normal}(0, 5^2) \\\\\n\\sigma_O, \\sigma_C & \\sim \\text{Gamma}(\\alpha=6, \\beta=2)\n\\end{aligned}\n\\]",
    "crumbs": [
      "Trabajos Pr√°cticos",
      "TP 2: El Dibu de la vida"
    ]
  },
  {
    "objectID": "trabajos_practicos/02_tp2.html#un-deep-dive-bien-profundo-en-metropolis-hastings",
    "href": "trabajos_practicos/02_tp2.html#un-deep-dive-bien-profundo-en-metropolis-hastings",
    "title": "El Dibu de la vida",
    "section": "Un deep dive bien profundo en Metropolis-Hastings",
    "text": "Un deep dive bien profundo en Metropolis-Hastings\nAdem√°s de brindarle una respuesta al pobre de Brian Tarkington, que hace d√©cadas que espera que alguien le ayude a resolver su problema, el objetivo principal de este trabajo pr√°ctico es profundizar en el uso de Metropolis-Hastings (MH) como algoritmo de inferencia bayesiana.\nEl algoritmo de Metropolis-Hastings permite generar muestras (pseudo-)aleatorias a partir de una distribuci√≥n de probabilidad \\(P\\) que no necesariamente pertence a una familia de distribuciones conocida. El √∫nico requisito es que se pueda evaluar la funci√≥n de densidad (o de masa de probabilidad) \\(p^*(\\boldsymbol{\\theta})\\) en cualquier valor de \\(\\boldsymbol{\\theta}\\), incluso cuando \\(p^*(\\boldsymbol{\\theta})\\) sea impropia (es decir, incluso aunque sea desconocida la constante de normalizaci√≥n que hace que la integral en el soporte de la funci√≥n sea igual a uno).\n\n\n\n\n\n\nAlgoritmo de Metropolis-Hastings\n\n\n\nSe desea generar una muestra de valores \\(\\{y^{(1)}, y^{(2)}, \\cdots, y^{(n)} \\}\\) a partir de una distribuci√≥n de probabilidad \\(P\\) con funci√≥n de densidad \\(p\\).\n\nSeleccionar un punto inicial \\(y^{(1)}\\).\nPara cada \\(t\\in \\{1, \\cdots, n\\}\\), repetir:\n\n\nRealizar propuesta\nObtener un valor aleatorio \\(y'\\) de una variable \\(Y'\\) cuya distribuci√≥n est√° dada por la distribuci√≥n de propuesta \\(Q\\), centrada en el valor de la √∫ltima muestra obtenida: \\[\n    Y' \\sim Q(y^{(t)})\n    \\]\nCalcular la probabilidad de aceptaci√≥n\nCalcular el cociente entre la funci√≥n de densidad en el punto propuesto y en el punto actual, ponderando por la densidad de la distribuci√≥n de propuesta. La probabilidad de aceptaci√≥n es igual a este cociente si es menor a 1, caso contrario es igual a 1. \\[\n    \\alpha = \\min \\left\\{ 1, \\frac{p(y')q(y^{(t)} \\mid y')}{p(y)q(y' \\mid y^{(t)})} \\right\\}\n    \\]\nSeleccionar el nuevo valor\nGenerar un valor aleatorio \\(u\\) de una distribuci√≥n \\(\\mathcal{U}(0, 1)\\) y determinar \\(y^{(t + 1)}\\) de la siguiente manera: \\[\n    y^{(t + 1)} = \\left\\{\n    \\begin{array}{ll}\n    y' & \\text{si} \\quad u \\le \\alpha \\\\\n    y^{(t)} & \\text{si} \\quad u &gt; \\alpha\n    \\end{array}\\right.\n    \\]\n\n\n\n\nMetropolis-Hastings en espacios param√©tricos acotados\nLa distribuci√≥n normal suele ser una elecci√≥n conveniente a la hora de proponer saltos, pero presenta desventajas cuando el espacio param√©trico es acotado: inevitablemente, algunas propuestas caer√°n fuera del dominio y rechazaremos m√°s que de costumbre.\nPara abordar este problema, existen dos estrategias principales. Una opci√≥n es utilizar distribuciones cuyo soporte coincida con el espacio objetivo. Sin embargo, encontrar parametrizaciones basadas en la media para todas ellas no es trivial.\nLa otra alternativa, m√°s general, consiste en aplicar una transformaci√≥n de variables para trabajar en un espacio no acotado, permitiendo as√≠ seguir utilizando una distribuci√≥n de propuesta normal. Esta estrategia requiere conocer la densidad objetivo en el nuevo espacio, lo que implica calcular el determinante del jacobiano de la transformaci√≥n. A su favor, muchas de las transformaciones m√°s utilizadas permiten un c√°lculo sencillo, y este proceso puede automatizarse con herramientas de diferenciaci√≥n autom√°tica existentes.\n\n\n\n\n\n\nTransformaci√≥n de variables aleatorias\n\n\n\nSean \\(X\\) e \\(Y = g(X)\\) variables aleatorias continuas, donde \\(g\\) es una funci√≥n uno a uno y \\(X\\) tiene funci√≥n de densidad \\(f_X(x)\\) conocida. Luego, la funci√≥n de densidad de \\(Y\\) es: \\[\nf_Y(y) = f_X(g^{-1}(y)) \\left\\lvert \\frac{d}{dy}g^{-1}(y) \\right\\rvert\n\\]\ndonde \\(\\left\\lvert \\frac{d}{dy}g^{-1}(y) \\right\\rvert\\) es el determinante del jacobiano de la transformaci√≥n. Por ejemplo, en el caso de la funci√≥n \\(g(u) = \\text{logit}(u)\\), se tiene: \\[\n\\begin{aligned}\ng : (0, 1) \\to \\mathbb{R} & = \\text{logit}(u) = \\log\\left(\\frac{u}{1 - u}\\right) \\\\\ng^{-1} : \\mathbb{R} \\to (0, 1) &= \\text{expit}(u) =  \\frac{1}{1 + \\exp(-u)}\n\\end{aligned}\n\\]\ny luego, utilizando \\(X\\) e \\(Y\\) definidas al principio: \\[\nf_Y(y) = f_X(\\text{expit}(y)) \\cdot \\text{expit}(y) \\cdot ( 1 - \\text{expit}(y))\n\\]\n\n\n\n\nMetropolis-Hastings en escala logar√≠tmica\nSin importar cu√°n moderna y potente sea nuestra computadora, siempre tendremos que lidiar con el tal√≥n de Aquiles del c√°lculo computacional: los problemas de subdesbordamiento y sobredesbordamiento (conocidos como underflow y overflow en ingl√©s). Por ejemplo, si intentamos multiplicar 100 n√∫meros del orden de 0.0001 en R, la computadora podr√≠a interpretar el resultado como 0, aunque matem√°ticamente esto no sea cierto.\nEl underflow es un problema frecuente en el c√≥mputo estad√≠stico. Un caso t√≠pico es el que se da al evaluar funciones de verosimilitud, donde se multiplican densidades correspondientes a cada observaci√≥n. Dado que estas densidades suelen ser valores muy peque√±os, es com√∫n enfrentarse a un problema de subdesbordamiento. Este riesgo aumenta con la cantidad de observaciones y la dimensionalidad de la distribuci√≥n objetivo.\nEl problema se agrava al utilizar algoritmos como Metropolis-Hastings, ya que la densidad objetivo se eval√∫a miles de veces en diferentes puntos del espacio param√©trico. Adem√°s, en inferencia bayesiana, esta funci√≥n suele involucrar una funci√≥n de verosimilitud, que puede implicar una multiplicaci√≥n de gran cantidad de n√∫meros peque√±os, lo que incrementa a√∫n m√°s las chances de que el c√≥mputo resulte en un underflow.\nSi nuestro algoritmo de muestreo encuentra un problema de underflow, pueden ocurrir dos situaciones:\n\nSe genera un error que detiene la ejecuci√≥n del programa.\nSe ignora la propuesta problem√°tica y se realiza una nueva, pero nuestro programa ya no sigue la cadena de Markov deseada.\n\nEntonces, ¬øc√≥mo solucionamos este problema? La respuesta es sencilla: realizar todos los c√°lculos en escala logar√≠tmica. Las multiplicaciones se vuelven sumas y el c√≥mputo se estabiliza.\n\n\n\nListado¬†1: Implementaci√≥n del algoritmo de Metropolis-Hastings en escala logar√≠tmica.\n\n\nmetropolis_hastings_log &lt;- function(logp, x, n, sigma = NULL) {\n  # Algortimo de Metropolis Hastings en escala logar√≠tmica\n  #\n  # Par√°metros\n  #  ------------------------------------------------------------------------\n  # | logp     | Funci√≥n de densidad objetivo, normalizada o sin normalizar, |\n  # |          | en escala logar√≠tmica.                                      |\n  # | x        | Posici√≥n inicial del algoritmo.                             |\n  # | n        | Cantidad de muestras a obtener.                             |\n  # | sigma    | Matriz de varianzas y covarianzas para la distribuci√≥n de   |\n  # |          | propuesta. Por defecto, es NULL y usa la matriz identidad.  |\n  #  ------------------------------------------------------------------------\n  #\n  # Salida\n  #  ------------------------------------------------------------------------\n  # | muestras | Matriz con las muestras obtenidas.                          |\n  #  ------------------------------------------------------------------------\n\n  # Obtener dimensionalidad de la distribuci√≥n objetivo a partir del punto inicial\n  p &lt;- length(x)\n\n  # Inicializar matriz donde se guardan las muestras\n  muestras &lt;- matrix(NA, nrow = n, ncol = p)\n\n  # Usar matriz diagonal unitaria para la distribuci√≥n de propuesta cuando no se especifica\n  if (is.null(sigma)) {\n    sigma &lt;- diag(p)\n  }\n\n  # Almacenar el punto inicial en la matriz de muestras\n  muestras[1, ] &lt;- x\n\n  for (i in 1:(n - 1)) {\n    # Obtener el valor de la muestra actual\n    muestra_actual &lt;- muestras[i, ]\n\n    # Proponer un nuevo valor\n    muestra_propuesta &lt;- mvtnorm::rmvnorm(1, mean = muestra_actual, sigma = sigma)\n\n    # Evaluar la log densidad en el valor actual y en el propuesto\n    logp_propuesta &lt;- logp(muestra_propuesta)\n    logp_actual &lt;- logp(muestra_actual)\n\n    # Log densidad al pasar de muestra_propuesta a muestra_actual y viceversa\n1    logq_actual &lt;- mvtnorm::dmvnorm(\n      muestra_actual, mean = muestra_propuesta, sigma = sigma, log = TRUE\n    )\n    logq_propuesta &lt;- mvtnorm::dmvnorm(\n      muestra_propuesta, mean = muestra_actual, sigma = sigma, log = TRUE\n    )\n\n    # Calcular log probabilidad de aceptaci√≥n\n2    log_alpha &lt;- ...\n\n    # Simular aceptaci√≥n o rechazo\n    log_u &lt;- log(runif(1))\n\n    aceptar &lt;- log_u &lt; log_alpha\n\n    # Determinar siguiente paso en base al criterio de selecci√≥n\n    if (aceptar) {\n      muestras[i + 1, ] &lt;- muestra_propuesta\n    } else {\n      muestras[i + 1, ] &lt;- muestra_actual\n    }\n  }\n  # Convertir 'muestras' a vector si se trata de una distrbuci√≥n univariada\n  if (p == 1) {\n    muestras &lt;- as.vector(muestras)\n  }\n\n  return(muestras)\n}\n\n1\n\n¬øEs necesario este paso?\n\n2\n\n¬øC√≥mo se calcula la log probabilidad de aceptaci√≥n?",
    "crumbs": [
      "Trabajos Pr√°cticos",
      "TP 2: El Dibu de la vida"
    ]
  },
  {
    "objectID": "trabajos_practicos/02_tp2.html#ahora-s√≠-cerebro-a-la-obra",
    "href": "trabajos_practicos/02_tp2.html#ahora-s√≠-cerebro-a-la-obra",
    "title": "El Dibu de la vida",
    "section": "Ahora s√≠, cerebro a la obra",
    "text": "Ahora s√≠, cerebro a la obra\nLa funci√≥n metropolis_hastings_log() es una herramienta poderosa que, en teor√≠a, permite obtener muestras de cualquier distribuci√≥n a posteriori que se pueda implementar en R en escala logar√≠tmica. Pero, ¬øde qu√© sirve tener una herramienta tan potente si no est√° nada claro c√≥mo dominarla?\nA esta altura, intentar usar metropolis_hastings_log() para muestrear del posterior en los modelos propuestos inicialmente resultar√≠a muy dificultoso. Ser√≠a como pasar de manejar un Golcito por las calles del pueblo a subirse a un F√≥rmula 1 en el medio de un Gran Premio.\nComo bien dice el refr√°n, la pr√°ctica hace al maestro. Y quienes conocen de primera mano la verdad de estas palabras son, casualmente, los pilotos de F√≥rmula 1. No solo acumulan experiencia tras a√±os de competencia en categor√≠as menores, desde el karting hasta la F√≥rmula 2, sino que tambi√©n, antes de cada Gran Premio, dan entre 400 y 500 vueltas al circuito‚Ä¶ ¬°pero en un simulador!\nDe este modo, en preparaci√≥n para la aplicaci√≥n final en este trabajo ‚Äînuestro propio Gran Premio‚Äî comenzaremos utilizando el algoritmo de Metropolis-Hastings en escenarios simples y controlados, aumentando gradualmente la complejidad. Esto nos permitir√° practicar su uso con transformaci√≥n de variables, en escala logar√≠tmica y en posteriors de m√∫ltiples dimensiones. Y al final, como en toda buena carrera, llegar√° el Gran Premio.\n\n1. En el simulador: ensayos con la distribuci√≥n Gamma\nEl objetivo de esta secci√≥n es familiarizarnos con el uso de transformaciones para obtener muestras de distribuciones con soporte acotado. Inicialmente, utilizaremos la funci√≥n metropolis_hastings() desarrollada en la pr√°ctica de la materia, que opera en escala original. Luego, utilizaremos la funci√≥n metropolis_hastings_log() incluida m√°s arriba, que trabaja en escala logar√≠tmica.\nSea \\(X \\sim \\text{Gamma}(\\alpha = 3, \\beta = 8)\\).\n\nObtenga y grafique la funci√≥n de densidad de \\(Y = \\log(X)\\).\nUtilice Metropolis-Hastings para obtener 10000 muestras de \\(X\\). Para ello, obtenga muestras de \\(Y\\) utilizando una propuesta normal con \\(\\sigma = 0.2\\) y realice la transformaci√≥n correspondiente para obtener las muestras de \\(X\\). Para evaluar la bondad del m√©todo, calcule el tama√±o de muestra efectivo, visualice la trayectoria de la cadena con un traceplot y compare la distribuci√≥n emp√≠rica con la funci√≥n de densidad te√≥rica utilizando un histograma.\nImplemente la funci√≥n de densidad de \\(Y\\) en escala logar√≠tmica y utilice metropolis_hastings_log() para obtener muestras de \\(X\\). Calcule el tama√±o efectivo de muestra efectivo y comp√°relo con el obtenido anteriormente. Finalmente, obtenga un histograma de las muestras y comp√°relo con la densidad emp√≠rica y con el histograma obtenido en el punto anterior.\n\n\n\n2. Free practice: m√∫ltiples dimensiones\nAl c√≥mputo en escala logar√≠tmica y el uso de transformaciones de variables, en esta etapa se le agrega que la distribuci√≥n objetivo tiene m√∫ltiples dimensiones y debe ser obtenida mediante el uso de la regla de Bayes.\nDado el siguiente modelo: \\[\n\\begin{aligned}\nY_i & \\sim \\text{Normal}(\\mu, \\sigma^2) \\\\\n\\mu & \\sim \\text{Normal}(0, 1^2) \\\\\n\\sigma & \\sim \\text{Gamma}(\\alpha=2, \\beta=2) \\\\\n\\end{aligned}\n\\]\ncon \\(i = 1, \\dots, N\\).\n\nImplemente en R una funci√≥n que permita calcular la densidad a posteriori en escala logar√≠tmica, sin incluir la constante de normalizaci√≥n. Esta funci√≥n tendr√° 2 par√°metros de entrada, uno para el vector de par√°metros y otro para el vector de valores observados.\nUtilice los valores de \\(y_i\\) en el archivo precalentamiento-mh.txt y la funci√≥n metropolis_hastings_log() para obtener muestras del posterior de \\(\\boldsymbol{\\theta} = \\{\\mu, \\sigma\\}\\). Corra dos cadenas de Markov. Calcule el tama√±o efectivo de muestra y eval√∫e mezcla y convergencia de manera gr√°fica.\n\n\n\n\n\n\n\nEvaluaci√≥n parcial de funciones\n\n\n\nEn las l√≠neas 41 y 42 del Listado¬†1 se puede ver que la funci√≥n logp que se pasa a metropolis_hastings_log() debe funcionar correctamente al ser llamada con un √∫nico argumento, que es el vector de par√°metros. Pero la funci√≥n que implementamos en el punto 4 tiene 2 argumentos, uno para el vector de par√°metros y otro para el vector de datos‚Ä¶ ¬øc√≥mo hacemos?\nLa funci√≥n partial(), de la librer√≠a purrr, permite pre-llenar argumentos de una funci√≥n. Toma como entrada una funci√≥n, los argumentos con sus valores a pre-llenar, y devuelve una nueva funci√≥n que hace lo mismo que la funci√≥n original, pero con valores de argumentos ya especificados.\nEn nuestro caso, podr√≠amos hacer:\nlibrary(purrr)\nlogp &lt;- partial(log_posterior, y = datos)\nLuego, la funci√≥n logp() puede ser llamada solamente pasando el vector de par√°metros. El valor de y ser√° el que se pas√≥ originalmente a la funci√≥n partial().\n\n\n\n\n3. El Gran Prix: ¬øqu√© le decimos a Brian?\nA esta altura del partido (o de la carrera mejor dicho) ya estamos ‚Äòretequecontra‚Äô preparados para trabajar con los modelos presentados inicialmente y dar respuesta a la pregunta sobre el efecto del ozono en desarrollo de las ratas de Brian Tarkington. Lleg√≥ la hora de realizar el an√°lisis estad√≠stico.\n\nRealice un an√°lisis exploratorio de los datos y describa sus hallazgos.\nPara una media y una varianza dada, ¬øen qu√© se diferencia la distribuci√≥n normal de la distribuci√≥n T de Student con 3 grados de libertad?\nTanto para el modelo normal como para el basado en la T de Student:\n\nEscriba la funci√≥n de densidad a posteriori en escala logar√≠tmica, sin considerar la constante de normalizaci√≥n. De ser necesario, realice las transformaciones de variables que crea conveniente.\nImplemente la funci√≥n anterior en R.\nUse metropolis_hastings_log() para obtener muestras del posterior. Utilice 4 cadenas de 5000 muestras cada una. Eval√∫e mezcla y convergencia mediante medidas num√©ricas y gr√°ficas. Visualice las distribuciones a posteriori marginales.\nConcluya en t√©rminos del problema.\n\n¬øLlega a la misma conclusi√≥n con ambos modelos? ¬øPor qu√©?\n\n\n\n\n\n\n\nT de Student no centrada y escalada\n\n\n\nLa funci√≥n dt() de R permite evaluar la funci√≥n de densidad de una distribuci√≥n T de Student est√°ndar, con media 0 y desv√≠o 1. Para evaluar la funci√≥n de densidad con media y desv√≠o arbitrario se puede utilizar la siguiente funci√≥n de R:\nscaled_t_density &lt;- function(x, df, mean = 0, sd = 1) {\n  dt((x - mean) / sd, df) / sd\n}",
    "crumbs": [
      "Trabajos Pr√°cticos",
      "TP 2: El Dibu de la vida"
    ]
  },
  {
    "objectID": "practica/practica_04.html",
    "href": "practica/practica_04.html",
    "title": "Pr√°ctica - Unidad 4",
    "section": "",
    "text": "El objetivo principal de esta unidad es la aplicaci√≥n de modelos de regresi√≥n lineales desde una perspectiva bayesiana, considerando a los par√°metros del modelo como cantidades aleatorias que se corresponden con una distribuci√≥n de probabilidad a priori. A diferencia del enfoque frecuentista o m√°ximo veros√≠mil, el resultado de la inferencia bayesiana es una distribuci√≥n de probabilidad a posteriori, la cual se utiliza como fuente de todas las conclusiones. Adem√°s, se emplean t√©cnicas propias de la estad√≠stica bayesiana para evaluar la adecuaci√≥n y comparar los modelos utilizados.\n\nüìå Mi primer regresi√≥n bayesiana\nEl conjunto de datos sales contiene los montos semanales de inversi√≥n en publicidad y de ingresos de una determinada compa√±√≠a. Considere el siguiente modelo de regresi√≥n lineal simple:\n\\[\n\\begin{aligned}\n\\text{ventas}_i &\\sim \\text{Normal}(\\mu_i, \\sigma^2) \\\\\n\\mu_i &= \\beta_0 + \\beta_1 \\text{publicidad}_i \\\\\n\\end{aligned}\n\\]\n\nAjuste el modelo utilizando {Rstan} y distribuciones uniformes como priors.\nConstruya un gr√°fico que muestre las ventas en funci√≥n de la inversi√≥n en publicidad y superponga la recta de regresi√≥n estimada.\n\nüìå Mejorando mi regresi√≥n bayesiana\nConsidere la siguiente versi√≥n del modelo del ejercicio anterior que propone distribuciones a priori para los par√°metros del modelo: \\[\n\\begin{aligned}\n\\text{ventas}_i &\\sim \\text{Normal}(\\mu_i, \\sigma^2) \\\\\n\\mu_i &= \\beta_0 + \\beta_1 \\text{publicidad}_i \\\\\n\\beta_0  &\\sim \\text{Normal}(\\overline{\\text{ventas}}, 10^2) \\\\\n\\beta_1  &\\sim \\text{Normal}(0, 0.5^2) \\\\\n\\sigma &\\sim \\text{Normal}^+(5)\n\\end{aligned}\n\\]\n\nAjuste el modelo utilizando {RStan} y los priors sugeridos.\nConstruya un gr√°fico que muestre las ventas en funci√≥n de la inversi√≥n en publicidad, superponga la recta de regresi√≥n estimada, y el intervalo de credibilidad del 95% para la recta de regresi√≥n.\n\nRegresiones frecuentistas y bayesianas\nUtilice datos simulados para comparar la estimaci√≥n por m√≠nimos cuadrados con la estimaci√≥n Bayesiana en modelos de regresi√≥n.\n\nSimule 100 observaciones del modelo \\(Y = 2 + 3X + \\varepsilon\\) donde los valores del predictor \\(X\\) se obtienen de una distribuci√≥n \\(\\text{Uniforme}(0, 20)\\) y los errores son obtenidos de manera independiente de una distribuci√≥n \\(\\text{Normal}(0, 5^2)\\).\nAjuste el modelo de regresi√≥n utilizando lm() y un modelo bayesiano mediante {RStan} utilizando priors uniformes.\nVerifique que ambos m√©todos arrojan resultados similares.\nRepresente gr√°ficamente los datos y las dos rectas de regresi√≥n.\nIntente repetir la simulaci√≥n, pero esta vez cree las condiciones para que ambos enfoques den resultados diferentes. \n\nüìå La altura‚Ä¶ ¬øse hereda?\nEl conjunto de datos de las alturas (heights) contiene las alturas (en pulgadas) de 5524 pares de madres e hijas registradas en un estudio realizado por Karl Pearson y Alice Lee en 1903.\n\nElabore un gr√°fico que permita ver la relaci√≥n entre las alturas de las madres y las hijas. Aplique las t√©cnicas que crea necesaria para obtener una visualizaci√≥n informativa y fidedigna.\n¬øPor qu√© es adecuado utilizar un modelo de regresi√≥n lineal?\nAjuste el siguiente modelo de regresi√≥n lineal utilizando {Rstan} y priors que crea convenientes:\n\n\\[\n\\begin{aligned}\n\\text{altura hija}_i &\\sim \\text{Normal}(\\mu_i, \\sigma^2) \\\\\n\\mu_i &= \\beta_0 + \\beta_1 \\text{altura madre}_i\n\\end{aligned}\n\\]\n\nCalcule la media, el desv√≠o est√°ndar y el intervalo de credibilidad del 95% para los par√°metros del modelo utilizando el posterior.\nInterprete los coeficientes del modelo.\nSuperponga la recta de regresi√≥n en el gr√°fico donde se visualiza la relaci√≥n entre las variables.\nObtenga el posterior del peso medio de una hija cuya madre mide 58 pulgadas. \n\nüìå Clima en Australia\nEl conjunto de datos weather_WU datos clim√°ticos correspondientes a 100 d√≠as en dos ciudades de Australia: Uluru y Wollongong. Se intentar√° predecir la temperatura a las 3 de la tarde, utilizando otras variables.\nConsidere los siguientes cuatro modelos:\n\n\\(m_1\\): temp3pm ~ temp9am;\n\\(m_2\\): temp3pm ~ location;\n\\(m_3\\): temp3pm ~ temp9am + location;\n\\(m_4\\): temp3pm ~ ..\n\n\nAjuste cada uno de los modelos y construya gr√°ficas para mostrar los par√°metros obtenidos.\nRealice pruebas predictivas a posteriori para comparar los modelos.\nCompare los ELPD de cada modelo utilizando LOO. \n\n\n\n\n\n\nParque Nacional Ulu·πüu-Kata Tju·πØa en Uluru, Australia. Foto de Snowscat en Unsplash\n\n\n\n\nüìå Ping√ºinos\nConsidere el dataset de ping√ºinos de Palmer (penguins) y los siguientes modelos:\n\n\\(m_1\\): body_mass_g ~ flipper_length_mm;\n\\(m_2\\): body_mass_g ~ species;\n\\(m_3\\): body_mass_g ~ flipper_length_mm + species;\n\\(m_4\\): body_mass_g ~ flipper_length_mm + species + flipper_length_mm:species;\n\\(m_5\\): body_mass_g ~ flipper_length_mm + bill_length_mm + bill_depth_mm.\n\n\nAjuste cada uno de los modelos y construya gr√°ficas para mostrar los par√°metros obtenidos.\nRealice pruebas predictivas a posteriori para comparar los modelos.\nCompare los ELPD de cada modelo utilizando LOO. \n\nüìå De tal palo‚Ä¶\nEl dataset child_iq contiene informaci√≥n de los resultados de tests de coeficiente intelectual de ni√±os de 3 a√±os, educaci√≥n de la madre, y edad de la madre cuando dio a luz.\n\nAjuste un modelo de regresi√≥n del puntaje del beb√© a los 3 a√±os en funci√≥n de la edad de la madre.\nAjsute ahora un modelo que incluya la educaci√≥n de la madre.\nConstruya gr√°ficas para mostrar los par√°metros obtenidos.\nRealice pruebas predictivas a posteriori para comparar los modelos.\nCompare los ELPD de cada modelo utilizando LOO.\n\n\nüìå Ingresos\nEl dataset earnings contiene los resultados de la encuesta realizada por Ross sobre Trabajo, Familia y Bienestar.\n\nAjuste un modelo que prediga ingreso en funci√≥n de altura e interprete los par√°metros.\n¬øQu√© transformaci√≥n ser√≠a necesaria para interpretar el intercepto como el ingreso promedio de una persona con altura promedio?\nAjuste un nuevo modelo utilizando la transformaci√≥n propuesta en el punto anterior y compare los posteriors de los coeficientes.\n\nüìå !Kung\nLos !Kung son un pueblo que habita en el desierto de Kalahari entre Botsuana, Namibia y Angola. Hablan la lengua !Kung, que se destaca por su amplio uso de consonantes clic (chasquido conson√°ntico). El !K del nombre «ÉKung es un sonido como cuando sale un corcho de una botella.\nEl archivo Howell1 contiene datos de un censo parcial realizado por Dobe Howell acerca de la poblaci√≥n !Kung.\nConsidere un modelo de altura en funci√≥n del peso.\n\nDetermine e interprete las distribuciones a posteriori de los par√°metros.\nConstruya un gr√°fico de altura en funci√≥n del peso, incluya las observaciones de los individuos, la recta de regresi√≥n MAP, el intervalo del 80% para la media y y el intervalo del 80% para la altura predicha.\nRealice predicciones para individuos cuyos pesos son: 46.95, 43.72, 64.78, 32.59 y 54.63. Calcule la altura esperada y el intervalo del 89%.\n\nüìå Zorros urbanos\nConsidere del conjunto de datos sobre zorros urbanos (foxes). Ajuste tres modelos:\n\n\\(m_1\\): weight ~ area;\n\\(m_2\\): weight ~ groupsize;\n\\(m_3\\): weight ~ area + groupsize.\n\n\nPara los modelos \\(m_1\\) y \\(m_2\\), represente gr√°ficamente los resultados, incluyendo la recta de regresi√≥n MAP, su intervalo del 89% y el intervalo de predicci√≥n del 89%. ¬øEs alguna de las dos variables importantes para predecir la masa de un zorro?\nRepresentar gr√°ficamente las predicciones del modelo para cada predictor, dejando el otro constante en su valor medio. ¬øQu√© puede decirse sobre la importancia de las variables para predecir la masa de un zorro?\n\nüìå Un prior informativo marca la diferencia\nConsidere el conjunto de datos sobre belleza y proporci√≥n de sexos (sexratio) . Estos datos provienen de un estudio de adolescentes estadounidenses cuyo atractivo en una escala de cinco puntos fue evaluado por entrevistadores en una encuesta cara a cara. A√±os m√°s tarde, muchos de estos encuestados tuvieron hijos y se registraron ciertos atributos entre los cuales se incluy√≥ el sexo. El objetivo del an√°lisis es comparar la proporci√≥n de sexos de los hijos seg√∫n la belleza de los padres. Para ello considere el siguiente modelo de regresi√≥n:\n\\[\n\\begin{aligned}\n\\text{pf}_i &\\sim \\text{Normal}(\\mu_i, \\sigma^2) \\\\\n\\mu_i &= \\beta_0 + \\beta_1 \\text{belleza}_i \\\\\n\\end{aligned}\n\\]\nDonde \\(\\text{pf}\\) representa la proporci√≥n de beb√©s de sexo femenino y \\(\\text{belleza}\\) representa el grupo de belleza de los padres.\n\nAjuste el modelo utilizando m√≠nimos cuadrados.\nAjuste el modelo con {RStan} y priors uniformes.\nCompare el ajuste de ambos modelos.\nExplore los priors utilizados por {RStan} y la distribuci√≥n predictiva a priori. ¬øQu√© puede concluir?\nProponga distribuciones a priori informativas.\nAjuste el modelo utilizando {RStan} y los priors informativos.\nCompare el resultado con los obtenidos anteriormente y concluya.\n\nüìå ¬°A la pesca de priors!\nEl conjunto de datos fish-market contiene mediciones morfol√≥gicas realizadas sobre pescados de diferentes especies. El objetivo es construir un modelo de regresi√≥n lineal que permita predecir el peso de los pescados en base a sus otros atributos.\nUno de los modelos propuestos es el siguiente:\n\\[\n\\begin{aligned}\n\\log(\\text{Weight}_i) &\\sim \\text{Normal}(\\mu_i, \\sigma) \\\\\n\\mu_i  &= \\beta_{0, j[i]} + \\beta_{1, j[i]} \\log(\\text{Length1}_i)\n\\end{aligned}\n\\]\n\\(\\text{Weight}_i\\) es el peso del i-√©simo pescado en gramos y \\(\\text{Length1}_i\\) es la longitud del i-√©simo pescado en cent√≠metros. La letra \\(j\\) indexa las especies de los pescados. Por lo tanto, en este modelo cada especie tiene su propio intercepto y pendiente.\n\nImplemente el modelo utilizando {RStan} y los siguientes priors no informativos: \\[\n\\begin{aligned}\n\\beta_{0, j[i]} &\\sim \\text{Normal}(0, 10) \\\\\n\\beta_{1, j[i]} &\\sim \\text{Normal}(0, 5)\n\\end{aligned}\n\\]\nObtenga y visualice la distribuci√≥n predictiva a priori.\nElabore un gr√°fico y describa la funci√≥n de densidad a priori de los par√°metros \\(\\beta_{0, j[i]}\\) y \\(\\beta_{1, j[i]}\\).\nProponga priors m√°s adecuados en base a la interpretaci√≥n de los par√°metros del modelo y la informaci√≥n que tenga del problema.\nNuevamente, obtenga y visualice la distribuci√≥n predictiva a priori y compare con el resultado obtenido anteriormente.\nAjuste el modelo, obtenga la distribuci√≥n predictiva a posteriori y graf√≠quela.\n\n\n\n\n\n\nEspecies de peces de todas variedades y tama√±os.\n\n\n\n\nEn b√∫squeda del modelo adecuado\nContinuando con los datos del ejercicio anterior, El objetivo es construir un modelo de regresi√≥n lineal que permita predecir el peso de los pescados en base a sus otros atributos.\nConsidere los siguientes modelos\n\n\\(m_1\\): log(Weight) ~ 0 + Species;\n\\(m_2\\): log(Weight) ~ 0 + Species + log(Length1);\n\\(m_3\\): log(Weight) ~ 0 + Species + log(Length1):Species;\n\\(m_4\\): log(Weight) ~ 0 + Species + log(Length1):Species + log(Height);\n\\(m_5\\): log(Weight) ~ 0 + Species + log(Length1):Species + log(Height):Species.\n\n\nAjuste cada uno de los modelos.\nEstime el ELPD de cada modelo utilizando LOO y seleccione el modelo m√°s adecuado de acuerdo a este criterio.\nExplique el resultado.\n\nComparaci√≥n de modelos\nSe recopilaron datos (mesquite) con el fin de desarrollar un m√©todo para estimar la producci√≥n total (biomasa) de hojas de mesquite utilizando par√°metros f√°cilmente medibles de la planta, antes de que se realice la cosecha real. Se tomaron dos conjuntos separados de mediciones, uno en un grupo de 26 arbustos de mesquite y otro en un grupo diferente de 20 arbustos de mesquite medidos en un momento diferente del a√±o. Todos los datos se obtuvieron en la misma ubicaci√≥n geogr√°fica, pero ninguno constituy√≥ una muestra estrictamente aleatoria. La variable de resultado es el peso total (en gramos) de material fotosint√©tico obtenido de la cosecha real del arbusto. Las variables de entrada son:\n\n\n\n\n\n\n\nNombre\nDescripci√≥n\n\n\n\n\ndiam1\nDi√°metro de la copa medido a lo largo del eje m√°s largo del arbusto (metros)\n\n\ndiam2\nDi√°metro de la copa medido a lo largo del eje m√°s corto (metros)\n\n\ncanopy_height\nAltura de la copa\n\n\ntotal_height\nAltura total del arbusto\n\n\ndensity\nN√∫mero de tallos primarios por planta\n\n\ngroup\nGrupo de mediciones (0 para el primer grupo, 1 para el segundo)\n\n\n\n\nRealice un an√°lisis exploratorio de los datos.\nAjuste el modelo weight ~ diam1 + diam2 + canopy_height + total_height + density + group.\nExplore y describa el posterior.\nEstime ELPD mediante PSIS-CV con la funci√≥n loo(), analice los valores de las estimaciones del par√°metro \\(k\\) de la distribuci√≥n generalizada de Pareto y otros valores de la salida que crea relevant, ¬øqu√© puede concluir?\nEstime ELPD mediante K-fold cross validation con \\(K=10\\). Compare la estimaci√≥n con el resultado obtenido mediante PSIS-CV y concluya.\nAjuste el modelo transformando todas las variables num√©ricas con la funci√≥n logar√≠tmica. ¬øC√≥mo afecta esta transformaci√≥n la interpretaci√≥n de los coeficientes?\nEstime ELPD mediante PSIS-CV con la funci√≥n loo(). Concluya acerca de la estabilidad del c√≥mputo. ¬øEs posible comparar la la estimaci√≥n con la obtenida en el inciso iv? ¬øPor qu√©?\nCon ambos modelos, obtenga y grafique la distribuci√≥n predictiva a posteriori compar√°ndola con los datos observados ¬øCu√°l de los modelos representa mejor a los datos?\n\n\n\n\n\n\nUn √°rbol de mesquite.Foto de Sergei Bogomyakov, Alamy Stock Photo.\n\n\n\n\n\n\nüß© Secundarios en Portugal\nSe cuenta con un conjunto de datos sobre 343 estudiantes de secundaria de Portugal (portugal) y se desea predecir la calificaci√≥n final en matem√°ticas del √∫ltimo a√±o en base a un gran n√∫mero de predictores potencialmente relevantes.\nEl listado de variables se compone por: escuela del estudiante, sexo del estudiante, edad del estudiante, tipo de domicilio del estudiante, tama√±o de la familia, estado de convivencia de los padres, educaci√≥n de la madre, educaci√≥n del padre, tiempo de viaje del hogar a la escuela, tiempo de estudio semanal, n√∫mero de fracasos escolares pasados, apoyo educativo adicional, clases pagadas adicionales dentro de la materia del curso, actividades extracurriculares, si el estudiante asisti√≥ a una guarder√≠a, si el estudiante desea cursar estudios superiores, acceso a Internet en el hogar, si el estudiante tiene una relaci√≥n rom√°ntica, calidad de las relaciones familiares, tiempo libre despu√©s de la escuela, si el estudiante sale con amigos, consumo de alcohol entre semana, consumo de alcohol los fines de semana, estado de salud actual y n√∫mero de ausencias escolares.\nPriors d√©bilmente informativos\n\nAjuste un modelo de regresi√≥n lineal utilizando todos los predictores luego de estandarizarlos y con los siguientes priors: \\[\n\\begin{aligned}\n\\beta_k &\\sim \\text{Normal}(0, 2.5) \\\\\n\\sigma  &\\sim \\text{Exponential}(1 / \\text{std}(y))\n\\end{aligned}\n\\]\nElabore un gr√°fico para visualizar los posteriors marginales y comp√°relos. ¬øQu√© puede concluir acerca de su incertidumbre?\nCalcule y compare la mediana del \\(R^2\\) bayesiano y del \\(R^2\\) calculado mediante LOO ¬øQu√© conclusi√≥n puede extraer de esta comparaci√≥n?\n¬øCu√°l es el n√∫mero efectivo de par√°metros seg√∫n LOO? ¬øQu√© indica?\nObtenga muestras del prior y del posterior del \\(R^2\\) bayesiano, comp√°relos utilizando una visualizaci√≥n y concluya considerando la elecci√≥n de los priors d√©bilmente informativos sobre \\(\\beta_k\\) y \\(\\sigma\\).\n\nPriors alternativos (I)\nSi se asume que muchos predictores pueden tener poca relevancia, se pueden escalar los priors independientes para que la suma de la varianza de los priors se encuentre alrededor de un valor razonable. En este caso, se cuenta con 26 predictores y se podr√≠a suponer que la proporci√≥n de la varianza explicada por los predictores est√° alrededor de 0.3. Entonces, un enfoque simple consiste en asignar priors independientes a los coeficientes de regresi√≥n con media 0 y desviaci√≥n est√°ndar \\(\\sqrt{0.3/26}\\text{sd}(y)\\) y un prior exponencial con media \\(\\sqrt{0.7}\\text{sd}(y)\\) para \\(\\sigma\\).\n\nAjuste el modelo nuevamente utilizando los siguientes priors: \\[\n\\begin{aligned}\n\\beta_k &\\sim \\text{Normal}(0, \\sqrt{\\frac{0.3}{26}}\\text{sd}(y)) \\\\\n\\sigma  &\\sim \\text{Exponential}(1 / \\sqrt{0.7}\\text{sd}(y))\n\\end{aligned}\n\\]\nExplore la distribuci√≥n a priori sobre \\(R^2\\) y comp√°rela con la distribuci√≥n obtenida con los priors d√©bilmente informativos.\nCalcule ELPD mediante LOO y compare este modelo con el anterior.\nElabore un gr√°fico para visualizar los posteriors marginales. Compare este resultado con el obtenido con los priors d√©bilmente informativos.\n\nPriors alternativos (II)\nOtra alternativa es asumir que solo algunos de los predictores tienen alta relevancia y que el resto de los predictores tienen una relevancia insignificante. Una posibilidad para modelar bajo este supuesto es el horseshoe prior regularizado1. Este prior utiliza distribuciones normales independientes con media 0 y varianza \\(\\tau^2\\lambda_k^2\\) para los coeficientes de regresi√≥n \\(\\beta_k\\) y se describe a continuaci√≥n:\n\\[\n\\begin{aligned}\n\\beta_k   &\\sim \\text{Normal}(0, \\tau^2\\tilde{\\lambda}_k^2) \\\\\n\\tilde{\\lambda}_k^2 &= \\frac{c^2\\lambda_k^2}{c^2 + \\tau^2\\lambda_k^2} \\\\\nc & = \\sqrt{c'} \\text{SS} \\\\\nc' &\\sim \\text{InvGamma}(0.5 \\cdot SDF, 0.5 \\cdot SDF) \\\\\n\\lambda_k &\\sim \\text{StudentT}^+(\\text{df} = 1, \\mu = 0, \\sigma = 1 ) \\\\\n\\tau      &\\sim \\text{StudentT}^+(\\text{df} = 1, \\mu = 0, \\sigma = \\text{GS})\n\\end{aligned}\n\\]\ncon \\[\n\\begin{aligned}\n\\text{GS} = \\frac{p_0}{p - p_0} \\frac{\\sigma}{\\sqrt{n}} \\\\\n\\text{SS} = \\sqrt{\\frac{0.3}{p_0}} \\text{sd}(y) \\\\\n\\text{SDF} = 4\n\\end{aligned}\n\\]\ndonde \\(\\text{GS}\\), \\(\\text{SS}\\) y \\(\\text{SDF}\\) representan global scale, slab scale y slab degrees of freedom, respectivamente. Adem√°s, \\(p\\) representa la cantidad de predictores, 26, y \\(p_0\\) la cantidad de predictores que se espera que sean relevantes.\nIntuitivamente, el par√°metro global \\(\\tau\\) empuja todos los \\(\\beta_k\\) hacia el 0, mientras que los par√°metros locales \\(\\lambda_k\\) contribuyen a que algunos de los \\(\\beta_k\\) escapen del 0.\n\nUtilice \\(p_0 = 6\\) para ajustar el modelo con todos los predictores y grafique y analice los posteriors marginales.\nCompare este modelo con los ajustados anteriormente en base a sus ELPD estimados con LOO y concluya.\n\nPriors d√©bilmente informativos con menos predictores\nAjuste el modelo de regresi√≥n con un subconjunto de predictores que crea conveniente y los priors d√©bilmente informativos que se utilizaron inicialmente.\n\nVisualice los posteriors marginales.\nNuevamente, calcule y compare la mediana del \\(R^2\\) bayesiano y del \\(R^2\\) calculado mediante.\nCompare este modelo con el ajustado anteriormente en base a sus ELPD estimados con LOO y concluya sobre la capacidad predictiva de este modelo.",
    "crumbs": [
      "Pr√°ctica",
      "Unidad 4"
    ]
  },
  {
    "objectID": "practica/practica_04.html#regresi√≥n-lineal",
    "href": "practica/practica_04.html#regresi√≥n-lineal",
    "title": "Pr√°ctica - Unidad 4",
    "section": "",
    "text": "El objetivo principal de esta unidad es la aplicaci√≥n de modelos de regresi√≥n lineales desde una perspectiva bayesiana, considerando a los par√°metros del modelo como cantidades aleatorias que se corresponden con una distribuci√≥n de probabilidad a priori. A diferencia del enfoque frecuentista o m√°ximo veros√≠mil, el resultado de la inferencia bayesiana es una distribuci√≥n de probabilidad a posteriori, la cual se utiliza como fuente de todas las conclusiones. Adem√°s, se emplean t√©cnicas propias de la estad√≠stica bayesiana para evaluar la adecuaci√≥n y comparar los modelos utilizados.\n\nüìå Mi primer regresi√≥n bayesiana\nEl conjunto de datos sales contiene los montos semanales de inversi√≥n en publicidad y de ingresos de una determinada compa√±√≠a. Considere el siguiente modelo de regresi√≥n lineal simple:\n\\[\n\\begin{aligned}\n\\text{ventas}_i &\\sim \\text{Normal}(\\mu_i, \\sigma^2) \\\\\n\\mu_i &= \\beta_0 + \\beta_1 \\text{publicidad}_i \\\\\n\\end{aligned}\n\\]\n\nAjuste el modelo utilizando {Rstan} y distribuciones uniformes como priors.\nConstruya un gr√°fico que muestre las ventas en funci√≥n de la inversi√≥n en publicidad y superponga la recta de regresi√≥n estimada.\n\nüìå Mejorando mi regresi√≥n bayesiana\nConsidere la siguiente versi√≥n del modelo del ejercicio anterior que propone distribuciones a priori para los par√°metros del modelo: \\[\n\\begin{aligned}\n\\text{ventas}_i &\\sim \\text{Normal}(\\mu_i, \\sigma^2) \\\\\n\\mu_i &= \\beta_0 + \\beta_1 \\text{publicidad}_i \\\\\n\\beta_0  &\\sim \\text{Normal}(\\overline{\\text{ventas}}, 10^2) \\\\\n\\beta_1  &\\sim \\text{Normal}(0, 0.5^2) \\\\\n\\sigma &\\sim \\text{Normal}^+(5)\n\\end{aligned}\n\\]\n\nAjuste el modelo utilizando {RStan} y los priors sugeridos.\nConstruya un gr√°fico que muestre las ventas en funci√≥n de la inversi√≥n en publicidad, superponga la recta de regresi√≥n estimada, y el intervalo de credibilidad del 95% para la recta de regresi√≥n.\n\nRegresiones frecuentistas y bayesianas\nUtilice datos simulados para comparar la estimaci√≥n por m√≠nimos cuadrados con la estimaci√≥n Bayesiana en modelos de regresi√≥n.\n\nSimule 100 observaciones del modelo \\(Y = 2 + 3X + \\varepsilon\\) donde los valores del predictor \\(X\\) se obtienen de una distribuci√≥n \\(\\text{Uniforme}(0, 20)\\) y los errores son obtenidos de manera independiente de una distribuci√≥n \\(\\text{Normal}(0, 5^2)\\).\nAjuste el modelo de regresi√≥n utilizando lm() y un modelo bayesiano mediante {RStan} utilizando priors uniformes.\nVerifique que ambos m√©todos arrojan resultados similares.\nRepresente gr√°ficamente los datos y las dos rectas de regresi√≥n.\nIntente repetir la simulaci√≥n, pero esta vez cree las condiciones para que ambos enfoques den resultados diferentes. \n\nüìå La altura‚Ä¶ ¬øse hereda?\nEl conjunto de datos de las alturas (heights) contiene las alturas (en pulgadas) de 5524 pares de madres e hijas registradas en un estudio realizado por Karl Pearson y Alice Lee en 1903.\n\nElabore un gr√°fico que permita ver la relaci√≥n entre las alturas de las madres y las hijas. Aplique las t√©cnicas que crea necesaria para obtener una visualizaci√≥n informativa y fidedigna.\n¬øPor qu√© es adecuado utilizar un modelo de regresi√≥n lineal?\nAjuste el siguiente modelo de regresi√≥n lineal utilizando {Rstan} y priors que crea convenientes:\n\n\\[\n\\begin{aligned}\n\\text{altura hija}_i &\\sim \\text{Normal}(\\mu_i, \\sigma^2) \\\\\n\\mu_i &= \\beta_0 + \\beta_1 \\text{altura madre}_i\n\\end{aligned}\n\\]\n\nCalcule la media, el desv√≠o est√°ndar y el intervalo de credibilidad del 95% para los par√°metros del modelo utilizando el posterior.\nInterprete los coeficientes del modelo.\nSuperponga la recta de regresi√≥n en el gr√°fico donde se visualiza la relaci√≥n entre las variables.\nObtenga el posterior del peso medio de una hija cuya madre mide 58 pulgadas. \n\nüìå Clima en Australia\nEl conjunto de datos weather_WU datos clim√°ticos correspondientes a 100 d√≠as en dos ciudades de Australia: Uluru y Wollongong. Se intentar√° predecir la temperatura a las 3 de la tarde, utilizando otras variables.\nConsidere los siguientes cuatro modelos:\n\n\\(m_1\\): temp3pm ~ temp9am;\n\\(m_2\\): temp3pm ~ location;\n\\(m_3\\): temp3pm ~ temp9am + location;\n\\(m_4\\): temp3pm ~ ..\n\n\nAjuste cada uno de los modelos y construya gr√°ficas para mostrar los par√°metros obtenidos.\nRealice pruebas predictivas a posteriori para comparar los modelos.\nCompare los ELPD de cada modelo utilizando LOO. \n\n\n\n\n\n\nParque Nacional Ulu·πüu-Kata Tju·πØa en Uluru, Australia. Foto de Snowscat en Unsplash\n\n\n\n\nüìå Ping√ºinos\nConsidere el dataset de ping√ºinos de Palmer (penguins) y los siguientes modelos:\n\n\\(m_1\\): body_mass_g ~ flipper_length_mm;\n\\(m_2\\): body_mass_g ~ species;\n\\(m_3\\): body_mass_g ~ flipper_length_mm + species;\n\\(m_4\\): body_mass_g ~ flipper_length_mm + species + flipper_length_mm:species;\n\\(m_5\\): body_mass_g ~ flipper_length_mm + bill_length_mm + bill_depth_mm.\n\n\nAjuste cada uno de los modelos y construya gr√°ficas para mostrar los par√°metros obtenidos.\nRealice pruebas predictivas a posteriori para comparar los modelos.\nCompare los ELPD de cada modelo utilizando LOO. \n\nüìå De tal palo‚Ä¶\nEl dataset child_iq contiene informaci√≥n de los resultados de tests de coeficiente intelectual de ni√±os de 3 a√±os, educaci√≥n de la madre, y edad de la madre cuando dio a luz.\n\nAjuste un modelo de regresi√≥n del puntaje del beb√© a los 3 a√±os en funci√≥n de la edad de la madre.\nAjsute ahora un modelo que incluya la educaci√≥n de la madre.\nConstruya gr√°ficas para mostrar los par√°metros obtenidos.\nRealice pruebas predictivas a posteriori para comparar los modelos.\nCompare los ELPD de cada modelo utilizando LOO.\n\n\nüìå Ingresos\nEl dataset earnings contiene los resultados de la encuesta realizada por Ross sobre Trabajo, Familia y Bienestar.\n\nAjuste un modelo que prediga ingreso en funci√≥n de altura e interprete los par√°metros.\n¬øQu√© transformaci√≥n ser√≠a necesaria para interpretar el intercepto como el ingreso promedio de una persona con altura promedio?\nAjuste un nuevo modelo utilizando la transformaci√≥n propuesta en el punto anterior y compare los posteriors de los coeficientes.\n\nüìå !Kung\nLos !Kung son un pueblo que habita en el desierto de Kalahari entre Botsuana, Namibia y Angola. Hablan la lengua !Kung, que se destaca por su amplio uso de consonantes clic (chasquido conson√°ntico). El !K del nombre «ÉKung es un sonido como cuando sale un corcho de una botella.\nEl archivo Howell1 contiene datos de un censo parcial realizado por Dobe Howell acerca de la poblaci√≥n !Kung.\nConsidere un modelo de altura en funci√≥n del peso.\n\nDetermine e interprete las distribuciones a posteriori de los par√°metros.\nConstruya un gr√°fico de altura en funci√≥n del peso, incluya las observaciones de los individuos, la recta de regresi√≥n MAP, el intervalo del 80% para la media y y el intervalo del 80% para la altura predicha.\nRealice predicciones para individuos cuyos pesos son: 46.95, 43.72, 64.78, 32.59 y 54.63. Calcule la altura esperada y el intervalo del 89%.\n\nüìå Zorros urbanos\nConsidere del conjunto de datos sobre zorros urbanos (foxes). Ajuste tres modelos:\n\n\\(m_1\\): weight ~ area;\n\\(m_2\\): weight ~ groupsize;\n\\(m_3\\): weight ~ area + groupsize.\n\n\nPara los modelos \\(m_1\\) y \\(m_2\\), represente gr√°ficamente los resultados, incluyendo la recta de regresi√≥n MAP, su intervalo del 89% y el intervalo de predicci√≥n del 89%. ¬øEs alguna de las dos variables importantes para predecir la masa de un zorro?\nRepresentar gr√°ficamente las predicciones del modelo para cada predictor, dejando el otro constante en su valor medio. ¬øQu√© puede decirse sobre la importancia de las variables para predecir la masa de un zorro?\n\nüìå Un prior informativo marca la diferencia\nConsidere el conjunto de datos sobre belleza y proporci√≥n de sexos (sexratio) . Estos datos provienen de un estudio de adolescentes estadounidenses cuyo atractivo en una escala de cinco puntos fue evaluado por entrevistadores en una encuesta cara a cara. A√±os m√°s tarde, muchos de estos encuestados tuvieron hijos y se registraron ciertos atributos entre los cuales se incluy√≥ el sexo. El objetivo del an√°lisis es comparar la proporci√≥n de sexos de los hijos seg√∫n la belleza de los padres. Para ello considere el siguiente modelo de regresi√≥n:\n\\[\n\\begin{aligned}\n\\text{pf}_i &\\sim \\text{Normal}(\\mu_i, \\sigma^2) \\\\\n\\mu_i &= \\beta_0 + \\beta_1 \\text{belleza}_i \\\\\n\\end{aligned}\n\\]\nDonde \\(\\text{pf}\\) representa la proporci√≥n de beb√©s de sexo femenino y \\(\\text{belleza}\\) representa el grupo de belleza de los padres.\n\nAjuste el modelo utilizando m√≠nimos cuadrados.\nAjuste el modelo con {RStan} y priors uniformes.\nCompare el ajuste de ambos modelos.\nExplore los priors utilizados por {RStan} y la distribuci√≥n predictiva a priori. ¬øQu√© puede concluir?\nProponga distribuciones a priori informativas.\nAjuste el modelo utilizando {RStan} y los priors informativos.\nCompare el resultado con los obtenidos anteriormente y concluya.\n\nüìå ¬°A la pesca de priors!\nEl conjunto de datos fish-market contiene mediciones morfol√≥gicas realizadas sobre pescados de diferentes especies. El objetivo es construir un modelo de regresi√≥n lineal que permita predecir el peso de los pescados en base a sus otros atributos.\nUno de los modelos propuestos es el siguiente:\n\\[\n\\begin{aligned}\n\\log(\\text{Weight}_i) &\\sim \\text{Normal}(\\mu_i, \\sigma) \\\\\n\\mu_i  &= \\beta_{0, j[i]} + \\beta_{1, j[i]} \\log(\\text{Length1}_i)\n\\end{aligned}\n\\]\n\\(\\text{Weight}_i\\) es el peso del i-√©simo pescado en gramos y \\(\\text{Length1}_i\\) es la longitud del i-√©simo pescado en cent√≠metros. La letra \\(j\\) indexa las especies de los pescados. Por lo tanto, en este modelo cada especie tiene su propio intercepto y pendiente.\n\nImplemente el modelo utilizando {RStan} y los siguientes priors no informativos: \\[\n\\begin{aligned}\n\\beta_{0, j[i]} &\\sim \\text{Normal}(0, 10) \\\\\n\\beta_{1, j[i]} &\\sim \\text{Normal}(0, 5)\n\\end{aligned}\n\\]\nObtenga y visualice la distribuci√≥n predictiva a priori.\nElabore un gr√°fico y describa la funci√≥n de densidad a priori de los par√°metros \\(\\beta_{0, j[i]}\\) y \\(\\beta_{1, j[i]}\\).\nProponga priors m√°s adecuados en base a la interpretaci√≥n de los par√°metros del modelo y la informaci√≥n que tenga del problema.\nNuevamente, obtenga y visualice la distribuci√≥n predictiva a priori y compare con el resultado obtenido anteriormente.\nAjuste el modelo, obtenga la distribuci√≥n predictiva a posteriori y graf√≠quela.\n\n\n\n\n\n\nEspecies de peces de todas variedades y tama√±os.\n\n\n\n\nEn b√∫squeda del modelo adecuado\nContinuando con los datos del ejercicio anterior, El objetivo es construir un modelo de regresi√≥n lineal que permita predecir el peso de los pescados en base a sus otros atributos.\nConsidere los siguientes modelos\n\n\\(m_1\\): log(Weight) ~ 0 + Species;\n\\(m_2\\): log(Weight) ~ 0 + Species + log(Length1);\n\\(m_3\\): log(Weight) ~ 0 + Species + log(Length1):Species;\n\\(m_4\\): log(Weight) ~ 0 + Species + log(Length1):Species + log(Height);\n\\(m_5\\): log(Weight) ~ 0 + Species + log(Length1):Species + log(Height):Species.\n\n\nAjuste cada uno de los modelos.\nEstime el ELPD de cada modelo utilizando LOO y seleccione el modelo m√°s adecuado de acuerdo a este criterio.\nExplique el resultado.\n\nComparaci√≥n de modelos\nSe recopilaron datos (mesquite) con el fin de desarrollar un m√©todo para estimar la producci√≥n total (biomasa) de hojas de mesquite utilizando par√°metros f√°cilmente medibles de la planta, antes de que se realice la cosecha real. Se tomaron dos conjuntos separados de mediciones, uno en un grupo de 26 arbustos de mesquite y otro en un grupo diferente de 20 arbustos de mesquite medidos en un momento diferente del a√±o. Todos los datos se obtuvieron en la misma ubicaci√≥n geogr√°fica, pero ninguno constituy√≥ una muestra estrictamente aleatoria. La variable de resultado es el peso total (en gramos) de material fotosint√©tico obtenido de la cosecha real del arbusto. Las variables de entrada son:\n\n\n\n\n\n\n\nNombre\nDescripci√≥n\n\n\n\n\ndiam1\nDi√°metro de la copa medido a lo largo del eje m√°s largo del arbusto (metros)\n\n\ndiam2\nDi√°metro de la copa medido a lo largo del eje m√°s corto (metros)\n\n\ncanopy_height\nAltura de la copa\n\n\ntotal_height\nAltura total del arbusto\n\n\ndensity\nN√∫mero de tallos primarios por planta\n\n\ngroup\nGrupo de mediciones (0 para el primer grupo, 1 para el segundo)\n\n\n\n\nRealice un an√°lisis exploratorio de los datos.\nAjuste el modelo weight ~ diam1 + diam2 + canopy_height + total_height + density + group.\nExplore y describa el posterior.\nEstime ELPD mediante PSIS-CV con la funci√≥n loo(), analice los valores de las estimaciones del par√°metro \\(k\\) de la distribuci√≥n generalizada de Pareto y otros valores de la salida que crea relevant, ¬øqu√© puede concluir?\nEstime ELPD mediante K-fold cross validation con \\(K=10\\). Compare la estimaci√≥n con el resultado obtenido mediante PSIS-CV y concluya.\nAjuste el modelo transformando todas las variables num√©ricas con la funci√≥n logar√≠tmica. ¬øC√≥mo afecta esta transformaci√≥n la interpretaci√≥n de los coeficientes?\nEstime ELPD mediante PSIS-CV con la funci√≥n loo(). Concluya acerca de la estabilidad del c√≥mputo. ¬øEs posible comparar la la estimaci√≥n con la obtenida en el inciso iv? ¬øPor qu√©?\nCon ambos modelos, obtenga y grafique la distribuci√≥n predictiva a posteriori compar√°ndola con los datos observados ¬øCu√°l de los modelos representa mejor a los datos?\n\n\n\n\n\n\nUn √°rbol de mesquite.Foto de Sergei Bogomyakov, Alamy Stock Photo.\n\n\n\n\n\n\nüß© Secundarios en Portugal\nSe cuenta con un conjunto de datos sobre 343 estudiantes de secundaria de Portugal (portugal) y se desea predecir la calificaci√≥n final en matem√°ticas del √∫ltimo a√±o en base a un gran n√∫mero de predictores potencialmente relevantes.\nEl listado de variables se compone por: escuela del estudiante, sexo del estudiante, edad del estudiante, tipo de domicilio del estudiante, tama√±o de la familia, estado de convivencia de los padres, educaci√≥n de la madre, educaci√≥n del padre, tiempo de viaje del hogar a la escuela, tiempo de estudio semanal, n√∫mero de fracasos escolares pasados, apoyo educativo adicional, clases pagadas adicionales dentro de la materia del curso, actividades extracurriculares, si el estudiante asisti√≥ a una guarder√≠a, si el estudiante desea cursar estudios superiores, acceso a Internet en el hogar, si el estudiante tiene una relaci√≥n rom√°ntica, calidad de las relaciones familiares, tiempo libre despu√©s de la escuela, si el estudiante sale con amigos, consumo de alcohol entre semana, consumo de alcohol los fines de semana, estado de salud actual y n√∫mero de ausencias escolares.\nPriors d√©bilmente informativos\n\nAjuste un modelo de regresi√≥n lineal utilizando todos los predictores luego de estandarizarlos y con los siguientes priors: \\[\n\\begin{aligned}\n\\beta_k &\\sim \\text{Normal}(0, 2.5) \\\\\n\\sigma  &\\sim \\text{Exponential}(1 / \\text{std}(y))\n\\end{aligned}\n\\]\nElabore un gr√°fico para visualizar los posteriors marginales y comp√°relos. ¬øQu√© puede concluir acerca de su incertidumbre?\nCalcule y compare la mediana del \\(R^2\\) bayesiano y del \\(R^2\\) calculado mediante LOO ¬øQu√© conclusi√≥n puede extraer de esta comparaci√≥n?\n¬øCu√°l es el n√∫mero efectivo de par√°metros seg√∫n LOO? ¬øQu√© indica?\nObtenga muestras del prior y del posterior del \\(R^2\\) bayesiano, comp√°relos utilizando una visualizaci√≥n y concluya considerando la elecci√≥n de los priors d√©bilmente informativos sobre \\(\\beta_k\\) y \\(\\sigma\\).\n\nPriors alternativos (I)\nSi se asume que muchos predictores pueden tener poca relevancia, se pueden escalar los priors independientes para que la suma de la varianza de los priors se encuentre alrededor de un valor razonable. En este caso, se cuenta con 26 predictores y se podr√≠a suponer que la proporci√≥n de la varianza explicada por los predictores est√° alrededor de 0.3. Entonces, un enfoque simple consiste en asignar priors independientes a los coeficientes de regresi√≥n con media 0 y desviaci√≥n est√°ndar \\(\\sqrt{0.3/26}\\text{sd}(y)\\) y un prior exponencial con media \\(\\sqrt{0.7}\\text{sd}(y)\\) para \\(\\sigma\\).\n\nAjuste el modelo nuevamente utilizando los siguientes priors: \\[\n\\begin{aligned}\n\\beta_k &\\sim \\text{Normal}(0, \\sqrt{\\frac{0.3}{26}}\\text{sd}(y)) \\\\\n\\sigma  &\\sim \\text{Exponential}(1 / \\sqrt{0.7}\\text{sd}(y))\n\\end{aligned}\n\\]\nExplore la distribuci√≥n a priori sobre \\(R^2\\) y comp√°rela con la distribuci√≥n obtenida con los priors d√©bilmente informativos.\nCalcule ELPD mediante LOO y compare este modelo con el anterior.\nElabore un gr√°fico para visualizar los posteriors marginales. Compare este resultado con el obtenido con los priors d√©bilmente informativos.\n\nPriors alternativos (II)\nOtra alternativa es asumir que solo algunos de los predictores tienen alta relevancia y que el resto de los predictores tienen una relevancia insignificante. Una posibilidad para modelar bajo este supuesto es el horseshoe prior regularizado1. Este prior utiliza distribuciones normales independientes con media 0 y varianza \\(\\tau^2\\lambda_k^2\\) para los coeficientes de regresi√≥n \\(\\beta_k\\) y se describe a continuaci√≥n:\n\\[\n\\begin{aligned}\n\\beta_k   &\\sim \\text{Normal}(0, \\tau^2\\tilde{\\lambda}_k^2) \\\\\n\\tilde{\\lambda}_k^2 &= \\frac{c^2\\lambda_k^2}{c^2 + \\tau^2\\lambda_k^2} \\\\\nc & = \\sqrt{c'} \\text{SS} \\\\\nc' &\\sim \\text{InvGamma}(0.5 \\cdot SDF, 0.5 \\cdot SDF) \\\\\n\\lambda_k &\\sim \\text{StudentT}^+(\\text{df} = 1, \\mu = 0, \\sigma = 1 ) \\\\\n\\tau      &\\sim \\text{StudentT}^+(\\text{df} = 1, \\mu = 0, \\sigma = \\text{GS})\n\\end{aligned}\n\\]\ncon \\[\n\\begin{aligned}\n\\text{GS} = \\frac{p_0}{p - p_0} \\frac{\\sigma}{\\sqrt{n}} \\\\\n\\text{SS} = \\sqrt{\\frac{0.3}{p_0}} \\text{sd}(y) \\\\\n\\text{SDF} = 4\n\\end{aligned}\n\\]\ndonde \\(\\text{GS}\\), \\(\\text{SS}\\) y \\(\\text{SDF}\\) representan global scale, slab scale y slab degrees of freedom, respectivamente. Adem√°s, \\(p\\) representa la cantidad de predictores, 26, y \\(p_0\\) la cantidad de predictores que se espera que sean relevantes.\nIntuitivamente, el par√°metro global \\(\\tau\\) empuja todos los \\(\\beta_k\\) hacia el 0, mientras que los par√°metros locales \\(\\lambda_k\\) contribuyen a que algunos de los \\(\\beta_k\\) escapen del 0.\n\nUtilice \\(p_0 = 6\\) para ajustar el modelo con todos los predictores y grafique y analice los posteriors marginales.\nCompare este modelo con los ajustados anteriormente en base a sus ELPD estimados con LOO y concluya.\n\nPriors d√©bilmente informativos con menos predictores\nAjuste el modelo de regresi√≥n con un subconjunto de predictores que crea conveniente y los priors d√©bilmente informativos que se utilizaron inicialmente.\n\nVisualice los posteriors marginales.\nNuevamente, calcule y compare la mediana del \\(R^2\\) bayesiano y del \\(R^2\\) calculado mediante.\nCompare este modelo con el ajustado anteriormente en base a sus ELPD estimados con LOO y concluya sobre la capacidad predictiva de este modelo.",
    "crumbs": [
      "Pr√°ctica",
      "Unidad 4"
    ]
  },
  {
    "objectID": "practica/practica_04.html#footnotes",
    "href": "practica/practica_04.html#footnotes",
    "title": "Pr√°ctica - Unidad 4",
    "section": "Notas",
    "text": "Notas\n\n\nEl nombre horseshoe (herradura) no se relaciona con la forma de la funci√≥n de densidad en s√≠, sino con la forma del prior impl√≠cito para los coeficientes de shrinkage (contracci√≥n) aplicados a cada par√°metro.‚Ü©Ô∏é",
    "crumbs": [
      "Pr√°ctica",
      "Unidad 4"
    ]
  },
  {
    "objectID": "practica/practica_05.html",
    "href": "practica/practica_05.html",
    "title": "Pr√°ctica - Unidad 5",
    "section": "",
    "text": "En esta √∫ltima unidad de la pr√°ctica se presentan ejercicios que requieren el desarrollo de modelos de regresi√≥n con un nivdel de complejidad mayor. La distribuci√≥n condicional de la respuesta ya no es necesariamente normal y la forma del predictor de la media incluye caracter√≠sticas que lo diferencian de un predictor lineal simple. Adem√°s, esta unidad presenta ejercicios con modelos jer√°rquicos.\n\nüìå Regresi√≥n Poisson\nConsidere el siguiente modelo para datos de conteo con un predictor \\(X\\) que toma valores entre -3 y 50:\n\\[\n\\begin{aligned}\nY_i   &\\sim \\mathrm{Poisson}(\\lambda_i) \\\\\n\\log(\\lambda_i) &= \\beta X \\\\\n\\end{aligned}\n\\]\n\nGenere 1000 valores de \\(X\\), asuma un valor conocido (y fijo) para \\(\\beta\\), simule los correspondientes valores de \\(\\lambda_i\\) y los de \\(Y_i\\). ¬øC√≥mo es \\(\\lambda\\) en funci√≥n de \\(X\\)? ¬øEs lineal la relaci√≥n entre \\(X\\) e \\(Y\\)? ¬øQu√© ocurre con la varianza de \\(Y\\) en funci√≥n de \\(X\\)? ¬øC√≥mo es la distribuci√≥n marginal de \\(Y\\)?\nAhora a√±ada incertidumbre al valor de \\(\\beta\\) (¬øc√≥mo se hace esto?) y simule nuevamente valores para \\(\\lambda_i\\) y \\(Y_i\\). Compare los resultados.\n\nüìå Regresi√≥n log√≠stica\nConsidere el siguiente modelo de clasificaci√≥n con un predictor \\(X\\) que toma valores entre -30 y 10:\n\\[\n\\begin{aligned}\nY_i   &\\sim \\mathrm{Bernoulli}(\\theta_i) \\\\\n\\log\\left(\\frac{\\theta_i}{1 - \\theta_i}\\right) &= \\beta X \\\\\n\\end{aligned}\n\\]\n\nGenere 1000 valores de \\(X\\), asuma un valor conocido (y fijo) para \\(\\beta\\), simule los correspondientes valores de \\(\\theta_i\\) y los de \\(Y_i\\). ¬øC√≥mo es \\(\\theta\\) en funci√≥n de \\(X\\)? ¬øEs lineal la relaci√≥n entre \\(X\\) e \\(Y\\)?\nAhora a√±ada incertidumbre al valor de \\(\\beta\\) (¬øc√≥mo se hace esto?) y simule nuevamente valores para \\(\\theta_i\\) y \\(Y_i\\). Compare los resultados.\n\nüìå Intenci√≥n de voto\nEl conjunto de datos elecciones.csv contiene los resultados de un estudio piloto sobre intenci√≥n de voto. Contiene las variables voto, edad y partido que indican respectivamente el candidato elegido, la edad y la afinidad partidaria del encuestado.\nUtilice un modelo de regresi√≥n log√≠stica para responder a las siguiente preguntas de investigaci√≥n:\n\n¬øC√≥mo se relaciona la edad de los encuestados con la intenci√≥n de voto?\n¬øEs esta relaci√≥n diferente para las diferentes afinidades partidarias?\n\nüìå D√≠as de ausencia\nUn organismo p√∫blico de un Estado de los Estados Unidos est√° intereaso en estudiar el comportamiento de la asistencia de los estudiantes de secundaria. Para eso se cuenta con datos de 314 estudiantes tercer a√±o en students.csv. Los predictores del n√∫mero de d√≠as de ausencia incluyen el tipo de programa en el que est√° inscrito el estudiante y una prueba estandarizada de matem√°ticas.\nLas variables de inter√©s en el conjunto de datos son:\n\ndaysabs: El n√∫mero de d√≠as de ausencia. Es nuestra variable de respuesta.\nprogr: El tipo de programa. Puede ser uno de los siguientes: \"General\", \"Academic\" o \"Vocational\".\nmath: Puntuaci√≥n en una prueba de matem√°ticas estandarizada.\n\nInteresa evaluar la asociaci√≥n entre el tipo de programa y la puntuaci√≥n en la prueba con los d√≠as de ausencia. Tambi√©n se desea ver ver si la asociaci√≥n entre el puntaje en la prueba y los d√≠as de ausencia es diferente en cada tipo de programa.\nRealice un an√°lisis exploratorio de los datos y elabore un modelo de regresi√≥n Poisson que permita explicar la asociaci√≥n entre las variables predictoras y la cantidad de d√≠as que se ausentan los estudiantes.\nüìå Baseball\nEl b√©isbol es uno de los deportes donde se m√°s intensivamente se utilizan herramientas estad√≠sticas y anal√≠ticas. La cantidad de m√©tricas que se calculan para los jugadores es muy elevada. Supongamos que estamos en un equipo de b√©isbol y nos gustar√≠a cuantificar el rendimiento de los jugadores, siendo una de las m√©tricas su promedio de bateo (definido por la cantidad de veces que un bateador golpea una pelota lanzada, dividido por el n√∫mero de veces que se presenta al bate) ¬øC√≥mo podr√≠amos utilizar la estad√≠stica bayesiana para resolver este problema?\nLa tabla batting es una compilaci√≥n de datos hist√≥ricos de b√©isbol realizada por el Baseball Databank. Entre otras, contiene las siguientes columnas de inter√©s:\n\nplayerID: Identificaci√≥n del jugador\nAB: Cantidad de veces que el jugador se presenta al bate\nH: Cantidad de veces que el jugador golpea la pelota al batear\nbatting_avg: El cociente entre H y AB\n\nProponga un modelo de regresi√≥n log√≠stica para estimar la probabilidad de bateo para cada jugador. Incorpore la identificaci√≥n del jugador en el modelo. Considere primero un modelo no jer√°rquico y luego un modelo jer√°rquico.\nüìå Privados del sue√±o\nEl conjunto de datos sleepstudy contiene el tiempo de reacci√≥n promedio en una serie de pruebas para un grupo de participantes en un estudio de privaci√≥n del sue√±o. Los primeros dos d√≠as del estudio se consideran de adaptaci√≥n y entrenamiento, el tercer d√≠a es una l√≠nea de base y la privaci√≥n del sue√±o comienza despu√©s del d√≠a 3. Los sujetos de este grupo estaban restringidos a 3 horas de sue√±o por noche. El objetivo era analizar c√≥mo la falta de sue√±o afectaba la capacidad de respuesta y la precisi√≥n en dicha tarea.\nLa variable de respuesta es \"Reaction\", que representa el promedio de las mediciones de tiempo de reacci√≥n de los participantes en un d√≠a determinado (en milisegundos). Las dos covariables son \"Days\", que indica el n√∫mero de d√≠as de privaci√≥n del sue√±o, y \"Subject\", que es el identificador del participante sobre el cual se realiz√≥ la medici√≥n.\nSe propone utilizar un modelo de regresi√≥n lineal de la forma:\n\\[\n\\begin{aligned}\n\\text{Reaction}_i & \\sim \\text{Normal}(\\mu_i, \\sigma) \\\\\n\\mu_i & = \\beta_0 + \\beta_1\\text{Days}_i \\\\\n\\end{aligned}\n\\]\nConsiderando a los sujetos como grupos, eval√∫e las siguientes alternativas para estimar el intercepto y la pendiente:\n\nComplete pooling\nNo pooling\nPartial pooling\n\nüìå Las 8 escuelas\nEl archivo escuelas.csv contiene la cantidad de horas semanales que dedican a estudiar los estudiantes de 8 escuelas. Obtener la distribuci√≥n a posteriori para las medias poblacionales de los 8 colegios considerando el modelo normal jer√°rquico: \\[\n\\begin{aligned}\nY_{i} &\\sim \\text{Normal}(\\theta_{j[i]}, \\sigma^2) \\\\\n\\theta_j &\\sim \\text{Normal}(\\mu, \\tau^2) \\\\\n\\mu &\\sim \\text{Normal}(\\mu_0, \\gamma_0^2) \\\\\n1/\\tau^2   &\\sim \\text{Gamma}(\\eta_0 / 2, \\eta_0 \\tau_0^2 / 2) \\\\\n1/\\sigma^2 &\\sim \\text{Gamma}(\\nu_0 / 2, \\nu_0 \\sigma_0^2 / 2)\n\\end{aligned}\n\\]\ndonde \\(i \\in \\{1, 2, \\cdots, 180\\}\\) y \\(j \\in \\{1, 2, \\cdots, 8\\}\\) y los valores de los hiperpar√°metros fijos son:\n\\[\n\\mu_0 = 7,\\ \\gamma_0^2=5, \\  \\tau_0^2 = 10,\\ \\eta_0 = 2,\\ \\sigma_0^2 = 15,\\ \\nu_0 = 2.\n\\]\n\nUse {Stan} para obtener una muestra de la distribuici√≥n a posteriori. Para verificar la convergencia de las cadenas de Markov, considere la medida \\(R\\) de Rubin y un tama√±o de muestra efectivo de al menos 1000.\nPresentar en una tabla la media, la mediana y el intervalo de credibilidad 95% para \\(\\sigma^2\\), \\(\\mu\\) y \\(\\tau^2\\).\nCompare las densidades a priori y a posteriori para cada uno de los par√°metros. Interprete.\nDibuje las densidades a priori y a posteriori para \\(\\gamma=\\frac{\\tau^2}{\\sigma^2 + \\tau^2}\\) y compare.\nDetermine \\(P(\\theta_4 &lt; \\theta_3 \\mid \\boldsymbol{y}_1, \\cdots , \\boldsymbol{y}_8)\\)\nHacer un gr√°fico que relacione las medias muestrales \\(\\bar{y}_1, \\cdots, \\bar{y}_8\\) con las medias a posteriori de \\(\\theta_1, \\cdots,\\theta_8\\). Compare tambi√©n la media a posteriori de \\(\\mu\\) con la media global de todas las observaciones.\n\nüìå Modelo lineal para elecciones en Estados Unidos\nUtilice el conjunto de datos de las elecciones presidenciales de Estados Unidos del a√±o 2016 que se provee en Reich y Ghosh (2019) (rep_2012_2016). Elabore un modelo de regresi√≥n lineal bayesiano donde la variable respuesta es la diferencia porcentual entre el porcentaje de votos que obtuvo el candidato Republicano en el 2016 versus los que tuvo en el 2012 en cada condado y utilice todas las dem√°s variables como predictoras.\n\nUtilice distribuciones a priori normales no informativas. Interprete las distribuciones a posteriori marginales de los coeficientes de regresi√≥n.\nCalcule los residuos \\(\\boldsymbol{r} = \\boldsymbol{y} - \\boldsymbol{X}\\hat{\\boldsymbol{\\beta}}\\) donde \\(\\hat{\\boldsymbol{\\beta}}\\) es la media a posteriori del vector de coeficientes de regresi√≥n ¬øPuede concluir que los residuos siguen una distribuci√≥n normal? ¬øQu√© condados presentan los residuos m√°s grandes y m√°s peque√±os? ¬øQu√© puede indicar sobre estos condados? \n\nControl de armas\nUtilice el conjunto de datos sobre el control de armas en Estados Unidos. Estos datos provienen de un estudio transversal. Para el estado \\(i\\), sea \\(Y_i\\) el numero de homicidios y \\(N_i\\) el tama√±o de la poblaci√≥n.\n\nAjuste el modelo \\(Y_i \\mid \\boldsymbol{\\beta} \\sim \\text{Poisson}(N_i\\lambda_i)\\) donde \\(\\text{log}(\\lambda_i) = \\boldsymbol{X}_i\\boldsymbol{\\beta}\\). Use distribuciones a priori no informativas y \\(p = 7\\) de las covariables en \\(\\boldsymbol{X}_i\\): el intercepto, los cinco confounders \\(\\boldsymbol{Z}_i\\), y el n√∫mero de leyes relacionadas a armas. Justifique que el sampler ha convergido y explorado suficientemente la distribuci√≥n a posteriori y resuma la distribuci√≥n a posteriori de \\(\\boldsymbol{\\beta}\\). \n\nüß© ¬øA cu√°ntas Sof√≠as conoces?\nDescargue el conjunto de datos babynames en R y calcule el log-odds de un beb√© llamado ‚ÄúSophia‚Äù en cada a√±o luego de 1950.\n\nlibrary(babynames)\ndat &lt;- babynames\ndat &lt;- dat[dat$name == \"Sophia\" & dat$sex == \"F\" & dat$year &gt; 1950, ]\nyr &lt;- dat$year\np &lt;- dat$prop\nt &lt;- dat$year - 1950\nY &lt;- log(p / (1 - p))\n\nSea \\(Y_t\\) el log-odds muestral en el a√±o \\(t + 1950\\). Ajuste el siguiente modelo auto-regresivo de orden 1:\n\\[\n\\begin{aligned}\nY_t   &= \\mu_t + \\rho(Y_{t - 1} + \\mu_{t - 1}) + \\varepsilon_t \\\\\n\\mu_t &= \\alpha + \\beta t \\\\\n\\varepsilon &\\underset{iid}{\\sim} \\text{Normal}(0, \\sigma^2) \\\\\n\\alpha, \\beta &\\sim \\text{Normal}(0, 100^2) \\\\\n\\rho &\\sim \\text{Uniforme}(-1, 1) \\\\\n\\sigma^2 &\\sim \\text{InvGamma}(0.1, 0.1)\n\\end{aligned}\n\\]\n\nInterprete los par√°metros del modelo (\\(\\alpha\\), \\(\\beta\\), \\(\\rho\\) y \\(\\sigma^2\\))\nAjuste el modelo utilizando {RStan} para \\(t &gt; 1\\). Verifique la convergencia y reporte la media a posteriori e intervalos del 95% para los par√°metros.\nGrafique la distribuci√≥n predictiva a posteriori para \\(Y_t\\) en el a√±o 2020. \n\nüß© Meta-an√°lisis\nEn este ejercicio se llevar√° a cabo un meta-an√°lisis, es decir, un an√°lisis que combina el resultado de varios estudios. Los datos provienen del paquete {rmeta} en R.\n\nlibrary(rmeta)\ndata(cochrane)\ncochrane\n\n          name ev.trt n.trt ev.ctrl n.ctrl\n1     Auckland     36   532      60    538\n2        Block      1    69       5     61\n3        Doran      4    81      11     63\n4        Gamsu     14   131      20    137\n5     Morrison      3    67       7     59\n6 Papageorgiou      1    71       7     75\n7      Tauesch      8    56      10     71\nLos datos provienen de siete ensayos aleatorizados que eval√∫an el efecto de la terapia con corticosteroides en la muerte neonatal. Para el ensayo \\(i \\in \\{1, \\dots, 7 \\}\\) \\(Y_{i0}\\) representa el n√∫mero de eventos que ocurren en el grupo de control de tama√±o \\(N_{i0}\\) y \\(Y_{i1}\\) representa el n√∫mero de eventos que ocurren en el grupo tratado de tama√±o \\(N_{i1}\\).\n\nAjuste el modelo \\(Y_{ij} \\mid \\theta_j \\underset{indep}{\\sim} \\text{Binomial}(N_{ij}, \\theta_j)\\) con \\(\\theta_0, \\theta_1 \\sim \\text{Uniforme}(0, 1)\\). ¬øSe puede concluir que el tratamiento est√° asociado a una reducci√≥n de la tasa de muerte?\nAjuste el modelo \\(Y_{ij} \\mid \\theta_j \\underset{indep}{\\sim} \\text{Binomial}(N_{ij}, \\theta_j)\\) con\n\n\\(\\text{logit}(\\theta_{ij}) = \\alpha_{ij}\\)\n\\(\\boldsymbol{\\alpha}_i = (\\alpha_{i0}, \\alpha_{i1})^T \\underset{iid}{\\sim} \\text{Normal}(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma})\\)\n\\(\\boldsymbol{\\mu} \\sim \\text{Normal}(0, 10^2I_2)\\)\n\\(\\boldsymbol{\\Sigma} \\sim \\text{InvWishart}(3, I_2)\\)\n\nInterprete los resultados indicando si estos sugieren que el tratamiento est√° asociado a una reducci√≥n en la tasa de muerte.\nDibuje un DAG para ambos modelos.\nDiscuta las ventajas y desventajas de ambos modelos.\n¬øCu√°l modelo es el preferido para estos datos? \n\nComparando modelos normales\nUtilice el conjuto de datos airquality que viene con el paquete {datasets} que se carga autom√°ticamente al crear una sesi√≥n de R.\n\nhead(airquality)\n\n  Ozone Solar.R Wind Temp Month Day\n1    41     190  7.4   67     5   1\n2    36     118  8.0   72     5   2\n3    12     149 12.6   74     5   3\n4    18     313 11.5   62     5   4\n5    NA      NA 14.3   56     5   5\n6    28      NA 14.9   66     5   6\nCompare los siguientes modelos utilizando 5-fold cross-validation:\n\\[\n\\begin{array}{l}\n\\mathcal{M}_1: \\text{Ozone}_i \\sim \\text{Normal}(\\beta_1 + \\beta_2 \\text{Solar.R}_i, \\sigma^2) \\\\\n\\mathcal{M}_2: \\text{Ozone}_i \\sim \\text{Normal}(\\beta_1 + \\beta_2 \\text{Solar.R}_i + \\beta_3 \\text{Temp}_i + \\beta_4 \\text{Wind}_i, \\sigma^2)\n\\end{array}\n\\]\nElija priors para los par√°metros de ambos modelos explicando su elecci√≥n. \n\n\n\nüß© Curvas de crecimiento de tiranos√°uridos\nSe analizan datos de 20 f√≥siles de tiranos√°uridos para estimar las curvas de crecimiento de cuatro especies: Albertosaurio, Daspletosaurio, Gorgosaurio y Tiranosaurio. Los datos se toman de la Tabla 1 de Erickson et¬†al. (2004) y se muestran en la Figura¬†1. El objetivo es determinar la curva de crecimiento, esto es, determinar el peso esperado por edad para todas las especies.\nEn el panel izquierdo de la Figura¬†1 se puede observar que hay una relaci√≥n no lineal entre la edad y el peso. Tambi√©n se observan ciertos patrones comunes a las especies. Por ejemplo, la relaci√≥n positiva entre las variables o el decrecimiento en la tasa de cambio conforme la edad es mayor.\n\n\n\n\n\n\n\n\nFigura¬†1: (Izquierda) Edad (a√±os) vs Peso (kilogramos). (Derecha) Los mismos datos luego de aplicar la transformaci√≥n logar√≠tmica a ambas variables.\n\n\n\n\n\nSea \\(Y_{ij}\\) el peso y \\(X_{ij}\\) y la edad de la muestra \\(i\\) de la especie \\(j\\), con \\(j = 1, 2, 3, 4\\). Se propone el siguiente modelo:\n\\[\nY_{ij} = f_j(X_{ij}) \\epsilon_{ij}\n\\]\ndonde \\(f_j\\) es la verdadera curva de crecimiento para la especie \\(j\\) y \\(\\epsilon_{ij} &gt; 0\\) es un error multiplicativo.\n\n¬øPor qu√© tiene sentido proponer un error multiplicativo?\n¬øCu√°l es un valor sensato para la media de la distribuci√≥n del error?\nUtilice una distribuci√≥n log-normal para el error, \\(\\log (\\epsilon_{ij}) \\sim \\text{Normal}\\). Proponga valores para la media y la varianza de forma tal que satisfagan la condici√≥n del punto anterior.\n\nEsto da lugar un al siguiente modelo log-normal para \\(Y_{ij}\\):\n\\[\n\\log (Y_{ij}) \\sim \\text{Normal}\n(\\log [f_j(X_{ij})] + \\mu_{\\log \\epsilon}, \\sigma^2_{\\log \\epsilon})\n\\]\ncon \\(\\mathbb{E}(Y_{ij}) = f_j(X_{ij})\\).\nA continuaci√≥n se proponen cuatro modelos que var√≠an seg√∫n la relaci√≥n funcional que se propone para \\(f_j\\) y la naturaleza de las distribuciones a priori que se utilizan.\nModelo 1\nObservando el panel derecho de la Figura¬†1 se puede concluir que luego de transformar ambas variables con la funci√≥n logaritmo la relaci√≥n se ve aproximadamente lineal. Por lo tanto, se propone el siguiente modelo log-lineal:\n\\[\n\\log [f_j(X)] = a_j + b_j \\log(X)\n\\]\ndonde \\(a_j\\) y \\(b_j\\) representan al intercepto y pendiente de la especie \\(j\\). La curva de crecimiento en la escala original resulta \\(f_j(X) = \\exp (a_j)X^{b_j}\\). Considere los siguientes priors:\n\\[\n\\begin{aligned}\na_j &\\sim \\text{Normal}(0, 10) \\\\\nb_j &\\sim \\text{Normal}(0, 10) \\\\\n\\sigma^2_j &\\sim \\text{InvGamma}(0.1, 0.1)\n\\end{aligned}\n\\]\n\nEscriba un programa en Stan que implemente el modelo y obtenga el posterior con {RStan}.\nAnalice los coeficientes del modelo y las curvas de crecimiento. Realice gr√°ficos que permitan observar la curva ajustada y su incertidumbre para cada especie.\n\nModelo 2\nEste modelo es el mismo que el Modelo 1, excepto que las especies tienen la misma varianza, \\(\\sigma^2_j = \\sigma^2\\) y los coeficientes de regresi√≥n son modelados de manera jer√°rquica. Utilice los siguientes priors:\n\\[\n\\begin{aligned}\n\\mu_a    &\\sim \\text{Normal}(0, 10) \\\\\n\\sigma_a &\\sim \\text{InvGamma}(0.1, 0.1) \\\\\n\\mu_b    &\\sim \\text{Normal}(0, 10) \\\\\n\\sigma_b &\\sim \\text{InvGamma}(0.1, 0.1) \\\\\na_j      &\\sim \\text{Normal}(\\mu_a, \\sigma^2_a) \\\\\nb_j      &\\sim \\text{Normal}(\\mu_b, \\sigma^2_b) \\\\\n\\sigma^2 &\\sim \\text{InvGamma}(0.1, 0.1)\n\\end{aligned}\n\\]\n\nEscriba un programa en Stan que implemente el modelo y obtenga el posterior con {RStan}.\nAnalice los coeficientes del modelo y las curvas de crecimiento. Genere gr√°ficos similares a los producidos en el punto anterior. Describa similitudes y diferencias respecto del modelo 1. Justifique su respuesta.\n¬øQu√© problemas detecta los modelos 1 y 2? Considere como evoluciona el peso conforme la edad seg√∫n el modelo.\n\nModelo 3\nComo alternativa al componente log-lineal anterior, se propone la siguiente curva de crecimiento log√≠stico:\n\\[\nf_j(X) = a_j + b_j \\frac{\\exp [d_j (\\log(X) - c_j)]}{1 + \\exp [d_j(\\log(X) - c_j)]}\n\\]\nEste modelo tiene cuatro par√°metros:\n\n\\(a_j\\) es el peso esperado cuando la edad es 0;\n\\(b_j\\) es el peso m√°ximo esperado (o la cota superior del peso);\n\\(\\log (c_j)\\) es la edad a la que la especie \\(j\\) alcanza la mitad del peso m√°ximo;\n\\(d_j &gt; 0\\) determina la tasa de crecimiento del peso conforme aumenta la edad.\n\nPara que la curva sea positiva y creciente para todas las edades, se debe cumplir que \\(a_j &gt; 0\\), \\(b_j &gt; a_j\\) y \\(d_j &gt; 0\\). Se pueden satisfacer estas restricciones expresando los par√°metros en funci√≥n de par√°metros cuyo dominio es \\(\\mathbb{R}\\):\n\n\\(a_j = \\exp (\\alpha_{j1})\\);\n\\(b_j = \\exp (\\alpha_{j2})\\);\n\\(c_j = \\alpha_{j3}\\);\n\\(d_j = \\exp (\\alpha_{j4})\\).\n\nConsidere las siguientes distribuciones a priori para los par√°metros del modelo:\n\\[\n\\begin{aligned}\n\\alpha_{jk} &\\sim \\text{Normal}(0, 10) \\\\\n\\sigma^2_j  &\\sim \\text{InvGamma}(0.1, 0.1)\n\\end{aligned}\n\\]\n\nEscriba un programa en Stan que implemente el modelo y obtenga el posterior con {RStan}.\nAnalice los diagn√≥sticos de la inferencia realizada.\nGrafique las curvas estimadas para cada especie junto a sus intervalos de credibilidad e interprete los resultados.\n\nModelo 4\nEste modelo es el mismo que el Modelo 3, excepto que las especies tienen la misma varianza, \\(\\sigma^2_j = \\sigma^2\\) y los coeficientes de regresi√≥n son modelados de manera jer√°rquica. Utilice los siguientes priors:\n\\[\n\\begin{aligned}\n\\mu_k             &\\sim \\text{Normal}(0, 10) \\\\\n\\sigma^2_k        &\\sim \\text{InvGamma}(0.1, 0.1) \\\\\n\\log(\\alpha_{jk}) &\\sim \\text{Normal}(\\mu_k, \\sigma^2_k) \\\\\n\\sigma^2          &\\sim \\text{InvGamma}(0.1, 0.1)\n\\end{aligned}\n\\]\n\nEscriba un programa en Stan que implemente el modelo y obtenga el posterior con {RStan}.\nAnalice los diagn√≥sticos de la inferencia y compare con los resultados del modelo 3.\nGrafique las curvas estimadas para cada especie junto a sus intervalos de credibilidad e interprete los resultados. Compare con los resultados del modelo 3. ¬øQu√© diferencias observa? ¬øPor qu√© se dan?\nEscriba una s√≠ntesis comparando todos los modelos desarrollados. Comente ventajas y desventajas de cada uno de ellos, explicando a que se deben en cada caso ¬øQu√© modelo resulta m√°s conveniente para estimar la curva de crecimiento de los tiranos√°uridos? Justifique su respuesta.",
    "crumbs": [
      "Pr√°ctica",
      "Unidad 5"
    ]
  },
  {
    "objectID": "practica/practica_05.html#modelos-de-regresi√≥n-avanzados",
    "href": "practica/practica_05.html#modelos-de-regresi√≥n-avanzados",
    "title": "Pr√°ctica - Unidad 5",
    "section": "",
    "text": "En esta √∫ltima unidad de la pr√°ctica se presentan ejercicios que requieren el desarrollo de modelos de regresi√≥n con un nivdel de complejidad mayor. La distribuci√≥n condicional de la respuesta ya no es necesariamente normal y la forma del predictor de la media incluye caracter√≠sticas que lo diferencian de un predictor lineal simple. Adem√°s, esta unidad presenta ejercicios con modelos jer√°rquicos.\n\nüìå Regresi√≥n Poisson\nConsidere el siguiente modelo para datos de conteo con un predictor \\(X\\) que toma valores entre -3 y 50:\n\\[\n\\begin{aligned}\nY_i   &\\sim \\mathrm{Poisson}(\\lambda_i) \\\\\n\\log(\\lambda_i) &= \\beta X \\\\\n\\end{aligned}\n\\]\n\nGenere 1000 valores de \\(X\\), asuma un valor conocido (y fijo) para \\(\\beta\\), simule los correspondientes valores de \\(\\lambda_i\\) y los de \\(Y_i\\). ¬øC√≥mo es \\(\\lambda\\) en funci√≥n de \\(X\\)? ¬øEs lineal la relaci√≥n entre \\(X\\) e \\(Y\\)? ¬øQu√© ocurre con la varianza de \\(Y\\) en funci√≥n de \\(X\\)? ¬øC√≥mo es la distribuci√≥n marginal de \\(Y\\)?\nAhora a√±ada incertidumbre al valor de \\(\\beta\\) (¬øc√≥mo se hace esto?) y simule nuevamente valores para \\(\\lambda_i\\) y \\(Y_i\\). Compare los resultados.\n\nüìå Regresi√≥n log√≠stica\nConsidere el siguiente modelo de clasificaci√≥n con un predictor \\(X\\) que toma valores entre -30 y 10:\n\\[\n\\begin{aligned}\nY_i   &\\sim \\mathrm{Bernoulli}(\\theta_i) \\\\\n\\log\\left(\\frac{\\theta_i}{1 - \\theta_i}\\right) &= \\beta X \\\\\n\\end{aligned}\n\\]\n\nGenere 1000 valores de \\(X\\), asuma un valor conocido (y fijo) para \\(\\beta\\), simule los correspondientes valores de \\(\\theta_i\\) y los de \\(Y_i\\). ¬øC√≥mo es \\(\\theta\\) en funci√≥n de \\(X\\)? ¬øEs lineal la relaci√≥n entre \\(X\\) e \\(Y\\)?\nAhora a√±ada incertidumbre al valor de \\(\\beta\\) (¬øc√≥mo se hace esto?) y simule nuevamente valores para \\(\\theta_i\\) y \\(Y_i\\). Compare los resultados.\n\nüìå Intenci√≥n de voto\nEl conjunto de datos elecciones.csv contiene los resultados de un estudio piloto sobre intenci√≥n de voto. Contiene las variables voto, edad y partido que indican respectivamente el candidato elegido, la edad y la afinidad partidaria del encuestado.\nUtilice un modelo de regresi√≥n log√≠stica para responder a las siguiente preguntas de investigaci√≥n:\n\n¬øC√≥mo se relaciona la edad de los encuestados con la intenci√≥n de voto?\n¬øEs esta relaci√≥n diferente para las diferentes afinidades partidarias?\n\nüìå D√≠as de ausencia\nUn organismo p√∫blico de un Estado de los Estados Unidos est√° intereaso en estudiar el comportamiento de la asistencia de los estudiantes de secundaria. Para eso se cuenta con datos de 314 estudiantes tercer a√±o en students.csv. Los predictores del n√∫mero de d√≠as de ausencia incluyen el tipo de programa en el que est√° inscrito el estudiante y una prueba estandarizada de matem√°ticas.\nLas variables de inter√©s en el conjunto de datos son:\n\ndaysabs: El n√∫mero de d√≠as de ausencia. Es nuestra variable de respuesta.\nprogr: El tipo de programa. Puede ser uno de los siguientes: \"General\", \"Academic\" o \"Vocational\".\nmath: Puntuaci√≥n en una prueba de matem√°ticas estandarizada.\n\nInteresa evaluar la asociaci√≥n entre el tipo de programa y la puntuaci√≥n en la prueba con los d√≠as de ausencia. Tambi√©n se desea ver ver si la asociaci√≥n entre el puntaje en la prueba y los d√≠as de ausencia es diferente en cada tipo de programa.\nRealice un an√°lisis exploratorio de los datos y elabore un modelo de regresi√≥n Poisson que permita explicar la asociaci√≥n entre las variables predictoras y la cantidad de d√≠as que se ausentan los estudiantes.\nüìå Baseball\nEl b√©isbol es uno de los deportes donde se m√°s intensivamente se utilizan herramientas estad√≠sticas y anal√≠ticas. La cantidad de m√©tricas que se calculan para los jugadores es muy elevada. Supongamos que estamos en un equipo de b√©isbol y nos gustar√≠a cuantificar el rendimiento de los jugadores, siendo una de las m√©tricas su promedio de bateo (definido por la cantidad de veces que un bateador golpea una pelota lanzada, dividido por el n√∫mero de veces que se presenta al bate) ¬øC√≥mo podr√≠amos utilizar la estad√≠stica bayesiana para resolver este problema?\nLa tabla batting es una compilaci√≥n de datos hist√≥ricos de b√©isbol realizada por el Baseball Databank. Entre otras, contiene las siguientes columnas de inter√©s:\n\nplayerID: Identificaci√≥n del jugador\nAB: Cantidad de veces que el jugador se presenta al bate\nH: Cantidad de veces que el jugador golpea la pelota al batear\nbatting_avg: El cociente entre H y AB\n\nProponga un modelo de regresi√≥n log√≠stica para estimar la probabilidad de bateo para cada jugador. Incorpore la identificaci√≥n del jugador en el modelo. Considere primero un modelo no jer√°rquico y luego un modelo jer√°rquico.\nüìå Privados del sue√±o\nEl conjunto de datos sleepstudy contiene el tiempo de reacci√≥n promedio en una serie de pruebas para un grupo de participantes en un estudio de privaci√≥n del sue√±o. Los primeros dos d√≠as del estudio se consideran de adaptaci√≥n y entrenamiento, el tercer d√≠a es una l√≠nea de base y la privaci√≥n del sue√±o comienza despu√©s del d√≠a 3. Los sujetos de este grupo estaban restringidos a 3 horas de sue√±o por noche. El objetivo era analizar c√≥mo la falta de sue√±o afectaba la capacidad de respuesta y la precisi√≥n en dicha tarea.\nLa variable de respuesta es \"Reaction\", que representa el promedio de las mediciones de tiempo de reacci√≥n de los participantes en un d√≠a determinado (en milisegundos). Las dos covariables son \"Days\", que indica el n√∫mero de d√≠as de privaci√≥n del sue√±o, y \"Subject\", que es el identificador del participante sobre el cual se realiz√≥ la medici√≥n.\nSe propone utilizar un modelo de regresi√≥n lineal de la forma:\n\\[\n\\begin{aligned}\n\\text{Reaction}_i & \\sim \\text{Normal}(\\mu_i, \\sigma) \\\\\n\\mu_i & = \\beta_0 + \\beta_1\\text{Days}_i \\\\\n\\end{aligned}\n\\]\nConsiderando a los sujetos como grupos, eval√∫e las siguientes alternativas para estimar el intercepto y la pendiente:\n\nComplete pooling\nNo pooling\nPartial pooling\n\nüìå Las 8 escuelas\nEl archivo escuelas.csv contiene la cantidad de horas semanales que dedican a estudiar los estudiantes de 8 escuelas. Obtener la distribuci√≥n a posteriori para las medias poblacionales de los 8 colegios considerando el modelo normal jer√°rquico: \\[\n\\begin{aligned}\nY_{i} &\\sim \\text{Normal}(\\theta_{j[i]}, \\sigma^2) \\\\\n\\theta_j &\\sim \\text{Normal}(\\mu, \\tau^2) \\\\\n\\mu &\\sim \\text{Normal}(\\mu_0, \\gamma_0^2) \\\\\n1/\\tau^2   &\\sim \\text{Gamma}(\\eta_0 / 2, \\eta_0 \\tau_0^2 / 2) \\\\\n1/\\sigma^2 &\\sim \\text{Gamma}(\\nu_0 / 2, \\nu_0 \\sigma_0^2 / 2)\n\\end{aligned}\n\\]\ndonde \\(i \\in \\{1, 2, \\cdots, 180\\}\\) y \\(j \\in \\{1, 2, \\cdots, 8\\}\\) y los valores de los hiperpar√°metros fijos son:\n\\[\n\\mu_0 = 7,\\ \\gamma_0^2=5, \\  \\tau_0^2 = 10,\\ \\eta_0 = 2,\\ \\sigma_0^2 = 15,\\ \\nu_0 = 2.\n\\]\n\nUse {Stan} para obtener una muestra de la distribuici√≥n a posteriori. Para verificar la convergencia de las cadenas de Markov, considere la medida \\(R\\) de Rubin y un tama√±o de muestra efectivo de al menos 1000.\nPresentar en una tabla la media, la mediana y el intervalo de credibilidad 95% para \\(\\sigma^2\\), \\(\\mu\\) y \\(\\tau^2\\).\nCompare las densidades a priori y a posteriori para cada uno de los par√°metros. Interprete.\nDibuje las densidades a priori y a posteriori para \\(\\gamma=\\frac{\\tau^2}{\\sigma^2 + \\tau^2}\\) y compare.\nDetermine \\(P(\\theta_4 &lt; \\theta_3 \\mid \\boldsymbol{y}_1, \\cdots , \\boldsymbol{y}_8)\\)\nHacer un gr√°fico que relacione las medias muestrales \\(\\bar{y}_1, \\cdots, \\bar{y}_8\\) con las medias a posteriori de \\(\\theta_1, \\cdots,\\theta_8\\). Compare tambi√©n la media a posteriori de \\(\\mu\\) con la media global de todas las observaciones.\n\nüìå Modelo lineal para elecciones en Estados Unidos\nUtilice el conjunto de datos de las elecciones presidenciales de Estados Unidos del a√±o 2016 que se provee en Reich y Ghosh (2019) (rep_2012_2016). Elabore un modelo de regresi√≥n lineal bayesiano donde la variable respuesta es la diferencia porcentual entre el porcentaje de votos que obtuvo el candidato Republicano en el 2016 versus los que tuvo en el 2012 en cada condado y utilice todas las dem√°s variables como predictoras.\n\nUtilice distribuciones a priori normales no informativas. Interprete las distribuciones a posteriori marginales de los coeficientes de regresi√≥n.\nCalcule los residuos \\(\\boldsymbol{r} = \\boldsymbol{y} - \\boldsymbol{X}\\hat{\\boldsymbol{\\beta}}\\) donde \\(\\hat{\\boldsymbol{\\beta}}\\) es la media a posteriori del vector de coeficientes de regresi√≥n ¬øPuede concluir que los residuos siguen una distribuci√≥n normal? ¬øQu√© condados presentan los residuos m√°s grandes y m√°s peque√±os? ¬øQu√© puede indicar sobre estos condados? \n\nControl de armas\nUtilice el conjunto de datos sobre el control de armas en Estados Unidos. Estos datos provienen de un estudio transversal. Para el estado \\(i\\), sea \\(Y_i\\) el numero de homicidios y \\(N_i\\) el tama√±o de la poblaci√≥n.\n\nAjuste el modelo \\(Y_i \\mid \\boldsymbol{\\beta} \\sim \\text{Poisson}(N_i\\lambda_i)\\) donde \\(\\text{log}(\\lambda_i) = \\boldsymbol{X}_i\\boldsymbol{\\beta}\\). Use distribuciones a priori no informativas y \\(p = 7\\) de las covariables en \\(\\boldsymbol{X}_i\\): el intercepto, los cinco confounders \\(\\boldsymbol{Z}_i\\), y el n√∫mero de leyes relacionadas a armas. Justifique que el sampler ha convergido y explorado suficientemente la distribuci√≥n a posteriori y resuma la distribuci√≥n a posteriori de \\(\\boldsymbol{\\beta}\\). \n\nüß© ¬øA cu√°ntas Sof√≠as conoces?\nDescargue el conjunto de datos babynames en R y calcule el log-odds de un beb√© llamado ‚ÄúSophia‚Äù en cada a√±o luego de 1950.\n\nlibrary(babynames)\ndat &lt;- babynames\ndat &lt;- dat[dat$name == \"Sophia\" & dat$sex == \"F\" & dat$year &gt; 1950, ]\nyr &lt;- dat$year\np &lt;- dat$prop\nt &lt;- dat$year - 1950\nY &lt;- log(p / (1 - p))\n\nSea \\(Y_t\\) el log-odds muestral en el a√±o \\(t + 1950\\). Ajuste el siguiente modelo auto-regresivo de orden 1:\n\\[\n\\begin{aligned}\nY_t   &= \\mu_t + \\rho(Y_{t - 1} + \\mu_{t - 1}) + \\varepsilon_t \\\\\n\\mu_t &= \\alpha + \\beta t \\\\\n\\varepsilon &\\underset{iid}{\\sim} \\text{Normal}(0, \\sigma^2) \\\\\n\\alpha, \\beta &\\sim \\text{Normal}(0, 100^2) \\\\\n\\rho &\\sim \\text{Uniforme}(-1, 1) \\\\\n\\sigma^2 &\\sim \\text{InvGamma}(0.1, 0.1)\n\\end{aligned}\n\\]\n\nInterprete los par√°metros del modelo (\\(\\alpha\\), \\(\\beta\\), \\(\\rho\\) y \\(\\sigma^2\\))\nAjuste el modelo utilizando {RStan} para \\(t &gt; 1\\). Verifique la convergencia y reporte la media a posteriori e intervalos del 95% para los par√°metros.\nGrafique la distribuci√≥n predictiva a posteriori para \\(Y_t\\) en el a√±o 2020. \n\nüß© Meta-an√°lisis\nEn este ejercicio se llevar√° a cabo un meta-an√°lisis, es decir, un an√°lisis que combina el resultado de varios estudios. Los datos provienen del paquete {rmeta} en R.\n\nlibrary(rmeta)\ndata(cochrane)\ncochrane\n\n          name ev.trt n.trt ev.ctrl n.ctrl\n1     Auckland     36   532      60    538\n2        Block      1    69       5     61\n3        Doran      4    81      11     63\n4        Gamsu     14   131      20    137\n5     Morrison      3    67       7     59\n6 Papageorgiou      1    71       7     75\n7      Tauesch      8    56      10     71\nLos datos provienen de siete ensayos aleatorizados que eval√∫an el efecto de la terapia con corticosteroides en la muerte neonatal. Para el ensayo \\(i \\in \\{1, \\dots, 7 \\}\\) \\(Y_{i0}\\) representa el n√∫mero de eventos que ocurren en el grupo de control de tama√±o \\(N_{i0}\\) y \\(Y_{i1}\\) representa el n√∫mero de eventos que ocurren en el grupo tratado de tama√±o \\(N_{i1}\\).\n\nAjuste el modelo \\(Y_{ij} \\mid \\theta_j \\underset{indep}{\\sim} \\text{Binomial}(N_{ij}, \\theta_j)\\) con \\(\\theta_0, \\theta_1 \\sim \\text{Uniforme}(0, 1)\\). ¬øSe puede concluir que el tratamiento est√° asociado a una reducci√≥n de la tasa de muerte?\nAjuste el modelo \\(Y_{ij} \\mid \\theta_j \\underset{indep}{\\sim} \\text{Binomial}(N_{ij}, \\theta_j)\\) con\n\n\\(\\text{logit}(\\theta_{ij}) = \\alpha_{ij}\\)\n\\(\\boldsymbol{\\alpha}_i = (\\alpha_{i0}, \\alpha_{i1})^T \\underset{iid}{\\sim} \\text{Normal}(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma})\\)\n\\(\\boldsymbol{\\mu} \\sim \\text{Normal}(0, 10^2I_2)\\)\n\\(\\boldsymbol{\\Sigma} \\sim \\text{InvWishart}(3, I_2)\\)\n\nInterprete los resultados indicando si estos sugieren que el tratamiento est√° asociado a una reducci√≥n en la tasa de muerte.\nDibuje un DAG para ambos modelos.\nDiscuta las ventajas y desventajas de ambos modelos.\n¬øCu√°l modelo es el preferido para estos datos? \n\nComparando modelos normales\nUtilice el conjuto de datos airquality que viene con el paquete {datasets} que se carga autom√°ticamente al crear una sesi√≥n de R.\n\nhead(airquality)\n\n  Ozone Solar.R Wind Temp Month Day\n1    41     190  7.4   67     5   1\n2    36     118  8.0   72     5   2\n3    12     149 12.6   74     5   3\n4    18     313 11.5   62     5   4\n5    NA      NA 14.3   56     5   5\n6    28      NA 14.9   66     5   6\nCompare los siguientes modelos utilizando 5-fold cross-validation:\n\\[\n\\begin{array}{l}\n\\mathcal{M}_1: \\text{Ozone}_i \\sim \\text{Normal}(\\beta_1 + \\beta_2 \\text{Solar.R}_i, \\sigma^2) \\\\\n\\mathcal{M}_2: \\text{Ozone}_i \\sim \\text{Normal}(\\beta_1 + \\beta_2 \\text{Solar.R}_i + \\beta_3 \\text{Temp}_i + \\beta_4 \\text{Wind}_i, \\sigma^2)\n\\end{array}\n\\]\nElija priors para los par√°metros de ambos modelos explicando su elecci√≥n. \n\n\n\nüß© Curvas de crecimiento de tiranos√°uridos\nSe analizan datos de 20 f√≥siles de tiranos√°uridos para estimar las curvas de crecimiento de cuatro especies: Albertosaurio, Daspletosaurio, Gorgosaurio y Tiranosaurio. Los datos se toman de la Tabla 1 de Erickson et¬†al. (2004) y se muestran en la Figura¬†1. El objetivo es determinar la curva de crecimiento, esto es, determinar el peso esperado por edad para todas las especies.\nEn el panel izquierdo de la Figura¬†1 se puede observar que hay una relaci√≥n no lineal entre la edad y el peso. Tambi√©n se observan ciertos patrones comunes a las especies. Por ejemplo, la relaci√≥n positiva entre las variables o el decrecimiento en la tasa de cambio conforme la edad es mayor.\n\n\n\n\n\n\n\n\nFigura¬†1: (Izquierda) Edad (a√±os) vs Peso (kilogramos). (Derecha) Los mismos datos luego de aplicar la transformaci√≥n logar√≠tmica a ambas variables.\n\n\n\n\n\nSea \\(Y_{ij}\\) el peso y \\(X_{ij}\\) y la edad de la muestra \\(i\\) de la especie \\(j\\), con \\(j = 1, 2, 3, 4\\). Se propone el siguiente modelo:\n\\[\nY_{ij} = f_j(X_{ij}) \\epsilon_{ij}\n\\]\ndonde \\(f_j\\) es la verdadera curva de crecimiento para la especie \\(j\\) y \\(\\epsilon_{ij} &gt; 0\\) es un error multiplicativo.\n\n¬øPor qu√© tiene sentido proponer un error multiplicativo?\n¬øCu√°l es un valor sensato para la media de la distribuci√≥n del error?\nUtilice una distribuci√≥n log-normal para el error, \\(\\log (\\epsilon_{ij}) \\sim \\text{Normal}\\). Proponga valores para la media y la varianza de forma tal que satisfagan la condici√≥n del punto anterior.\n\nEsto da lugar un al siguiente modelo log-normal para \\(Y_{ij}\\):\n\\[\n\\log (Y_{ij}) \\sim \\text{Normal}\n(\\log [f_j(X_{ij})] + \\mu_{\\log \\epsilon}, \\sigma^2_{\\log \\epsilon})\n\\]\ncon \\(\\mathbb{E}(Y_{ij}) = f_j(X_{ij})\\).\nA continuaci√≥n se proponen cuatro modelos que var√≠an seg√∫n la relaci√≥n funcional que se propone para \\(f_j\\) y la naturaleza de las distribuciones a priori que se utilizan.\nModelo 1\nObservando el panel derecho de la Figura¬†1 se puede concluir que luego de transformar ambas variables con la funci√≥n logaritmo la relaci√≥n se ve aproximadamente lineal. Por lo tanto, se propone el siguiente modelo log-lineal:\n\\[\n\\log [f_j(X)] = a_j + b_j \\log(X)\n\\]\ndonde \\(a_j\\) y \\(b_j\\) representan al intercepto y pendiente de la especie \\(j\\). La curva de crecimiento en la escala original resulta \\(f_j(X) = \\exp (a_j)X^{b_j}\\). Considere los siguientes priors:\n\\[\n\\begin{aligned}\na_j &\\sim \\text{Normal}(0, 10) \\\\\nb_j &\\sim \\text{Normal}(0, 10) \\\\\n\\sigma^2_j &\\sim \\text{InvGamma}(0.1, 0.1)\n\\end{aligned}\n\\]\n\nEscriba un programa en Stan que implemente el modelo y obtenga el posterior con {RStan}.\nAnalice los coeficientes del modelo y las curvas de crecimiento. Realice gr√°ficos que permitan observar la curva ajustada y su incertidumbre para cada especie.\n\nModelo 2\nEste modelo es el mismo que el Modelo 1, excepto que las especies tienen la misma varianza, \\(\\sigma^2_j = \\sigma^2\\) y los coeficientes de regresi√≥n son modelados de manera jer√°rquica. Utilice los siguientes priors:\n\\[\n\\begin{aligned}\n\\mu_a    &\\sim \\text{Normal}(0, 10) \\\\\n\\sigma_a &\\sim \\text{InvGamma}(0.1, 0.1) \\\\\n\\mu_b    &\\sim \\text{Normal}(0, 10) \\\\\n\\sigma_b &\\sim \\text{InvGamma}(0.1, 0.1) \\\\\na_j      &\\sim \\text{Normal}(\\mu_a, \\sigma^2_a) \\\\\nb_j      &\\sim \\text{Normal}(\\mu_b, \\sigma^2_b) \\\\\n\\sigma^2 &\\sim \\text{InvGamma}(0.1, 0.1)\n\\end{aligned}\n\\]\n\nEscriba un programa en Stan que implemente el modelo y obtenga el posterior con {RStan}.\nAnalice los coeficientes del modelo y las curvas de crecimiento. Genere gr√°ficos similares a los producidos en el punto anterior. Describa similitudes y diferencias respecto del modelo 1. Justifique su respuesta.\n¬øQu√© problemas detecta los modelos 1 y 2? Considere como evoluciona el peso conforme la edad seg√∫n el modelo.\n\nModelo 3\nComo alternativa al componente log-lineal anterior, se propone la siguiente curva de crecimiento log√≠stico:\n\\[\nf_j(X) = a_j + b_j \\frac{\\exp [d_j (\\log(X) - c_j)]}{1 + \\exp [d_j(\\log(X) - c_j)]}\n\\]\nEste modelo tiene cuatro par√°metros:\n\n\\(a_j\\) es el peso esperado cuando la edad es 0;\n\\(b_j\\) es el peso m√°ximo esperado (o la cota superior del peso);\n\\(\\log (c_j)\\) es la edad a la que la especie \\(j\\) alcanza la mitad del peso m√°ximo;\n\\(d_j &gt; 0\\) determina la tasa de crecimiento del peso conforme aumenta la edad.\n\nPara que la curva sea positiva y creciente para todas las edades, se debe cumplir que \\(a_j &gt; 0\\), \\(b_j &gt; a_j\\) y \\(d_j &gt; 0\\). Se pueden satisfacer estas restricciones expresando los par√°metros en funci√≥n de par√°metros cuyo dominio es \\(\\mathbb{R}\\):\n\n\\(a_j = \\exp (\\alpha_{j1})\\);\n\\(b_j = \\exp (\\alpha_{j2})\\);\n\\(c_j = \\alpha_{j3}\\);\n\\(d_j = \\exp (\\alpha_{j4})\\).\n\nConsidere las siguientes distribuciones a priori para los par√°metros del modelo:\n\\[\n\\begin{aligned}\n\\alpha_{jk} &\\sim \\text{Normal}(0, 10) \\\\\n\\sigma^2_j  &\\sim \\text{InvGamma}(0.1, 0.1)\n\\end{aligned}\n\\]\n\nEscriba un programa en Stan que implemente el modelo y obtenga el posterior con {RStan}.\nAnalice los diagn√≥sticos de la inferencia realizada.\nGrafique las curvas estimadas para cada especie junto a sus intervalos de credibilidad e interprete los resultados.\n\nModelo 4\nEste modelo es el mismo que el Modelo 3, excepto que las especies tienen la misma varianza, \\(\\sigma^2_j = \\sigma^2\\) y los coeficientes de regresi√≥n son modelados de manera jer√°rquica. Utilice los siguientes priors:\n\\[\n\\begin{aligned}\n\\mu_k             &\\sim \\text{Normal}(0, 10) \\\\\n\\sigma^2_k        &\\sim \\text{InvGamma}(0.1, 0.1) \\\\\n\\log(\\alpha_{jk}) &\\sim \\text{Normal}(\\mu_k, \\sigma^2_k) \\\\\n\\sigma^2          &\\sim \\text{InvGamma}(0.1, 0.1)\n\\end{aligned}\n\\]\n\nEscriba un programa en Stan que implemente el modelo y obtenga el posterior con {RStan}.\nAnalice los diagn√≥sticos de la inferencia y compare con los resultados del modelo 3.\nGrafique las curvas estimadas para cada especie junto a sus intervalos de credibilidad e interprete los resultados. Compare con los resultados del modelo 3. ¬øQu√© diferencias observa? ¬øPor qu√© se dan?\nEscriba una s√≠ntesis comparando todos los modelos desarrollados. Comente ventajas y desventajas de cada uno de ellos, explicando a que se deben en cada caso ¬øQu√© modelo resulta m√°s conveniente para estimar la curva de crecimiento de los tiranos√°uridos? Justifique su respuesta.",
    "crumbs": [
      "Pr√°ctica",
      "Unidad 5"
    ]
  },
  {
    "objectID": "practica/practica_03.html",
    "href": "practica/practica_03.html",
    "title": "Pr√°ctica - Unidad 3",
    "section": "",
    "text": "Esta secci√≥n contiene una lista exhaustiva de ejercicios que requieren el uso de herramientas computacionales para resolver problemas que involucran una variedad de c√°lculos, como el c√°lculo de probabilidades y el c√°lculo de integrales. Se vuelve indispensable el uso de R y se promueve el uso de buenas pr√°cticas de programaci√≥n cient√≠fica, como el uso de funciones compartimentar los componentes de un programa.\n\nüìå ¬°A calcular probabilidades! (I)\nSea \\(X \\sim \\text{Normal}(\\mu=3, \\sigma=1.2)\\).\n\nElabore un gr√°fico que permita visualizar la funci√≥n de densidad de probabilidad de \\(X\\).\n¬øCu√°l es la probabilidad de que \\(X\\) sea menor a 2.5?\n¬øCu√°l es la probabilidad de que \\(X\\) sea mayor a 4?\n¬øCu√°l es la probabilidad de que \\(X\\) sea mayor 2 y menor 3?\n\nüìå ¬°A calcular probabilidades! (II)\nSea \\(X \\sim \\text{Beta}(\\alpha=10, \\beta=2)\\).\n\nElabore un gr√°fico que permita visualizar la funci√≥n de densidad de probabilidad de \\(X\\).\n¬øCu√°l es la probabilidad de que \\(X\\) sea menor a 0.5?\n¬øCu√°l es la probabilidad de que \\(X\\) sea mayor a 0.8?\n¬øCu√°l es la probabilidad de que \\(X\\) sea mayor 0.25 y menor 0.75?\n\nüìå Magia blanca: Obtener probabilidades mediante simulaci√≥n\nResponda los dos puntos anteriores sin evaluar la funci√≥n de densidad ni la funci√≥n de distribuci√≥n de las variables aleatorias mencionadas. Para eso genere muestras que provengan de las correspondientes distribuciones y util√≠celas para responder las preguntas mencionadas. Reflexione sobre las ventajas y desventajas de utilizar un enfoque basado en la simulaci√≥n para resolver problemas.\nüìå Media y varianza de una variable aleatoria\nUna variable aleatoria \\(X\\) toma valores en el conjunto \\(\\{2, 4, 6, 8, 10\\}\\) con igual probabilidad. Encuentre la media y el desv√≠o est√°ndar de las variables \\(X\\) e \\(Y = 2X + 1\\). \nüìå Probabilidades a posteriori\nEn un problema determinado la distribuci√≥n a posteriori de la par√°metro de inte≈ïes \\(\\alpha\\) es \\(\\Gamma(k=3, \\theta=1.5)\\), donde \\(k\\) es el par√°metro de forma y \\(\\theta\\) es el par√°metro de escala. Calcule la probabilidad de que \\(\\alpha^2\\) sea mayor a 10. \nüìå Probabilidades con dos variables aleatorias\nSean \\(X\\) e \\(Y\\) dos variables aleatorias independientes con distribuci√≥n uniforme en el intervalo \\([0, 1]\\).\n\n¬øCu√°l es la probabilidad de que \\(X \\le Y\\)?\nGrafique los puntos muestreados coloreando de acuerdo a si la muestra satisface el evento antes mencionado o no.\n\nTe veo en la fotocopiadora\nDos estudiantes de estad√≠stica deciden encontrarse en la fotocopiadora de la Facultad entre las 10 y las 11 de la ma√±ana, eligiendo el tiempo de llegada al azar. La estudiante A esperar√° 10 minutos luego de llegar. Si el estudiante B no llega en ese intervalo, se ir√°. Lo mismo hace el estudiante B, pero este decide esperar 14 minutos. ¬øCu√°l es la probabilidad de que se produzca el encuentro en la fotocopiadora entre la estudiante A y el estudiante B?\nüß© Armando celulares en Tierra del Fuego\nUna m√°quina que se utiliza para ensamblar tel√©fonos celulares en una f√°brica en Tierra del Fuego cuenta con tres componentes cr√≠ticos para su funcionamiento. Ante una falla en cualquiera de estos componentes, la m√°quina se detiene. Las probabilidades de que estos elementos operen correctamente durante un d√≠a cualquiera son \\(p_1 = 0.8\\), \\(p_2 = 0.9\\) y \\(p_3 = 0.7\\). Responda las siguientes preguntas utilizando t√©cnicas de simulaci√≥n:\n\n¬øCu√°l es la probabilidad de que la m√°quina falle en el primer d√≠a de uso?\n¬øCu√°l es la probabilidad de que la m√°quina siga funcionando luego de 10 d√≠as?\n¬øCu√°l es la probabilidad de que la m√°quina falle en el d√≠a 7 de uso?\nSea \\(T=\\) Cantidad de d√≠as que la m√°quina funciona ininterrumpidamente. Grafique la funci√≥n de densidad de probabilidad de \\(T\\).\n\nBolas infinitas\nEste tuit propone un problema muy interesante. Una urna contiene una bola azul y una amarilla. Se elije una bola al azar y se la vuelve a colocar junto con otra bola adicional del mismo color. Se repite este proceso indefinidamente. ¬øQu√© ocurre con la proporci√≥n de bolas azules en la urna a medida que repetimos m√°s y m√°s veces?\n\nTiende a 1/2.\nTiende a 0 √≥ a 1.\nNo se estabiliza.\nNinguna de las anteriores.\n\nEscriba un programa en R para responder esta pregunta utilizando simulaciones. Genere gr√°ficos que faciliten la comprensi√≥n del resultado. \nüìå Estimando el valor de \\(\\pi\\)\nImagine un c√≠rculo de radio \\(r\\) y un cuadrado de lado \\(2r\\), ambos centrados en el mismo punto, que de manera arbitraria puede ser el punto \\((0, 0)\\). Obtenga muestras de una distribuci√≥n uniforme en el plano \\((x, y)\\), cuyo dominio est√° acotado por el cuadrado antes mencionado. Para cada muestra extraida, determine si se encuentra dentro del c√≠rculo o no ‚Äì todos las muestras se encontrar√°n dentro del cuadrado. Utilice esta informaci√≥n para estimar el valor de \\(\\pi\\).\n\n\n\n\n\n\n\n\n\nAlgunos datos √∫tiles\n\nArea de un c√≠rculo: \\(\\pi \\cdot r^2\\).\nArea de un cuadrado: \\(a^2\\), donde \\(a\\) es la longitud del lado.\n\nLos puntos uniformes\nSe seleccionan dos puntos de manera uniforme e independiente dentro de un c√≠rculo. ¬øCu√°l es la probabilidad de que la distancia entre dos puntos sea menor al radio?\n\nResuelva el problema utilizando R.\nElabore una visualizaci√≥n que facilite la comunicaci√≥n de los resultados.\n\n\n\n\n\n\n\nAyuda\n\n\n\n\n\nTO DO Algo sobre como simular de una uniforme en 2d\n\n\n\nSobre el hist√≥rico 7 a 1 del 2014\nEn la Copa del Mundo de la FIFA 2014, Alemania jug√≥ contra Brasil en la semifinal. Los alemanes hicieron el primer gol a los 11 minutos y el segundo a los 23. Asuma que el tiempo entre goles sigue una distribuci√≥n exponencial. Elija una distribuci√≥n a priori para el tiempo entre goles (puede ser conjugada o no). En ese momento del partido,\n\n¬øCu√°l es la distribuci√≥n a posteriori del tiempo entre goles de Alemania?\n¬øCu√°ntos goles cabr√≠a esperar que Alemania hiciera al finalizar los 90 minutos?\n¬øCu√°l era la probabilidad de que Alemania hiciera m√°s de 5 goles (cosa que ocurri√≥)? \n\nüíª ¬øTendr√© que esperar mucho?\nEl tiempo que un empleado de recursos humanos demora en hacer una entrevista tiene distribuci√≥n exponencial con media 30 minutos. Los tiempos de duraci√≥n de cada entrevista se pueden considerar independientes entre s√≠. Las entrevistas a postulantes para un trabajo est√°n programadas cada 15 minutos, comenzando desde las 8. Es v√°lido considerar que todos los postulantes llegan puntuales a su entrevista. Cuando la persona del turno de las 8:15 llega a la oficina\n\n¬øCu√°l es la probabilidad de que tenga que esperar antes de ser entrevistada?\n¬øCu√°l es el horario esperado al que terminar√° su entrevista? \n\nüß© ¬°Qu√© casualidad!\nDos personas se conocen en la fila de embarque para un vuelo en un avi√≥n Airbus A330-300. Considere que el Airbus A330-300 tiene 30 filas de 2-4-2 asientos.\n\n¬øCu√°l es la probabilidad de que tengan asientos en la misma fila?\n¬øCu√°l es la probabilidad de que est√©n sentados en asientos adyacentes?\n\n‚úçÔ∏è üíª üìå El Problema de Monty Hall\nEl Problema de Monty Hall es un problema de probabilidad basado en un juego del concurso televisivo estadounidense ‚ÄúTrato hecho‚Äù. En este problema, el concursante debe elegir una puerta entre tres, todas cerradas. El premio consiste en llevarse lo que se encuentra detr√°s de la elegida. Se sabe con certeza que tras una de ellas se oculta un autom√≥vil, y tras las otras dos hay cabras. Una vez que el concursante haya elegido una puerta y comunicado su elecci√≥n a los presentes, el presentador, que sabe lo que hay detr√°s de cada puerta, abrir√° una de las otras dos en la que haya una cabra. A continuaci√≥n, le da la opci√≥n al concursante de cambiar, si lo desea, de puerta (tiene dos opciones). ¬øDebe el concursante mantener su elecci√≥n original o escoger la otra puerta? ¬øHay alguna diferencia? Resuelva este ejercicio utilizando simulaciones. \n\n\n\n\n\nLas 3 puertas del problema de Monty Hall\n\n\n\n\nüíª Que los cumplan feliz\nBas√°ndose en el siguiente tuit y conociendo el problema del cumplea√±os (¬øcu√°ntas personas debe haber en una habitaci√≥n para que la probabilidad de que dos de ellas cumplan a√±os el mismo d√≠a sea mayor a X%?) construir un gr√°fico similar al del tuit donde se grafique la probabilidad de que haya \\(n\\) personas que cumplan a√±os el mismo d√≠a para \\(K\\) personas presentes en la habitaci√≥n.\nüíª Qu√© suerte, ¬øno?\nPrevio a la final de la Copa Am√©rica 2021, los jugadores de la Selecci√≥n Argentina se re√∫nen en la habitaci√≥n del hotel como se describe en este tuit.\n\n¬øCu√°l es la probabilidad de que un jugador adivine una de diez cartas?\n¬øCu√°l es la probabilidad de que tres de ellos adivinen una de diez cartas?\n\nEl √°lbum del Campe√≥n\nEl √°lbum oficial del Mundial de F√∫tbol de Qatar 2022 consta de 638 figuritas. Cada paquete trae cinco figuritas.\n\nComprando cinco paquetes, ¬øcu√°l es la probabilidad de tener a Messi?\nComprando cinco paquetes, ¬øcu√°l es la probabilidad de sacar a Messi repetido?\n¬øCu√°ntos paquetes se necesitan, en promedio, para completar el √°lbum?\nSi a una persona le faltan diez figuritas para completar el √°lbum, ¬øcu√°ntos paquetes tiene que comprar para asegurarse de lograrlo?\n\nüíª ¬øQue t√°n raras son estas secuencias raras?\nSi se arroja una moneda \\(n\\) veces, ¬øcu√°l es la probabilidad de que no haya secuencias de \\(k\\) caras?\nüìå Un viaje por el elevador\n¬øCu√°l es la probabilidad de que tres personas en un ascensor con doce pisos presionen para ir a tres pisos consecutivos? ¬øQu√© supuestos realiza para resolver el problema? Escr√≠balos en una lista de manera expl√≠cita.\nLa vida es muy corta como para perderla ordenando medias\nUn caj√≥n contiene 10 pares de medias. No hay dos pares iguales. Por fiaca, el due√±o de las medias no las agrupa despu√©s de lavarlas y simplemente las pone en el caj√≥n. Al momento de necesitar un par de medias, saca una tras una hasta que se forma un par. En promedio, ¬øcu√°ntas medias sacar√° hasta encontrar un par? \n¬øVale la pena hacer un ensayo cl√≠nico a gran escala?\nDados los resultados de un estudio piloto, la probabilidad a posteriori de que la droga desarrollada por tu compa√±√≠a sea mas efectiva que el tratamiento actual es \\(\\theta \\in [0, 1]\\). Tu compa√±√≠a est√° considerando realizar un ensayo cl√≠nico a gran escala para confirmar que la droga que desarrollan es de hecho mejor. El costo del estudio es $X. Si la droga es mejor, la probabilidad de que esto se confirme en el ensayo es del 80%. Si la droga no es mejor, hay una probabilidad del 5% de que el estudio confirme que es mejor. Si el ensayo sugiere que tu droga es mejor, ganar√°s $cX. ¬øPara qu√© valores de \\(\\theta\\) y \\(c\\) tiene sentido realizar el estudio? \nEl problema de concordancia\nResuelva el problema de concordancia de de Montmort presentado en la Pr√°ctica 0 utilizando simulaciones. \nEl problema de los sobres\nResuelva el problema de los dos sobres presentado en la Pr√°ctica 0 utilizando simulaciones.\nüß© Integraci√≥n por muestreo\nCalcule las siguientes integrales utilizando muestras.\n\n\\(\\displaystyle \\int_{-\\infty}^{\\infty}{\\frac{x^2}{\\sqrt{2\\pi}}\\exp \\left(-\\frac{x^2}{2} \\right)dx}\\)\n\\(\\displaystyle \\int_{1}^{\\infty}{\\frac{x^3}{\\sqrt{2\\pi}}\\exp \\left(-\\frac{x^2}{2} \\right)dx}\\)\n\\(\\displaystyle \\int_{1}^{\\infty}{\\frac{x^6}{\\sqrt{2\\pi}}\\exp \\left(-\\frac{x^2 - 4x}{2} \\right)dx}\\)\n\\(\\displaystyle \\int_{1}^{10}{x^6\\frac{e^{-x^4/2}}{\\sqrt{2\\pi}}dx}\\) \n\n¬øCu√°l es la distribuci√≥n de muestreo aproximada al usar muestreo independiente para evaluar integrales?",
    "crumbs": [
      "Pr√°ctica",
      "Unidad 3"
    ]
  },
  {
    "objectID": "practica/practica_03.html#m√©todos-computacionales",
    "href": "practica/practica_03.html#m√©todos-computacionales",
    "title": "Pr√°ctica - Unidad 3",
    "section": "",
    "text": "Esta secci√≥n contiene una lista exhaustiva de ejercicios que requieren el uso de herramientas computacionales para resolver problemas que involucran una variedad de c√°lculos, como el c√°lculo de probabilidades y el c√°lculo de integrales. Se vuelve indispensable el uso de R y se promueve el uso de buenas pr√°cticas de programaci√≥n cient√≠fica, como el uso de funciones compartimentar los componentes de un programa.\n\nüìå ¬°A calcular probabilidades! (I)\nSea \\(X \\sim \\text{Normal}(\\mu=3, \\sigma=1.2)\\).\n\nElabore un gr√°fico que permita visualizar la funci√≥n de densidad de probabilidad de \\(X\\).\n¬øCu√°l es la probabilidad de que \\(X\\) sea menor a 2.5?\n¬øCu√°l es la probabilidad de que \\(X\\) sea mayor a 4?\n¬øCu√°l es la probabilidad de que \\(X\\) sea mayor 2 y menor 3?\n\nüìå ¬°A calcular probabilidades! (II)\nSea \\(X \\sim \\text{Beta}(\\alpha=10, \\beta=2)\\).\n\nElabore un gr√°fico que permita visualizar la funci√≥n de densidad de probabilidad de \\(X\\).\n¬øCu√°l es la probabilidad de que \\(X\\) sea menor a 0.5?\n¬øCu√°l es la probabilidad de que \\(X\\) sea mayor a 0.8?\n¬øCu√°l es la probabilidad de que \\(X\\) sea mayor 0.25 y menor 0.75?\n\nüìå Magia blanca: Obtener probabilidades mediante simulaci√≥n\nResponda los dos puntos anteriores sin evaluar la funci√≥n de densidad ni la funci√≥n de distribuci√≥n de las variables aleatorias mencionadas. Para eso genere muestras que provengan de las correspondientes distribuciones y util√≠celas para responder las preguntas mencionadas. Reflexione sobre las ventajas y desventajas de utilizar un enfoque basado en la simulaci√≥n para resolver problemas.\nüìå Media y varianza de una variable aleatoria\nUna variable aleatoria \\(X\\) toma valores en el conjunto \\(\\{2, 4, 6, 8, 10\\}\\) con igual probabilidad. Encuentre la media y el desv√≠o est√°ndar de las variables \\(X\\) e \\(Y = 2X + 1\\). \nüìå Probabilidades a posteriori\nEn un problema determinado la distribuci√≥n a posteriori de la par√°metro de inte≈ïes \\(\\alpha\\) es \\(\\Gamma(k=3, \\theta=1.5)\\), donde \\(k\\) es el par√°metro de forma y \\(\\theta\\) es el par√°metro de escala. Calcule la probabilidad de que \\(\\alpha^2\\) sea mayor a 10. \nüìå Probabilidades con dos variables aleatorias\nSean \\(X\\) e \\(Y\\) dos variables aleatorias independientes con distribuci√≥n uniforme en el intervalo \\([0, 1]\\).\n\n¬øCu√°l es la probabilidad de que \\(X \\le Y\\)?\nGrafique los puntos muestreados coloreando de acuerdo a si la muestra satisface el evento antes mencionado o no.\n\nTe veo en la fotocopiadora\nDos estudiantes de estad√≠stica deciden encontrarse en la fotocopiadora de la Facultad entre las 10 y las 11 de la ma√±ana, eligiendo el tiempo de llegada al azar. La estudiante A esperar√° 10 minutos luego de llegar. Si el estudiante B no llega en ese intervalo, se ir√°. Lo mismo hace el estudiante B, pero este decide esperar 14 minutos. ¬øCu√°l es la probabilidad de que se produzca el encuentro en la fotocopiadora entre la estudiante A y el estudiante B?\nüß© Armando celulares en Tierra del Fuego\nUna m√°quina que se utiliza para ensamblar tel√©fonos celulares en una f√°brica en Tierra del Fuego cuenta con tres componentes cr√≠ticos para su funcionamiento. Ante una falla en cualquiera de estos componentes, la m√°quina se detiene. Las probabilidades de que estos elementos operen correctamente durante un d√≠a cualquiera son \\(p_1 = 0.8\\), \\(p_2 = 0.9\\) y \\(p_3 = 0.7\\). Responda las siguientes preguntas utilizando t√©cnicas de simulaci√≥n:\n\n¬øCu√°l es la probabilidad de que la m√°quina falle en el primer d√≠a de uso?\n¬øCu√°l es la probabilidad de que la m√°quina siga funcionando luego de 10 d√≠as?\n¬øCu√°l es la probabilidad de que la m√°quina falle en el d√≠a 7 de uso?\nSea \\(T=\\) Cantidad de d√≠as que la m√°quina funciona ininterrumpidamente. Grafique la funci√≥n de densidad de probabilidad de \\(T\\).\n\nBolas infinitas\nEste tuit propone un problema muy interesante. Una urna contiene una bola azul y una amarilla. Se elije una bola al azar y se la vuelve a colocar junto con otra bola adicional del mismo color. Se repite este proceso indefinidamente. ¬øQu√© ocurre con la proporci√≥n de bolas azules en la urna a medida que repetimos m√°s y m√°s veces?\n\nTiende a 1/2.\nTiende a 0 √≥ a 1.\nNo se estabiliza.\nNinguna de las anteriores.\n\nEscriba un programa en R para responder esta pregunta utilizando simulaciones. Genere gr√°ficos que faciliten la comprensi√≥n del resultado. \nüìå Estimando el valor de \\(\\pi\\)\nImagine un c√≠rculo de radio \\(r\\) y un cuadrado de lado \\(2r\\), ambos centrados en el mismo punto, que de manera arbitraria puede ser el punto \\((0, 0)\\). Obtenga muestras de una distribuci√≥n uniforme en el plano \\((x, y)\\), cuyo dominio est√° acotado por el cuadrado antes mencionado. Para cada muestra extraida, determine si se encuentra dentro del c√≠rculo o no ‚Äì todos las muestras se encontrar√°n dentro del cuadrado. Utilice esta informaci√≥n para estimar el valor de \\(\\pi\\).\n\n\n\n\n\n\n\n\n\nAlgunos datos √∫tiles\n\nArea de un c√≠rculo: \\(\\pi \\cdot r^2\\).\nArea de un cuadrado: \\(a^2\\), donde \\(a\\) es la longitud del lado.\n\nLos puntos uniformes\nSe seleccionan dos puntos de manera uniforme e independiente dentro de un c√≠rculo. ¬øCu√°l es la probabilidad de que la distancia entre dos puntos sea menor al radio?\n\nResuelva el problema utilizando R.\nElabore una visualizaci√≥n que facilite la comunicaci√≥n de los resultados.\n\n\n\n\n\n\n\nAyuda\n\n\n\n\n\nTO DO Algo sobre como simular de una uniforme en 2d\n\n\n\nSobre el hist√≥rico 7 a 1 del 2014\nEn la Copa del Mundo de la FIFA 2014, Alemania jug√≥ contra Brasil en la semifinal. Los alemanes hicieron el primer gol a los 11 minutos y el segundo a los 23. Asuma que el tiempo entre goles sigue una distribuci√≥n exponencial. Elija una distribuci√≥n a priori para el tiempo entre goles (puede ser conjugada o no). En ese momento del partido,\n\n¬øCu√°l es la distribuci√≥n a posteriori del tiempo entre goles de Alemania?\n¬øCu√°ntos goles cabr√≠a esperar que Alemania hiciera al finalizar los 90 minutos?\n¬øCu√°l era la probabilidad de que Alemania hiciera m√°s de 5 goles (cosa que ocurri√≥)? \n\nüíª ¬øTendr√© que esperar mucho?\nEl tiempo que un empleado de recursos humanos demora en hacer una entrevista tiene distribuci√≥n exponencial con media 30 minutos. Los tiempos de duraci√≥n de cada entrevista se pueden considerar independientes entre s√≠. Las entrevistas a postulantes para un trabajo est√°n programadas cada 15 minutos, comenzando desde las 8. Es v√°lido considerar que todos los postulantes llegan puntuales a su entrevista. Cuando la persona del turno de las 8:15 llega a la oficina\n\n¬øCu√°l es la probabilidad de que tenga que esperar antes de ser entrevistada?\n¬øCu√°l es el horario esperado al que terminar√° su entrevista? \n\nüß© ¬°Qu√© casualidad!\nDos personas se conocen en la fila de embarque para un vuelo en un avi√≥n Airbus A330-300. Considere que el Airbus A330-300 tiene 30 filas de 2-4-2 asientos.\n\n¬øCu√°l es la probabilidad de que tengan asientos en la misma fila?\n¬øCu√°l es la probabilidad de que est√©n sentados en asientos adyacentes?\n\n‚úçÔ∏è üíª üìå El Problema de Monty Hall\nEl Problema de Monty Hall es un problema de probabilidad basado en un juego del concurso televisivo estadounidense ‚ÄúTrato hecho‚Äù. En este problema, el concursante debe elegir una puerta entre tres, todas cerradas. El premio consiste en llevarse lo que se encuentra detr√°s de la elegida. Se sabe con certeza que tras una de ellas se oculta un autom√≥vil, y tras las otras dos hay cabras. Una vez que el concursante haya elegido una puerta y comunicado su elecci√≥n a los presentes, el presentador, que sabe lo que hay detr√°s de cada puerta, abrir√° una de las otras dos en la que haya una cabra. A continuaci√≥n, le da la opci√≥n al concursante de cambiar, si lo desea, de puerta (tiene dos opciones). ¬øDebe el concursante mantener su elecci√≥n original o escoger la otra puerta? ¬øHay alguna diferencia? Resuelva este ejercicio utilizando simulaciones. \n\n\n\n\n\nLas 3 puertas del problema de Monty Hall\n\n\n\n\nüíª Que los cumplan feliz\nBas√°ndose en el siguiente tuit y conociendo el problema del cumplea√±os (¬øcu√°ntas personas debe haber en una habitaci√≥n para que la probabilidad de que dos de ellas cumplan a√±os el mismo d√≠a sea mayor a X%?) construir un gr√°fico similar al del tuit donde se grafique la probabilidad de que haya \\(n\\) personas que cumplan a√±os el mismo d√≠a para \\(K\\) personas presentes en la habitaci√≥n.\nüíª Qu√© suerte, ¬øno?\nPrevio a la final de la Copa Am√©rica 2021, los jugadores de la Selecci√≥n Argentina se re√∫nen en la habitaci√≥n del hotel como se describe en este tuit.\n\n¬øCu√°l es la probabilidad de que un jugador adivine una de diez cartas?\n¬øCu√°l es la probabilidad de que tres de ellos adivinen una de diez cartas?\n\nEl √°lbum del Campe√≥n\nEl √°lbum oficial del Mundial de F√∫tbol de Qatar 2022 consta de 638 figuritas. Cada paquete trae cinco figuritas.\n\nComprando cinco paquetes, ¬øcu√°l es la probabilidad de tener a Messi?\nComprando cinco paquetes, ¬øcu√°l es la probabilidad de sacar a Messi repetido?\n¬øCu√°ntos paquetes se necesitan, en promedio, para completar el √°lbum?\nSi a una persona le faltan diez figuritas para completar el √°lbum, ¬øcu√°ntos paquetes tiene que comprar para asegurarse de lograrlo?\n\nüíª ¬øQue t√°n raras son estas secuencias raras?\nSi se arroja una moneda \\(n\\) veces, ¬øcu√°l es la probabilidad de que no haya secuencias de \\(k\\) caras?\nüìå Un viaje por el elevador\n¬øCu√°l es la probabilidad de que tres personas en un ascensor con doce pisos presionen para ir a tres pisos consecutivos? ¬øQu√© supuestos realiza para resolver el problema? Escr√≠balos en una lista de manera expl√≠cita.\nLa vida es muy corta como para perderla ordenando medias\nUn caj√≥n contiene 10 pares de medias. No hay dos pares iguales. Por fiaca, el due√±o de las medias no las agrupa despu√©s de lavarlas y simplemente las pone en el caj√≥n. Al momento de necesitar un par de medias, saca una tras una hasta que se forma un par. En promedio, ¬øcu√°ntas medias sacar√° hasta encontrar un par? \n¬øVale la pena hacer un ensayo cl√≠nico a gran escala?\nDados los resultados de un estudio piloto, la probabilidad a posteriori de que la droga desarrollada por tu compa√±√≠a sea mas efectiva que el tratamiento actual es \\(\\theta \\in [0, 1]\\). Tu compa√±√≠a est√° considerando realizar un ensayo cl√≠nico a gran escala para confirmar que la droga que desarrollan es de hecho mejor. El costo del estudio es $X. Si la droga es mejor, la probabilidad de que esto se confirme en el ensayo es del 80%. Si la droga no es mejor, hay una probabilidad del 5% de que el estudio confirme que es mejor. Si el ensayo sugiere que tu droga es mejor, ganar√°s $cX. ¬øPara qu√© valores de \\(\\theta\\) y \\(c\\) tiene sentido realizar el estudio? \nEl problema de concordancia\nResuelva el problema de concordancia de de Montmort presentado en la Pr√°ctica 0 utilizando simulaciones. \nEl problema de los sobres\nResuelva el problema de los dos sobres presentado en la Pr√°ctica 0 utilizando simulaciones.\nüß© Integraci√≥n por muestreo\nCalcule las siguientes integrales utilizando muestras.\n\n\\(\\displaystyle \\int_{-\\infty}^{\\infty}{\\frac{x^2}{\\sqrt{2\\pi}}\\exp \\left(-\\frac{x^2}{2} \\right)dx}\\)\n\\(\\displaystyle \\int_{1}^{\\infty}{\\frac{x^3}{\\sqrt{2\\pi}}\\exp \\left(-\\frac{x^2}{2} \\right)dx}\\)\n\\(\\displaystyle \\int_{1}^{\\infty}{\\frac{x^6}{\\sqrt{2\\pi}}\\exp \\left(-\\frac{x^2 - 4x}{2} \\right)dx}\\)\n\\(\\displaystyle \\int_{1}^{10}{x^6\\frac{e^{-x^4/2}}{\\sqrt{2\\pi}}dx}\\) \n\n¬øCu√°l es la distribuci√≥n de muestreo aproximada al usar muestreo independiente para evaluar integrales?",
    "crumbs": [
      "Pr√°ctica",
      "Unidad 3"
    ]
  },
  {
    "objectID": "practica/practica_03.html#aproximaci√≥n-de-una-distribuci√≥n-mediante-una-grilla",
    "href": "practica/practica_03.html#aproximaci√≥n-de-una-distribuci√≥n-mediante-una-grilla",
    "title": "Pr√°ctica - Unidad 3",
    "section": "üíª Aproximaci√≥n de una distribuci√≥n mediante una grilla",
    "text": "üíª Aproximaci√≥n de una distribuci√≥n mediante una grilla\nEn esta parte de la pr√°ctica se comienza a utilizar t√©cnicas computacionales que se asocian directamente a la pr√°ctica de la estad√≠stica bayesiana. Los problemas tienen como objetivo la familiarizaci√≥n con el uso pr√°ctico de estas t√©cnicas y la comprensi√≥n de sus caracter√≠sticas principales.\n\nüìå Aproximaci√≥n de grilla\nSe tiene un experimento binomial donde \\(n=80\\) y se observan \\(y=7\\) √©xitos. Considere que el prior de la probabilidad de √©xito \\(\\theta\\) es \\(\\text{Beta}(2, 10)\\).\n\nObtenga la distribuci√≥n a posteriori de \\(\\theta\\) de manera anal√≠tica y graf√≠quela.\nObtenga la distribuci√≥n a posteriori de \\(\\theta\\) utilizando el m√©todo de la grilla en base a una grilla de 10 puntos y dibuje la curva obtenida en el gr√°fico creado anteriormente.\nRepita el proceso del punto anterior utilizando una grilla de 100 puntos.\nConcluya sobre la fidelidad de las aproximaciones. ¬øConsidera que es necesario utilizar una grilla m√°s densa? ¬øCu√°les ser√≠an las ventajas y desventajas?\n\nüìå C√°lculo de probabilidades en base a la grilla (I)\nEn base al posterior obtenido en el ejercicio anterior mediante el m√©todo de la grilla calcule las siguientes probabilidades\n\n\\(P(\\theta &lt; 0.7)\\).\n\\(P(\\theta &gt; 0.05)\\).\n\\(P(0.05 &lt; \\theta &lt; 0.15)\\).\n\nDe ser necesario, obtenga el posterior mediante una grilla de mayor densidad.\nüìå C√°lculo de probabilidades en base a la grilla (II)\nUtilice los valores de la grilla y sus probabilidades a posteriori para obtener muestras del posterior y calcular las mismas probabilidades que en el ejercicio anterior en base a muestras. Ayuda: Para obtener muestras utilice la funci√≥n sample().\nüß©üìå Aproximaci√≥n de grilla en 2 dimensiones\nSean dos variables aleatorias continuas \\(X\\) e \\(Y\\) tales que \\((X, Y) \\in \\mathbb{R}^2\\), y sea el siguiente modelo de regresi√≥n lineal simple:\n\\[\n\\begin{aligned}\n\\alpha &\\sim \\text{Normal}(0, 1.5) \\\\\n\\beta  &\\sim \\text{Normal}(0, 2) \\\\\nY      &\\sim \\text{Normal}(\\alpha + \\beta X, 0.8)\n\\end{aligned}\n\\]\n\nObtenga el posterior conjunto del vector de par√°metros \\([\\alpha, \\beta]^T\\) mediante el m√©todo de la grilla. Elabore un gr√°fico que permita visualizar esta distribuci√≥n.\nObtenga el posterior marginal de \\(\\alpha\\) y graf√≠quelo.\nObtenga el posterior marginal de \\(\\beta\\) y graf√≠quelo.\nCalcule la probabilidad de que el intercepto sea mayor a 0.95.\nCalcule la probabilidad de que la pendiente sea menor a -2.\n\nPara responder las consignas utilice los datos simulados que se obtienen con el siguiente bloque de c√≥digo:\n\nset.seed(121195)\nalpha &lt;- 1\nbeta &lt;- -2\nsigma &lt;- 0.8\nn &lt;- 80\nx &lt;- rnorm(n)\ny &lt;- rnorm(n, alpha + beta * x, sigma)\ndf &lt;- data.frame(x = x, y = y)\n\nBonus: ¬øC√≥mo podr√≠a responder las consignas (ii)-(v) utilizando muestras del posterior?\nüìå Escalando la aproximaci√≥n de la grilla\nSuponga que se tiene que estimar un posterior utilizando la aproximaci√≥n mediante una grilla de 200 puntos en cada dimensi√≥n. Calcule c√∫antas veces se tiene que evaluar el posterior en cada uno de los siguientes escenarios:\n\n1 dimensi√≥n.\n2 dimensiones.\n3 dimensiones.\n5 dimensiones.\n10 dimensiones.\n\nConcluya sobre las ventajas y desventajas de la aproximaci√≥n de la grilla teniendo en cuenta sus caracter√≠sticas conforme se incrementa el n√∫mero de dimensiones del posterior.\nBenchmark de la aproximaci√≥n de la grilla\nEl siguiente bloque de c√≥digo define una funci√≥n llamada create_and_eval_grid() que eval√∫a la funci√≥n de densidad normal en una cantidad arbitraria dimensiones. El argumento dimension_n indica la dimensionalidad de la distribuci√≥n normal, y grid_n indica la cantidad de puntos en la grilla de cada dimensi√≥n. Debajo, se utiliza la funci√≥n mark() del paquete {bench} para comparar el desempe√±o de la funci√≥n create_and_eval_grid() con diferentes n√∫meros de dimensiones.\n\ncreate_and_eval_grid &lt;- function(dimension_n, grid_n = 100) {\n    grid &lt;- seq(-3, 3, length.out = grid_n)\n    grids &lt;- replicate(dimension_n, grid, simplify = FALSE)\n    df &lt;- expand.grid(grids, KEEP.OUT.ATTRS = FALSE)\n    Mu &lt;- rep(0, dimension_n)\n    Sigma &lt;- diag(dimension_n)\n    mvtnorm::dmvnorm(df, mean = Mu, sigma = Sigma)\n}\nbench::mark(\n    create_and_eval_grid(1),\n    create_and_eval_grid(2),\n    check = FALSE,\n    max_iterations = 500\n)\n\nModifique el c√≥digo brindado para evaluar la funci√≥n de densidad en hasta un m√°ximo de 10 dimensiones. Concluya sobre el tiempo de ejecuci√≥n, el consumo de memoria, y otras cantidades que se encuentren en la salida y crea adecuadas para el an√°lisis.",
    "crumbs": [
      "Pr√°ctica",
      "Unidad 3"
    ]
  },
  {
    "objectID": "practica/practica_03.html#sec-mh",
    "href": "practica/practica_03.html#sec-mh",
    "title": "Pr√°ctica - Unidad 3",
    "section": "üíª Metropolis-Hastings",
    "text": "üíª Metropolis-Hastings\n\n\nEn esta secci√≥n, se profundiza en la pr√°ctica de uno de los algoritmos fundamentales de la inferencia estad√≠stica bayesiana: el algoritmo de Metropolis-Hastings.\n\nüìå Muestreo utilizando el algoritmo de Metropolis-Hastings (I)\nUse el algoritmo de Metropolis-Hastings y una distribuci√≥n de propuesta \\(\\text{Normal}(0, 0.1)\\) para obtener 5000 muestras de las siguientes distribuciones de probabilidad:\n\n\\(\\text{Normal}(\\mu = 3, \\sigma = 6)\\).\n\\(\\text{StudentT}(\\nu = 5)\\).\n\\(\\frac{2}{3}\\text{Normal}(\\mu = 0, \\sigma = 0.5) + \\frac{1}{3}\\text{Normal}(\\mu = 3, \\sigma = 2)\\).\n\nGrafique las distribuciones obtenidas utilizando un histograma o una estimaci√≥n de densidad y superponga la funci√≥n de densidad verdadera para realizar una comparaci√≥n. Concluya sobre la similitud de las mismas y la aptitud de la distribuci√≥n de propuesta utilizada.\nüìå Simplificando el algoritmo de Metropolis-Hastings\nLa distribuci√≥n de propuesta utilizada en el ejercicio anterior goza de una propiedad que permite simplificar el algoritmo de Metropolis-Hastings.\n\n¬øCu√°l es esta propiedad?\n¬øQu√© simplificaci√≥n se puede hacer?\n¬øQu√© nombre recibe la versi√≥n simplificada del algoritmo?\nImplemente la versi√≥n simplificada del algoritmo de Metropolis-Hastings y obtenga nuevamente 5000 muestras para las distribuciones presentadas en el ejercicio anterior utilizando la nueva implementaci√≥n.\n\nüìå Muestreo utilizando el algoritmo de Metropolis-Hastings (II)\nUse el algoritmo de Metropolis-Hastings y una distribuci√≥n de propuesta que crea conveniente para obtener 5000 muestras de las siguientes distribuciones de probabilidad:\n\n\\(\\text{Beta}(\\alpha=4, \\beta=8)\\).\n\\(\\text{Gamma}(k = 3, \\theta = 2)\\).\n\\(\\frac{1}{2}\\text{Beta}(\\alpha=10, \\beta=3) + \\frac{1}{2}\\text{Beta}(\\alpha=3, \\beta=10)\\).\n\nRealice un an√°lisis similar al realizado en el Ejercicio 1.\n¬°A jugar con la distribuci√≥n de propuesta!\nSuponga que se desea obtener muestras de una distribuci√≥n \\(\\text{Normal}(4, 1)\\) utilizando Metropolis-Hastings y la siguiente distribuci√≥n de propuesta:\n\\[\n\\mu' \\mid \\mu \\sim \\text{Uniforme}(\\mu - w, \\mu + w)\n\\]\nObtenga \\(n=5000\\) muestras con \\(w \\in \\{0.01, 1, 100\\}\\) y compute la probabilidad de aceptaci√≥n. Luego, para cada \\(w\\), grafique la distribuci√≥n obtenida y visualice la cadena de Markov utilizando un traceplot.\n\n¬øC√≥mo se relaciona \\(w\\) con el desempe√±o del muestreo?\n¬øC√≥mo se relaciona \\(w\\) con la probabilidad de aceptaci√≥n? Justifique su respuesta utilizando la ecuaci√≥n del criterio de aceptaci√≥n.\n\nüìå Modelo Normal-Normal\nSe desea estudiar el tiempo promedio que los estudiantes de estad√≠stica dedican por semana a la materia Estad√≠stica Bayesiana. Para eso se propone utilizar un modelo Normal-Normal, con \\(\\sigma=1.2\\) conocido.\n\nElija una distribuci√≥n a priori para el par√°metro \\(\\mu\\).\nDescriba el modelo matem√°ticamente.\nDetermine una distribuci√≥n de propuesta adecuada para este problema. Explique.\nObtenga 2000 muestras del posterior de \\(\\mu\\). Ajuste los par√°metros de la distribuci√≥n de propuesta hasta que los resultados se vean adecuados.\nGrafique un histograma de las muestras obtenidas y concluya sobre el desempe√±o de la aproximaci√≥n.\n\nPara resolver este problema utilice los datos que se leen con el siguiente codigo:\n\nurl &lt;- paste0(\n    \"https://raw.githubusercontent.com/estadisticaunr/\",\n    \"estadistica-bayesiana/main/datos/tiempo-estudio-eb.csv\"\n)\ndf_estudio &lt;- readr::read_csv(url)\n\nüìå Modelo Beta-Binomial\nSe desea estimar el posterior del par√°metro \\(\\pi\\) en el siguiente modelo Beta-Binomial:\n\\[\n\\begin{aligned}\nY &\\sim \\text{Binomial}(n, \\pi) \\\\\n\\pi &\\sim \\text{Beta}(2, 3)\n\\end{aligned}\n\\]\nSe observan \\(n=10\\) ensayos de Bernoulli y se registran \\(y=3\\) √©xitos. Determine una distribuci√≥n de propuesta adecuada y obtenga muestras de la distribuci√≥n a posteriori del par√°metro \\(\\pi\\) utilizando el algoritmo de Metropolis-Hastings.\nDe ser necesario, ajuste los par√°metros de la distribuci√≥n de propuesta.\nüìå Modelo Poisson\nEn el ejercicio ¬°Ostras! ¬°Estoy haciendo inferencia bayesiana! de la Pr√°ctica 1 se report√≥ que la cantidad de especies marinas bivalvas descubiertas cada a√±o entre 2010 y 2015 fue 64, 13, 33, 18, 30 y 20.\nSea \\(Y_t\\) la cantidad de especies descubiertas en el a√±o \\(2009 + t\\) (e.g.¬†\\(Y_1 = 64\\) es el conteo para el a√±o 2010) y el siguiente modelo:\n\\[\n\\begin{aligned}\nY_t        &\\sim \\text{Poisson}(\\lambda_t) \\\\\n\\lambda_t  &= \\exp (\\alpha + \\beta t) \\\\\n\\alpha     &\\sim \\text{Normal}(0, 10^2) \\\\\n\\beta      &\\sim \\text{Normal}(0, 10^2)\n\\end{aligned}\n\\]\nAjuste el modelo utilizando Metropolis-Hastings y verifique la convergencia de las cadenas de Markov. \nüìå Metropolis-Hastings multivariado\nUse el algoritmo de Metropolis Hastings para obtener 1000 muestras de la distribuci√≥n \\(\\text{MVN}(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma})\\) donde\n\\[\n\\begin{array}{cc}\n    \\boldsymbol{\\mu} = \\begin{bmatrix}1.2 \\\\ 0.8 \\end{bmatrix}, &\n    \\boldsymbol{\\Sigma} = \\begin{bmatrix}3 & 0.2 \\\\ 0.2 & 2 \\end{bmatrix}\n\\end{array}\n\\]\nUtilice una distribuci√≥n de propuesta \\(\\text{MVN}(\\boldsymbol{0}_2, \\boldsymbol{I}_2\\sigma)\\) con \\(\\sigma = 0.2\\).\nüìå Metropolis-Hastings para regresi√≥n\nRepita el ejercicio Aproximaci√≥n de grilla en 2 dimensiones pero utilice Metropolis-Hastings para obtener muestras del posterior en vez del m√©todo de la grilla.",
    "crumbs": [
      "Pr√°ctica",
      "Unidad 3"
    ]
  },
  {
    "objectID": "practica/practica_03.html#diagn√≥sticos",
    "href": "practica/practica_03.html#diagn√≥sticos",
    "title": "Pr√°ctica - Unidad 3",
    "section": "Diagn√≥sticos",
    "text": "Diagn√≥sticos\nEl uso de algoritmos de MCMC provee de un gran poder que conlleva una gran responsabilidad. En los ejercicios de esta secci√≥n ya se cuenta con un posterior y se busca evaluar la fiabilidad de las muestras utilizando diferentes medidas de diagn√≥stico an√°liticas y gr√°ficas.\n\nüìå Describir medidas de diagn√≥stico (I)\nEn sus propias palabras, explique que son \\(\\text{ESS}\\), \\(\\hat{R}\\) y \\(\\text{MCSE}\\). Considere:\n\n¬øQu√© miden?\n¬øQu√© potencial problema de MCMC detectan? \n\nüìå Describir medidas de diagn√≥stico (II)\nEn sus propias palabras, explique por qu√© las t√©cnicas de estimaci√≥n del posterior basadas en MCMC necesitan diagn√≥sticos de convergencia. En particular, contraste estos con los m√©todos conjugados descritos en la Unidad 2 que no necesitan esos diagn√≥sticos. ¬øQu√© es diferente entre los dos m√©todos de inferencia? \nüìå Problemitas de MCMC\nPara cada escenario de simulaci√≥n mediante MCMC descrito a continuaci√≥n, explique c√≥mo el escenario podr√≠a afectar la aproximaci√≥n del posterior.\n\nLa cadena se mezcla muy lentamente.\nLa cadena presenta alta auto-correlaci√≥n.\nLa cadena tiende a quedarse ‚Äútrabada‚Äù. \n\nüìå Vamos de paseo\nElabore traceplots que le permitan visualizar la traza de la cadena de Markov utilizada en el ejercicio Modelo Normal-Normal de la secci√≥n Metropolis-Hastings.\nLuego, repita el ejercicio utilizando 4 cadenas independientes y grafique sus trazas en un mismo gr√°fico. ¬øQu√© puede concluir sobre la convergencia y la mezcla de las cadenas?\nüìå Primeros pasitos con \\(\\hat{R}\\)\nRepita lo realizado en el ejercicio Modelo Beta-Binomial de la secci√≥n Metropolis-Hastings utilizando 4 cadenas independientes. Luego:\n\nCalcule la varianza intra-cadenas \\(W\\).\nCalcule la varianza entre-cadenas \\(B\\).\nCalcule \\(\\hat{R}\\) y concluya sobre el resultado obtenido.\n\n\n\\(\\hat{R}\\) para un muestreo independiente\nEn un determinado problema se encuentra que el posterior del par√°metro \\(\\pi\\) de un modelo con verosimilitud binomial est√° dado por\n\\[\n\\pi \\mid \\boldsymbol{y} \\sim \\text{Beta}(12, 6)\n\\]\n\nObtenga 4 conjuntos independientes de 1000 muestras independientes de esta distribuci√≥n.\nCalcule \\(W\\), \\(B\\), y \\(\\hat{R}\\) considerando que cada conjunto representa una cadena.\nExplique el resultado de \\(\\hat{R}\\).\n\nBonus\n\n¬øSe puede concluir que cada uno de los muestreos realizados representan realizaciones de una cadena de Markov? ¬øPor qu√©?\n¬øPor qu√© no fue necesario descartar un conjunto de muestras de warm-up?\n\nüìå Funci√≥n de autocorrelaci√≥n\nUtilice las 4 cadenas obtenidas en el ejercicio Primeros pasitos con \\(\\hat{R}\\) y grafique la funci√≥n de autocorrelaci√≥n y calcule el coeficiente de autocorrelaci√≥n utilizando un rezago unitario. Concluya sobre la dependencia entre las muestras obtenidas.\nüìå Tama√±o de muestra efectivo\nUtilice las muestras obtenidas en el ejercicio anterior para calcular el tama√±o de muestra efectivo.\nExperimentos con el tama√±o de muestra efectivo\nSuponga una distribuci√≥n \\(\\text{Normal}(0, 1^2)\\). Obtenga \\(n\\) muestras independientes utilizando rnorm() y \\(n\\) muestras dependientes utilizando Metropolis-Hastings con \\(n \\in \\{10, 50, 100, 500, 1000, 10000\\}\\).\n\nCalcule el tema√±o de muestra efectivo en todos los escenarios simulados.\nDescriba el comportamiento del tama√±o de muestra efectivo conforme se incrementa el n√∫mero de muestras.\n¬øPor qu√© se observan los comportamientos descritos?\n\nüìå ¬°Rompan todo! Un caso simulado\nLos siguientes gr√°ficos muestran las trazas y las distribuciones que resultan al obtener muestras de un posterior utilizando dos cadenas de Markov independientes. Estos muestreos fueron realizados de manera que presenten algunos problemas. Describa cuales son los problemas que puede observar en los siguientes gr√°ficos y explique por qu√© no utilizar√≠a estas muestras para obtener conclusiones sobre el posterior.\n\n\n\n\n\nGr√°fico 1\n\n\n\n\n\n\n\n\n\nGr√°fico 2\n\n\n\n\n\n\n\n\n\nGr√°fico 3\n\n\n\n\n\n\n\n\n\nGr√°fico 4\n\n\n\n\nüìå ¬°Rompan todo! Un caso real\nAl igual que en el ejercicio anterior, se presentan traceplots donde el muestreo del posterior presenta problemas. En este caso, los gr√°ficos que se observan fueron obtenidos en el marco de un problema real, y se usan 4 cadenas en vez de 2. Nuevamente, describa cuales son los problemas que puede observar en los siguientes gr√°ficos y explique por qu√© no utilizar√≠a estas muestras para obtener conclusiones sobre el posterior.\n\n\n\n\n\nGr√°fico 1\n\n\n\n\n\n\n\n\n\nGr√°fico 2\n\n\n\n\nAdem√°s, responda:\n\n¬øCu√°l gr√°fico se asocia a un mayor tama√±o de muestra efectivo?\n¬øQu√© grafico muestra peor mezcla entre cadenas?\n¬øEn qu√© gr√°fico se puede observar que las cadenas convergen a la misma distribuci√≥n?",
    "crumbs": [
      "Pr√°ctica",
      "Unidad 3"
    ]
  },
  {
    "objectID": "practica/practica_03.html#programaci√≥n-probabil√≠stica---stan-y-rstan",
    "href": "practica/practica_03.html#programaci√≥n-probabil√≠stica---stan-y-rstan",
    "title": "Pr√°ctica - Unidad 3",
    "section": "Programaci√≥n probabil√≠stica - Stan y RStan",
    "text": "Programaci√≥n probabil√≠stica - Stan y RStan\nEn esta secci√≥n se comienza a utilizar Stan para realizar los c√°lculos relacionados a la inferencia bayesiana. Stan es uno de los lenguajes de programaci√≥n probabil√≠stica mas potentes y populares en la actualidad. Los ejercicios contienen modelos estad√≠sticos que deben ser resueltos con la interface de Stan a R, {RStan}.\n\nüìå MCMC con RStan: Precalentamiento (I)\nUtilice la informaci√≥n proporcionada para definir la estructura del modelo bayesiano utilizando {RStan}. No es necesario ejecutar nada, solo necesita proporcionar el c√≥digo correcto.\n\n\\(Y \\mid \\pi \\sim \\text{Binomial}(\\pi, 20)\\) con \\(\\pi \\sim \\text{Beta}(1, 1)\\).\n\\(Y \\mid \\lambda \\sim \\text{Poisson}(\\lambda)\\) con \\(\\lambda \\sim \\text{Gamma}(4, 2)\\).\n\\(Y \\mid \\mu \\sim \\text{Normal}(\\mu, 1^2)\\) con \\(\\mu \\sim \\text{Normal}(0, 10^2)\\). \n\nüìå MCMC con RStan: Precalentamiento (II)\nUtilice la informaci√≥n proporcionada para (1) definir la estructura del modelo bayesiano y (2) obtener muestras del posterior utilizando {RStan}. No es necesario ejecutar nada, solo necesita proporcionar el c√≥digo correcto.\n\n\\(Y \\mid \\pi \\sim \\text{Binomial}(\\pi, 20)\\) con \\(\\pi \\sim \\text{Beta}(1, 1)\\) e \\(y = 12\\).\n\\(Y \\mid \\lambda \\sim \\text{Poisson}(\\lambda)\\) con \\(\\lambda \\sim \\text{Gamma}(4, 2)\\) e \\(y = 3\\).\n\\(Y \\mid \\mu \\sim \\text{Normal}(\\mu, 1^2)\\) con \\(\\mu \\sim \\text{Normal}(0, 10^2)\\) e \\(y = 12.2\\). \n\nüìå Modelo Beta-Binomial con RStan (I)\nConsidere el modelo Beta-Binomial para \\(\\pi\\) con \\(Y \\mid \\pi \\sim \\text{Binomial}(\\pi, n)\\) y \\(\\pi \\sim \\text{Beta}(3, 8)\\). Suponga que en \\(n = 10\\) ensayos independientes observa \\(y = 2\\) √©xitos.\n\nObtenga muestras del posterior de \\(\\pi\\) con {RStan} utilizando 3 cadenas y 12000 iteraciones por cadena.\nGrafique la traza de cada una de las tres cadenas.\n¬øCu√°l es el rango de valores en el eje x del traceplot? ¬øPor qu√© el valor m√°ximo de este rango no es 12000?\nCree un gr√°fico que permita visualizar la funci√≥n de densidad de los valores obtenidos con cada una de las tres cadenas.\nUtilizando lo estudiado en la Unidad 2, especifique el posterior de \\(\\pi\\). ¬øC√≥mo se compara con la aproximaci√≥n mediante MCMC?\n\nüìå Modelo Beta-Binomial con RStan (II)\nRepita el ejercicio anterior utilizando \\(\\pi \\sim \\text{Beta}(4, 3)\\), donde observa \\(y = 4\\) √©xitos en \\(n = 12\\) ensayos independientes.\nüìå Modelo Gamma-Poisson con RStan (I)\nConsidere el modelo Gamma-Poisson para \\(\\lambda\\) con \\(Y_i \\mid \\lambda \\sim \\text{Poisson}(\\lambda)\\) y \\(\\lambda \\sim \\text{Gamma}(20, 5)\\). Suponga que cuenta con \\(n = 3\\) observaciones independientes \\((y_1, y_2, y_3) = (0, 1, 0)\\)\n\nObtenga muestras del posterior de \\(\\lambda\\) con {RStan} utilizando 4 cadenas y 10000 iteraciones por cadena.\nGrafique la traza y la funci√≥n de densidad de cada una de las tres cadenas.\nA partir del gr√°fico de la funci√≥n de densidad, ¬øcu√°les suelen ser, a posteriori, los valores m√°s probables de \\(\\lambda\\)?\nUtilizando lo estudiado en la Unidad 2, especifique el posterior de \\(\\lambda\\). ¬øC√≥mo se compara con la aproximaci√≥n mediante MCMC?\n\nüìå Modelo Gamma-Poisson con RStan (II)\nRepita el ejercicio anterior utilizando el prior \\(\\lambda \\sim \\text{Gamma}(5, 5)\\).\nüìå Modelo Normal-Normal con RStan (I)\nRepita los mismos pasos del ejercicio Modelo Gamma-Poisson con Rstan (I) pero considere el modelo Normal-Normal para \\(\\mu\\) con \\(Y_i \\mid \\mu \\sim \\text{Normal}(\\mu, 1.3^2)\\) y \\(\\mu \\sim \\text{Normal}(10, 1.2^2)\\). Suponga que cuenta con \\(n = 4\\) observaciones independientes \\((y_1, y_2, y_3, y_4) = (7.1, 8.9, 8.4, 8.6)\\)\nüìå Modelo Normal-Normal con RStan (II)\nRepita el ejercicio anterior con el modelo Normal-Normal pero ahora considere \\(Y_i \\mid \\mu \\sim \\text{Normal}(\\mu, 8^2)\\) y \\(\\mu \\sim \\text{Normal}(-15, 2^2)\\). Suponga que en \\(n = 5\\) observaciones independientes observa \\((y_1, y_2, y_3, y_4, y_5) = (‚àí10.1, 5.5, 0.1,‚àí1.4, 11.5)\\)\nUn modelo que es un poco ¬øcomplicado?\nConsidere el siguiente modelo: \\[\n\\begin{aligned}\n\\text{peso}_i &\\sim \\text{Normal}(\\mu_i, \\sigma^2) \\\\\n\\mu_i         &= \\theta_1 + \\theta_2 + \\text{edad}_i^{\\theta_3} \\\\\n\\theta_1      &\\sim \\text{Normal}(0, 100^2) \\\\\n\\theta_2      &\\sim \\text{Uniforme}(0, 20000) \\\\\n\\theta_3      &\\sim \\text{Normal}(0, 1) \\\\\n\\sigma^2      &\\sim \\text{InvGamma}(0.01, 0.01)\n\\end{aligned}\n\\]\ny los siguientes datos:\n\nedad &lt;- c(2, 15, 14, 16, 18, 22, 28)\npeso &lt;- c(29.9, 1761, 1807, 2984, 3230, 5040, 5654)\nn &lt;- length(edad)\ndata_list &lt;- data.frame(peso = peso, edad = edad, n = n)\n\n\nAjuste el modelo utilizando {RStan}.\nObtenga una visualizaci√≥n de la edad versus el peso junto con una curva que indique la media a posteriori de \\(\\mu_i\\) para evaluar si el modelo ajusta bien.\nEstudie la convergencia de las cadenas de Markov.\nMencione tres medidas que podr√≠a tomar para mejorar la convergencia.",
    "crumbs": [
      "Pr√°ctica",
      "Unidad 3"
    ]
  },
  {
    "objectID": "practica/practica_03.html#otros",
    "href": "practica/practica_03.html#otros",
    "title": "Pr√°ctica - Unidad 3",
    "section": "Otros",
    "text": "Otros\n\nüìå El peso de los pescados\nUna compa√±√≠a pesquera de Comodoro Rivadavia se encuentra probando un nuevo m√©todo para estimar el peso de los peces que extrae del Mar Argentino. El objetivo de este m√©todo es obtener una estimaci√≥n lo suficientemente buena del peso de cada pescado sin tener que pesarlos uno por uno, ya que es un proceso costoso en tiempo y labor. Para eso, seleccionaron una muestra de pescados, los pesaron y les midieron ciertos aspectos morfol√≥gicos (ancho, alto y largo). En el futuro, esperan recolectar estas mismas medidas morfol√≥gicas mediante una c√°mara especializada y utilizar un modelo para estimar el peso.\nEl modelo propuesto por el equipo de investigaci√≥n es el siguiente:\n\\[\n\\begin{aligned}\n\\log(\\text{Peso}_i) &\\sim \\text{Normal}(\\mu_i, \\sigma) \\\\\n\\mu_i               &= \\beta_0 + \\beta_1 \\log(\\text{Largo}_i) \\\\\n\\sigma              &\\sim \\text{Gamma}(k, \\theta)\n\\end{aligned}\n\\]\nEl peso se encuentra medido en gramos y la longitud en cent√≠metros. El equipo provee las muestras que obtuvieron del posterior. Las mismas se pueden leer en R utilizando el siguiente bloque de c√≥digo.\n\nurl &lt;- paste0(\n    \"https://raw.githubusercontent.com/estadisticaunr/\",\n    \"estadistica-bayesiana/main/datos/fish-market-posterior.csv\"\n)\ndf_posterior &lt;- readr::read_csv(url)\nhead(df_posterior)\n\n# A tibble: 6 √ó 3\n  intercepto pendiente sigma\n       &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;\n1      -4.44      3.08 0.408\n2      -4.30      3.05 0.431\n3      -4.49      3.12 0.433\n4      -4.04      2.96 0.341\n5      -4.76      3.18 0.413\n6      -4.65      3.15 0.350\n\nAnalice de manera gr√°fica y anal√≠tica los posteriors marginales de los par√°metros del modelo. Realice las transformaciones de par√°metros que crea conveniente para facilitar la comprensi√≥n del an√°lisis.\nConsidere un pescado cuya longitud es de 30 cent√≠metros.\n\nObtenga y grafique la distribuci√≥n a posteriori del peso medio.\nObtenga y grafique la distribuci√≥n predictiva a posteriori del peso.\nInterprete los resultados.\n\nGrafique la curva de regresi√≥n junto a una banda de credibilidad del 95% en el plano de las variables originales y en el plano de las variables transformadas.\nAgregue a los gr√°ficos anteriores una banda de credibilidad del 95% para la distribuci√≥n predictiva a posteriori. Interprete los resultados.",
    "crumbs": [
      "Pr√°ctica",
      "Unidad 3"
    ]
  },
  {
    "objectID": "teoria/u2_teoria_03.html",
    "href": "teoria/u2_teoria_03.html",
    "title": "Unidad 2 - Distribuciones Conjugadas",
    "section": "",
    "text": "Instrucciones para guardar como PDF\n\nHacer clic ac√°\nAbrir la ventana de impresi√≥n del navegador con CTRL+P\nSi es necesario, cambiar el ‚ÄúDestino‚Äù a ‚ÄúGuardar como PDF‚Äù\nCliquear en ‚ÄúGuardar‚Äù\nElegir el nombre del archivo de destino\n¬°Listo! üéâ",
    "crumbs": [
      "Teor√≠a",
      "U2 - Distribuciones Conjugadas"
    ]
  },
  {
    "objectID": "teoria/u5_teoria_07.html",
    "href": "teoria/u5_teoria_07.html",
    "title": "Unidad 5 - Modelos Avanzados",
    "section": "",
    "text": "Instrucciones para guardar como PDF\n\nHacer clic ac√°\nAbrir la ventana de impresi√≥n del navegador con CTRL+P\nSi es necesario, cambiar el ‚ÄúDestino‚Äù a ‚ÄúGuardar como PDF‚Äù\nCliquear en ‚ÄúGuardar‚Äù\nElegir el nombre del archivo de destino\n¬°Listo! üéâ",
    "crumbs": [
      "Teor√≠a",
      "U5 - Modelos Avanzados"
    ]
  },
  {
    "objectID": "teoria/u1_teoria_01.html",
    "href": "teoria/u1_teoria_01.html",
    "title": "Unidad 1 - Introducci√≥n",
    "section": "",
    "text": "Instrucciones para guardar como PDF\n\nHacer clic ac√°\nAbrir la ventana de impresi√≥n del navegador con CTRL+P\nSi es necesario, cambiar el ‚ÄúDestino‚Äù a ‚ÄúGuardar como PDF‚Äù\nCliquear en ‚ÄúGuardar‚Äù\nElegir el nombre del archivo de destino\n¬°Listo! üéâ",
    "crumbs": [
      "Teor√≠a",
      "U1 - Introducci√≥n"
    ]
  },
  {
    "objectID": "teoria/u4_teoria_06.html",
    "href": "teoria/u4_teoria_06.html",
    "title": "Unidad 4 - Modelos Lineales",
    "section": "",
    "text": "Instrucciones para guardar como PDF\n\nHacer clic ac√°\nAbrir la ventana de impresi√≥n del navegador con CTRL+P\nSi es necesario, cambiar el ‚ÄúDestino‚Äù a ‚ÄúGuardar como PDF‚Äù\nCliquear en ‚ÄúGuardar‚Äù\nElegir el nombre del archivo de destino\n¬°Listo! üéâ",
    "crumbs": [
      "Teor√≠a",
      "U4 - Modelos Lineales"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Estad√≠stica Bayesiana",
    "section": "",
    "text": "Licenciatura en Estad√≠stica\n ¬† Facultad de Ciencias Econ√≥micas y Estad√≠stica (UNR)\n ¬† 1¬∞ Cuatrimestre 2025\n ¬† Mi√© ‚Äì 7:00 a 11:00 ¬† | ¬† Vie ‚Äì 7:00 a 9:00"
  },
  {
    "objectID": "index.html#profesores",
    "href": "index.html#profesores",
    "title": "Estad√≠stica Bayesiana",
    "section": "Profesores",
    "text": "Profesores\n\n\nNacho Evangelista\n\n ¬† ignacio.evangelista@fcecon.unr.edu.ar\n ¬† Consultas: Lunes 15:00\n\n\n\nTom√°s Capretto\n\n ¬† tomas.capretto@fcecon.unr.edu.ar\n ¬† Consultas: Martes 16:00"
  },
  {
    "objectID": "teoria/u3_teoria_05.html",
    "href": "teoria/u3_teoria_05.html",
    "title": "Unidad 3 - M√©todos Computacionales",
    "section": "",
    "text": "Instrucciones para guardar como PDF\n\nHacer clic ac√°\nAbrir la ventana de impresi√≥n del navegador con CTRL+P\nSi es necesario, cambiar el ‚ÄúDestino‚Äù a ‚ÄúGuardar como PDF‚Äù\nCliquear en ‚ÄúGuardar‚Äù\nElegir el nombre del archivo de destino\n¬°Listo! üéâ",
    "crumbs": [
      "Teor√≠a",
      "U3 - M√©todos Computacionales"
    ]
  },
  {
    "objectID": "teoria/u1_teoria_02.html",
    "href": "teoria/u1_teoria_02.html",
    "title": "Unidad 1 - Inferencia Bayesiana",
    "section": "",
    "text": "Instrucciones para guardar como PDF\n\nHacer clic ac√°\nAbrir la ventana de impresi√≥n del navegador con CTRL+P\nSi es necesario, cambiar el ‚ÄúDestino‚Äù a ‚ÄúGuardar como PDF‚Äù\nCliquear en ‚ÄúGuardar‚Äù\nElegir el nombre del archivo de destino\n¬°Listo! üéâ",
    "crumbs": [
      "Teor√≠a",
      "U1 - Inferencia Bayesiana"
    ]
  },
  {
    "objectID": "teoria/u2_teoria_04.html",
    "href": "teoria/u2_teoria_04.html",
    "title": "Unidad 2 - Teor√≠a de la Decisi√≥n",
    "section": "",
    "text": "Instrucciones para guardar como PDF\n\nHacer clic ac√°\nAbrir la ventana de impresi√≥n del navegador con CTRL+P\nSi es necesario, cambiar el ‚ÄúDestino‚Äù a ‚ÄúGuardar como PDF‚Äù\nCliquear en ‚ÄúGuardar‚Äù\nElegir el nombre del archivo de destino\n¬°Listo! üéâ",
    "crumbs": [
      "Teor√≠a",
      "U2 - Teor√≠a de la Decisi√≥n"
    ]
  },
  {
    "objectID": "teoria/u5_teoria_08.html",
    "href": "teoria/u5_teoria_08.html",
    "title": "Unidad 5 - Modelos Jer√°rquicos",
    "section": "",
    "text": "Instrucciones para guardar como PDF\n\nHacer clic ac√°\nAbrir la ventana de impresi√≥n del navegador con CTRL+P\nSi es necesario, cambiar el ‚ÄúDestino‚Äù a ‚ÄúGuardar como PDF‚Äù\nCliquear en ‚ÄúGuardar‚Äù\nElegir el nombre del archivo de destino\n¬°Listo! üéâ",
    "crumbs": [
      "Teor√≠a",
      "U5 - Modelos Jer√°rquicos"
    ]
  },
  {
    "objectID": "practica/practica_01.html",
    "href": "practica/practica_01.html",
    "title": "Pr√°ctica - Unidad 1",
    "section": "",
    "text": "El prop√≥sito de esta secci√≥n de la pr√°ctica es resolver situaciones que impliquen la aplicaci√≥n de la Regla de Bayes como se presenta tradicionalmente en un curso de Probabilidad.\n\nDemostraci√≥n\nDemuestra la validez de la siguiente expresi√≥n de la Regla de Bayes\n\\[\nP(B_j \\mid A) = \\frac{P(A \\mid B_j) P(B_j)}{\\sum_{k=1}^{K}P(A \\mid B_k) P(B_k)}\n\\]\ndonde \\(A\\) es un evento cualquiera y \\(\\{B_1, \\cdots, B_K\\}\\) forman una partici√≥n. \n\n\n\n\n\n\nüìå El test infalible\nEn una poblaci√≥n dada, una de cada mil personas tiene una enfermedad. Se toma una persona al azar de la poblaci√≥n, se le aplica un test para detectar dicha enfermedad, y el resultado es positivo. El test se caracteriza por dar positivo el 99% de las veces que una persona tiene la enfermedad. Adem√°s, dicho test tiene una tasa de falsos positivos del 5%.\n\n¬øCu√°l es la probabilidad de que la persona tenga efectivamente la enfermedad?\nSi realizamos el mismo an√°lisis una segunda vez sobre el mismo paciente y obtenemos nuevamente positivo,\n\n¬øCu√°l seria la probabilidad que el paciente est√© enfermo?\n¬øY si diera negativo?\n¬øEs el prior el mismo cuando se analiza el resultado del segundo an√°lisis que cuando solo se analiza el primero? \n\n\n¬øEs verdad que existen los vampiros? Versi√≥n Crep√∫sculo\nEdward quiere probarle a Bella que los vampiros existen. Seg√∫n Bella, hay una probabilidad del 5% de que los vampiros existan. Tambi√©n cree que la probabilidad de que exista alguien con la piel brillante dado que los vampiros existen es del 70%, y que la probabilidad de que alguien tenga la piel brillante si los vampiros no existen es del 3%. Edward lleva a Bella al bosque y le muestra que de hecho su piel brilla como un üíé ¬øCu√°l es la probabilidad que existan los vampiros? \n\n\n\n\n\nRobert Pattinson como Edward en Crep√∫sculo\n\n\n\n\nüìå √Årboles enfermos\nUn vivero de la ciudad se destaca por vender una variedad de √°rboles nativos, incluyendo al jacarand√°, ceibo, omb√∫, entre otros. Lamentablemente, el 18% de los √°rboles del vivero estan infectados con moho. Los √°rboles enfermos se componen en un 15% por jacarand√°s, 80% de ceibos, y 5% de otras especies. Los √°rboles sanos se componen por un 20% de jacarand√°s, 10% de ceibos, y 70% de otras especies. Con el objetivo de monitorear cuanto se propag√≥ la enfermedad, una de las personas que trabaja en el vivero selecciona al azar uno de los √°rboles para testear.\n\n¬øCu√°l es la probabilidad a priori de que el √°rbol tenga moho?\nResulta que el √°rbol seleccionado es un ceibo. ¬øCu√°l es la probabilidad de haber seleccionado un ceibo?\n¬øCu√°l es la probabilidad a posteriori de que el ceibo seleccionado tenga moho?\nCompare las probabilidades a priori y a posteriori de que el √°rbol tenga moho. ¬øC√≥mo afecta el an√°lisis el saber que el √°rbol es un ceibo? \n\n\n\n\n\n\nFlor del Ceibo, la flor nacional\n\n\n\n\nüìå Transporte ‚ÄúEl Impuntual‚Äù\nUna cierta empresa de transporte regional, que decidimos llamar ‚ÄúEl Impuntual‚Äù, tiene servicios que van desde Rosario hasta Wheelwright varias veces al d√≠a, todos los d√≠as de la semana. Un 30% de los viajes salen a la ma√±ana, otro 30% salen a la tarde, y el restante 40% salen a la noche. Los pasajeros suelen estar muy frustrados ya que un 25% de los viajes salen tarde. De estos viajes demorados, el 40% corresponden a la ma√±ana, un 50% suceden a la tarde, y el 10% restante ocurre a la noche1.\nLucio y Franco son dos amigos del pueblo, y se volvieron a sus casas en colectivos diferentes.\n\nLucio se fue en uno de los colectivos de la ma√±ana. ¬øCu√°l es la probabilidad que su viaje est√© demorado?\nEl colectivo de Franco no est√° demorado. ¬øCu√°l es la probabilidad de que est√© viajando en uno de los colectivos de la ma√±ana? \n\n\n\n\n\n\nFoto de Markus Winkler en Unsplash\n\n\n\n\nüìå Beb√© panda\nSupongamos que hay dos especies de osos panda. Ambas especies son igual de frecuentes y viven en la misma regi√≥n. Es m√°s, lucen de la misma forma y comen la misma comida. A√∫n no existe una prueba gen√©tica que pueda diferenciarlos. Lo √∫nico que los diferencia es la cantidad de cr√≠as que suelen tener. Las madres de la especie A dan luz a mellizos el 10% del tiempo. Y las madres de la especie B dan a luz mellizos el 20% del tiempo. En todos los otros casos, estas madres dan a luz un solo beb√© panda.\nUsando un poco la imaginaci√≥n, supongamos que somos la persona encargada de un programa de reproducci√≥n de pandas. Tenemos una panda femenina que acaba de dar a luz a un par de mellizos, pero no sabemos a que especie pertenece.\n\n¬øCu√°l es la probabilidad que la mam√° panda sea de la especie A?\n¬øCu√°l es la probabilidad que vuelva a tener mellizos en la pr√≥xima parici√≥n?\nUn tiempo despu√©s sos encontramos con que en la segunda parici√≥n da a luz a un √∫nico beb√© panda. ¬øCu√°l es la probabilidad de que este panda sea de la especie A? \n\n\n\n\n\n\nFoto de Stone Wang en Unsplash\n\n\n\n\nüìå Paraguas\nEst√°s a punto de subir a un avi√≥n rumbo a Mendoza. Quer√©s saber si ten√©s que llevar un paraguas o no. Llam√°s a tres amigos que viven en Mendoza y les pregunt√°s si est√° lloviendo. Cada uno de ellos tiene una probabilidad de \\(2/3\\) de decirte la verdad y \\(1/3\\) de mentirte para hacerte una broma. Los tres responden que s√≠ est√° lloviendo. ¬øCu√°l es la probabilidad de que realmente est√© lloviendo en las Mendoza? Se puede asumir que en Mendoza llueve en 1 de cada 10 d√≠as. \n\n\n\n\n\n\nAyuda\n\n\n\n\n\nSi \\(LLL\\) es ‚Äúlos tres amigos dijeron que llov√≠a‚Äù, buscamos \\(P(\\text{lluvia} \\mid LLL) = P(LLL \\mid \\text{lluvia}) P(\\text{lluvia}) / P(LLL)\\).\n\\(P(LLL \\mid \\text{lluvia})\\) es la probabilidad de que ninguno de los tres mienta.\n\\(P(LLL) = P(LLL \\mid \\text{lluvia}) P(\\text{lluvia}) + P(LLL \\mid \\text{no lluvia}) P(\\text{no lluvia})\\).\n\n\n\nüìå Sherlock\nDos personas dejaron rastros de sangre en la escena del crimen. La sangre de Guido, un sospechoso, es analizada y resulta ser de tipo ‚Äò0‚Äô. Los rastros de sangre de la escena son de tipo ‚Äò0‚Äô (un tipo com√∫n en la poblaci√≥n, presente en el 60% de las personas) y de tipo ‚ÄòAB‚Äô (un tipo raro, con una frecuencia del 1% en la poblaci√≥n). ¬øEstos datos representan evidencia de que Guido estaba presente en la escena del crimen? \n\n\n\n\n\nSherlock Holmes, el detective privado\n\n\n\n\n\n\n\n\n\n\nAyuda\n\n\n\n\n\nLlamemos \\(S\\) a ‚ÄúGuido y otra persona estuvieron en la escena del crimen‚Äù versus \\(S'\\), ‚Äúdos personas desconocidas estuvieron en la escena del crimen‚Äù. Adem√°s, contamos con la evidencia \\(E\\), ‚Äúobservar sangre 0 y sangre AB‚Äù.\nQueremos saber qu√© es mayor, si \\(P(E \\mid S)\\) o \\(P(E \\mid S')\\) (si luego quisi√©ramos comparar \\(P(S \\mid E)\\) con \\(P(S' \\mid E)\\) necesitar√≠amos las probabilidades a priori de \\(P(S)\\) y \\(P(S')\\)). Si \\(P(E \\mid S) &lt; P(E \\mid S')\\) entonces los datos aportan evidencia en contra de que Guido estaba en la escena del crimen (\\(E\\) es m√°s probable bajo \\(S'\\) que bajo \\(S\\)).\n\\(P(E \\mid S)\\) es la probabilidad de observar sangre 0 y sangre AB, si Guido estaba en la escena del crimen. Esto es b√°sicamente la probabilidad de encontrar sangre AB (porque la cuota de 0 ya est√° cubierta por Guido). Eso equivale a tener en la escena del crimen a una de las personas que tienen sangre AB (1/100 de la poblaci√≥n). Luego, \\(P(E \\mid S) = 0.01\\).\n\\(P(E \\mid S')\\) es la probabilidad de observar sangre 0 y sangre AB, si Guido no estaba en la escena del crimen. Esto es la probabilidad de que la primera persona tuviera sangre 0 y la segunda AB m√°s la probabilidad de que la primera persona tuviera sangre AB y la segunda sangre 0: \\((60 / 100) \\cdot (1 / 100) + (1 / 100) \\cdot (60 / 100) = 0.012\\).\nPara pensarlo intuitivamente: supongamos que hay 200 personas en un pueblo. 120 personas (Guido y 119 m√°s) tienen sangre 0, 2 personas tienen sangre AB y el resto tiene otro tipo. Con Guido en la escena del crimen hay \\(2/200\\) personas que podr√≠an haber estado con √©l; sin Guido en la escena, pudieron haber estado cualquiera de las 119 personas de sangre 0 con cualquiera de las 2 personas de sangre AB.\n\n\n\nHijos de la probabilidad\nNos encontramos con alguien en la calle y nos dice que tiene dos hijos. Le preguntamos si alguno de ellos es mujer y nos responde que s√≠. ¬øCu√°l es la probabilidad de que ambos sean ni√±as? \nLos Reyes del Rock\nElvis Presley ten√≠a un hermano var√≥n que naci√≥ en el mismo parto pero que muri√≥ al poco tiempo. ¬øCu√°l es la probabilidad de que Elvis tuviera un gemelo? Alguna informaci√≥n adicional: en 1935, cuando Elvis naci√≥, 1/3 de los hermanos del mismo parto eran gemelos y 2/3 mellizos; adem√°s, la probabilidad de que dos mellizos sean del mismo sexo biol√≥gico puede estimarse en 50%, mientras que dos gemelos son siempre del mismo sexo biol√≥gico. \nüìå ¬øAlguien ordena las medias?\nDos cajones contienen medias. Uno de ellos tiene igual cantidad de medias blancas y negras. El otro contiene un n√∫mero igual de medias rojas, verdes y azules. Se elige un caj√≥n al azar, se sacan dos medias sin mirar y resultan ser las dos iguales. ¬øCu√°l es la probabilidad de que las medias sean blancas? Sup√≥ngase que sacar la primera media no altera las proporciones. \n\n\n\n\n\n\nAyuda\n\n\n\n\n\nSean los eventos\n\\(C\\): elegir el caj√≥n BN (y no el RVA)\n\\(I\\): elegir un par de medias iguales\n\\(B\\): elegir un par de medias blancas\nMatem√°ticamente, buscamos \\(P(B \\mid I) = P(B, C \\mid I) + P(B, C' \\mid I)\\) (b√°sicamente estamos marginalizando la variable ‚Äúcaj√≥n‚Äù). El segundo t√©rmino es 0 porque \\(B\\) y \\(C'\\) no pueden darse nunca (nunca voy a sacar un par blanco del caj√≥n RVA). Entonces queda \\(P(B \\mid I) = P(B,C \\mid I) = P(B \\mid C, I) P(C \\mid I)\\), que son los dosvalores que encontramos arriba.\n¬øDe d√≥nde sale que \\(P(B, C \\mid I) = P(B \\mid C, I) P(C \\mid I)\\)?\n\\(P(B, C \\mid I) = P(B,C,I)/P(I)\\) y el numerador, por regla de la cadena, es \\(P(A) P(C \\mid I) P(B \\mid C,I)\\).\n\n\n\nüìå La Falacia del Fiscal\nSally Clark era una abogada brit√°nica que fue err√≥neamente sentenciada a prisi√≥n perpetua en 1999 por la muerte de sus dos hijos beb√©s. Su hijo mayor, Christopher, muri√≥ con 11 semanas en diciembre de 1996 y su hijo m√°s joven, Harry, con 8 semanas en enero de 1998. Durante el juicio, la defensa argument√≥ que las muertes se debieron al s√≠ndrome de muerte s√∫bita del lactante (SIDS). Clark fue condenada a partir del testimonio del pediatra Sir Roy Meadow, quien argument√≥ en la corte lo siguiente:\n\nEn familias sanas, la chance de muerte por SIDS es de \\(\\frac{1}{8500}\\)\nLa probabilidad de dos muertes por SIDS en la misma familia es aproximadamente \\(\\frac{1}{8500^2} \\approx \\frac{1}{73000000}\\)\nEs, por ende, muy poco probable que Clark sea inocente\n\nLuego de pasar 3 a√±os en prisi√≥n, Clark fue liberada en 2003 luego de que se determinara que el testimonio experto de Meadows era equivocado. Dos mujeres, a las cuales el testimonio de Meadows hab√≠a enviado a prisi√≥n, tambi√©n fueron liberadas.\n\nIdentifica una falla en la probabilidad de \\(\\frac{1}{73000000}\\) dada por Meadows.\nIncluso aceptando el n√∫mero anterior como correcto, ¬øcu√°l es el problema de interpretar esa probabilidad como la probabilidad de inocencia de Clark?\n\n\n\n\n\n\n\n\nAyuda\n\n\n\n\n\nA√∫n asumiendo que \\(P(E \\mid I)\\) es lo que dice Meadows, esa no es la probabilidad de inter√©s. La probabilidad que interesa es la de inocencia (o culpabilidad) dada la evidencia. Es decir, la probabilidad que nos interesa es \\(P(I \\mid E)\\) que la podemos escribir como \\(P(I \\mid E) = P(E \\mid I) P(I) / P(E)\\).\n\\(P(I \\mid E)\\) es solo similar a \\(P(E \\ mid I)\\) si \\(P(I)\\) es similar a \\(P(E)\\). Este no es el caso, ¬øpor qu√©?",
    "crumbs": [
      "Pr√°ctica",
      "Unidad 1"
    ]
  },
  {
    "objectID": "practica/practica_01.html#regla-de-bayes",
    "href": "practica/practica_01.html#regla-de-bayes",
    "title": "Pr√°ctica - Unidad 1",
    "section": "",
    "text": "El prop√≥sito de esta secci√≥n de la pr√°ctica es resolver situaciones que impliquen la aplicaci√≥n de la Regla de Bayes como se presenta tradicionalmente en un curso de Probabilidad.\n\nDemostraci√≥n\nDemuestra la validez de la siguiente expresi√≥n de la Regla de Bayes\n\\[\nP(B_j \\mid A) = \\frac{P(A \\mid B_j) P(B_j)}{\\sum_{k=1}^{K}P(A \\mid B_k) P(B_k)}\n\\]\ndonde \\(A\\) es un evento cualquiera y \\(\\{B_1, \\cdots, B_K\\}\\) forman una partici√≥n. \n\n\n\n\n\n\nüìå El test infalible\nEn una poblaci√≥n dada, una de cada mil personas tiene una enfermedad. Se toma una persona al azar de la poblaci√≥n, se le aplica un test para detectar dicha enfermedad, y el resultado es positivo. El test se caracteriza por dar positivo el 99% de las veces que una persona tiene la enfermedad. Adem√°s, dicho test tiene una tasa de falsos positivos del 5%.\n\n¬øCu√°l es la probabilidad de que la persona tenga efectivamente la enfermedad?\nSi realizamos el mismo an√°lisis una segunda vez sobre el mismo paciente y obtenemos nuevamente positivo,\n\n¬øCu√°l seria la probabilidad que el paciente est√© enfermo?\n¬øY si diera negativo?\n¬øEs el prior el mismo cuando se analiza el resultado del segundo an√°lisis que cuando solo se analiza el primero? \n\n\n¬øEs verdad que existen los vampiros? Versi√≥n Crep√∫sculo\nEdward quiere probarle a Bella que los vampiros existen. Seg√∫n Bella, hay una probabilidad del 5% de que los vampiros existan. Tambi√©n cree que la probabilidad de que exista alguien con la piel brillante dado que los vampiros existen es del 70%, y que la probabilidad de que alguien tenga la piel brillante si los vampiros no existen es del 3%. Edward lleva a Bella al bosque y le muestra que de hecho su piel brilla como un üíé ¬øCu√°l es la probabilidad que existan los vampiros? \n\n\n\n\n\nRobert Pattinson como Edward en Crep√∫sculo\n\n\n\n\nüìå √Årboles enfermos\nUn vivero de la ciudad se destaca por vender una variedad de √°rboles nativos, incluyendo al jacarand√°, ceibo, omb√∫, entre otros. Lamentablemente, el 18% de los √°rboles del vivero estan infectados con moho. Los √°rboles enfermos se componen en un 15% por jacarand√°s, 80% de ceibos, y 5% de otras especies. Los √°rboles sanos se componen por un 20% de jacarand√°s, 10% de ceibos, y 70% de otras especies. Con el objetivo de monitorear cuanto se propag√≥ la enfermedad, una de las personas que trabaja en el vivero selecciona al azar uno de los √°rboles para testear.\n\n¬øCu√°l es la probabilidad a priori de que el √°rbol tenga moho?\nResulta que el √°rbol seleccionado es un ceibo. ¬øCu√°l es la probabilidad de haber seleccionado un ceibo?\n¬øCu√°l es la probabilidad a posteriori de que el ceibo seleccionado tenga moho?\nCompare las probabilidades a priori y a posteriori de que el √°rbol tenga moho. ¬øC√≥mo afecta el an√°lisis el saber que el √°rbol es un ceibo? \n\n\n\n\n\n\nFlor del Ceibo, la flor nacional\n\n\n\n\nüìå Transporte ‚ÄúEl Impuntual‚Äù\nUna cierta empresa de transporte regional, que decidimos llamar ‚ÄúEl Impuntual‚Äù, tiene servicios que van desde Rosario hasta Wheelwright varias veces al d√≠a, todos los d√≠as de la semana. Un 30% de los viajes salen a la ma√±ana, otro 30% salen a la tarde, y el restante 40% salen a la noche. Los pasajeros suelen estar muy frustrados ya que un 25% de los viajes salen tarde. De estos viajes demorados, el 40% corresponden a la ma√±ana, un 50% suceden a la tarde, y el 10% restante ocurre a la noche1.\nLucio y Franco son dos amigos del pueblo, y se volvieron a sus casas en colectivos diferentes.\n\nLucio se fue en uno de los colectivos de la ma√±ana. ¬øCu√°l es la probabilidad que su viaje est√© demorado?\nEl colectivo de Franco no est√° demorado. ¬øCu√°l es la probabilidad de que est√© viajando en uno de los colectivos de la ma√±ana? \n\n\n\n\n\n\nFoto de Markus Winkler en Unsplash\n\n\n\n\nüìå Beb√© panda\nSupongamos que hay dos especies de osos panda. Ambas especies son igual de frecuentes y viven en la misma regi√≥n. Es m√°s, lucen de la misma forma y comen la misma comida. A√∫n no existe una prueba gen√©tica que pueda diferenciarlos. Lo √∫nico que los diferencia es la cantidad de cr√≠as que suelen tener. Las madres de la especie A dan luz a mellizos el 10% del tiempo. Y las madres de la especie B dan a luz mellizos el 20% del tiempo. En todos los otros casos, estas madres dan a luz un solo beb√© panda.\nUsando un poco la imaginaci√≥n, supongamos que somos la persona encargada de un programa de reproducci√≥n de pandas. Tenemos una panda femenina que acaba de dar a luz a un par de mellizos, pero no sabemos a que especie pertenece.\n\n¬øCu√°l es la probabilidad que la mam√° panda sea de la especie A?\n¬øCu√°l es la probabilidad que vuelva a tener mellizos en la pr√≥xima parici√≥n?\nUn tiempo despu√©s sos encontramos con que en la segunda parici√≥n da a luz a un √∫nico beb√© panda. ¬øCu√°l es la probabilidad de que este panda sea de la especie A? \n\n\n\n\n\n\nFoto de Stone Wang en Unsplash\n\n\n\n\nüìå Paraguas\nEst√°s a punto de subir a un avi√≥n rumbo a Mendoza. Quer√©s saber si ten√©s que llevar un paraguas o no. Llam√°s a tres amigos que viven en Mendoza y les pregunt√°s si est√° lloviendo. Cada uno de ellos tiene una probabilidad de \\(2/3\\) de decirte la verdad y \\(1/3\\) de mentirte para hacerte una broma. Los tres responden que s√≠ est√° lloviendo. ¬øCu√°l es la probabilidad de que realmente est√© lloviendo en las Mendoza? Se puede asumir que en Mendoza llueve en 1 de cada 10 d√≠as. \n\n\n\n\n\n\nAyuda\n\n\n\n\n\nSi \\(LLL\\) es ‚Äúlos tres amigos dijeron que llov√≠a‚Äù, buscamos \\(P(\\text{lluvia} \\mid LLL) = P(LLL \\mid \\text{lluvia}) P(\\text{lluvia}) / P(LLL)\\).\n\\(P(LLL \\mid \\text{lluvia})\\) es la probabilidad de que ninguno de los tres mienta.\n\\(P(LLL) = P(LLL \\mid \\text{lluvia}) P(\\text{lluvia}) + P(LLL \\mid \\text{no lluvia}) P(\\text{no lluvia})\\).\n\n\n\nüìå Sherlock\nDos personas dejaron rastros de sangre en la escena del crimen. La sangre de Guido, un sospechoso, es analizada y resulta ser de tipo ‚Äò0‚Äô. Los rastros de sangre de la escena son de tipo ‚Äò0‚Äô (un tipo com√∫n en la poblaci√≥n, presente en el 60% de las personas) y de tipo ‚ÄòAB‚Äô (un tipo raro, con una frecuencia del 1% en la poblaci√≥n). ¬øEstos datos representan evidencia de que Guido estaba presente en la escena del crimen? \n\n\n\n\n\nSherlock Holmes, el detective privado\n\n\n\n\n\n\n\n\n\n\nAyuda\n\n\n\n\n\nLlamemos \\(S\\) a ‚ÄúGuido y otra persona estuvieron en la escena del crimen‚Äù versus \\(S'\\), ‚Äúdos personas desconocidas estuvieron en la escena del crimen‚Äù. Adem√°s, contamos con la evidencia \\(E\\), ‚Äúobservar sangre 0 y sangre AB‚Äù.\nQueremos saber qu√© es mayor, si \\(P(E \\mid S)\\) o \\(P(E \\mid S')\\) (si luego quisi√©ramos comparar \\(P(S \\mid E)\\) con \\(P(S' \\mid E)\\) necesitar√≠amos las probabilidades a priori de \\(P(S)\\) y \\(P(S')\\)). Si \\(P(E \\mid S) &lt; P(E \\mid S')\\) entonces los datos aportan evidencia en contra de que Guido estaba en la escena del crimen (\\(E\\) es m√°s probable bajo \\(S'\\) que bajo \\(S\\)).\n\\(P(E \\mid S)\\) es la probabilidad de observar sangre 0 y sangre AB, si Guido estaba en la escena del crimen. Esto es b√°sicamente la probabilidad de encontrar sangre AB (porque la cuota de 0 ya est√° cubierta por Guido). Eso equivale a tener en la escena del crimen a una de las personas que tienen sangre AB (1/100 de la poblaci√≥n). Luego, \\(P(E \\mid S) = 0.01\\).\n\\(P(E \\mid S')\\) es la probabilidad de observar sangre 0 y sangre AB, si Guido no estaba en la escena del crimen. Esto es la probabilidad de que la primera persona tuviera sangre 0 y la segunda AB m√°s la probabilidad de que la primera persona tuviera sangre AB y la segunda sangre 0: \\((60 / 100) \\cdot (1 / 100) + (1 / 100) \\cdot (60 / 100) = 0.012\\).\nPara pensarlo intuitivamente: supongamos que hay 200 personas en un pueblo. 120 personas (Guido y 119 m√°s) tienen sangre 0, 2 personas tienen sangre AB y el resto tiene otro tipo. Con Guido en la escena del crimen hay \\(2/200\\) personas que podr√≠an haber estado con √©l; sin Guido en la escena, pudieron haber estado cualquiera de las 119 personas de sangre 0 con cualquiera de las 2 personas de sangre AB.\n\n\n\nHijos de la probabilidad\nNos encontramos con alguien en la calle y nos dice que tiene dos hijos. Le preguntamos si alguno de ellos es mujer y nos responde que s√≠. ¬øCu√°l es la probabilidad de que ambos sean ni√±as? \nLos Reyes del Rock\nElvis Presley ten√≠a un hermano var√≥n que naci√≥ en el mismo parto pero que muri√≥ al poco tiempo. ¬øCu√°l es la probabilidad de que Elvis tuviera un gemelo? Alguna informaci√≥n adicional: en 1935, cuando Elvis naci√≥, 1/3 de los hermanos del mismo parto eran gemelos y 2/3 mellizos; adem√°s, la probabilidad de que dos mellizos sean del mismo sexo biol√≥gico puede estimarse en 50%, mientras que dos gemelos son siempre del mismo sexo biol√≥gico. \nüìå ¬øAlguien ordena las medias?\nDos cajones contienen medias. Uno de ellos tiene igual cantidad de medias blancas y negras. El otro contiene un n√∫mero igual de medias rojas, verdes y azules. Se elige un caj√≥n al azar, se sacan dos medias sin mirar y resultan ser las dos iguales. ¬øCu√°l es la probabilidad de que las medias sean blancas? Sup√≥ngase que sacar la primera media no altera las proporciones. \n\n\n\n\n\n\nAyuda\n\n\n\n\n\nSean los eventos\n\\(C\\): elegir el caj√≥n BN (y no el RVA)\n\\(I\\): elegir un par de medias iguales\n\\(B\\): elegir un par de medias blancas\nMatem√°ticamente, buscamos \\(P(B \\mid I) = P(B, C \\mid I) + P(B, C' \\mid I)\\) (b√°sicamente estamos marginalizando la variable ‚Äúcaj√≥n‚Äù). El segundo t√©rmino es 0 porque \\(B\\) y \\(C'\\) no pueden darse nunca (nunca voy a sacar un par blanco del caj√≥n RVA). Entonces queda \\(P(B \\mid I) = P(B,C \\mid I) = P(B \\mid C, I) P(C \\mid I)\\), que son los dosvalores que encontramos arriba.\n¬øDe d√≥nde sale que \\(P(B, C \\mid I) = P(B \\mid C, I) P(C \\mid I)\\)?\n\\(P(B, C \\mid I) = P(B,C,I)/P(I)\\) y el numerador, por regla de la cadena, es \\(P(A) P(C \\mid I) P(B \\mid C,I)\\).\n\n\n\nüìå La Falacia del Fiscal\nSally Clark era una abogada brit√°nica que fue err√≥neamente sentenciada a prisi√≥n perpetua en 1999 por la muerte de sus dos hijos beb√©s. Su hijo mayor, Christopher, muri√≥ con 11 semanas en diciembre de 1996 y su hijo m√°s joven, Harry, con 8 semanas en enero de 1998. Durante el juicio, la defensa argument√≥ que las muertes se debieron al s√≠ndrome de muerte s√∫bita del lactante (SIDS). Clark fue condenada a partir del testimonio del pediatra Sir Roy Meadow, quien argument√≥ en la corte lo siguiente:\n\nEn familias sanas, la chance de muerte por SIDS es de \\(\\frac{1}{8500}\\)\nLa probabilidad de dos muertes por SIDS en la misma familia es aproximadamente \\(\\frac{1}{8500^2} \\approx \\frac{1}{73000000}\\)\nEs, por ende, muy poco probable que Clark sea inocente\n\nLuego de pasar 3 a√±os en prisi√≥n, Clark fue liberada en 2003 luego de que se determinara que el testimonio experto de Meadows era equivocado. Dos mujeres, a las cuales el testimonio de Meadows hab√≠a enviado a prisi√≥n, tambi√©n fueron liberadas.\n\nIdentifica una falla en la probabilidad de \\(\\frac{1}{73000000}\\) dada por Meadows.\nIncluso aceptando el n√∫mero anterior como correcto, ¬øcu√°l es el problema de interpretar esa probabilidad como la probabilidad de inocencia de Clark?\n\n\n\n\n\n\n\n\nAyuda\n\n\n\n\n\nA√∫n asumiendo que \\(P(E \\mid I)\\) es lo que dice Meadows, esa no es la probabilidad de inter√©s. La probabilidad que interesa es la de inocencia (o culpabilidad) dada la evidencia. Es decir, la probabilidad que nos interesa es \\(P(I \\mid E)\\) que la podemos escribir como \\(P(I \\mid E) = P(E \\mid I) P(I) / P(E)\\).\n\\(P(I \\mid E)\\) es solo similar a \\(P(E \\ mid I)\\) si \\(P(I)\\) es similar a \\(P(E)\\). Este no es el caso, ¬øpor qu√©?",
    "crumbs": [
      "Pr√°ctica",
      "Unidad 1"
    ]
  },
  {
    "objectID": "practica/practica_01.html#inferencia-bayesiana",
    "href": "practica/practica_01.html#inferencia-bayesiana",
    "title": "Pr√°ctica - Unidad 1",
    "section": "Inferencia Bayesiana",
    "text": "Inferencia Bayesiana\nEn esta parte de la pr√°ctica, se le otorga un significado a las cantidades que aparecen en la Regla de Bayes modificando conceptualmente el enfoque de las situaciones problem√°ticas. Ahora los problemas se tratan de realizar inferencias sobre posibles causas de ciertos datos observados. Se incrementa el rigor matem√°tico, aparecen distribuciones de probabilidad y la necesidad de dejar ciertos c√°lculos en manos de la computadora.\n\nüìå El lenguaje de las probabilidades\nEscribir la expresi√≥n matem√°tica para cada una de las siguientes descripciones verbales:\n\nProbabilidad de un par√°metro dados los datos observados.\nLa distribuci√≥n de probabilidad de los par√°metros antes de ver los datos.\nLa verosimilitud de los datos para un valor dado de los par√°metros.\nLa probabilidad de una observaci√≥n nueva luego de observar los datos.\nLa probabilidad de una observaci√≥n antes de ver los datos. \n\nQu√© datazo me tiraste, rey\nLos M&Ms azul fueron introducidos en el a√±o 1995 (antes hab√≠a dos tipos de marr√≥n)\n\nAntes de 1995, la mezcla de colores en una bolsa de M&Ms era: 30% marron, 20% amarillo, 20% rojo, 10% verde, 10% naranja y 10% marr√≥n bronceado.\nLuego de 1995, la mezcla pas√≥ a ser: 24% azul, 20% verde, 16% naranja, 14% amarillo, 13% rojo y 13% marr√≥n.\n\nUn amigo tiene dos bolsas de M&M y nos dice que una bolsa es de 1994 y la otra es de 1996, pero no nos dice cu√°l es cu√°l. Nos da un M&M de cada bolsa: uno es amarillo y el otro es verde (ambos posiblemente est√©n vencidos). ¬øCu√°l es la probabilidad de que el amarillo venga de la bolsa de 1994?\nüíª La Gran Estafa\nHay dos monedas en una caja. Una de ellas es una moneda com√∫n y la otra es una moneda que tiene dos caras.\n\nSe elige una moneda al azar, se arroja, y se obtiene cara. ¬øCu√°l es la probabilidad de que la moneda elegida sea la falsa?\nSe elige una moneda al azar y se arroja al aire tres veces, obteni√©ndose tres caras. ¬øCu√°l es la probabilidad de que la moneda elegida sea la falsa?\n\nüìå Vocabulario limitado\nSupongamos que existe un idioma con seis palabras:\n\\[\n\\text{\\{perro, parra, farra, carro, corro, tarro\\}}\n\\]\nUn an√°lisis ling√º√≠stico exhaustivo de esta lengua ha descubierto que todas las palabras son igualmente probables, excepto por ‚Äòperro‚Äô, que es \\(\\alpha\\) veces m√°s probable que las otras.\nAdem√°s:\n\nCuando se tipean, un caracter se introduce err√≥neamente con probabilidad \\(\\theta\\).\nTodas las letras tienen la misma probabilidad de producir un error de tipeo.\nSi una letra se tipe√≥ mal, la probabilidad de cometer un error en otro caracter no cambia.\nLos errores son independientes a lo largo de una palabras.\n\n\n¬øCu√°l es la probabilidad de escribir correctamente ‚Äòtarro‚Äô?\n¬øCu√°l es la probabilidad de tipear ‚Äòcerro‚Äô o ‚Äòcurro‚Äô al querer escribir ‚Äòcarro‚Äô?\nUtilizando la Regla de Bayes, desarrollar un corrector gramatical para esta lengua. Para las palabras tipeadas ‚Äòfarra‚Äô, ‚Äòbirra‚Äô y ‚Äòlocos‚Äô, hallar la probabilidad de que cada palabra del diccionario sea la palabra que se hab√≠a querido escribir. Utilizar las siguientes combinaciones de par√°metros:\n\n\\(\\alpha = 2\\) y \\(\\theta = 0.1\\)\n\\(\\alpha = 50\\) y \\(\\theta = 0.1\\)\n\\(\\alpha = 2\\) y \\(\\theta = 0.9\\)\n\n\nüìå Que el √°rbol no tape el bosque\nSea \\(X_1 \\sim \\text{Bernoulli}(\\theta)\\) una variable que indica si una especie de √°rboles se halla en un determinado bosque y \\(\\theta \\in [0, 1]\\) representa la probabilidad a priori de que la especie se encuentre en el bosque. Una investigadora selecciona una muestra de \\(n\\) √°rboles del bosque y encuentra que \\(X_2\\) de ellas pertenecen a la especie de inter√©s.\nEl modelo luego es\n\\[\n\\begin{array}{lc}\nX_2 \\mid X_1 = x_1 \\sim \\text{Binomial}(\\lambda x_1, n) & \\text{con} \\ \\lambda \\in [0, 1]\n\\end{array}\n\\]\n\\(\\lambda\\) representa la probabilidad de detectar la especie, dado que la especie se encuentra en el bosque.\nEncuntre expresiones matem√°ticas en t√©rmino de \\(n\\), \\(\\theta\\) y \\(\\lambda\\) para las siguientes probabilidades:\n\n\\(P(X_1 = 0)\\).\n\\(P(X_1 = 0, X_2 = 0)\\).\n\\(P(X_2 = 0)\\).\n\\(P(X_1 = 0 \\mid X_2 = 0)\\).\n\\(P(X_2 = 0 \\mid X_1 = 0)\\).\n\\(P(X_1 = 0 \\mid X_2 = 1)\\).\n\\(P(X_2 = 0 \\mid X_1 = 1)\\).\nExplique de manera intuitiva c√≥mo es que las probabilidades calculadas en (iv)-(vii) cambian seg√∫n \\(n\\), \\(\\theta\\) y \\(\\lambda\\).\nAsuma \\(\\theta=0.5\\), \\(\\lambda=0.1\\) y \\(X_2 = 0\\) ¬øCu√°n grande debe ser \\(n\\) para que se puede concluir con 95% de confianza que la especie no se encuentra en el bosque? \n\n\n\n\n\n\nFoto de Sergei A en Unsplash\n\n\n\n\nüíª üìå ¬°Ostras! ¬°Estoy haciendo inferencia bayesiana!\nEn un estudio que utiliza m√©todos de la Estad√≠stica Bayesiana para predecir el n√∫mero de especies que ser√°n descubiertas en el futuro se reporta que la cantidad de especies marinas bivalvas2 descubiertas cada a√±o entre 2010 y 2015 fue 64, 13, 33, 18, 30 y 20.\nSi se representa con \\(Y_t\\) a la cantidad de especies descubierta en el a√±o \\(t\\), y asumiendo:\n\\[\n\\begin{aligned}\nY_t \\mid \\lambda &\\underset{iid}{\\sim} \\text{Poisson}(\\lambda) \\\\\n\\lambda       &\\sim \\text{Uniforme}(0, 100)\n\\end{aligned}\n\\]\nGraficar la distribuci√≥n a posteriori de \\(\\lambda\\). \nüíª üìå Es negocio‚Ä¶\nSea \\(n\\) la cantidad desconocida de clientes que visitan una tienda en un dia cualquiera. El n√∫mero de clientes que realizan una compra es \\(Y\\) y se cumple que\n\\[\nY \\mid \\theta, n \\sim \\text{Binomial}(\\theta, n)\n\\]\ndonde \\(\\theta\\) es la probabilidad de compra, dado que se produce la visita a la tienda. La distribuci√≥n a priori de \\(n\\) es \\(n \\sim \\text{Poisson}(5)\\). Bajo el supuesto que \\(\\theta\\) es conocido y que \\(n\\) es desconocido, graficar la distribuci√≥n a posteriori de \\(n\\) para todas las combinaciones de \\(Y \\in \\{0, 5, 10 \\}\\) y \\(\\theta \\in \\{0.2, 0.5\\}\\). Explique cual es del efecto de cambiar \\(Y\\) y \\(\\theta\\) sobre la distribuci√≥n a posteriori.  \nüìå Con amigos as√≠, qui√©n necesita enemigos\nUn amigo arroja un dado y anota en secreto el n√∫mero que sale (llam√©moslo \\(T\\)). A continuaci√≥n, nosotros, con los ojos vendados, arrojamos el dado varias veces. No podemos ver el n√∫mero que sale pero nuestro amigo nos dice si el n√∫mero que sacamos es mayor, menor o igual a \\(T\\).\nSupongamos que nos da la secuencia: \\(G,\\ G,\\ C,\\ I,\\ C,\\ C,\\ C, I,\\ G,\\ C\\) (siendo \\(G\\) m√°s grande, \\(C\\) m√°s chico e \\(I\\) igual). ¬øCu√°l es la distribuci√≥n a posteriori de los valores de \\(T\\)? \n\n\n\n\n\n\nAyuda\n\n\n\n\n\nEstamos tratando de hacer inferencias sobre \\(T\\), es decir, la distribuci√≥n a posteriori debe ser \\(P(T=1), P(T=2), \\dots P(T=6)\\).\nConviene analizar este problema en forma secuencial. Observamos \\(G\\), ¬øqu√© significa eso? ¬øc√≥mo obtenemos la verosimilitud? \\(P(G \\mid T=1) = 5/6\\), \\(P(G \\mid T = 2) = 4 / 6\\), \\(P(G \\mid T=3) = 3/6\\)‚Ä¶\n\n\n\nüíª Orden en la sala\nEn las Jornadas Rosarinas de Ciencia de Datos, una expositora est√° dando una charla en un sal√≥n cuando el personal de seguridad la interrumpe porque cree que puede haber m√°s de 1000 personas en la sala, superando el m√°ximo permitido.\nLa expositora piensa que hay menos de 1000 personas y se ofrece a demostrarlo, aunque piensa que contarlas podr√≠a llevar mucho tiempo. Decide hacer un experimento:\n\nPregunta cu√°ntas personas nacieron el 11 de mayo. Dos personas levantan la mano.\nPregunta cu√°ntas personas nacieron el 23 de mayo. Una persona levanta la mano.\nPregunta cu√°ntas personas nacieron el 1 de agosto. Nadie levanta la mano.\n\n¬øCu√°ntas personas hay en la sala? O, mejor dicho, ¬øcu√°l es la probabilidad de que haya m√°s de 1000 personas en la sala? \n\n\n\n\n\n\nAyuda\n\n\n\n\n\nEstamos tratando de hacer inferencias sobre la cantidad de personas X (que puede ser un n√∫mero entre, digamos, 1 y 3000).\nConviene analizar el problema en forma secuencial. Para la primera observaci√≥n (dos personas cumpliendo a√±os el mismo d√≠a), la verosimilitud es la probabilidad de que haya dos personas con el mismo cumplea√±os si en la sala hay 1, 2, 3, ‚Ä¶ personas. Lo podemos pensar como una binomial\nSi \\(X=1\\), \\(P(Y=2 \\mid X = 1) = 0\\)\nSi \\(X=2\\), \\(P(Y=2 \\mid X=2) =\\) dbinom(2, size = 2, prob = 1/365)\nSi \\(X=100\\), \\(P(Y=2 \\mid X=100) =\\) dbinom(2, size = 100, prob = 1/365)\n\n\n\nüíª House of Cards\nHay 538 miembros en el Congreso de Estados Unidos. Supongamos que se auditan sus inversiones y se encuentra que 312 de ellos obtuvieron rendimientos por encima del mercado. Asumamos que un miembro honesto del Congreso tiene solo una probabilidad del 50% de tener rendimientos por encima del mercado, pero uno deshonesto que opera con informaci√≥n confidencial tiene una chance del 90% de hacerlo. ¬øCu√°ntos miembros del Congreso son honestos? \n\n\n\n\n\nRobin Wright y Kevin Spacey, protagonistas de House of Cards\n\n\n\n\nüíª üìå Puede fallar‚Ä¶\nCansada de los experimentos de arrojar una moneda cientos de veces al aire, una estudiante dise√±a un sistema de reconocimiento de im√°genes que determina si sali√≥ cara o ceca y registra el resultado.\nL√≥gicamente, el sistema dise√±ado no es perfecto sino que presenta una tasa de error. En particular, la probabilidad de que clasificar mal es de 0.2 (20% de las veces que sale cara, el sistema dice ceca, y viceversa).\nSe arroja la moneda 250 veces y el sistema detecta 140 caras,\n\n¬øCu√°l es la distribuci√≥n a posteriori de \\(\\theta\\), la probabilidad de obtener cara?\n¬øQu√© ocurre a medida que la probabilidad de clasificar mal var√≠a? \n\n\n\n\n\n\n\nAyuda\n\n\n\n\n\nConsiderar:\n\n\\(\\theta\\) la probabilidad de obtener cara (es decir, que la moneda caiga en cara)\n\\(\\pi\\) es la probabilidad de observar cara\n\\(p\\) es la probabilidad de falla\n\n\\[\n\\begin{aligned}\n\\theta &\\sim \\text{Beta}(1, 1) \\\\\nY &=\n    \\begin{cases}\n    1 & \\text{Si la moneda cae en cara} \\\\\n    0 & \\text{Si la moneda cae en ceca}\n    \\end{cases} \\\\\nX &=\n    \\begin{cases}\n    1 & \\text{Si se clasifica bien} \\\\\n    0 & \\text{Si se clasifica mal}\n    \\end{cases} \\\\\nZ &=\n    \\begin{cases}\n    1 & \\text{Si se observa cara} \\\\\n    0 & \\text{Si se obserca ceca}\n    \\end{cases}\n\\end{aligned}\n\\]\nObtener \\(\\pi\\) en base a la distribuci√≥n conjunta de \\(X\\) e \\(Y\\) y luego realizar la inferencia con un modelo binomial para \\(T = \\sum_{i=1}^{250}{Z_i}\\) utilizando \\(\\pi\\) como probabilidad de √©xito en vez de \\(\\theta\\).\nEl par√°metro \\(\\pi\\) quedar√° determinado en funci√≥n de \\(\\theta\\), y luego la verosimilitud para \\(\\theta\\) se puede obtener a partir de la verosimilitud para el valor \\(\\pi = f(\\theta)\\) \\[\np_\\theta(T = t \\mid \\theta) = p_\\pi(T = t \\mid \\pi)\n\\]\ndonde \\(p_\\pi\\) es la funci√≥n de masa de probabilidad de una binomial. Luego,\n\\[\n\\begin{aligned}\np_\\theta(\\theta \\mid T = t) &\\propto p_\\theta(T = t \\mid \\theta) p_\\theta(\\theta) \\\\\n&= p_\\theta(T = t \\mid \\theta)\n\\end{aligned}\n\\]\n\n\n\n¬°Saludos a los cubos con puntos! (‚Ä¶) Ser√°n dados\nDos dados de seis caras son arrojados. Se sabe que la suma de los dos puntajes obtenidos es 9. ¬øCu√°l es la distribuci√≥n a posteriori de los puntajes de los dados? \n\n\n\n\n\n\nAyuda\n\n\n\n\n\nEstamos tratando de hacer inferencias sobre \\(D_1\\) y \\(D_2\\), los valores de cada uno de los datos. Definir la distribuci√≥n a priori no es complicado (es una distribuci√≥n sobre \\(D_1\\) y \\(D_2\\)).\nEl truco est√° en determinar la verosimilitud de cada par (\\(D_1, D_2\\)) luego de haber observado un 9.\n¬øCu√°l es la probabilidad de observar 9 en la suma de los dos dado que ‚Ä¶?\n\\(D_1 = 1\\) y \\(D_2 = 1 -\\) la probabilidad de observar 9 es 0\n\\(D_1 = 1\\) y \\(D_2 = 2 -\\) la probabilidad de observar 9 es 0\n\\(D_1 = 4\\) y \\(D_2 = 5 -\\) la probabilidad de observar 9 es 1\n\\(D_1 = 5\\) y \\(D_2 = 4 -\\) la probabilidad de observar 9 es 1",
    "crumbs": [
      "Pr√°ctica",
      "Unidad 1"
    ]
  },
  {
    "objectID": "practica/practica_01.html#conceptuales",
    "href": "practica/practica_01.html#conceptuales",
    "title": "Pr√°ctica - Unidad 1",
    "section": "Conceptuales",
    "text": "Conceptuales\nEn esta secci√≥n, se nos invita a pensar sobre las caracter√≠sticas de la Estad√≠stica Bayesiana. En lugar de encontrar una respuesta √∫nica mediante c√°lculos matem√°ticos, se necesita comprender en profundidad tanto el enfoque frecuentista como el bayesiano para interpretar estas visiones en diferentes escenarios.\n\nüìå Voy a conseguir esa pasant√≠a\nLa empresa de tecnolog√≠a en la que todo el mundo quiere trabajar tiene varias vacantes para pasantes en ciencia de datos. Luego de leer la descripci√≥n de la b√∫squeda, te das cuenta que sos una persona calificada para el puesto: estos son tus datos. Tu objetivo es averiguar si te van a ofrecer el puesto: esta es tu hip√≥tesis.\n\nDesde la perspectiva de una persona con un razonamiento frecuentista, ¬øQu√© es lo que se responde al evaluar la hip√≥tesis de que te ofrecen el puesto?\nRepita el punto anterior considerando la perspectiva de una persona con un razonamiento Bayesiano.\n¬øQu√© pregunta tiene m√°s sentido responder: la frecuentista o la Bayesiana? Justifica tu respuesta. \n\nüìå Beneficios de la Estad√≠stica Bayesiana\nUna amiga te cuenta que est√° interesada en aprender m√°s sobre Estad√≠stica Bayesiana. Expl√≠cale lo siguiente:\n\n¬øPor qu√© es √∫til el enfoque Bayesiano?\n¬øCu√°les son las similitudes entre el enfoque frecuentista y el Bayesiano?",
    "crumbs": [
      "Pr√°ctica",
      "Unidad 1"
    ]
  },
  {
    "objectID": "practica/practica_01.html#footnotes",
    "href": "practica/practica_01.html#footnotes",
    "title": "Pr√°ctica - Unidad 1",
    "section": "Notas",
    "text": "Notas\n\n\nHay gente que dice que ‚Äúcada dos por tres‚Äù te deja a pata. No nos vamos a pelear explic√°ndoles que estan siendo demasiado exigentes, ya que un 25% tambi√©n es un mont√≥n!‚Ü©Ô∏é\nUna clase de molusco. El mejill√≥n, la ostra y la almeja son bivalvos‚Ü©Ô∏é",
    "crumbs": [
      "Pr√°ctica",
      "Unidad 1"
    ]
  },
  {
    "objectID": "practica/practica_00.html",
    "href": "practica/practica_00.html",
    "title": "Pr√°ctica - Unidad 0",
    "section": "",
    "text": "El objetivo de los ejercicios de esta unidad es refrescar y consolidar los conocimientos relacionados con el c√°lculo de probabilidades y la manipulaci√≥n de distribuciones de probabilidad.\n\nüìå ¬°Argentina campe√≥n!\nDe las siguientes expresiones cual(es) se corresponde(n) con el enunciado ‚Äúla probabilidad de que Argentina gane la copa del mundo dado que es 18 de Diciembre de 2022‚Äù?\n\n\\(P(\\text{18 de Diciembre de 2022} \\mid \\text{Argentina campe√≥n})\\).\n\\(P(\\text{Argentina campe√≥n})\\).\n\\(P(\\text{Argentina campe√≥n}, \\text{18 de Diciembre de 2022}) / P(\\text{18 de Diciembre de 2022})\\).\n\\(P(\\text{Argentina campe√≥n} \\mid \\text{Diciembre})\\).\n\\(P(\\text{Argentina campe√≥n} \\mid \\text{18 de Diciembre de 2022})\\). \n\n\n\n\n\n\nEl festejo de los campeones del mundo\n\n\n\n\nüìå De formulas al espa√±ol\nEnuncie con palabras cada una de las expresiones del punto anterior. \nüìå Probabilidades condicionales\nSeg√∫n la definici√≥n de probabilidad condicional\n\n¬øCu√°l es el valor de \\(P(A \\mid A)\\)?\n¬øCu√°l es la probabilidad de \\(P(A, B)\\)?\n¬øCu√°l es la probabilidad de \\(P(A, B)\\) en el caso que \\(A\\) y \\(B\\) sean independientes?\n¬øCu√°ndo se cumple que \\(P(A \\mid B) = P(A)\\)?\n¬øEs posible que \\(P(A \\mid B) &gt; P(A)\\)? ¬øCu√°ndo?\n¬øEs posible que \\(P(A \\mid B) &lt; P(A)\\)? ¬øCu√°ndo? \n\nüìå Amigarse con la funci√≥n de densidad (I)\nSuponga \\(X \\sim \\text{Uniforme}(a, b)\\). Su soporte es \\(\\mathcal{S} = [a, b]\\) y su funci√≥n de densidad de probabilidad es \\(p(x) = 1 / (b - a)\\) para todo \\(x \\in \\mathcal{S}\\).\n\nPruebe que \\(p(x)\\) es una funci√≥n de densidad de probabilidad v√°lida.\nEncuentre la media y la varianza de \\(X\\). \n\nüìå Amigarse con la funci√≥n de densidad (II)\nSea \\(X\\) una variable aleatoria con soporte \\(X \\in \\mathcal{S} = [1, \\infty)\\). Encuentre la constante \\(c\\), en funci√≥n de \\(\\theta\\), que haga que \\(p(x) = c \\exp(-x / \\theta)\\) sea una funci√≥n de densidad de probabilidad v√°lida. \nüìå En b√∫squeda de la distribuci√≥n deseada\nSeg√∫n personas expertas en un problema determinado, se indica que el valor de un par√°metro debe ser positivo y su distribuci√≥n a priori debe tener media igual a 5 y varianza igual a 3. Encuentre una distribuci√≥n que satisfaga estas condiciones. \nüìå Distribuci√≥n conjunta, marginal y condicional\nSean \\(X_1\\) y \\(X_2\\) dos variables aleatorias con funci√≥n de probabilidad conjunta dada por la siguiente tabla:\n\n\n\n\n\n\\(X_1\\) / \\(X_2\\)\n\\(X_2=0\\)\n\\(X_2=1\\)\n\n\n\n\n\\(X_1=0\\)\n\\(0.15\\)\n\\(0.15\\)\n\n\n\\(X_1=1\\)\n\\(0.15\\)\n\\(0.20\\)\n\n\n\\(X_1=2\\)\n\\(0.15\\)\n\\(0.20\\)\n\n\n\n\n\ndonde la celda de la primer fila y primer columna se lee \\(P(X_1=0, X_2=0)=0.15\\)\n\nObtenga la distribuci√≥n marginal de \\(X_1\\).\nObtenga la distribuci√≥n marginal de \\(X_2\\).\nObtenga la distribuci√≥n condicional de \\(X_1\\) dado \\(X_2\\).\nObtenga la distribuci√≥n condicional de \\(X_2\\) dado \\(X_1\\). \n\nDistribuciones marginales y condicionales de una normal\nSean \\(X_1\\) y \\(X_2\\) tales que el vector \\((X_1, X_2)\\) sigue una distribuci√≥n normal bivariada con \\(\\mathbb{E}(X_1) = \\mathbb{E}(X_2) = 0\\), \\(\\mathbb{V}(X_1) = \\mathbb{V}(X_2) = 1\\) y \\(\\text{cor}(X_1, X_2) = \\rho\\)\n\nEncuentre la distribuci√≥n marginal de \\(X_1\\).\nEncuentre la distribuci√≥n condicional de \\(X_1\\) dado \\(X_2\\). \n\nüìå De bolas, pesos y distribuciones de probabilidad\nSuponga una urna \\(S\\) contiene un 30% de bolas verdes y un 70% de bolas rojas, y otra urna \\(E\\) contiene un 45% de bolas verdes y un 55% de bolas rojas. Una persona arroja una moneda de un peso argentino y selecciona una bola de una de las dos urnas dependiendo de si la moneda cae en sol o escudo. Si la moneda cae en sol, saca una bola de la urna \\(S\\) y si la moneda cae en escudo, saca una bola de la urna \\(E\\).\nConsidere las siguientes variables aleatorias:\n\\[\n\\begin{aligned}\nX &=\n    \\begin{cases}\n    1 & \\text{Si la moneda cae en sol} \\\\\n    0 & \\text{Si la moneda cae en escudo}\n    \\end{cases}\n\\\\\n\\\\\nY &=\n    \\begin{cases}\n    1 & \\text{Si la bola es verde} \\\\\n    0 & \\text{Si la bola es roja}\n    \\end{cases}\n\\end{aligned}\n\\]\n\nEncuentre la distribuci√≥n conjunta de \\(X\\) e \\(Y\\) en una tabla.\nEncuentre \\(\\mathbb{E}(Y)\\). ¬øCu√°l es la probabilidad de que la bola sea verde?\nEncuentre \\(\\mathbb{V}(Y \\mid X = 0)\\), \\(\\mathbb{V}(Y \\mid X = 1)\\) Y \\(\\mathbb{V}(Y)\\). Considerando a la varianza como una medida de incertidumbre, explique de manera intuitiva por que algunas variancias son mas grandes que otras.\nSuponga que observa que la bola es verde. ¬øCu√°l es la probabilidad de que la moneda haya caido en escudo? \n\n\n\n\n\n\nMoneda de un peso argentino acu√±ada en 1995\n\n\n\n\nLuces de giro\nLas luces de giro en los autom√≥viles se utilizan para indicar que se va a realizar alguna acci√≥n determinada. La acci√≥n depende del escenario donde se conduzca (urbano, ruta, rotonda, etc.) y la luz que se encienda (izquierda o derecha). En el uso urbano, se debe colocar la luz de giro correspondiente para indicar que se va a girar en un sentido determinado. Sin embargo, esto no siempre se realiza. Muchas veces sucede que un veh√≠culo no muestra luz de giro, y sin embargo, gira. Aunque menos frecuente, tambi√©n se da que el veh√≠culo coloca la luz de giro, pero no gira. Un estudio revel√≥ un veh√≠culo gira en una de cada diez intersecciones que cruza, que la probabilidad de colocar la luz de giro cuando se va a doblar es de 0.87 y que la probabilidad de que se coloque la luz de giro cuando no se va a doblar es de 0.04. Si observa que un veh√≠culo coloca la luz de giro, ¬øcu√°l es la probabilidad de que efectivamente doble?\nProblema del cumplea√±os\nHay \\(k\\) personas en una sala. Suponga que el cumplea√±os de cada persona tiene la misma probabilidad de ocurrir en cualquiera de los 365 d√≠as del a√±o (se excluye el 29 de febrero) y que los cumplea√±os de las personas son independientes entre si. ¬øCu√°l es la probabilidad de que al menos un par de personas en el grupo cumplan los a√±os el mismo d√≠a?\n\n\n\n\n\n\nFoto de Adi Goldstein en Unsplash\n\n\n\n\nüß© Problema de concordancia de de Montmort\nConsidere un mazo de \\(n\\) cartas bien mezcladas, etiquetadas con n√∫meros de 1 a \\(n\\). Se seleccionan las cartas de a una y se la da vuelta, diciendo en voz alta el n√∫mero de cartas dadas vueltas desde 1 a \\(n\\). Para ganar el juego tiene que coincidir el n√∫mero que se dice en voz alta con el n√∫mero de la carta que se est√° dando vuelta ‚Äì por ejemplo, si la s√©ptima carta dada vuelta contiene el n√∫mero 7. ¬øCu√°l es la probabilidad de ganar? ¬øDepende de \\(n\\)? \nüß© Problema de los dos sobres\nSupongamos que te presentan dos sobres con dinero. Un sobre contiene el doble de dinero que el otro, pero a simple vista son indistinguibles. Se te pide que escojas uno de los sobres. Antes de abrirlo se te ofrece la posibilidad de cambiarlo por el otro. ¬øCambiar√≠as el sobre? ¬øPor qu√©?",
    "crumbs": [
      "Pr√°ctica",
      "Unidad 0"
    ]
  },
  {
    "objectID": "practica/practica_00.html#repaso-de-probabilidad",
    "href": "practica/practica_00.html#repaso-de-probabilidad",
    "title": "Pr√°ctica - Unidad 0",
    "section": "",
    "text": "El objetivo de los ejercicios de esta unidad es refrescar y consolidar los conocimientos relacionados con el c√°lculo de probabilidades y la manipulaci√≥n de distribuciones de probabilidad.\n\nüìå ¬°Argentina campe√≥n!\nDe las siguientes expresiones cual(es) se corresponde(n) con el enunciado ‚Äúla probabilidad de que Argentina gane la copa del mundo dado que es 18 de Diciembre de 2022‚Äù?\n\n\\(P(\\text{18 de Diciembre de 2022} \\mid \\text{Argentina campe√≥n})\\).\n\\(P(\\text{Argentina campe√≥n})\\).\n\\(P(\\text{Argentina campe√≥n}, \\text{18 de Diciembre de 2022}) / P(\\text{18 de Diciembre de 2022})\\).\n\\(P(\\text{Argentina campe√≥n} \\mid \\text{Diciembre})\\).\n\\(P(\\text{Argentina campe√≥n} \\mid \\text{18 de Diciembre de 2022})\\). \n\n\n\n\n\n\nEl festejo de los campeones del mundo\n\n\n\n\nüìå De formulas al espa√±ol\nEnuncie con palabras cada una de las expresiones del punto anterior. \nüìå Probabilidades condicionales\nSeg√∫n la definici√≥n de probabilidad condicional\n\n¬øCu√°l es el valor de \\(P(A \\mid A)\\)?\n¬øCu√°l es la probabilidad de \\(P(A, B)\\)?\n¬øCu√°l es la probabilidad de \\(P(A, B)\\) en el caso que \\(A\\) y \\(B\\) sean independientes?\n¬øCu√°ndo se cumple que \\(P(A \\mid B) = P(A)\\)?\n¬øEs posible que \\(P(A \\mid B) &gt; P(A)\\)? ¬øCu√°ndo?\n¬øEs posible que \\(P(A \\mid B) &lt; P(A)\\)? ¬øCu√°ndo? \n\nüìå Amigarse con la funci√≥n de densidad (I)\nSuponga \\(X \\sim \\text{Uniforme}(a, b)\\). Su soporte es \\(\\mathcal{S} = [a, b]\\) y su funci√≥n de densidad de probabilidad es \\(p(x) = 1 / (b - a)\\) para todo \\(x \\in \\mathcal{S}\\).\n\nPruebe que \\(p(x)\\) es una funci√≥n de densidad de probabilidad v√°lida.\nEncuentre la media y la varianza de \\(X\\). \n\nüìå Amigarse con la funci√≥n de densidad (II)\nSea \\(X\\) una variable aleatoria con soporte \\(X \\in \\mathcal{S} = [1, \\infty)\\). Encuentre la constante \\(c\\), en funci√≥n de \\(\\theta\\), que haga que \\(p(x) = c \\exp(-x / \\theta)\\) sea una funci√≥n de densidad de probabilidad v√°lida. \nüìå En b√∫squeda de la distribuci√≥n deseada\nSeg√∫n personas expertas en un problema determinado, se indica que el valor de un par√°metro debe ser positivo y su distribuci√≥n a priori debe tener media igual a 5 y varianza igual a 3. Encuentre una distribuci√≥n que satisfaga estas condiciones. \nüìå Distribuci√≥n conjunta, marginal y condicional\nSean \\(X_1\\) y \\(X_2\\) dos variables aleatorias con funci√≥n de probabilidad conjunta dada por la siguiente tabla:\n\n\n\n\n\n\\(X_1\\) / \\(X_2\\)\n\\(X_2=0\\)\n\\(X_2=1\\)\n\n\n\n\n\\(X_1=0\\)\n\\(0.15\\)\n\\(0.15\\)\n\n\n\\(X_1=1\\)\n\\(0.15\\)\n\\(0.20\\)\n\n\n\\(X_1=2\\)\n\\(0.15\\)\n\\(0.20\\)\n\n\n\n\n\ndonde la celda de la primer fila y primer columna se lee \\(P(X_1=0, X_2=0)=0.15\\)\n\nObtenga la distribuci√≥n marginal de \\(X_1\\).\nObtenga la distribuci√≥n marginal de \\(X_2\\).\nObtenga la distribuci√≥n condicional de \\(X_1\\) dado \\(X_2\\).\nObtenga la distribuci√≥n condicional de \\(X_2\\) dado \\(X_1\\). \n\nDistribuciones marginales y condicionales de una normal\nSean \\(X_1\\) y \\(X_2\\) tales que el vector \\((X_1, X_2)\\) sigue una distribuci√≥n normal bivariada con \\(\\mathbb{E}(X_1) = \\mathbb{E}(X_2) = 0\\), \\(\\mathbb{V}(X_1) = \\mathbb{V}(X_2) = 1\\) y \\(\\text{cor}(X_1, X_2) = \\rho\\)\n\nEncuentre la distribuci√≥n marginal de \\(X_1\\).\nEncuentre la distribuci√≥n condicional de \\(X_1\\) dado \\(X_2\\). \n\nüìå De bolas, pesos y distribuciones de probabilidad\nSuponga una urna \\(S\\) contiene un 30% de bolas verdes y un 70% de bolas rojas, y otra urna \\(E\\) contiene un 45% de bolas verdes y un 55% de bolas rojas. Una persona arroja una moneda de un peso argentino y selecciona una bola de una de las dos urnas dependiendo de si la moneda cae en sol o escudo. Si la moneda cae en sol, saca una bola de la urna \\(S\\) y si la moneda cae en escudo, saca una bola de la urna \\(E\\).\nConsidere las siguientes variables aleatorias:\n\\[\n\\begin{aligned}\nX &=\n    \\begin{cases}\n    1 & \\text{Si la moneda cae en sol} \\\\\n    0 & \\text{Si la moneda cae en escudo}\n    \\end{cases}\n\\\\\n\\\\\nY &=\n    \\begin{cases}\n    1 & \\text{Si la bola es verde} \\\\\n    0 & \\text{Si la bola es roja}\n    \\end{cases}\n\\end{aligned}\n\\]\n\nEncuentre la distribuci√≥n conjunta de \\(X\\) e \\(Y\\) en una tabla.\nEncuentre \\(\\mathbb{E}(Y)\\). ¬øCu√°l es la probabilidad de que la bola sea verde?\nEncuentre \\(\\mathbb{V}(Y \\mid X = 0)\\), \\(\\mathbb{V}(Y \\mid X = 1)\\) Y \\(\\mathbb{V}(Y)\\). Considerando a la varianza como una medida de incertidumbre, explique de manera intuitiva por que algunas variancias son mas grandes que otras.\nSuponga que observa que la bola es verde. ¬øCu√°l es la probabilidad de que la moneda haya caido en escudo? \n\n\n\n\n\n\nMoneda de un peso argentino acu√±ada en 1995\n\n\n\n\nLuces de giro\nLas luces de giro en los autom√≥viles se utilizan para indicar que se va a realizar alguna acci√≥n determinada. La acci√≥n depende del escenario donde se conduzca (urbano, ruta, rotonda, etc.) y la luz que se encienda (izquierda o derecha). En el uso urbano, se debe colocar la luz de giro correspondiente para indicar que se va a girar en un sentido determinado. Sin embargo, esto no siempre se realiza. Muchas veces sucede que un veh√≠culo no muestra luz de giro, y sin embargo, gira. Aunque menos frecuente, tambi√©n se da que el veh√≠culo coloca la luz de giro, pero no gira. Un estudio revel√≥ un veh√≠culo gira en una de cada diez intersecciones que cruza, que la probabilidad de colocar la luz de giro cuando se va a doblar es de 0.87 y que la probabilidad de que se coloque la luz de giro cuando no se va a doblar es de 0.04. Si observa que un veh√≠culo coloca la luz de giro, ¬øcu√°l es la probabilidad de que efectivamente doble?\nProblema del cumplea√±os\nHay \\(k\\) personas en una sala. Suponga que el cumplea√±os de cada persona tiene la misma probabilidad de ocurrir en cualquiera de los 365 d√≠as del a√±o (se excluye el 29 de febrero) y que los cumplea√±os de las personas son independientes entre si. ¬øCu√°l es la probabilidad de que al menos un par de personas en el grupo cumplan los a√±os el mismo d√≠a?\n\n\n\n\n\n\nFoto de Adi Goldstein en Unsplash\n\n\n\n\nüß© Problema de concordancia de de Montmort\nConsidere un mazo de \\(n\\) cartas bien mezcladas, etiquetadas con n√∫meros de 1 a \\(n\\). Se seleccionan las cartas de a una y se la da vuelta, diciendo en voz alta el n√∫mero de cartas dadas vueltas desde 1 a \\(n\\). Para ganar el juego tiene que coincidir el n√∫mero que se dice en voz alta con el n√∫mero de la carta que se est√° dando vuelta ‚Äì por ejemplo, si la s√©ptima carta dada vuelta contiene el n√∫mero 7. ¬øCu√°l es la probabilidad de ganar? ¬øDepende de \\(n\\)? \nüß© Problema de los dos sobres\nSupongamos que te presentan dos sobres con dinero. Un sobre contiene el doble de dinero que el otro, pero a simple vista son indistinguibles. Se te pide que escojas uno de los sobres. Antes de abrirlo se te ofrece la posibilidad de cambiarlo por el otro. ¬øCambiar√≠as el sobre? ¬øPor qu√©?",
    "crumbs": [
      "Pr√°ctica",
      "Unidad 0"
    ]
  },
  {
    "objectID": "practica/practica_02.html",
    "href": "practica/practica_02.html",
    "title": "Pr√°ctica - Unidad 2",
    "section": "",
    "text": "Esta secci√≥n contiene ejercicios para trabajar con modelos basados en distribuciones conjugadas. En general, los ejercicios requieren c√°lculos o derivaciones que se pueden realizar a mano. Sin embargo, se promueve fuertemente el uso de la computadora y el lenguaje R para verificar los resultados, mostrar soluciones alternativas y ejercitar el uso de una herramienta que ser√° de suma utilidad a lo largo de todo el curso y de la vida profesional.\n\nüíªüìå ¬øQui√©n domina el posterior?\nPara cada una de las situaciones siguientes, se da una distribuci√≥n a priori Beta para el par√°metro \\(\\pi\\) de un ensayo binomial. Para cada escenario, identificar cu√°l de estos se cumple: el prior tiene mayor influencia en el posterior, los datos tienen m√°s influencia en el posteriori, o la creencia a priori y los datos influyen de manera similar en la creencia a posteriori\n\nPrior: \\(\\pi \\sim \\text{Beta}(1,4)\\), observaciones: \\(y=8\\) √©xitos en \\(n=10\\) ensayos.\nPrior: \\(\\pi \\sim \\text{Beta}(20,3)\\), observaciones: \\(y=0\\) √©xitos en \\(n=1\\) ensayos.\nPrior: \\(\\pi \\sim \\text{Beta}(4,2)\\), observaciones: \\(y=1\\) √©xitos en \\(n=3\\) ensayos.\nPrior: \\(\\pi \\sim \\text{Beta}(3,10)\\), observaciones: \\(y=10\\) √©xitos en \\(n=13\\) ensayos.\nPrior: \\(\\pi \\sim \\text{Beta}(20,2)\\), observaciones: \\(y=10\\) √©xitos en \\(n=200\\) ensayos.\n\nüíªüìå M√°s o menos certeza\nSea \\(\\theta\\) la proporci√≥n de personas que prefieren los perros a los gatos. Suponga que se elige una distribuci√≥n \\(\\text{Beta}(7,2)\\) para representar la creencia a priori\n\nDe acuerdo al prior ¬øcu√°les son valores razonables para \\(\\theta\\)?\nSe observa en una encuesta que \\(y=19\\) de \\(n=20\\) personas prefieren perros, ¬øc√≥mo cambia eso el conocimiento acerca de \\(\\theta\\)? Comenta en t√©rminos de la evoluci√≥n de la credibilidad media y del grado de certidumbre acerca de \\(\\theta\\).\nSi, en lugar de eso, se determina que \\(y=1\\) de \\(n=20\\) personas prefieren perros, ¬øc√≥mo cambia ahora el grado de credibilidad de los diferentes valores de \\(\\theta\\)?\nSi, en lugar de eso, se determina que \\(y=10\\) de \\(n=20\\) personas prefieren perros, ¬øc√≥mo cambia ahora el grado de credibilidad de los diferentes valores de \\(\\theta\\)?\n\nüíªüìå Pasito a pasito\nSea \\(\\theta\\) la probabilidad de √©xito de un evento de inter√©s. Sea \\(\\text{Beta}(2,3)\\) la distribuci√≥n a priori para \\(\\theta\\). Actualiza la distribuci√≥n a posteriori para \\(\\theta\\) secuencialmente:\n\nPrimera observaci√≥n: √©xito.\nSegunda observaci√≥n: √©xito.\nTercera observaci√≥n: fracaso.\nCuarta observaci√≥n: √©xito.\n\nüíªüìå Pasitos tras pasitos\nSea \\(\\theta\\) la probabilidad de √©xito de un evento de inter√©s. Sea \\(\\text{Beta}(2,3)\\) la distribuci√≥n a priori para \\(\\theta\\). Actualiza la distribuci√≥n a posteriori para \\(\\theta\\) secuencialmente dados conjuntos de cinco observaciones:\n\nPrimeras observaciones: tres √©xitos.\nSegundas observaciones: un √©xito.\nTerceras observaciones: un √©xito.\nCuartas observaci√≥nes: dos √©xitos.\n\nüíªüìå Diferentes observaciones, diferentes posteriors\nUna empresa que fabrica zapatillas est√° dise√±ando una publicidad para Instagram. Tres empleados comparten que la creencia a priori para \\(\\pi\\), la probabilidad de que un cliente haga clic en el anuncio cuando lo ve, puede expresarse con una distribuci√≥n \\(\\text{Beta}(4, 3)\\). No obstante, los tres empleados realizan tres experimentos distintos y por ende tienen acceso a datos diferentes. El primer empleado prueba el anuncio en una persona, que no cliquea el anuncio. El segundo lo prueba en 10 personas, de las cuales 3 cliquean el anuncio. El √∫ltimo lo prueba en 100 personas, 20 de las cuales cliquean el anuncio.\n\nDescriba el entendimiento a priori que los empleados tienen sobre \\(\\pi\\).\nEspecifique la distribuci√≥n a posteriori de cada uno de los empleados.\nCompare las distribuciones a posteriori de cada empleado.\n\nüíªüìå ¬øGalletitas o masitas?\nLa UNR re√∫ne cada a√±o a estudiantes provenientes de diferentes localidades. Cu√°ntas cuadras constituyen una distancia ‚Äúcaminable‚Äù suele ser motivo de discusi√≥n, entre otros. Pero la verdadera grieta est√° entre la denominaci√≥n galletitas versus masitas. Un rosarino pone un prior \\(\\text{Beta}(20,2)\\) a la proporci√≥n de personas que dicen galletitas, mientras que un oriundo de una localidad del interior dir√° que la credibilidad a priori es \\(\\text{Beta}(2,8)\\).\n\nResuma ambas distribuciones a priori y explique con sus palabras lo que implican.\nCon la informaci√≥n de sus compa√±eros de curso, actualice ambas distribuciones a priori. ¬øEs suficiente esa informaci√≥n para acercar ambas posturas?\n\nüíª üìå Mi primera huerta\nEn un campamento de verano para infantes se realizaron actividades que promueven el contacto con la naturaleza. Una de las tareas consisti√≥ en germinar semillas de tomate. Josefina plant√≥ 18 semillas en su almaciguera. Al cabo de 5 d√≠as, 8 de ellas germinaron. Sea \\(\\theta\\) la probabilidad de que una semilla de tomate germine y sea \\(\\text{Beta}(1, 1)\\) su distribuci√≥n a priori.\n\n¬øQu√© informaci√≥n implica el prior sobre la probabilidad de germinaci√≥n?\nCalcule la media y el desv√≠o est√°ndar a posteriori de \\(\\theta\\) a mano.\nVerifique el c√°lculo utilizando R.\nObtenga un intervalo de credibilidad del 95% para \\(\\theta\\).\n\n\n\n\n\n\nFoto de Markus Spiske en Unsplash\n\n\n\n\nüíª ¬øQui√©n dijo que el f√∫tbol siempre da revancha?\nEn la final del 2018 de la Copa del Mundo de la FIFA, Francia le gan√≥ a Croacia por 4 a 2. Considere que el n√∫mero de goles que un equipo hace en un partido puede modelizarse con una distribuci√≥n de Poisson. Suponga un par√°metro \\(\\lambda_F\\) para Francia y uno \\(\\lambda_C\\) para Croacia. Elija una distribuci√≥n Gamma a priori para el n√∫mero medio de goles por partido (es decir, \\(\\lambda_F\\) y \\(\\lambda_C\\) compartir√°n la distribuci√≥n a priori). \\(\\lambda_F\\) da una idea de la capacidad de Francia de hacer goles (\\(\\lambda_C\\) lo mismo, pero para Croacia).\nEn funci√≥n del resultado del partido, obtenga las distribuciones a posteriori de \\(\\lambda_F\\) y \\(\\lambda_C\\) y responda utilizando R:\n\n¬øQu√© probabilidad hay de que Francia fuera un mejor equipo que Croacia?\nSi el mismo partido se jugara de nuevo (cosa que los franceses en aquella oportunidad no pidieron), ¬øcu√°l es la probabilidad de que Francia ganara de nuevo? \n\n\n\n\n\n\n\nAyuda\n\n\n\n\n\nPlantee el modelo: \\[\n\\begin{aligned}\nY_F & \\sim \\text{Poisson}(\\lambda_F) \\\\\nY_C & \\sim \\text{Poisson}(\\lambda_C) \\\\\n\\lambda_F, \\lambda_C &\\sim \\text{Gamma}(\\alpha, \\beta)\n\\end{aligned}\n\\]\nutilizando valores razonables para \\(\\alpha\\) y \\(\\beta\\). Luego obtener el posterior y calcular \\(P(\\lambda_F &gt; \\lambda_C)\\). Para el segundo punto calcular \\(P(\\tilde{y}_F \\mid y_F) &gt; P(\\tilde{y}_C \\mid y_C)\\).\n\n\n\nMir√° si me va a pasar a mi‚Ä¶\nDurante el desarrollo de las vacunas contra el COVID-19, un medio anunci√≥ para una determinada vacuna una eficacia del 100%.\n\nEn la fase 3 de un ensayo en adolescentes de entre 12 y 15 a√±os, la vacuna BNT162b2 de Pfizer-BioNTech para el COVID-19 demostr√≥ una eficacia del 100% y una respuesta robusta de anticuerpos. El ensayo cl√≠nico involucr√≥ 2260 j√≥venes estadounidenses. En el ensayo, 18 casos de COVID-19 fueron observados en el grupo placebo (\\(n=1129\\)) y ninguno en el grupo vacunado (\\(n=1131\\))\n\nEs de esperar que, en un ensayo m√°s grande, aparezca alg√∫n caso de COVID-19 en el grupo que recibi√≥ el tratamiento. ¬øC√≥mo se estima la probabilidad de algo que a√∫n no ocurri√≥? \n\n\n\n\n\n\nAyuda\n\n\n\n\n\nSea \\(Y\\) laf cantidad de personas que reciben la vacuna y se contagian COVID-19. Plantee un modelo y obtenga el posterior de \\(\\theta_v\\), la probabilidad de que una persona vacunada se contagie de COVID-19. Luego obtenga la distribuci√≥n predictiva a posteriori utilizando un \\(N_v\\) grande y calcule \\(P(\\tilde{Y} &gt; 0)\\).\n\n\n\nüíª La regla del tres\nUna estudiante de Licenciatura en Estad√≠stica est√° releyendo su tesina antes de entregarla. Si en 20 p√°ginas encontrase 5 de ellas con al menos un typo, ser√≠a razonable estimar que la probabilidad de que una p√°gina contenga un typo es \\(\\frac{5}{20} = \\frac{1}{4}\\). ¬øPero qu√© ocurre si en 20 p√°ginas no encuentra ning√∫n error?\nVerifcar que, partiendo de un prior uniforme, \\(\\frac{3}{N}\\) es una estimaci√≥n razonable para \\(\\tau\\) (la probabilidad de que una p√°gina contenga un typo), siendo \\(N\\) el n√∫mero de p√°ginas. Para ello, grafique la distribuci√≥n a posteriori que se obtiene al haber observado 0 typos en 10 p√°ginas y luego halle la probabilidad de que \\(\\tau &lt; \\frac{3}{N}\\) para diferentes valores de \\(N\\) (10, 100, 1000, 10000). \n\n\n\n\n\n\nAyuda\n\n\n\n\n\nConsiderar \\(Y =\\) cantidad de p√°ginas con al menos un typo en \\(N\\) p√°ginas. Asumir que la ocurrencia de typos entre p√°ginas son independientes. El prior uniforme refiere a la probabilidad de cometer al menos un typo en una p√°gina, \\(\\tau\\). Para los diferentes valores de \\(N\\), primero encontrar el posterior de \\(\\tau\\) y luego calcular \\(P(\\tau &lt; \\frac{3}{N})\\). ¬øQu√© se observa?\n\n\n\n¬øTen√©s alguien para recomendar?\nUna colega quiere comprar un producto por Internet. Tres vendedores ofrecen el mismo producto al mismo precio. Un vendedor tiene 100% de evaluaciones positivas, con 10 reviews. Otro tiene 96% de evaluaciones positivas, con 50 reviews. El √∫ltimo tiene 90% de comentarios positivos, con 200 evaluaciones. ¬øCu√°l de los tres vendedores le recomendar√≠as? \nüíªüìå Bichos\nUn bi√≥logo quiere determinar la densidad de un insecto en su regi√≥n. Su conocimiento a priori del n√∫mero promedio de insectos por unidad de √°rea (\\(\\text{m}^2\\)) se puede representar con una distribuci√≥n Gamma de media 0.50 y desv√≠o est√°ndar 0.25. En una investigaci√≥n en 20 \\(\\text{m}^2\\) de √°rea, se hallan 3, 2, 5, 1 y 2 insectos en los primeros 5 \\(\\text{m}^2\\) y ninguno en la fracci√≥n de tierra restante.\n\nHalle la distribuci√≥n a posteriori del n√∫mero medio de insectos por unidad de √°rea.\nHalle la distribuci√≥n predictiva a posteriori del n√∫mero de insectos que se espera encontrar en una exploraci√≥n de un √°rea de 10 \\(\\text{m}^2\\)\n\n\n\n\n\n\nUna gran variedad de insectosIm√°gen de Freepik\n\n\n\n\n\n\n\n\n\n\nAyuda\n\n\n\n\n\n\nUna distribuci√≥n conveniente para la cantidad de bichos por metro cuadrado dado que se sabe que su promedio tiene un prior Gamma es la Poisson. A partir de eso es sencillo obtener el posterior.\nPrimero resolverlo para 1 metro cuadrado, y luego para 10. Se puede hacer computacionalmente muestreando \\(\\lambda\\) del posterior y obteniendo muestras de la distribuci√≥n de la respuesta usando los valores muestreados de \\(\\lambda\\). Tambi√©n se puede resolver de manera te√≥rica calculando la integral de la distribuci√≥n predictiva a posteriori. Para eso tener en cuenta: \\[\n\\begin{aligned}\n\\Gamma(x + 1) &= x! \\\\\n\\int_0^{\\infty} x^b e^{-ax} dx &= \\frac{\\Gamma(b + 1)}{a ^ {b+1}}\n\\end{aligned}\n\\] y que la funci√≥n de masa de probabiliad la distribuci√≥n binomial negativa se puede escribir como: \\[\n\\begin{array}{ccc}\np(k, \\mid r, \\pi) = P(X = k) = \\displaystyle \\frac{\\Gamma(k + r)}{\\Gamma(k + 1) \\Gamma({r})}(1 - \\pi)^k \\pi^r,\n& k \\in \\{0, 1, 2, \\dots\\},\n& r &gt; 0\n\\end{array}\n\\]\n\n\n\n\nüìå Alter-ego\nEl profesor Caprista y el profesor Evangetto est√°n dando sus primeros cursos de Estad√≠stica Bayesiana. Sus colegas les dijeron que el puntaje promedio en un examen final, \\(\\mu\\), var√≠a normalmente a√±o a a√±o con media 8 y desv√≠o est√°ndar 0.4. Y adem√°s, que los puntajes individuales de los estudiantes \\(Y\\) var√≠an normalmente alrededor de \\(\\mu\\) con una desviaci√≥n est√°ndar de 0.4\n\n¬øCu√°l es la probabilidad a priori de que un estudiante se saque m√°s de 9 en un examen final?\nEl profesor Caprista toma el examen final y observa que sus 20 estudiantes obtuvieron una nota media de 8.6. Halle la distribuci√≥n a posteriori de \\(\\mu\\).\nEl profesor Evangetto toma el examen final y observa que sus 20 estudiantes obtuvieron una nota media de 8.2. Halle la distribuci√≥n a posteriori de \\(\\mu\\).\nCombine las notas de ambos ex√°menes para obtener la distribuci√≥n a posteriori de \\(\\mu\\)\n¬øCu√°l es la probabilidad a posteriori de que un estudiante se saque m√°s de 9 en un examen final?\n\nüìå Inferencia sobre una distribuci√≥n de Poisson\nLa distribuci√≥n de masa de probabilidad Poisson se define como\n\\[\n\\begin{array}{lcr}\n\\displaystyle p(x \\mid \\lambda) = \\frac{e^{-\\lambda}\\lambda^x}{x!} &\n\\text{con} &\nx \\in \\{0, 1, 2, \\cdots \\}\n\\end{array}\n\\]\ndonde \\(\\lambda &gt; 0\\) es la cantidad promedio de veces que ocurre el evento de inter√©s en un periodo o espacio determinado.\n\nDerive el estimador de m√°xima verosimilitud del par√°metro \\(\\lambda\\).\nDerive el posterior \\(p(\\lambda \\mid \\boldsymbol{x})\\) suponiendo que el prior sobre \\(\\lambda\\) es \\(\\text{Gamma}(\\lambda \\mid \\alpha, \\beta)\\) con \\(p(\\lambda \\mid \\alpha, \\beta)  \\propto \\lambda^{\\alpha - 1}e ^ {-\\lambda \\beta}\\).\nAyuda: El posterior tambi√©n es una distribuci√≥n Gamma.\n¬øA qu√© valor tiende la media a posteriori cuando \\(\\alpha \\to 0\\) y \\(\\beta \\to 0\\)?\nRecuerde que la media de una distribuci√≥n \\(\\text{Gamma}(\\alpha, \\beta)\\) es \\(\\alpha/\\beta\\). \n\nüìå El modelo Gamma-Poisson\nSea \\(\\lambda\\) la tasa de mensajes de WhatsApp que una persona recibe en una hora. Suponga inicialmente que se cree que la tasa de mensajes por hora tiene media 5 con desv√≠o est√°ndar de 0.25 mensajes.\n\nElija una distribuci√≥n Gamma que represente adecuadamente lo que se cree acerca de \\(\\lambda\\)\n¬øCu√°l es la probabilidad a priori de que la tasa de mensajes sea mayor a 10?\n¬øCu√°ntos mensajes se espera que reciba una persona en promedio en una hora?\n\nSe sondea a un grupo de seis personas que recibieron 7, 3, 8, 9, 10 y 12 mensajes en la √∫ltima hora.\n\nGraficar la funci√≥n de verosimilitud.\nDeterminar la distribuci√≥n a posteriori de \\(\\lambda\\).\n¬øCu√°l es la probabilidad a posteriori de que la tasa de mensajes sea mayor a 10?\n¬øCu√°ntos mensajes se espera ahora que reciba una persona en promedio en una hora?\n\nüß© üìå Inferencia sobre una distribuci√≥n Uniforme\nConsidere una distribuci√≥n \\(\\text{Uniforme}(0, \\theta)\\). La funci√≥n de densidad de probabilidad es\n\\[\np(x) = \\frac{1}{\\theta}\\mathbb{I}(x \\in [0, \\theta])\n\\]\nSea \\(\\boldsymbol{X} = (X_1,..., X_n)\\) un vector de \\(n\\) variables aleatorias independientes e id√©nticamente distribuidas seg√∫n \\(p(x)\\)\nInferencia m√°ximo-veros√≠mil\n\n¬øCu√°l es el estimador m√°ximo veros√≠mil de \\(\\theta\\) (ll√°melo \\(\\hat{\\theta}_{\\text{MV}}\\))?\n¬øQu√© probabilidad le asigna el modelo a una nueva observaci√≥n \\(x_{n + 1}\\) usando \\(\\hat{\\theta}_{\\text{MV}}\\)?\n¬øObserva alg√∫n problema con el resultado anterior? Si es as√≠, sugiera una alternativa mejor.\n\nInferencia Bayesiana\nEl prior conjugado de la distribuci√≥n uniforme es la distribuci√≥n de Pareto.\nSi \\(X \\sim \\text{Pareto}(\\alpha, m)\\), luego \\[\np(x \\mid \\alpha, m) = \\frac{\\alpha m^\\alpha}{x^{\\alpha+1}} \\mathbb{I}(x \\ge m)\n\\]\nSi el prior es una distribuci√≥n de Pareto, la distribuci√≥n conjunta de \\(\\theta\\) y \\(\\boldsymbol{X} = (X_1,..., X_n)\\) es \\[\np(\\theta, \\boldsymbol{X})\n    = \\frac{\\alpha m^\\alpha}{\\theta^{n + \\alpha + 1}}\n    \\mathbb{I}(\\theta \\ge \\max(M_x, m))\n\\]\ndonde \\(M_x = \\text{max}(\\boldsymbol{X})\\). La evidencia es: \\[\n\\begin{aligned}\np(\\boldsymbol{X}) &= \\int_{M_x}^\\infty\n                 \\frac{\\alpha m^\\alpha}{\\theta^{n + \\alpha + 1}}\n                 d\\theta \\\\\n&=  \\begin{cases}\n    \\displaystyle \\frac{\\alpha}{(n+\\alpha)m^n} & \\text{Si } M_x \\le m \\\\\n    \\displaystyle \\frac{\\alpha m^\\alpha}{(n+\\alpha)M_x^{n+\\alpha}} & \\text{Si } M_x &gt; m \\\\\n    \\end{cases}\n\\end{aligned}\n\\]\nDerive el posterior y muestre que puede ser expresado como una distribuci√≥n de Pareto. \nüß© üìå Inferencia sobre una distribuci√≥n Exponencial\nEl tiempo de vida de una m√°quina en a√±os \\(X\\) es modelado con una distribuci√≥n exponencial con par√°metro \\(\\theta\\) desconocido. La funci√≥n de densidad es: \\[\n\\begin{array}{lcrr}\np(x \\mid \\theta) = \\theta e^{-\\theta x} & \\text{con} & x \\ge 0, & \\theta \\ge 0\n\\end{array}\n\\]\n\nMuestre que el estimador m√°ximo veros√≠mil (MV) es \\(\\hat{\\theta}_\\text{MV} = 1/\\bar{x}\\).\nSuponga que se observan los siguientes tiempos de vida de tres m√°quinas independientes \\(x_1 = 5\\), \\(x_2 = 6\\), \\(x_3 = 4\\). ¬øCu√°l es el valor del estimador MV?\nUna experta del √°rea sugiere que \\(\\theta\\) debe tener una distribuci√≥n a priori que tambi√©n sea exponencial. \\[\n\\begin{aligned}\n\\theta \\mid \\lambda &\\sim \\text{Exp}(\\lambda) \\\\\np(\\theta \\mid \\lambda) &= \\lambda e^{-\\lambda \\theta}\n\\end{aligned}\n\\] Elija un valor para el hiperpar√°metro \\(\\lambda\\) de la distribuci√≥n a priori tal que \\(\\mathbb{E}(\\theta) = 1/3\\). Utilice \\(\\lambda_0\\) para representar al valor.\n¬øCu√°l es el posterior \\(p(\\theta \\mid \\boldsymbol{x}, \\lambda_0)\\)?\n¬øEs la distribuci√≥n exponencial conjugada con un likelihood exponencial?\nEncuentre la media del posterior, \\(\\mathbb{E}(\\theta \\mid \\boldsymbol{x}, \\lambda_0)\\)\nExplique por que difieren el estimador MV de la media a posteriori. ¬øCu√°l es m√°s razonable en este ejemplo? \n\nüìå Otras distribuciones conjugadas (I)\nConsidere el siguiente modelo: \\[\n\\begin{array}{l}\nY \\mid \\theta \\sim \\text{Geom√©trica}(\\theta) \\\\\n\\theta \\sim \\text{Beta}(\\alpha, \\beta)\n\\end{array}\n\\]\ndonde la funci√≥n de densidad de la distribuci√≥n geom√©trica es \\(p(y \\mid \\theta) = \\theta (1 - \\theta)^{y-1}\\) para \\(y \\in {1,2,\\dots}\\)\n\n¬øQu√© deber√≠a ocurrir con la distribuci√≥n a posteriori de \\(\\theta\\) para poder afirmar que la distribuci√≥n geom√©trica es conjugada de la beta?\nDerive la distribuci√≥n a posteriori de \\(\\theta\\) y concluya.\n\nüìå Otras distribuciones conjugadas (II)\nConsidere el siguiente modelo:\n\\[\n\\begin{aligned}\nY\\mid\\theta &\\sim \\text{BinomialNeg}(\\theta, m) \\\\\n\\theta &\\sim \\text{Beta}(\\alpha,\\beta)\n\\end{aligned}\n\\]\ndonde la funci√≥n de densidad de la distribuci√≥n binomial negativa es: \\[\np(y \\mid \\theta, m) = {y+m-1 \\choose y} \\theta^{m} (1-\\theta)^y\n\\]\nObtenga la distribuci√≥n a posteriori de \\(\\theta\\).\nüß© Otras distribuciones conjugadas (III)\nConsidere el siguiente modelo: \\[\nY \\mid \\theta \\sim \\text{Exponencial}(\\theta) = \\text{Gamma}(1,\\theta)\n\\]\ndonde la funci√≥n de densidad exponencial es \\(p(y \\mid \\theta) = \\theta e^{-\\theta y}\\).\nElija una distribuci√≥n a priori conjugada de la verosimilitud propuesta y obtenga la expresi√≥n para la distribuci√≥n de probabilidad a posteriori.",
    "crumbs": [
      "Pr√°ctica",
      "Unidad 2"
    ]
  },
  {
    "objectID": "practica/practica_02.html#modelos-de-distribuciones-conjugadas",
    "href": "practica/practica_02.html#modelos-de-distribuciones-conjugadas",
    "title": "Pr√°ctica - Unidad 2",
    "section": "",
    "text": "Esta secci√≥n contiene ejercicios para trabajar con modelos basados en distribuciones conjugadas. En general, los ejercicios requieren c√°lculos o derivaciones que se pueden realizar a mano. Sin embargo, se promueve fuertemente el uso de la computadora y el lenguaje R para verificar los resultados, mostrar soluciones alternativas y ejercitar el uso de una herramienta que ser√° de suma utilidad a lo largo de todo el curso y de la vida profesional.\n\nüíªüìå ¬øQui√©n domina el posterior?\nPara cada una de las situaciones siguientes, se da una distribuci√≥n a priori Beta para el par√°metro \\(\\pi\\) de un ensayo binomial. Para cada escenario, identificar cu√°l de estos se cumple: el prior tiene mayor influencia en el posterior, los datos tienen m√°s influencia en el posteriori, o la creencia a priori y los datos influyen de manera similar en la creencia a posteriori\n\nPrior: \\(\\pi \\sim \\text{Beta}(1,4)\\), observaciones: \\(y=8\\) √©xitos en \\(n=10\\) ensayos.\nPrior: \\(\\pi \\sim \\text{Beta}(20,3)\\), observaciones: \\(y=0\\) √©xitos en \\(n=1\\) ensayos.\nPrior: \\(\\pi \\sim \\text{Beta}(4,2)\\), observaciones: \\(y=1\\) √©xitos en \\(n=3\\) ensayos.\nPrior: \\(\\pi \\sim \\text{Beta}(3,10)\\), observaciones: \\(y=10\\) √©xitos en \\(n=13\\) ensayos.\nPrior: \\(\\pi \\sim \\text{Beta}(20,2)\\), observaciones: \\(y=10\\) √©xitos en \\(n=200\\) ensayos.\n\nüíªüìå M√°s o menos certeza\nSea \\(\\theta\\) la proporci√≥n de personas que prefieren los perros a los gatos. Suponga que se elige una distribuci√≥n \\(\\text{Beta}(7,2)\\) para representar la creencia a priori\n\nDe acuerdo al prior ¬øcu√°les son valores razonables para \\(\\theta\\)?\nSe observa en una encuesta que \\(y=19\\) de \\(n=20\\) personas prefieren perros, ¬øc√≥mo cambia eso el conocimiento acerca de \\(\\theta\\)? Comenta en t√©rminos de la evoluci√≥n de la credibilidad media y del grado de certidumbre acerca de \\(\\theta\\).\nSi, en lugar de eso, se determina que \\(y=1\\) de \\(n=20\\) personas prefieren perros, ¬øc√≥mo cambia ahora el grado de credibilidad de los diferentes valores de \\(\\theta\\)?\nSi, en lugar de eso, se determina que \\(y=10\\) de \\(n=20\\) personas prefieren perros, ¬øc√≥mo cambia ahora el grado de credibilidad de los diferentes valores de \\(\\theta\\)?\n\nüíªüìå Pasito a pasito\nSea \\(\\theta\\) la probabilidad de √©xito de un evento de inter√©s. Sea \\(\\text{Beta}(2,3)\\) la distribuci√≥n a priori para \\(\\theta\\). Actualiza la distribuci√≥n a posteriori para \\(\\theta\\) secuencialmente:\n\nPrimera observaci√≥n: √©xito.\nSegunda observaci√≥n: √©xito.\nTercera observaci√≥n: fracaso.\nCuarta observaci√≥n: √©xito.\n\nüíªüìå Pasitos tras pasitos\nSea \\(\\theta\\) la probabilidad de √©xito de un evento de inter√©s. Sea \\(\\text{Beta}(2,3)\\) la distribuci√≥n a priori para \\(\\theta\\). Actualiza la distribuci√≥n a posteriori para \\(\\theta\\) secuencialmente dados conjuntos de cinco observaciones:\n\nPrimeras observaciones: tres √©xitos.\nSegundas observaciones: un √©xito.\nTerceras observaciones: un √©xito.\nCuartas observaci√≥nes: dos √©xitos.\n\nüíªüìå Diferentes observaciones, diferentes posteriors\nUna empresa que fabrica zapatillas est√° dise√±ando una publicidad para Instagram. Tres empleados comparten que la creencia a priori para \\(\\pi\\), la probabilidad de que un cliente haga clic en el anuncio cuando lo ve, puede expresarse con una distribuci√≥n \\(\\text{Beta}(4, 3)\\). No obstante, los tres empleados realizan tres experimentos distintos y por ende tienen acceso a datos diferentes. El primer empleado prueba el anuncio en una persona, que no cliquea el anuncio. El segundo lo prueba en 10 personas, de las cuales 3 cliquean el anuncio. El √∫ltimo lo prueba en 100 personas, 20 de las cuales cliquean el anuncio.\n\nDescriba el entendimiento a priori que los empleados tienen sobre \\(\\pi\\).\nEspecifique la distribuci√≥n a posteriori de cada uno de los empleados.\nCompare las distribuciones a posteriori de cada empleado.\n\nüíªüìå ¬øGalletitas o masitas?\nLa UNR re√∫ne cada a√±o a estudiantes provenientes de diferentes localidades. Cu√°ntas cuadras constituyen una distancia ‚Äúcaminable‚Äù suele ser motivo de discusi√≥n, entre otros. Pero la verdadera grieta est√° entre la denominaci√≥n galletitas versus masitas. Un rosarino pone un prior \\(\\text{Beta}(20,2)\\) a la proporci√≥n de personas que dicen galletitas, mientras que un oriundo de una localidad del interior dir√° que la credibilidad a priori es \\(\\text{Beta}(2,8)\\).\n\nResuma ambas distribuciones a priori y explique con sus palabras lo que implican.\nCon la informaci√≥n de sus compa√±eros de curso, actualice ambas distribuciones a priori. ¬øEs suficiente esa informaci√≥n para acercar ambas posturas?\n\nüíª üìå Mi primera huerta\nEn un campamento de verano para infantes se realizaron actividades que promueven el contacto con la naturaleza. Una de las tareas consisti√≥ en germinar semillas de tomate. Josefina plant√≥ 18 semillas en su almaciguera. Al cabo de 5 d√≠as, 8 de ellas germinaron. Sea \\(\\theta\\) la probabilidad de que una semilla de tomate germine y sea \\(\\text{Beta}(1, 1)\\) su distribuci√≥n a priori.\n\n¬øQu√© informaci√≥n implica el prior sobre la probabilidad de germinaci√≥n?\nCalcule la media y el desv√≠o est√°ndar a posteriori de \\(\\theta\\) a mano.\nVerifique el c√°lculo utilizando R.\nObtenga un intervalo de credibilidad del 95% para \\(\\theta\\).\n\n\n\n\n\n\nFoto de Markus Spiske en Unsplash\n\n\n\n\nüíª ¬øQui√©n dijo que el f√∫tbol siempre da revancha?\nEn la final del 2018 de la Copa del Mundo de la FIFA, Francia le gan√≥ a Croacia por 4 a 2. Considere que el n√∫mero de goles que un equipo hace en un partido puede modelizarse con una distribuci√≥n de Poisson. Suponga un par√°metro \\(\\lambda_F\\) para Francia y uno \\(\\lambda_C\\) para Croacia. Elija una distribuci√≥n Gamma a priori para el n√∫mero medio de goles por partido (es decir, \\(\\lambda_F\\) y \\(\\lambda_C\\) compartir√°n la distribuci√≥n a priori). \\(\\lambda_F\\) da una idea de la capacidad de Francia de hacer goles (\\(\\lambda_C\\) lo mismo, pero para Croacia).\nEn funci√≥n del resultado del partido, obtenga las distribuciones a posteriori de \\(\\lambda_F\\) y \\(\\lambda_C\\) y responda utilizando R:\n\n¬øQu√© probabilidad hay de que Francia fuera un mejor equipo que Croacia?\nSi el mismo partido se jugara de nuevo (cosa que los franceses en aquella oportunidad no pidieron), ¬øcu√°l es la probabilidad de que Francia ganara de nuevo? \n\n\n\n\n\n\n\nAyuda\n\n\n\n\n\nPlantee el modelo: \\[\n\\begin{aligned}\nY_F & \\sim \\text{Poisson}(\\lambda_F) \\\\\nY_C & \\sim \\text{Poisson}(\\lambda_C) \\\\\n\\lambda_F, \\lambda_C &\\sim \\text{Gamma}(\\alpha, \\beta)\n\\end{aligned}\n\\]\nutilizando valores razonables para \\(\\alpha\\) y \\(\\beta\\). Luego obtener el posterior y calcular \\(P(\\lambda_F &gt; \\lambda_C)\\). Para el segundo punto calcular \\(P(\\tilde{y}_F \\mid y_F) &gt; P(\\tilde{y}_C \\mid y_C)\\).\n\n\n\nMir√° si me va a pasar a mi‚Ä¶\nDurante el desarrollo de las vacunas contra el COVID-19, un medio anunci√≥ para una determinada vacuna una eficacia del 100%.\n\nEn la fase 3 de un ensayo en adolescentes de entre 12 y 15 a√±os, la vacuna BNT162b2 de Pfizer-BioNTech para el COVID-19 demostr√≥ una eficacia del 100% y una respuesta robusta de anticuerpos. El ensayo cl√≠nico involucr√≥ 2260 j√≥venes estadounidenses. En el ensayo, 18 casos de COVID-19 fueron observados en el grupo placebo (\\(n=1129\\)) y ninguno en el grupo vacunado (\\(n=1131\\))\n\nEs de esperar que, en un ensayo m√°s grande, aparezca alg√∫n caso de COVID-19 en el grupo que recibi√≥ el tratamiento. ¬øC√≥mo se estima la probabilidad de algo que a√∫n no ocurri√≥? \n\n\n\n\n\n\nAyuda\n\n\n\n\n\nSea \\(Y\\) laf cantidad de personas que reciben la vacuna y se contagian COVID-19. Plantee un modelo y obtenga el posterior de \\(\\theta_v\\), la probabilidad de que una persona vacunada se contagie de COVID-19. Luego obtenga la distribuci√≥n predictiva a posteriori utilizando un \\(N_v\\) grande y calcule \\(P(\\tilde{Y} &gt; 0)\\).\n\n\n\nüíª La regla del tres\nUna estudiante de Licenciatura en Estad√≠stica est√° releyendo su tesina antes de entregarla. Si en 20 p√°ginas encontrase 5 de ellas con al menos un typo, ser√≠a razonable estimar que la probabilidad de que una p√°gina contenga un typo es \\(\\frac{5}{20} = \\frac{1}{4}\\). ¬øPero qu√© ocurre si en 20 p√°ginas no encuentra ning√∫n error?\nVerifcar que, partiendo de un prior uniforme, \\(\\frac{3}{N}\\) es una estimaci√≥n razonable para \\(\\tau\\) (la probabilidad de que una p√°gina contenga un typo), siendo \\(N\\) el n√∫mero de p√°ginas. Para ello, grafique la distribuci√≥n a posteriori que se obtiene al haber observado 0 typos en 10 p√°ginas y luego halle la probabilidad de que \\(\\tau &lt; \\frac{3}{N}\\) para diferentes valores de \\(N\\) (10, 100, 1000, 10000). \n\n\n\n\n\n\nAyuda\n\n\n\n\n\nConsiderar \\(Y =\\) cantidad de p√°ginas con al menos un typo en \\(N\\) p√°ginas. Asumir que la ocurrencia de typos entre p√°ginas son independientes. El prior uniforme refiere a la probabilidad de cometer al menos un typo en una p√°gina, \\(\\tau\\). Para los diferentes valores de \\(N\\), primero encontrar el posterior de \\(\\tau\\) y luego calcular \\(P(\\tau &lt; \\frac{3}{N})\\). ¬øQu√© se observa?\n\n\n\n¬øTen√©s alguien para recomendar?\nUna colega quiere comprar un producto por Internet. Tres vendedores ofrecen el mismo producto al mismo precio. Un vendedor tiene 100% de evaluaciones positivas, con 10 reviews. Otro tiene 96% de evaluaciones positivas, con 50 reviews. El √∫ltimo tiene 90% de comentarios positivos, con 200 evaluaciones. ¬øCu√°l de los tres vendedores le recomendar√≠as? \nüíªüìå Bichos\nUn bi√≥logo quiere determinar la densidad de un insecto en su regi√≥n. Su conocimiento a priori del n√∫mero promedio de insectos por unidad de √°rea (\\(\\text{m}^2\\)) se puede representar con una distribuci√≥n Gamma de media 0.50 y desv√≠o est√°ndar 0.25. En una investigaci√≥n en 20 \\(\\text{m}^2\\) de √°rea, se hallan 3, 2, 5, 1 y 2 insectos en los primeros 5 \\(\\text{m}^2\\) y ninguno en la fracci√≥n de tierra restante.\n\nHalle la distribuci√≥n a posteriori del n√∫mero medio de insectos por unidad de √°rea.\nHalle la distribuci√≥n predictiva a posteriori del n√∫mero de insectos que se espera encontrar en una exploraci√≥n de un √°rea de 10 \\(\\text{m}^2\\)\n\n\n\n\n\n\nUna gran variedad de insectosIm√°gen de Freepik\n\n\n\n\n\n\n\n\n\n\nAyuda\n\n\n\n\n\n\nUna distribuci√≥n conveniente para la cantidad de bichos por metro cuadrado dado que se sabe que su promedio tiene un prior Gamma es la Poisson. A partir de eso es sencillo obtener el posterior.\nPrimero resolverlo para 1 metro cuadrado, y luego para 10. Se puede hacer computacionalmente muestreando \\(\\lambda\\) del posterior y obteniendo muestras de la distribuci√≥n de la respuesta usando los valores muestreados de \\(\\lambda\\). Tambi√©n se puede resolver de manera te√≥rica calculando la integral de la distribuci√≥n predictiva a posteriori. Para eso tener en cuenta: \\[\n\\begin{aligned}\n\\Gamma(x + 1) &= x! \\\\\n\\int_0^{\\infty} x^b e^{-ax} dx &= \\frac{\\Gamma(b + 1)}{a ^ {b+1}}\n\\end{aligned}\n\\] y que la funci√≥n de masa de probabiliad la distribuci√≥n binomial negativa se puede escribir como: \\[\n\\begin{array}{ccc}\np(k, \\mid r, \\pi) = P(X = k) = \\displaystyle \\frac{\\Gamma(k + r)}{\\Gamma(k + 1) \\Gamma({r})}(1 - \\pi)^k \\pi^r,\n& k \\in \\{0, 1, 2, \\dots\\},\n& r &gt; 0\n\\end{array}\n\\]\n\n\n\n\nüìå Alter-ego\nEl profesor Caprista y el profesor Evangetto est√°n dando sus primeros cursos de Estad√≠stica Bayesiana. Sus colegas les dijeron que el puntaje promedio en un examen final, \\(\\mu\\), var√≠a normalmente a√±o a a√±o con media 8 y desv√≠o est√°ndar 0.4. Y adem√°s, que los puntajes individuales de los estudiantes \\(Y\\) var√≠an normalmente alrededor de \\(\\mu\\) con una desviaci√≥n est√°ndar de 0.4\n\n¬øCu√°l es la probabilidad a priori de que un estudiante se saque m√°s de 9 en un examen final?\nEl profesor Caprista toma el examen final y observa que sus 20 estudiantes obtuvieron una nota media de 8.6. Halle la distribuci√≥n a posteriori de \\(\\mu\\).\nEl profesor Evangetto toma el examen final y observa que sus 20 estudiantes obtuvieron una nota media de 8.2. Halle la distribuci√≥n a posteriori de \\(\\mu\\).\nCombine las notas de ambos ex√°menes para obtener la distribuci√≥n a posteriori de \\(\\mu\\)\n¬øCu√°l es la probabilidad a posteriori de que un estudiante se saque m√°s de 9 en un examen final?\n\nüìå Inferencia sobre una distribuci√≥n de Poisson\nLa distribuci√≥n de masa de probabilidad Poisson se define como\n\\[\n\\begin{array}{lcr}\n\\displaystyle p(x \\mid \\lambda) = \\frac{e^{-\\lambda}\\lambda^x}{x!} &\n\\text{con} &\nx \\in \\{0, 1, 2, \\cdots \\}\n\\end{array}\n\\]\ndonde \\(\\lambda &gt; 0\\) es la cantidad promedio de veces que ocurre el evento de inter√©s en un periodo o espacio determinado.\n\nDerive el estimador de m√°xima verosimilitud del par√°metro \\(\\lambda\\).\nDerive el posterior \\(p(\\lambda \\mid \\boldsymbol{x})\\) suponiendo que el prior sobre \\(\\lambda\\) es \\(\\text{Gamma}(\\lambda \\mid \\alpha, \\beta)\\) con \\(p(\\lambda \\mid \\alpha, \\beta)  \\propto \\lambda^{\\alpha - 1}e ^ {-\\lambda \\beta}\\).\nAyuda: El posterior tambi√©n es una distribuci√≥n Gamma.\n¬øA qu√© valor tiende la media a posteriori cuando \\(\\alpha \\to 0\\) y \\(\\beta \\to 0\\)?\nRecuerde que la media de una distribuci√≥n \\(\\text{Gamma}(\\alpha, \\beta)\\) es \\(\\alpha/\\beta\\). \n\nüìå El modelo Gamma-Poisson\nSea \\(\\lambda\\) la tasa de mensajes de WhatsApp que una persona recibe en una hora. Suponga inicialmente que se cree que la tasa de mensajes por hora tiene media 5 con desv√≠o est√°ndar de 0.25 mensajes.\n\nElija una distribuci√≥n Gamma que represente adecuadamente lo que se cree acerca de \\(\\lambda\\)\n¬øCu√°l es la probabilidad a priori de que la tasa de mensajes sea mayor a 10?\n¬øCu√°ntos mensajes se espera que reciba una persona en promedio en una hora?\n\nSe sondea a un grupo de seis personas que recibieron 7, 3, 8, 9, 10 y 12 mensajes en la √∫ltima hora.\n\nGraficar la funci√≥n de verosimilitud.\nDeterminar la distribuci√≥n a posteriori de \\(\\lambda\\).\n¬øCu√°l es la probabilidad a posteriori de que la tasa de mensajes sea mayor a 10?\n¬øCu√°ntos mensajes se espera ahora que reciba una persona en promedio en una hora?\n\nüß© üìå Inferencia sobre una distribuci√≥n Uniforme\nConsidere una distribuci√≥n \\(\\text{Uniforme}(0, \\theta)\\). La funci√≥n de densidad de probabilidad es\n\\[\np(x) = \\frac{1}{\\theta}\\mathbb{I}(x \\in [0, \\theta])\n\\]\nSea \\(\\boldsymbol{X} = (X_1,..., X_n)\\) un vector de \\(n\\) variables aleatorias independientes e id√©nticamente distribuidas seg√∫n \\(p(x)\\)\nInferencia m√°ximo-veros√≠mil\n\n¬øCu√°l es el estimador m√°ximo veros√≠mil de \\(\\theta\\) (ll√°melo \\(\\hat{\\theta}_{\\text{MV}}\\))?\n¬øQu√© probabilidad le asigna el modelo a una nueva observaci√≥n \\(x_{n + 1}\\) usando \\(\\hat{\\theta}_{\\text{MV}}\\)?\n¬øObserva alg√∫n problema con el resultado anterior? Si es as√≠, sugiera una alternativa mejor.\n\nInferencia Bayesiana\nEl prior conjugado de la distribuci√≥n uniforme es la distribuci√≥n de Pareto.\nSi \\(X \\sim \\text{Pareto}(\\alpha, m)\\), luego \\[\np(x \\mid \\alpha, m) = \\frac{\\alpha m^\\alpha}{x^{\\alpha+1}} \\mathbb{I}(x \\ge m)\n\\]\nSi el prior es una distribuci√≥n de Pareto, la distribuci√≥n conjunta de \\(\\theta\\) y \\(\\boldsymbol{X} = (X_1,..., X_n)\\) es \\[\np(\\theta, \\boldsymbol{X})\n    = \\frac{\\alpha m^\\alpha}{\\theta^{n + \\alpha + 1}}\n    \\mathbb{I}(\\theta \\ge \\max(M_x, m))\n\\]\ndonde \\(M_x = \\text{max}(\\boldsymbol{X})\\). La evidencia es: \\[\n\\begin{aligned}\np(\\boldsymbol{X}) &= \\int_{M_x}^\\infty\n                 \\frac{\\alpha m^\\alpha}{\\theta^{n + \\alpha + 1}}\n                 d\\theta \\\\\n&=  \\begin{cases}\n    \\displaystyle \\frac{\\alpha}{(n+\\alpha)m^n} & \\text{Si } M_x \\le m \\\\\n    \\displaystyle \\frac{\\alpha m^\\alpha}{(n+\\alpha)M_x^{n+\\alpha}} & \\text{Si } M_x &gt; m \\\\\n    \\end{cases}\n\\end{aligned}\n\\]\nDerive el posterior y muestre que puede ser expresado como una distribuci√≥n de Pareto. \nüß© üìå Inferencia sobre una distribuci√≥n Exponencial\nEl tiempo de vida de una m√°quina en a√±os \\(X\\) es modelado con una distribuci√≥n exponencial con par√°metro \\(\\theta\\) desconocido. La funci√≥n de densidad es: \\[\n\\begin{array}{lcrr}\np(x \\mid \\theta) = \\theta e^{-\\theta x} & \\text{con} & x \\ge 0, & \\theta \\ge 0\n\\end{array}\n\\]\n\nMuestre que el estimador m√°ximo veros√≠mil (MV) es \\(\\hat{\\theta}_\\text{MV} = 1/\\bar{x}\\).\nSuponga que se observan los siguientes tiempos de vida de tres m√°quinas independientes \\(x_1 = 5\\), \\(x_2 = 6\\), \\(x_3 = 4\\). ¬øCu√°l es el valor del estimador MV?\nUna experta del √°rea sugiere que \\(\\theta\\) debe tener una distribuci√≥n a priori que tambi√©n sea exponencial. \\[\n\\begin{aligned}\n\\theta \\mid \\lambda &\\sim \\text{Exp}(\\lambda) \\\\\np(\\theta \\mid \\lambda) &= \\lambda e^{-\\lambda \\theta}\n\\end{aligned}\n\\] Elija un valor para el hiperpar√°metro \\(\\lambda\\) de la distribuci√≥n a priori tal que \\(\\mathbb{E}(\\theta) = 1/3\\). Utilice \\(\\lambda_0\\) para representar al valor.\n¬øCu√°l es el posterior \\(p(\\theta \\mid \\boldsymbol{x}, \\lambda_0)\\)?\n¬øEs la distribuci√≥n exponencial conjugada con un likelihood exponencial?\nEncuentre la media del posterior, \\(\\mathbb{E}(\\theta \\mid \\boldsymbol{x}, \\lambda_0)\\)\nExplique por que difieren el estimador MV de la media a posteriori. ¬øCu√°l es m√°s razonable en este ejemplo? \n\nüìå Otras distribuciones conjugadas (I)\nConsidere el siguiente modelo: \\[\n\\begin{array}{l}\nY \\mid \\theta \\sim \\text{Geom√©trica}(\\theta) \\\\\n\\theta \\sim \\text{Beta}(\\alpha, \\beta)\n\\end{array}\n\\]\ndonde la funci√≥n de densidad de la distribuci√≥n geom√©trica es \\(p(y \\mid \\theta) = \\theta (1 - \\theta)^{y-1}\\) para \\(y \\in {1,2,\\dots}\\)\n\n¬øQu√© deber√≠a ocurrir con la distribuci√≥n a posteriori de \\(\\theta\\) para poder afirmar que la distribuci√≥n geom√©trica es conjugada de la beta?\nDerive la distribuci√≥n a posteriori de \\(\\theta\\) y concluya.\n\nüìå Otras distribuciones conjugadas (II)\nConsidere el siguiente modelo:\n\\[\n\\begin{aligned}\nY\\mid\\theta &\\sim \\text{BinomialNeg}(\\theta, m) \\\\\n\\theta &\\sim \\text{Beta}(\\alpha,\\beta)\n\\end{aligned}\n\\]\ndonde la funci√≥n de densidad de la distribuci√≥n binomial negativa es: \\[\np(y \\mid \\theta, m) = {y+m-1 \\choose y} \\theta^{m} (1-\\theta)^y\n\\]\nObtenga la distribuci√≥n a posteriori de \\(\\theta\\).\nüß© Otras distribuciones conjugadas (III)\nConsidere el siguiente modelo: \\[\nY \\mid \\theta \\sim \\text{Exponencial}(\\theta) = \\text{Gamma}(1,\\theta)\n\\]\ndonde la funci√≥n de densidad exponencial es \\(p(y \\mid \\theta) = \\theta e^{-\\theta y}\\).\nElija una distribuci√≥n a priori conjugada de la verosimilitud propuesta y obtenga la expresi√≥n para la distribuci√≥n de probabilidad a posteriori.",
    "crumbs": [
      "Pr√°ctica",
      "Unidad 2"
    ]
  },
  {
    "objectID": "practica/practica_02.html#simulaciones",
    "href": "practica/practica_02.html#simulaciones",
    "title": "Pr√°ctica - Unidad 2",
    "section": "üíª Simulaciones",
    "text": "üíª Simulaciones\nA diferencia de la secci√≥n anterior, que requiere resolver los ejercicios a mano y promueve el uso de la computadora y R de manera complementaria, esta secci√≥n contiene ejercicios que deben ser resueltos mediante t√©cnicas de simulaci√≥n implementadas en R. Es posible que en algunos casos tambi√©n se pueda obtener una soluci√≥n anal√≠tica. En estos casos, puede resultar de utilidad obtener tambi√©n una soluci√≥n a mano para validar el resultado, evaluar el nivel de dificultad y ver que tan intuitivo resultan ambos enfoques.\n\nüìå Entrada en calor\nPara cada una de las siguientes situaciones, hallar los intervalos centrales de credibilidad.\n\nIntervalo del 95% para \\(\\pi\\) siendo \\(\\pi\\mid \\boldsymbol{y} \\sim \\text{Beta}(4,5)\\).\nIntervalo del 60% para \\(\\pi\\) siendo \\(\\pi\\mid \\boldsymbol{y} \\sim \\text{Beta}(4,5)\\).\nIntervalo del 89% para \\(\\lambda\\) siendo \\(\\lambda\\mid \\boldsymbol{y} \\sim \\text{Gamma}(1,8)\\).\nIntervalo del 95% para \\(\\lambda\\) siendo \\(\\lambda\\mid \\boldsymbol{y} \\sim \\text{Gamma}(2,5)\\).\nIntervalo del 81% para \\(\\mu\\) siendo \\(\\mu\\mid \\boldsymbol{y} \\sim \\mathcal{N}(10,2^2)\\).\nIntervalo del 99% para \\(\\pi\\) siendo \\(\\mu\\mid \\boldsymbol{y} \\sim \\mathcal{N}(-3,1^2)\\).\n\nPropiedades frecuentistas de inferencias bayesianas (!!)\nSea una variable \\(Y\\) tal que \\(Y \\mid \\theta \\sim \\text{Binomial}(n, \\theta)\\) y \\(\\theta \\sim \\text{Beta}(1/2, 1/2)\\). Mediante un estudio de simulaci√≥n calcule la cobertura emp√≠rica del intervalo de credibilidad del 95% con \\(n \\in \\{1, 5, 10, 25\\}\\) y \\(\\theta \\in \\{0.05, 0.10, \\dots, 0.50 \\}\\). Describa las propiedades frecuentistas del intervalo de credibilidad bayesiano. \nüìå ¬øTe preguntaste alguna vez cu√°l es la distribuci√≥n de un p-value?\nConsidere un problema conocido. Se desean comparar dos muestras independientes de tama√±o 5 utilizando un test t y utilizando el test de Mann-Whitney.\n\nConsidere el caso en que las dos muestras provienen de poblaciones con igual media y desv√≠o est√°ndar (supongamos normal de media nula y varianza unitaria). Si se repitiera muchas veces el proceso de tomar las muestras y realizar los tests, ¬øqu√© distribuci√≥n tendr√°n los p-values obtenidos para cada test?\nConsidere ahora el caso en que las dos muestras provienen de poblaciones con diferente media e igual desv√≠o est√°ndar (\\(\\mathcal{N}(0,1)\\) y \\(\\mathcal{N}(1,1)\\)). Si se repitiera muchas veces el proceso de tomar las muestras y realizar los tests, ¬øqu√© distribuci√≥n tendr√°n los p-values obtenidos para cada test?",
    "crumbs": [
      "Pr√°ctica",
      "Unidad 2"
    ]
  },
  {
    "objectID": "practica/practica_02.html#elecci√≥n-de-distribuciones-a-priori",
    "href": "practica/practica_02.html#elecci√≥n-de-distribuciones-a-priori",
    "title": "Pr√°ctica - Unidad 2",
    "section": "Elecci√≥n de distribuciones a priori",
    "text": "Elecci√≥n de distribuciones a priori\nEsta √∫tima secci√≥n de la pr√°ctica tiene como prop√≥sito ejercitar el uso de distribuciones de probabilidad como herramienta para reflejar informaci√≥n de un problema determinado.\n\nüìå Esbozar la distribuci√≥n de las siguientes variables\n\nEl n√∫mero de personas que compran caf√© en el bar de la facultad asumiendo distribuci√≥n de Poisson.\nEl peso de perros adultos en kilogramos asumiendo una distribuci√≥n Uniforme.\nEl peso de elefantes adultos en kilogramos asumiendo una distribuci√≥n Normal.\nEl peso de humanos adultos en libras asumiendo una distribuci√≥n asim√©trica hacia la derecha.\n\nüìå Verificar los resultados de manera computacional\nPara cada uno cada uno de los ejemplos del ejercicio anterior, graficar la distribuci√≥n usando R. Seleccionar los par√°metros que creas razonable, tomar una muestra aleatoria de tama√±o 1000 y graficar la distribuci√≥n en base a las muestras. ¬øSe refleja tu conocimiento del problema en la distribuci√≥n graficada? Si no, ajustar los par√°metros y repetir el proceso hasta que el resultado tenga concuerde con el conocimiento del problema.\nüíª üìå Hay que amigarse con de la distribuci√≥n Beta\nComparar las siguientes distribuciones a priori.\n\n\\(\\text{Beta}(0.5, 0.5)\\).\n\\(\\text{Beta}(1, 1)\\).\n\\(\\text{Beta}(1, 4)\\).\n\\(\\text{Beta}(5, 1.5)\\).\n\n\n¬øEn qu√© se diferencian?\n¬øCu√°l de ellas es m√°s informativa?\n¬øC√≥mo lo determinaste?\n\nüíª üìå Elicitaci√≥n de priors\nEn cada una de la situaciones que se describen debajo, ajustar manualmente los par√°metros de una distribuci√≥n \\(\\text{Beta}\\) para que reflejen la informaci√≥n brindada. No siempre existe una √∫nica respuesta correcta.\n\nUn amigo se postul√≥ para un empleo en LinkedIn y te dijo: ‚ÄúDir√≠a que tengo una chance del 40% de que me den el trabajo, pero no estoy seguro‚Äù. Cuando le preguntamos un poco mas, dijo que estima sus chances entre un 20% y un 60%.\nUn grupo de investigaci√≥n del CONICET desarroll√≥ una nueva prueba para una enfermedad bastante rara. El grupo espera que esta prueba arroje resultados correctos el 80% de las veces, con una varianza de 0.05.\nEl primo de un amigo es un apasionado de la pesca, lo practica muy seguido, y se dice ser muy bueno. Seg√∫n comenta tu amigo, en el asado de los Jueves el pescador dijo lo siguiente:\n\n\nSi tengo que hacer un promedio, 9 de cada 10 veces que salgo, vuelvo con algo. Pero √∫ltimamente te dir√≠a que siempre es 10 de 10. Estoy infalible. La verdad es que soy un crack de la pesca.\n\nAnte el descreimiento de algunos de los comensales supo reconocer que no siempre le fue tan bien:\n\nTuve mis malas rachas, pero nunca menos de 8 pescas de cada 10 salidas.\n\nüíª üìå Efecto de la parametrizaci√≥n\nSea \\(\\theta\\) la probabilidad de √©xito en un experimento binomial y sea \\(\\gamma = \\frac{\\theta}{1-\\theta}\\) la chance de √©xito. Utilizar simulaciones para explorar los efectos de las siguientes elecciones de distribuciones a priori\n\nSi \\(\\theta \\sim \\text{Uniforme}(0, 1)\\), ¬øcu√°l es el prior inducido para \\(\\gamma\\)?\nSi \\(\\theta \\sim \\text{Beta}(5, 5)\\), ¬øcu√°l es el prior inducido para \\(\\gamma\\)?\nSi \\(\\gamma \\sim \\text{Uniforme}(0, 100)\\), ¬øcu√°l es el prior inducido para \\(\\theta\\)?\nSi \\(\\gamma \\sim \\text{Gamma}(1, 1)\\), ¬øcu√°l es el prior inducido para \\(\\theta\\)?",
    "crumbs": [
      "Pr√°ctica",
      "Unidad 2"
    ]
  },
  {
    "objectID": "practica/practica_02.html#teor√≠a-de-la-decisi√≥n",
    "href": "practica/practica_02.html#teor√≠a-de-la-decisi√≥n",
    "title": "Pr√°ctica - Unidad 2",
    "section": "Teor√≠a de la Decisi√≥n",
    "text": "Teor√≠a de la Decisi√≥n\n\nDada la distribuci√≥n a posteriori \\(p(\\theta \\mid y)\\), probar que el estimador de Bayes que minimiza la funci√≥n de p√©rdida \\(L_1\\) es la mediana de \\(p(\\theta \\mid y)\\). \nSuponga que la distribuci√≥n a posteriori de \\(\\pi\\), \\(p(\\pi \\mid y)\\), es \\(\\text{Beta}(12,4)\\). Determine mediante simulaci√≥n el estimador que minimiza la p√©rdida de Huber: \\[\n\\mathcal{L}(\\delta,\\pi) =\n\\begin{cases}\n\\frac{1}{2} (\\pi - \\delta)^2 \\text{ si } |\\pi - \\delta| \\leq \\alpha \\\\\n\\alpha \\cdot (|\\pi - \\delta|-\\frac{1}{2}\\alpha) \\text{ en cualquier otro caso}\n\\end{cases}\n\\]",
    "crumbs": [
      "Pr√°ctica",
      "Unidad 2"
    ]
  },
  {
    "objectID": "practica/practica_02.html#otros",
    "href": "practica/practica_02.html#otros",
    "title": "Pr√°ctica - Unidad 2",
    "section": "Otros",
    "text": "Otros\n\n‚úçÔ∏è üìå Distribuci√≥n predictiva a posteriori\nConsidere un modelo \\(Y \\mid \\theta \\sim \\text{Binomial}(\\theta, n)\\), donde \\(\\theta\\) puede tomar valores discretos \\(0,\\ 0.1,\\ 0.2,\\ \\dots,\\ 1\\). Se realizaron inferencias sobre \\(\\theta\\) y se obtuvo la distribuci√≥n a posteriori que se muestra en la parte superior de la Figura¬†1.\nSe desea obtener la distribuci√≥n predictiva a posteriori para el n√∫mero de √©xitos \\(\\tilde{y}\\) en \\(n=5\\) nuevas realizaciones del experimento. Cada valor de \\(\\theta\\) da lugar a una posible distribuci√≥n de \\(\\tilde{y}\\) de acuerdo a la verosimilitud binomial, como se observa en la parte inferior de la Figura¬†1.\n\n\n\n\n\n\n\n\n\nFigura¬†1: Distribuci√≥n a posteriori y distribuci√≥n predictiva a posteriori\n\n\n\n\n\n\nCalcule \\(p(\\tilde{y}\\mid\\theta)\\) para cada valor posible de \\(\\theta\\) y compruebe que los gr√°ficos de la parte inferior de la Figura¬†1 son correctos.\nCombine los \\(p(\\tilde{y}\\mid\\theta)\\) ponderando por las probabilidades a posteriori de \\(\\theta\\), \\(p(\\theta\\mid y)\\) para obtener la distribuci√≥n predictiva a posteriori\nCompare la varianza de una de las \\(p(\\tilde{y}\\mid\\theta)\\) (por ejemplo, la de \\(\\theta=0.5\\)) con la varianza de la distribuci√≥n predictiva a posteriori ¬øqu√© observa?\nA partir de \\(p(\\theta\\mid y)\\) y de la verosimilitud binomial, obtenga muestras de \\(p(\\tilde{y}\\mid y)\\) y grafique su distribuci√≥n.",
    "crumbs": [
      "Pr√°ctica",
      "Unidad 2"
    ]
  },
  {
    "objectID": "trabajos_practicos/descripcion.html",
    "href": "trabajos_practicos/descripcion.html",
    "title": "Generalidades",
    "section": "",
    "text": "Para aprobar la materia es necesario completar tres trabajos pr√°cticos cortos. La denominaci√≥n cortos hace referencia a que los trabajos son guiados y las tareas a realizar est√°n delimitadas.\nLos trabajos pr√°cticos tienen como objetivo repasar y afianzar los conocimientos adquiridos durante las clases, adquirir pr√°ctica en la aplicaci√≥n de conceptos trabajados, mejorar las habilidades de programaci√≥n y el uso de R, e incorporar algunos conceptos complementarios.\nCada trabajo pr√°ctico ser√° presentado y discutido en clase. Se destinar√° una fracci√≥n de la clase a comenzar a pensar algunas de las actividades.\nLa fecha de entrega de cada trabajo pr√°ctico ser√° de dos semanas luego de la fecha de presentaci√≥n. Se podr√° entregar el trabajo pr√°ctico una semana despu√©s de la fecha de entrega con una penalizaci√≥n del 25% de la nota final.\nPara cada trabajo pr√°ctico, cada grupo deber√° entregar un informe en formato pdf donde se resuelvan las actividades propuestas. El informe debe estar obligatoriamente elaborado utilizando \\(\\mathrm{\\LaTeX{}}\\) (a trav√©s de Quarto, RMarkdown o alguna otra variante). Tener en cuenta que los apartados presentados en el enunciado del trabajo pr√°ctico constituyen una gu√≠a de actividades a resolver y no deben responderse uno a uno como si se tratara de un cuestionario. El informe deber√° permitir una lectura fluida de los resultados y an√°lisis presentados. Cuando la resoluci√≥n de una problem√°tica consista en una funci√≥n o porci√≥n de c√≥digo en R, el c√≥digo deber√° mostrarse en el informe.\nSe evaluar√°n los siguientes aspectos del informe: presentaci√≥n, redacci√≥n (claridad, coherencia y cohesi√≥n), est√©tica, resultados obtenidos, profundidad del an√°lisis.",
    "crumbs": [
      "Trabajos Pr√°cticos",
      "Generalidades"
    ]
  },
  {
    "objectID": "trabajos_practicos/01_tp1.html",
    "href": "trabajos_practicos/01_tp1.html",
    "title": "¬øLa gloria es el dinero?",
    "section": "",
    "text": "La idea de convertir unos pocos pesos en una fortuna con un solo clic tiene algo de magia, de esa que seduce con la promesa de √©xito instant√°neo. Masivo. ¬øQu√© puede salir mal? Total‚Ä¶ ‚ÄúEst√° todo bajo control, yo decido cu√°ndo frenar‚Äù. El problema es que, a veces de manera silenciosa, el juego compulsivo puede desencadenar problemas de largo plazo en la salud mental y el funcionamiento social de los individuos.\nLas apuestas en l√≠nea han ganado una creciente popularidad entre los adolescentes, impulsadas por la accesibilidad de plataformas digitales y la constante exposici√≥n a la publicidad en redes sociales y eventos deportivos. A esto se suma que se trata de una actividad escasamente regulada por el Estado y a la que cualquier persona puede acceder con relativa facilidad.\nEstas apuestas pueden dividirse en dos grandes categor√≠as: los casinos virtuales, que incluyen juegos como ruleta o blackjack, y las apuestas deportivas, donde los j√≥venes apuestan sobre el resultado de eventos deportivos, a menudo influenciados por pron√≥sticos y estrategias compartidas en comunidades en l√≠nea. Ambas modalidades presentan riesgos relevantes, ya que la facilidad para depositar dinero y la sensaci√≥n de control pueden llevar a un espiral de p√©rdidas y nuevas apuestas en busca de recuperar lo perdido.\n\n\n\n\n\nTanto los casinos virtuales como las apuestas deportivas se han vuelto m√°s populares en los √∫ltimos a√±os\n\n\n\n\nLa sensaci√≥n de ganar y perder plata genera adrenalina. La adolescencia, una etapa crucial de desarrollo de los seres humanos, es especialmente susceptible a la promesa del dinero f√°cil y a la gratificaci√≥n instant√°nea que pueden otorgar los juegos de apuestas, debido a la inmadurez de las √°reas cerebrales encargadas del control de impulsos, lo que incrementa la vulnerabilidad a comportamientos adictivos. A esto se suman factores sociales como la presi√≥n de grupo y la b√∫squeda de aceptaci√≥n, donde ganar una apuesta se convierte en una forma de demostrar conocimiento y obtener reputaci√≥n entre pares.\nLa ilusi√≥n de ganar dinero r√°pido, junto con la normalizaci√≥n de las apuestas a trav√©s de la publicidad, crea un c√≥ctel peligroso que atrapa a los j√≥venes en un ciclo de riesgo y compulsi√≥n. A esto se suma la aparici√≥n de gur√∫es financieros y esquemas piramidales que prometen riqueza r√°pida, donde quienes no participan son vistos como ingenuos o fracasados. Esta presi√≥n social y la promesa de ganancias f√°ciles refuerzan la participaci√≥n en un entorno cada vez m√°s riesgoso y manipulador.\nComo si todo esto fuera poco, indagar en un grupo sobre temas sensibles puede resultar complejo. Muchas personas, por verg√ºenza, miedo al juicio social o simplemente por desconfianza, optan por no responder con sinceridad a ciertas preguntas, por ejemplo en el marco de una encuesta. En el caso de las apuestas, admitir la frecuencia con la que se participa, las cantidades apostadas o las p√©rdidas sufridas puede generar resistencia a contestar con la verdad. Algunas personas evitan responder, mientras que otras pueden proporcionar respuestas err√≥neas de manera deliberada. Este fen√≥meno, conocido como sesgo de respuesta, representa un desaf√≠o para quienes buscan obtener datos fiables en encuestas y estudios sobre este tema.\nPartiendo de la premisa de que una forma de incrementar la cooperaci√≥n de los encuestados es garantizar la protecci√≥n de informaci√≥n sensible, una posible forma de mitigar este sesgo es la t√©cnica de respuesta aleatorizada, que permite que los encuestados respondan de manera m√°s sincera sin temor a ser identificados. En la t√©cnica de respuesta aleatorizada, se introduce una cuota de azar con el objetivo de preservar la privacidad de la persona que responde.\n\n\n\n\n\nEn la t√©cnica de respuesta aleatorizada, se introduce una cuota de azar al momento de que el encuestado responda\n\n\n\n\nNos centraremos en el problema de querer realizar inferencias sobre \\(\\pi_A\\), la proporci√≥n de estudiantes de una escuela que participan de apuestas deportivas en l√≠nea.\nEstudiaremos dos t√©cnicas de respuesta aleatorizada. En lugar de de responder directamente a la pregunta ‚Äú¬øParticipaste alguna vez en apuestas deportivas en l√≠nea?‚Äù, se le pide al encuestado que utilice un mecanismo aleatorio, como arrojar una moneda. Dependiendo del resultado de este mecanismo, el encuestado sigue ciertas instrucciones para responder. En las variantes que estudiaremos, por ejemplo, si sale cara, el estudiante contesta a la pregunta ‚Äú¬øParticipaste alguna vez en apuestas deportivas en l√≠nea?‚Äù mientras que si sale cruz, responde a otra pregunta. As√≠, como el investigador no conoce el resultado del mecanismo aleatorio no puede saber a qu√© pregunta est√° respondiendo el encuestado. No obstante, a partir de los resultados obtenidos para toda la muestra, pueden obtenerse conclusiones v√°lidas.\nEn el caso m√°s general, diremos que \\(p\\) es la probabilidad (conocida), debida al mecanismo aleatorio, de que el encuestado responda a la pregunta ‚Äú¬øParticipaste alguna vez en apuestas deportivas en l√≠nea?‚Äù mientras que \\((1-p)\\) es la probabilidad de que responda a la otra pregunta.",
    "crumbs": [
      "Trabajos Pr√°cticos",
      "TP 1: ¬øLa gloria es el dinero?"
    ]
  },
  {
    "objectID": "trabajos_practicos/01_tp1.html#introducci√≥n",
    "href": "trabajos_practicos/01_tp1.html#introducci√≥n",
    "title": "¬øLa gloria es el dinero?",
    "section": "",
    "text": "La idea de convertir unos pocos pesos en una fortuna con un solo clic tiene algo de magia, de esa que seduce con la promesa de √©xito instant√°neo. Masivo. ¬øQu√© puede salir mal? Total‚Ä¶ ‚ÄúEst√° todo bajo control, yo decido cu√°ndo frenar‚Äù. El problema es que, a veces de manera silenciosa, el juego compulsivo puede desencadenar problemas de largo plazo en la salud mental y el funcionamiento social de los individuos.\nLas apuestas en l√≠nea han ganado una creciente popularidad entre los adolescentes, impulsadas por la accesibilidad de plataformas digitales y la constante exposici√≥n a la publicidad en redes sociales y eventos deportivos. A esto se suma que se trata de una actividad escasamente regulada por el Estado y a la que cualquier persona puede acceder con relativa facilidad.\nEstas apuestas pueden dividirse en dos grandes categor√≠as: los casinos virtuales, que incluyen juegos como ruleta o blackjack, y las apuestas deportivas, donde los j√≥venes apuestan sobre el resultado de eventos deportivos, a menudo influenciados por pron√≥sticos y estrategias compartidas en comunidades en l√≠nea. Ambas modalidades presentan riesgos relevantes, ya que la facilidad para depositar dinero y la sensaci√≥n de control pueden llevar a un espiral de p√©rdidas y nuevas apuestas en busca de recuperar lo perdido.\n\n\n\n\n\nTanto los casinos virtuales como las apuestas deportivas se han vuelto m√°s populares en los √∫ltimos a√±os\n\n\n\n\nLa sensaci√≥n de ganar y perder plata genera adrenalina. La adolescencia, una etapa crucial de desarrollo de los seres humanos, es especialmente susceptible a la promesa del dinero f√°cil y a la gratificaci√≥n instant√°nea que pueden otorgar los juegos de apuestas, debido a la inmadurez de las √°reas cerebrales encargadas del control de impulsos, lo que incrementa la vulnerabilidad a comportamientos adictivos. A esto se suman factores sociales como la presi√≥n de grupo y la b√∫squeda de aceptaci√≥n, donde ganar una apuesta se convierte en una forma de demostrar conocimiento y obtener reputaci√≥n entre pares.\nLa ilusi√≥n de ganar dinero r√°pido, junto con la normalizaci√≥n de las apuestas a trav√©s de la publicidad, crea un c√≥ctel peligroso que atrapa a los j√≥venes en un ciclo de riesgo y compulsi√≥n. A esto se suma la aparici√≥n de gur√∫es financieros y esquemas piramidales que prometen riqueza r√°pida, donde quienes no participan son vistos como ingenuos o fracasados. Esta presi√≥n social y la promesa de ganancias f√°ciles refuerzan la participaci√≥n en un entorno cada vez m√°s riesgoso y manipulador.\nComo si todo esto fuera poco, indagar en un grupo sobre temas sensibles puede resultar complejo. Muchas personas, por verg√ºenza, miedo al juicio social o simplemente por desconfianza, optan por no responder con sinceridad a ciertas preguntas, por ejemplo en el marco de una encuesta. En el caso de las apuestas, admitir la frecuencia con la que se participa, las cantidades apostadas o las p√©rdidas sufridas puede generar resistencia a contestar con la verdad. Algunas personas evitan responder, mientras que otras pueden proporcionar respuestas err√≥neas de manera deliberada. Este fen√≥meno, conocido como sesgo de respuesta, representa un desaf√≠o para quienes buscan obtener datos fiables en encuestas y estudios sobre este tema.\nPartiendo de la premisa de que una forma de incrementar la cooperaci√≥n de los encuestados es garantizar la protecci√≥n de informaci√≥n sensible, una posible forma de mitigar este sesgo es la t√©cnica de respuesta aleatorizada, que permite que los encuestados respondan de manera m√°s sincera sin temor a ser identificados. En la t√©cnica de respuesta aleatorizada, se introduce una cuota de azar con el objetivo de preservar la privacidad de la persona que responde.\n\n\n\n\n\nEn la t√©cnica de respuesta aleatorizada, se introduce una cuota de azar al momento de que el encuestado responda\n\n\n\n\nNos centraremos en el problema de querer realizar inferencias sobre \\(\\pi_A\\), la proporci√≥n de estudiantes de una escuela que participan de apuestas deportivas en l√≠nea.\nEstudiaremos dos t√©cnicas de respuesta aleatorizada. En lugar de de responder directamente a la pregunta ‚Äú¬øParticipaste alguna vez en apuestas deportivas en l√≠nea?‚Äù, se le pide al encuestado que utilice un mecanismo aleatorio, como arrojar una moneda. Dependiendo del resultado de este mecanismo, el encuestado sigue ciertas instrucciones para responder. En las variantes que estudiaremos, por ejemplo, si sale cara, el estudiante contesta a la pregunta ‚Äú¬øParticipaste alguna vez en apuestas deportivas en l√≠nea?‚Äù mientras que si sale cruz, responde a otra pregunta. As√≠, como el investigador no conoce el resultado del mecanismo aleatorio no puede saber a qu√© pregunta est√° respondiendo el encuestado. No obstante, a partir de los resultados obtenidos para toda la muestra, pueden obtenerse conclusiones v√°lidas.\nEn el caso m√°s general, diremos que \\(p\\) es la probabilidad (conocida), debida al mecanismo aleatorio, de que el encuestado responda a la pregunta ‚Äú¬øParticipaste alguna vez en apuestas deportivas en l√≠nea?‚Äù mientras que \\((1-p)\\) es la probabilidad de que responda a la otra pregunta.",
    "crumbs": [
      "Trabajos Pr√°cticos",
      "TP 1: ¬øLa gloria es el dinero?"
    ]
  },
  {
    "objectID": "trabajos_practicos/01_tp1.html#t√©cnicas-de-respuesta-aleatorizada",
    "href": "trabajos_practicos/01_tp1.html#t√©cnicas-de-respuesta-aleatorizada",
    "title": "¬øLa gloria es el dinero?",
    "section": "T√©cnicas de respuesta aleatorizada",
    "text": "T√©cnicas de respuesta aleatorizada\n\nWarner\nUn mecanismo aleatorio selecciona la pregunta que se le presenta a la persona que responde. Con probabilidad \\(p\\) se le pregunta si es cierto que alguna vez particip√≥ en apuestas deportivas en l√≠nea (\\(Q_1\\)), mientras que con probabilidad \\((1-p)\\) se le pregunta si es cierto que nunca particip√≥ en apuestas deportivas en l√≠nea (\\(Q_2\\)). Es decir, en general, con probabilidad \\(p\\) se le pregunta si pertenece a una categor√≠a \\(A\\), mientras que con probabilidad \\((1-p)\\) se le pregunta si pertenece a la categor√≠a \\(A^c\\) (el complemento de \\(A\\)).\n\n\nGreenberg y otros\nUn mecanismo aleatorio selecciona la pregunta que se le presenta a la persona que responde. Con probabilidad \\(p\\) se le pregunta si es cierto que alguna vez particip√≥ en apuestas deportivas en l√≠nea (\\(Q_1\\)), mientras que con probabilidad \\((1-p)\\) se le pregunta si naci√≥ en un mes de 31 d√≠as (\\(Q_2\\)). Es decir, en general, con probabilidad \\(p\\) se le pregunta si pertenece a una categor√≠a \\(A\\), mientras que con probabilidad \\((1-p)\\) se le pregunta si pertenece a una categor√≠a \\(B\\) (no controversial o no pol√©mica).\nExisten dos versiones de esta modalidad, en relaci√≥n al conocimiento o desconocimiento de \\(\\pi_B\\), la proporci√≥n de elementos en la poblaci√≥n que pertenece a la categor√≠a \\(B\\). En el primer caso, se conoce de antemano \\(\\pi_B\\) (como en el caso presentado, en que \\(\\pi_B = \\frac{7}{12}\\)). Mientras que en el segundo caso, no se conoce \\(\\pi_B\\). Nos limitaremos al estudio de la primera situaci√≥n.",
    "crumbs": [
      "Trabajos Pr√°cticos",
      "TP 1: ¬øLa gloria es el dinero?"
    ]
  },
  {
    "objectID": "trabajos_practicos/01_tp1.html#actividades",
    "href": "trabajos_practicos/01_tp1.html#actividades",
    "title": "¬øLa gloria es el dinero?",
    "section": "Actividades",
    "text": "Actividades\nPara todo lo que sigue, tenga en consideraci√≥n lo siguiente:\n\nSe toma una muestra al azar con reemplazo de tama√±o \\(n=100\\) de una poblaci√≥n de tama√±o \\(N=1000\\).\nPara nuestros estudios comparativos, supondremos que el porcentaje de alumnos que apuesta (lo que se quisiera estimar) es 40%.\nCuando se les pregunta directamente si han hecho alguna vez apuestas, los estudiantes que s√≠ han apostado alguna vez mienten con probabilidad \\(\\mu\\).\nSi se utilizan t√©cnicas de respuesta aleatorizada, los estudiantes no mienten.\n\nComenzaremos realizando un estudio de simulaci√≥n para estudiar el efecto de la mentira en las estimaciones. En esta primera aproximaci√≥n, consideraremos que se realiza la pregunta directa.\n\nProponga un modelo bayesiano que, a partir de encuestar a \\(n\\) estudiantes, permita estimar \\(\\pi_a\\). Explique la elecci√≥n de la funci√≥n de verosimilitud y el prior e indique c√≥mse obtiene el posterior.\nUtilizando R, simule la obtenci√≥n de una muestra para el caso en que los estudiantes no mienten y para el caso en que los estudiantes mienten con tres niveles de mentira \\(\\mu\\) bajo, medio y alto. Compare los resultados de la inferencia.\nRealice ahora 1000 simulaciones y compare los resultados de las inferencias.\n\nConsidere ahora el caso del m√©todo propuesto por Warner:\n\nSeg√∫n este m√©todo, ¬øcu√°l es la probabilidad (llam√©mosla \\(\\lambda_W\\)) de que un estudiante responda que apuesta? ¬øcu√°l es la probabilidad de que un estudiante responda queno apuesta?\nA partir de lo anterior, proponga un modelo razonable sobre c√≥mo se generan los datos.\nConsidere un prior uniforme y halle el posterior exacto.\n\n\n\n\n\n\n\nAyuda\n\n\n\nEscriba el posterior dejando el denominador de la Regla de Bayes expresado como \\(Z\\). Luego, muestre que:\n\\[\nZ = \\frac{B(1 - p; y + 1, n - y + 1) - B(p; y + 1, n - y + 1)}{1 - 2p}\n\\]\ndonde \\(B(x; a, b) = \\int_0^x t^{a - 1} (1 - t)^{b - 1} dt\\) es la funci√≥n beta incompleta, \\(p\\) es la probabilidad con la que se hace la pregunta alternativa e \\(y\\) es la cantidad depersonas que responden que apuestan.\nPara resolver la integral, utilice el m√©todo de sustituci√≥n y h√°galo en t√©rminos de \\(\\lambda_W\\). No es tan terrible como parece.\n\n\n\nGrafique el posterior para diferentes valores de \\(p\\) y concluya.\n¬øQu√© pasar√≠a si el porcentaje de alumnos que apuesta fuera diferente al \\(40\\%\\)? Analice los resultados en funci√≥n de diferentes niveles de \\(p\\) y \\(\\pi_a\\).\nEscriba una funci√≥n de R que le permita realizar la inferencia (en forma aproximada) con un prior beta no necesariamente uniforme.\n\nPara el m√©todo propuesto por Greenberg:\n\n¬øCu√°l es la probabilidad \\(\\lambda_G\\) de que un estudiante responda que apuesta? ¬øy de que responda que no apuesta?\nEscriba una funci√≥n de R que le permita realizar la inferencia (en forma aproximada) con un prior beta no necesariamente uniforme.\n\nPara terminar, es hora de comparar todos los escenarios.\n\nUtilizando R, simule la obtenci√≥n de una muestra para el caso en que los estudiantes no mienten, mienten con tres niveles de mentira \\(\\mu\\) (bajo, medio y alto), se utiliza el m√©todo de Werner y se utiliza el m√©todo de Greenberg. Compare los resultados de las inferencias en cada caso.\nRealice ahora 1000 simulaciones y analice los resultados.",
    "crumbs": [
      "Trabajos Pr√°cticos",
      "TP 1: ¬øLa gloria es el dinero?"
    ]
  },
  {
    "objectID": "recursos/codigo/02_ostras.html",
    "href": "recursos/codigo/02_ostras.html",
    "title": "02 - ¬°Ostras! ¬°Estoy haciendo inferencia bayesiana!",
    "section": "",
    "text": "El siguiente programa muestra diferentes alternativas para obtener la densidad de la distribuci√≥n a posteriori en el ejercicio ¬°Ostras! ¬°Estoy haciendo inferencia bayesiana! de la Pr√°ctica 1.\nPara graficar la densidad a posteriori de \\(\\lambda\\) basta con ver que: \\[\n\\begin{array}{lr}\np(\\lambda \\mid \\boldsymbol{y})\n  \\propto p(\\lambda) p(\\boldsymbol{y} \\mid \\lambda)\n  \\propto p(\\boldsymbol{y} \\mid \\lambda),\n& \\lambda \\in (0, 100)\n\\end{array}\n\\]\npor ser el prior sobre \\(\\lambda\\) uniforme. Es decir, la forma del posterior queda completamente determinada por la forma de la funci√≥n de verosimilitud.\nTener presente: \\[\n\\begin{aligned}\np(\\boldsymbol{y} \\mid \\lambda) &= \\prod_{i=1}^n p(y_i \\mid \\lambda) \\\\\np(\\boldsymbol{y} \\mid \\lambda) &= \\prod_{i=1}^n {\\frac{e^{-\\lambda}\\lambda^{y_i}}{y_i!}} \\\\\n& = \\frac{e^{-n\\lambda}\\lambda^{\\sum_i y_i}}{\\prod_{i=1}^{n}{y_i!}} \\\\\n\\log p(\\boldsymbol{y} \\mid \\lambda) &= \\sum_{i=1}^{n} \\log(p(y_i \\mid \\lambda))\n\\end{aligned}\n\\]\n\nlibrary(ggplot2)\n\ncolores &lt;- c(\"#f08533\", \"#3b78b0\", \"#d1352c\")\n\n# Valores observados de \"Y\"\ny &lt;- c(64, 13, 33, 18, 30, 20)\n\n# Altura de la densidad a priori uniforme\nprior &lt;- rep(1 / 100, 400)\n\n# Grilla con los valorse de lambda\nlambda_grid &lt;- seq(0, 100, length.out = 400)\n\nOpci√≥n 1: Replicar la f√≥rmula de la funci√≥n de verosimilitud en una funci√≥n de R\n\nf_likelihood &lt;- function(lambda, y_vector) {\n  n &lt;- length(y_vector)\n  y_suma &lt;- sum(y_vector)\n  denominador &lt;- prod(factorial(y_vector))\n  (exp(- n * lambda) * lambda ^ y_suma) / denominador\n}\n\nlikelihood &lt;- f_likelihood(lambda_grid, y)\nposterior_ &lt;- prior * likelihood # ¬øEs necesario multiplicar por 'prior'?\nposterior_\n\n  [1]  0.000000e+00 8.341450e-313 7.104056e-260 3.488800e-229 1.344921e-207\n  [6] 5.316241e-191 1.468223e-177 2.693061e-166 1.258167e-156 3.562976e-148\n [11] 1.105533e-140 5.733178e-134 6.787109e-128 2.324156e-122 2.767353e-117\n [16] 1.325696e-112 2.873971e-108 3.104267e-104 1.809183e-100  6.081954e-97\n [21]  1.247863e-93  1.639788e-90  1.438521e-87  8.732918e-85  3.785569e-82\n [26]  1.204418e-79  2.881624e-77  5.297329e-75  7.627146e-73  8.749872e-71\n [31]  8.122076e-69  6.185727e-67  3.914092e-65  2.081334e-63  9.397953e-62\n [36]  3.637672e-60  1.217538e-58  3.551973e-57  9.098475e-56  2.060222e-54\n [41]  4.149713e-53  7.478107e-52  1.212173e-50  1.776239e-49  2.363842e-48\n [46]  2.869426e-47  3.189974e-46  3.260149e-45  3.073864e-44  2.682700e-43\n [51]  2.173984e-42  1.640637e-41  1.156224e-40  7.629166e-40  4.724840e-39\n [56]  2.752848e-38  1.512230e-37  7.848722e-37  3.856409e-36  1.797148e-35\n [61]  7.957451e-35  3.353418e-34  1.347173e-33  5.167090e-33  1.894912e-32\n [66]  6.653603e-32  2.239884e-31  7.238448e-31  2.248240e-30  6.719236e-30\n [71]  1.934454e-29  5.370553e-29  1.439272e-28  3.726947e-28  9.333742e-28\n [76]  2.262769e-27  5.314719e-27  1.210418e-26  2.675170e-26  5.741957e-26\n [81]  1.197791e-25  2.430089e-25  4.798220e-25  9.226574e-25  1.728934e-24\n [86]  3.159075e-24  5.631721e-24  9.800977e-24  1.666033e-23  2.767673e-23\n [91]  4.495581e-23  7.143525e-23  1.110973e-22  1.691846e-22  2.523943e-22\n [96]  3.690191e-22  5.289958e-22  7.438185e-22  1.026279e-21  1.389998e-21\n[101]  1.848737e-21  2.415490e-21  3.101393e-21  3.914514e-21  4.858608e-21\n[106]  5.931961e-21  7.126440e-21  8.426877e-21  9.810887e-21  1.124921e-20\n[111]  1.270659e-20  1.414317e-20  1.551637e-20  1.678304e-20  1.790181e-20\n[116]  1.883539e-20  1.955269e-20  2.003058e-20  2.025504e-20  2.022183e-20\n[121]  1.993649e-20  1.941370e-20  1.867617e-20  1.775307e-20  1.667820e-20\n[126]  1.548806e-20  1.421993e-20  1.291006e-20  1.159221e-20  1.029641e-20\n[131]  9.048144e-21  7.867903e-21  6.771014e-21  5.767820e-21  4.864064e-21\n[136]  4.061454e-21  3.358320e-21  2.750318e-21  2.231130e-21  1.793113e-21\n[141]  1.427871e-21  1.126747e-21  8.812014e-22  6.831092e-22  5.249581e-22\n[146]  3.999734e-22  3.021764e-22  2.263932e-22  1.682243e-22  1.239895e-22\n[151]  9.065646e-23  6.576228e-23  4.733305e-23  3.380696e-23  2.396323e-23\n[156]  1.685873e-23  1.177299e-23  8.161536e-24  5.617212e-24  3.838602e-24\n[161]  2.604758e-24  1.755262e-24  1.174720e-24  7.808739e-25  5.156059e-25\n[166]  3.382054e-25  2.203960e-25  1.426992e-25  9.180523e-26  5.869137e-26\n[171]  3.728846e-26  2.354505e-26  1.477683e-26  9.218273e-27  5.716563e-27\n[176]  3.524253e-27  2.160105e-27  1.316397e-27  7.976853e-28  4.806581e-28\n[181]  2.880237e-28  1.716462e-28  1.017374e-28  5.997816e-29  3.517201e-29\n[186]  2.051719e-29  1.190639e-29  6.873967e-30  3.948428e-30  2.256596e-30\n[191]  1.283274e-30  7.261782e-31  4.089292e-31  2.291691e-31  1.278170e-31\n[196]  7.095239e-32  3.920239e-32  2.155985e-32  1.180285e-32  6.432150e-33\n[201]  3.489582e-33  1.884768e-33  1.013513e-33  5.426324e-34  2.892720e-34\n[206]  1.535499e-34  8.116204e-35  4.272035e-35  2.239301e-35  1.168970e-35\n[211]  6.077494e-36  3.146971e-36  1.623022e-36  8.337508e-37  4.266228e-37\n[216]  2.174523e-37           Inf           Inf           Inf           Inf\n[221]           Inf           Inf           Inf           Inf           Inf\n[226]           Inf           Inf           Inf           Inf           Inf\n[231]           Inf           Inf           Inf           Inf           Inf\n[236]           Inf           Inf           Inf           Inf           Inf\n[241]           Inf           Inf           Inf           Inf           Inf\n[246]           Inf           Inf           Inf           Inf           Inf\n[251]           Inf           Inf           Inf           Inf           Inf\n[256]           Inf           Inf           Inf           Inf           Inf\n[261]           Inf           Inf           Inf           Inf           Inf\n[266]           Inf           Inf           Inf           Inf           Inf\n[271]           Inf           Inf           Inf           Inf           Inf\n[276]           Inf           Inf           Inf           Inf           Inf\n[281]           Inf           Inf           Inf           Inf           Inf\n[286]           Inf           Inf           Inf           Inf           Inf\n[291]           Inf           Inf           Inf           Inf           Inf\n[296]           Inf           Inf           Inf           Inf           Inf\n[301]           Inf           Inf           Inf           Inf           Inf\n[306]           Inf           Inf           Inf           Inf           Inf\n[311]           Inf           Inf           Inf           Inf           Inf\n[316]           Inf           Inf           Inf           Inf           Inf\n[321]           Inf           Inf           Inf           Inf           Inf\n[326]           Inf           Inf           Inf           Inf           Inf\n[331]           Inf           Inf           Inf           Inf           Inf\n[336]           Inf           Inf           Inf           Inf           Inf\n[341]           Inf           Inf           Inf           Inf           Inf\n[346]           Inf           Inf           Inf           Inf           Inf\n[351]           Inf           Inf           Inf           Inf           Inf\n[356]           Inf           Inf           Inf           Inf           Inf\n[361]           Inf           Inf           Inf           Inf           Inf\n[366]           Inf           Inf           Inf           Inf           Inf\n[371]           Inf           Inf           Inf           Inf           Inf\n[376]           Inf           Inf           Inf           Inf           Inf\n[381]           Inf           Inf           Inf           Inf           Inf\n[386]           Inf           Inf           Inf           Inf           Inf\n[391]           Inf           Inf           Inf           Inf           Inf\n[396]           Inf           Inf           Inf           Inf           Inf\n\n\nLa densidad a posteriori no normalizada resulta Inf en algunos casos. Esto indica que el computo no es estable. Veamos otras alternativas.\nOpci√≥n 2: Utilizar la funcion dpois de R para evaluar la funci√≥n de masa de probabilidad en cada observaci√≥n y luego obtener la funci√≥n de verosimilitud multiplicando estos resultados.\n\nf_likelihood &lt;- function(lambda, y_vector) {\n  output &lt;- numeric(length(lambda))\n  for (i in seq_along(output)) {\n    output[i] &lt;- prod(dpois(y_vector, lambda[i]))\n  }\n  output\n}\n\nlikelihood &lt;- f_likelihood(lambda_grid, y)\nposterior_ &lt;- prior * likelihood\narea &lt;- integrate(f_likelihood, lower = 0, upper = 100, y_vector = y)$value\nposterior &lt;- posterior_ / area\n\ndf &lt;- data.frame(\n  grupo = factor(\n    rep(c(\"prior\", \"likelihood\", \"posterior\"), each = 400),\n    levels = c(\"prior\", \"likelihood\", \"posterior\"),\n    ordered = TRUE\n  ),\n  lambda = rep(lambda_grid, 3),\n  valor = c(prior, likelihood, posterior)\n)\n\nggplot(df) +\n  geom_line(aes(x = lambda, y = valor, color = grupo), linewidth = 1) +\n  scale_color_manual(values = colores) +\n  labs(x = expression(lambda), y = NULL) +\n  facet_wrap(~ grupo, scales = \"free_y\") +\n  theme_bw() +\n    theme(\n        panel.grid.major = element_blank(),\n        panel.grid.minor = element_blank(),\n        legend.position = \"none\"\n    )\n\n\n\n\n\n\n\n\nOpci√≥n 3: Utilizar la funcion dpois de R, pero hacer cuentas en escalas logaritmica.\n\nf_likelihood &lt;- function(lambda, y_vector) {\n  output &lt;- numeric(length(lambda))\n  for (i in seq_along(output)) {\n    # El producto se transforma en suma\n    output[i] &lt;- sum(dpois(y_vector, lambda[i], log = TRUE))\n  }\n  # Volver a la escala original\n  exp(output)\n}\n\nlikelihood &lt;- f_likelihood(lambda_grid, y)\nposterior_ &lt;- prior * likelihood\narea &lt;- integrate(f_likelihood, lower = 0, upper = 100, y_vector = y)$value\nposterior &lt;- posterior_ / area\n\ndf &lt;- data.frame(\n  grupo = factor(\n    rep(c(\"prior\", \"likelihood\", \"posterior\"), each = 400),\n    levels = c(\"prior\", \"likelihood\", \"posterior\"),\n    ordered = TRUE\n  ),\n  lambda = rep(lambda_grid, 3),\n  valor = c(prior, likelihood, posterior)\n)\n\nggplot(df) +\n  geom_line(aes(x = lambda, y = valor, color = grupo), linewidth = 1) +\n  scale_color_manual(values = colores) +\n  labs(x = expression(lambda), y = NULL) +\n  facet_wrap(~ grupo, scales = \"free_y\") +\n  theme_bw() +\n    theme(\n        panel.grid.major = element_blank(),\n        panel.grid.minor = element_blank(),\n        legend.position = \"none\"\n    )\n\n\n\n\n\n\n\n\nPregunta\n\n¬øQu√© explica que los valores del eje vertical para el likelihood sean tan distintos del resto?",
    "crumbs": [
      "Recursos",
      "C√≥digo",
      "02 - ¬°Ostras! ¬°Estoy haciendo inferencia bayesiana!"
    ]
  },
  {
    "objectID": "recursos/codigo/04_diferentes_posteriors.html",
    "href": "recursos/codigo/04_diferentes_posteriors.html",
    "title": "04 - Diferentes observaciones, diferentes posteriors",
    "section": "",
    "text": "Este programa obtiene la distribuci√≥n a posteriori para los diferentes trabajadores del ejercicio Diferentes observaciones, diferentes posteriors de la Pr√°ctica 2.\n\nlibrary(ggplot2)\n\ncolores &lt;- c(\"#f08533\", \"#3b78b0\", \"#d1352c\")\n\n# Crear grilla para los valores de \"pi\"\ngrid_n &lt;- 200\npi_grid &lt;- seq(0, 1, length.out = grid_n)\n\n# Hiperpar√°metros del prior\na_prior &lt;- 4\nb_prior &lt;- 3\n\n# Cantidad de √©xitos y ensayos para cada trabajador\ny_1 &lt;- 0\nN_1 &lt;- 1\n\ny_2 &lt;- 3\nN_2 &lt;- 10\n\ny_3 &lt;- 20\nN_3 &lt;- 100\n\n# Hiperpar√°metros del posterior, para cada trabajador\na_posterior_1 &lt;- a_prior + y_1\nb_posterior_1 &lt;- b_prior + N_1 - y_1\n\na_posterior_2 &lt;- a_prior + y_2\nb_posterior_2 &lt;- b_prior + N_2 - y_2\n\na_posterior_3 &lt;- a_prior + y_3\nb_posterior_3 &lt;- b_prior + N_3 - y_3\n\n# Evaluar la funcion de densidad del posterior de cada trabajador\n# en cada uno de los puntos de \"pi_grid\"\nposterior_1 &lt;- dbeta(pi_grid, a_posterior_1, b_posterior_1)\nposterior_2 &lt;- dbeta(pi_grid, a_posterior_2, b_posterior_2)\nposterior_3 &lt;- dbeta(pi_grid, a_posterior_3, b_posterior_3)\n\n# Crear un data.frame, necesario para trabajar con ggplot2\ndatos &lt;- data.frame(\n    p = rep(pi_grid, times = 3),\n    posterior = c(posterior_1, posterior_2, posterior_3),\n    trabajador = rep(c(\"T1\", \"T2\", \"T3\"), each = grid_n)\n)\n\n# Crear el grafico con ggplot2 con los siguientes mapeos\n# * Los valores del eje horizontal \"x\" salen de \"pi\"\n# * Los valores de la altura en el eje vertical \"y\" salen de \"posterior\"\n# * Los colores se mapean a cada uno de los valores de \"trabajador\"\n# * Las areas tienen un color de relleno distinto para cada valor en \"trabajador\"\nggplot(datos, aes(x = p, y = posterior, color = trabajador)) +\n    geom_line() +\n    geom_area(aes(fill = trabajador), alpha = 0.4, position = \"identity\") +\n    scale_color_manual(values = colores) +\n    scale_fill_manual(values = colores) +\n    labs(x = expression(pi), y = expression(\"p(\" ~ pi ~ \"| y)\")) +\n    theme_bw() +\n    theme(\n        panel.grid.major = element_blank(),\n        panel.grid.minor = element_blank(),\n    )",
    "crumbs": [
      "Recursos",
      "C√≥digo",
      "04 - Diferentes observaciones, diferentes posteriors"
    ]
  },
  {
    "objectID": "recursos/codigo/03_quien_domina_el_posterior.html",
    "href": "recursos/codigo/03_quien_domina_el_posterior.html",
    "title": "03 - ¬øQui√©n domina el posterior?",
    "section": "",
    "text": "Este programa obtiene la distribuci√≥n a posteriori para los diferentes casos del ejercicio ¬øQui√©n domina el posterior? de la Pr√°ctica 2.\nCreamos dos funciones para simplificar los bloques de c√≥digo en cada caso. La primera, generar_datos(), recibe una grilla para \\(\\pi\\), los valores de \\(p(\\pi)\\), \\(p(y \\mid \\pi)\\) y \\(p(\\pi, \\mid y)\\) en cada valor de la grilla, y devuelve un data.frame que permite graficar las 3 curvas con {ggplot2}. La segunda, generar_grafico(), simplemente toma el data.frame generado por generar_datos() y produce la visualizaci√≥n.\n\nlibrary(ggplot2)\n\ngrid_n &lt;- 200\npi_grid &lt;- seq(0, 1, length.out = grid_n)\n\ngenerar_datos &lt;- function(pi_grid, pi_prior, pi_likelihood, pi_posterior) {\n    grid_n &lt;- length(pi_grid)\n\n    datos &lt;- data.frame(\n        x = rep(pi_grid, times = 3),\n        y = c(pi_prior, pi_likelihood, pi_posterior),\n1        grupo = factor(\n            rep(c(\"prior\", \"likelihood\", \"posterior\"), each = grid_n),\n            levels = c(\"prior\", \"likelihood\", \"posterior\"),\n            ordered = TRUE\n        )\n    )\n    return(datos)\n}\n\ngenerar_grafico &lt;- function(datos) {\n    colores &lt;- c(\"#f08533\", \"#3b78b0\", \"#d1352c\")\n\n    plt &lt;- ggplot(datos) +\n        geom_line(aes(x = x, y = y, color = grupo), linewidth = 1) +\n        scale_color_manual(values = colores) +\n        labs(x = expression(pi), y = NULL) +\n        facet_wrap(~ grupo, scales = \"free_y\") +\n        theme_bw() +\n        theme(\n            panel.grid.major = element_blank(),\n            panel.grid.minor = element_blank(),\n            axis.ticks.y = element_blank(),\n            axis.text.y = element_blank(),\n            legend.position = \"none\"\n        )\n    return(plt)\n}\n\n\n1\n\nSe usa un factor() ordenado para indicarle a {ggplot2} que primero se ubica el prior, luego el likelihood, y finalmente el posterior.\n\n\n\n\nCaso i\n\na_prior &lt;- 1\nb_prior &lt;- 4\ny &lt;- 8\nN &lt;- 10\n\npi_prior &lt;- dbeta(pi_grid, a_prior, b_prior)\npi_likelihood &lt;- dbinom(y, N, pi_grid)\npi_posterior &lt;- dbeta(pi_grid, a_prior + y, b_prior + N - y)\n\ndatos &lt;- generar_datos(pi_grid, pi_prior, pi_likelihood, pi_posterior)\ngenerar_grafico(datos)\n\n\n\n\n\n\n\n\nCaso ii\n\na_prior &lt;- 20\nb_prior &lt;- 3\ny &lt;- 0\nN &lt;- 1\n\npi_prior &lt;- dbeta(pi_grid, a_prior, b_prior)\npi_likelihood &lt;- dbinom(y, N, pi_grid)\npi_posterior &lt;- dbeta(pi_grid, a_prior + y, b_prior + N - y)\n\ndatos &lt;- generar_datos(pi_grid, pi_prior, pi_likelihood, pi_posterior)\ngenerar_grafico(datos)\n\n\n\n\n\n\n\n\nCaso iii\n\na_prior &lt;- 4\nb_prior &lt;- 2\ny &lt;- 1\nN &lt;- 3\n\npi_prior &lt;- dbeta(pi_grid, a_prior, b_prior)\npi_likelihood &lt;- dbinom(y, N, pi_grid)\npi_posterior &lt;- dbeta(pi_grid, a_prior + y, b_prior + N - y)\n\ndatos &lt;- generar_datos(pi_grid, pi_prior, pi_likelihood, pi_posterior)\ngenerar_grafico(datos)\n\n\n\n\n\n\n\n\nCaso iv\n\na_prior &lt;- 3\nb_prior &lt;- 10\ny &lt;- 10\nN &lt;- 13\n\npi_prior &lt;- dbeta(pi_grid, a_prior, b_prior)\npi_likelihood &lt;- dbinom(y, N, pi_grid)\npi_posterior &lt;- dbeta(pi_grid, a_prior + y, b_prior + N - y)\n\ndatos &lt;- generar_datos(pi_grid, pi_prior, pi_likelihood, pi_posterior)\ngenerar_grafico(datos)\n\n\n\n\n\n\n\n\nCaso v\n\na_prior &lt;- 20\nb_prior &lt;- 2\ny &lt;- 10\nN &lt;- 200\n\npi_prior &lt;- dbeta(pi_grid, a_prior, b_prior)\npi_likelihood &lt;- dbinom(y, N, pi_grid)\npi_posterior &lt;- dbeta(pi_grid, a_prior + y, b_prior + N - y)\n\ndatos &lt;- generar_datos(pi_grid, pi_prior, pi_likelihood, pi_posterior)\ngenerar_grafico(datos)",
    "crumbs": [
      "Recursos",
      "C√≥digo",
      "03 - ¬øQui√©n domina el _posterior_?"
    ]
  },
  {
    "objectID": "recursos/software/index.html",
    "href": "recursos/software/index.html",
    "title": "Instalaci√≥n de software",
    "section": "",
    "text": "En este curso se utiliza el lenguaje de programaci√≥n probabil√≠stica Stan mediante su interface RStan.\nLa instalaci√≥n de RStan en sistemas operativos basados en Linux es relativamente sencilla. Pero no lo es as√≠ en el sistema operativo Windows. En este √∫ltimo caso se necesita tener instalado un conjunto de herramientas conocido como RTools.\nEn las gu√≠as debajo se explica como instalar R, RTools, RStudio y RStan en una computadora que utiliza Windows.\nInstalar RInstalar RToolsInstalar RStudioInstalar RStan",
    "crumbs": [
      "Recursos",
      "Instalaci√≥n de software"
    ]
  },
  {
    "objectID": "recursos/software/04_instalar_rstan.html",
    "href": "recursos/software/04_instalar_rstan.html",
    "title": "Instalar RStan",
    "section": "",
    "text": "Si bien los pasos a continuaci√≥n pueden funcionar con m√∫ltiples versiones de R, en esta gu√≠a se asume que se tiene instalado R 4.3.3, RTools 4.3 y RStudio (aunque este √∫ltimo no sea estrictamente necesario).\nNo se recomienda utilizar las versiones 4.0, 4.1 y 4.2 de R para trabajar con RStan en Windows debido a la cantidad de problemas que han reportado en el pasado.\nPrevio a instalar RStan, se debe verificar que RTools se encuentra instalado y R tiene acceso al mismo. Una forma de verificarlo es ejecutando Sys.which(\"make\"), como se explica en Instalar RTools.\n\n\n\n\n\n\n\n\n\nDado que RTools funciona en el sistema, instalar RStan es tan sencillo como correr install.packages(\"rstan\").\n\n\n\n\n\n\n\n\n\nComienza un proceso de descarga de dependencias (ya compiladas):\n\n\n\n\n\n\n\n\n\ny luego se instalan y verifican las mismas:\n\n\n\n\n\n\n\n\n\nUna vez finalizado este proceso, se puede ejecutar library(\"rstan\"). Es de esperar que aparezcan varios mensajes, pero no hay que alarmarse ya que suelen ser a modo informativo. Lo m√°s relevante son las versiones de RStan y Stan.\n\n\n\n\n\n\n\n\n\nFinalmente, la mejor forma de determinar si RStan funciona correctamente es obteniendo muestras de un posterior. En las capturas debajo se utiliza el siguiente bloque de c√≥digo:\n\nlibrary(\"rstan\")\n\nN &lt;- 20\ny &lt;- 4\n\nmodel_beta1_stan &lt;- \"\ndata {\n  int N;     \n  int Y; \n}\nparameters {\n  real&lt;lower=0, upper=1&gt; pi;\n}\nmodel {\n  pi ~ beta(2,2); // prior\n  Y ~ binomial(N, pi);  // likelihood\n}\"\n\nmodel_beta1 &lt;- stan_model(model_code = model_beta1_stan)\n\ndata_list &lt;- list(Y = y, N = N)\n\nmodel_beta1_fit &lt;- sampling(\n  object = model_beta1, \n  data = data_list, \n  chains = 2, \n  iter = 500,\n  warmup = 100\n)\n\nEn el caso de que RStan haya funcionado correctamente, los mensajes en la consola ser√°n similares a los que se muestran a continuaci√≥n:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSi se quiere obtener m√°s informaci√≥n sobre la instalaci√≥n de RStan se puede consultar la wiki del repositorio de RStan o el foro de Stan donde se discuten m√∫ltiples temas.",
    "crumbs": [
      "Recursos",
      "Instalaci√≥n de software",
      "Instalar RStan"
    ]
  },
  {
    "objectID": "recursos/distribuciones/distribuciones.html",
    "href": "recursos/distribuciones/distribuciones.html",
    "title": "Distribuciones de probabilidad",
    "section": "",
    "text": "\\[\nX \\sim \\text{Normal}(\\mu, \\sigma)\n\\]\n\\[\np(x \\mid \\mu, \\sigma) = \\frac{1}{\\sqrt{2\\pi} \\sigma}e^{-\\frac{(x - \\mu) ^ 2}{2\\sigma^2}}\n\\]\n\\[\nP(X \\le x) = \\int_{-\\infty}^{x}{p(x | \\mu, \\sigma) dx}\n\\]\no tambi√©n\n\\[\nP(X \\le x) = \\frac{1}{2} \\left[1 + \\text{erf}\\left(\\frac{x - \\mu}{\\sigma\\sqrt{2}} \\right) \\right]\n\\]\ncon\n\\[\n\\text{erf}(x) = \\frac{2}{\\sqrt{\\pi}}\\int_0^{x} {e^{-t^2}dt}\n\\]\n\n\\(X \\in \\mathbb{R}\\)\n\\(\\mu \\in \\mathbb{R}\\)\n\\(\\sigma &gt; 0\\)\n\\(\\mathbb{E}(X) = \\mu\\)\n\\(\\mathbb{V}(X) = \\sigma^2\\)\n\n\n\n\n\\[\nX \\sim \\text{StudentT}(\\nu)\n\\]\n\\[\np(x \\mid \\nu) =\n    \\frac{\\Gamma(\\frac{\\nu + 1}{2})}{\\Gamma(\\frac{\\nu}{2})}\n    \\left(\\frac{1}{\\pi\\nu}\\right) ^ {\\frac{1}{2}}\n    \\left[1 + \\frac{x^2}{\\nu}\\right]^{-\\frac{\\nu + 1}{2}}\n\\]\n\\[\nP(X \\le x) = \\frac{1}{2} + x \\Gamma\\left(\\frac{\\nu + 1}{2}\\right)\n             \\frac{{}_2F_1\\left(\\frac{1}{2}, \\frac{v + 1}{2}, \\frac{3}{2}, \\frac{-x^2}{\\nu} \\right)}\n             {\\sqrt{\\pi \\nu} \\Gamma(\\frac{\\nu}{2})}\n\\]\ndonde \\({}_2F_1\\) es la funci√≥n hipergeom√©trica.\n\n\\(X \\in \\mathbb{R}\\)\n\\(\\nu &gt; 0\\)\n\\(\\mathbb{E}(X) = 0\\) si \\(\\nu &gt; 1\\)\n\\(\\mathbb{V}(X) = \\nu / (\\nu - 2)\\) si \\(\\nu &gt; 2\\)\n\n\n\n\n\n\n\\[\nX \\sim \\text{Gamma}(k, \\theta)\n\\]\n\\[\np(x \\mid k, \\theta) = \\frac{1}{\\Gamma(k)\\theta^k}x^{k-1}e^{-\\frac{x}{\\theta}}\n\\]\n\\[\nP(X \\le x) = \\frac{1}{\\Gamma(k)} \\gamma \\left(k, \\frac{x}{\\theta}\\right)\n\\]\n\n\\(X &gt; 0\\)\n\\(k &gt; 0\\)\n\\(\\theta &gt; 0\\)\n\\(\\mathbb{E}(X) = k\\theta\\)\n\\(\\mathbb{V}(X) = k\\theta^2\\)\n\n\n\n\n\\[\nX \\sim \\text{Gamma}(\\alpha, \\beta)\n\\]\n\\[\np(x | \\alpha, \\beta) = \\frac{1}{\\Gamma(\\alpha)}x^{\\alpha-1}e^{-\\beta x}\\beta^\\alpha\n\\]\n\\[\nP(X \\le x) = \\frac{1}{\\Gamma(\\alpha)} \\gamma(\\alpha, \\beta x)\n\\]\n\n\\(X &gt; 0\\)\n\\(\\alpha &gt; 0\\)\n\\(\\beta &gt; 0\\)\n\\(\\mathbb{E}(X) = \\alpha/\\beta\\)\n\\(\\mathbb{V}(X) = \\alpha/\\beta^2\\)\n\n\n\n\n\n\\[\nX \\sim \\text{Exponencial}(\\lambda)\n\\]\n\\[\np(x \\mid \\lambda) = \\lambda e^{-\\lambda x}\n\\]\n\\[\nP(X \\le x) = 1 - e^{-\\lambda x}\n\\]\n\n\\(X &gt; 0\\)\n\\(\\lambda &gt; 0\\)\n\\(\\mathbb{E}(X) = 1 / \\lambda\\)\n\\(\\mathbb{V}(X) = 1 / \\lambda ^ 2\\)\n\nEs un caso particular de \\(\\text{Gamma}(\\alpha, \\beta)\\) con \\(\\alpha = 1\\) y \\(\\beta = \\lambda\\)\n\n\n\n\\[\nX \\sim \\text{Beta}(a, b)\n\\]\n\\[\np(x \\mid a, b) = \\frac{x^{a-1} (1-x)^{b-1}}{B(a, b)}\n\\]\n\\[\nB(a, b) = \\int_0^1 x^{a-1} (1-x)^{b-1} dx = \\frac{\\Gamma(a)\\Gamma(b)}{\\Gamma(a+b)}\n\\]\n\\[\n\\Gamma(x) = \\int_0^\\infty u^{x-1} e^{-u} du\n\\]\n\n\\(X \\in (0, 1)\\)\n\\(\\displaystyle \\mathbb{E}(X) = \\frac{a}{a + b}\\)\n\\(\\displaystyle \\mathbb{V}(X) = \\frac{ab}{(a + b) ^ 2 (a + b + 1)}\\)\n\n\n\n\n\\[\nX \\sim \\text{Binomial}(\\theta, n)\n\\]\n\\[\np(x \\mid \\theta, n) = {n \\choose x} \\theta^x (1 - \\theta)^{(n - x)}\n\\]\n\\[\nP(X \\le x) = \\sum_{i = 0} ^ {x} {n \\choose i} \\theta^i (1 - \\theta)^{(n - i)}\n\\]\n\n\\(X \\in \\{0, 1, 2, \\cdots, n\\}\\)\n\\(\\theta \\in [0, 1]\\)\n\\(n \\in \\{0, 1, 2, \\cdots \\}\\)\n\\(\\mathbb{E}(X) = n \\theta\\)\n\\(\\mathbb{V}(X) = n \\theta (1 - \\theta)\\)\n\n\n\n\n\\[\nX \\sim \\text{Poisson}(\\lambda)\n\\]\n\\[\np(x \\mid \\lambda) = \\frac{\\lambda^x e^{-\\lambda}}{x!}\n\\]\n\\[\nP(X \\le x) = e^{-\\lambda} \\sum_{i = 0} ^ {x} \\frac{\\lambda^i}{i!}\n\\]\n\n\\(X \\in \\{0, 1, 2, \\cdots\\}\\)\n\\(\\lambda &gt; 0\\)\n\\(\\mathbb{E}(X) = \\lambda\\)\n\\(\\mathbb{V}(X) = \\lambda\\)\n\n\n\n\n\\[\nX \\sim \\text{BinomialNegativa}(r, p)\n\\]\n\\[\np(x \\mid k, p)= \\binom{x + r - 1}{x}(1 - p)^x p^r\n\\]\n\n\\(X \\in \\{0, 1, 2, \\cdots \\}\\)\n\\(r \\in \\{1, 2, 3, \\cdots \\}\\)\n\\(p \\in [0, 1]\\)\n\\(\\mathbb{E}(X) = r(1 - p) / p\\)\n\\(\\mathbb{V}(X) = r(1 - p) / p^2\\)",
    "crumbs": [
      "Recursos",
      "Distribuciones"
    ]
  },
  {
    "objectID": "recursos/distribuciones/distribuciones.html#normal",
    "href": "recursos/distribuciones/distribuciones.html#normal",
    "title": "Distribuciones de probabilidad",
    "section": "",
    "text": "\\[\nX \\sim \\text{Normal}(\\mu, \\sigma)\n\\]\n\\[\np(x \\mid \\mu, \\sigma) = \\frac{1}{\\sqrt{2\\pi} \\sigma}e^{-\\frac{(x - \\mu) ^ 2}{2\\sigma^2}}\n\\]\n\\[\nP(X \\le x) = \\int_{-\\infty}^{x}{p(x | \\mu, \\sigma) dx}\n\\]\no tambi√©n\n\\[\nP(X \\le x) = \\frac{1}{2} \\left[1 + \\text{erf}\\left(\\frac{x - \\mu}{\\sigma\\sqrt{2}} \\right) \\right]\n\\]\ncon\n\\[\n\\text{erf}(x) = \\frac{2}{\\sqrt{\\pi}}\\int_0^{x} {e^{-t^2}dt}\n\\]\n\n\\(X \\in \\mathbb{R}\\)\n\\(\\mu \\in \\mathbb{R}\\)\n\\(\\sigma &gt; 0\\)\n\\(\\mathbb{E}(X) = \\mu\\)\n\\(\\mathbb{V}(X) = \\sigma^2\\)",
    "crumbs": [
      "Recursos",
      "Distribuciones"
    ]
  },
  {
    "objectID": "recursos/distribuciones/distribuciones.html#t-student",
    "href": "recursos/distribuciones/distribuciones.html#t-student",
    "title": "Distribuciones de probabilidad",
    "section": "",
    "text": "\\[\nX \\sim \\text{StudentT}(\\nu)\n\\]\n\\[\np(x \\mid \\nu) =\n    \\frac{\\Gamma(\\frac{\\nu + 1}{2})}{\\Gamma(\\frac{\\nu}{2})}\n    \\left(\\frac{1}{\\pi\\nu}\\right) ^ {\\frac{1}{2}}\n    \\left[1 + \\frac{x^2}{\\nu}\\right]^{-\\frac{\\nu + 1}{2}}\n\\]\n\\[\nP(X \\le x) = \\frac{1}{2} + x \\Gamma\\left(\\frac{\\nu + 1}{2}\\right)\n             \\frac{{}_2F_1\\left(\\frac{1}{2}, \\frac{v + 1}{2}, \\frac{3}{2}, \\frac{-x^2}{\\nu} \\right)}\n             {\\sqrt{\\pi \\nu} \\Gamma(\\frac{\\nu}{2})}\n\\]\ndonde \\({}_2F_1\\) es la funci√≥n hipergeom√©trica.\n\n\\(X \\in \\mathbb{R}\\)\n\\(\\nu &gt; 0\\)\n\\(\\mathbb{E}(X) = 0\\) si \\(\\nu &gt; 1\\)\n\\(\\mathbb{V}(X) = \\nu / (\\nu - 2)\\) si \\(\\nu &gt; 2\\)",
    "crumbs": [
      "Recursos",
      "Distribuciones"
    ]
  },
  {
    "objectID": "recursos/distribuciones/distribuciones.html#gamma",
    "href": "recursos/distribuciones/distribuciones.html#gamma",
    "title": "Distribuciones de probabilidad",
    "section": "",
    "text": "\\[\nX \\sim \\text{Gamma}(k, \\theta)\n\\]\n\\[\np(x \\mid k, \\theta) = \\frac{1}{\\Gamma(k)\\theta^k}x^{k-1}e^{-\\frac{x}{\\theta}}\n\\]\n\\[\nP(X \\le x) = \\frac{1}{\\Gamma(k)} \\gamma \\left(k, \\frac{x}{\\theta}\\right)\n\\]\n\n\\(X &gt; 0\\)\n\\(k &gt; 0\\)\n\\(\\theta &gt; 0\\)\n\\(\\mathbb{E}(X) = k\\theta\\)\n\\(\\mathbb{V}(X) = k\\theta^2\\)\n\n\n\n\n\\[\nX \\sim \\text{Gamma}(\\alpha, \\beta)\n\\]\n\\[\np(x | \\alpha, \\beta) = \\frac{1}{\\Gamma(\\alpha)}x^{\\alpha-1}e^{-\\beta x}\\beta^\\alpha\n\\]\n\\[\nP(X \\le x) = \\frac{1}{\\Gamma(\\alpha)} \\gamma(\\alpha, \\beta x)\n\\]\n\n\\(X &gt; 0\\)\n\\(\\alpha &gt; 0\\)\n\\(\\beta &gt; 0\\)\n\\(\\mathbb{E}(X) = \\alpha/\\beta\\)\n\\(\\mathbb{V}(X) = \\alpha/\\beta^2\\)",
    "crumbs": [
      "Recursos",
      "Distribuciones"
    ]
  },
  {
    "objectID": "recursos/distribuciones/distribuciones.html#exponencial",
    "href": "recursos/distribuciones/distribuciones.html#exponencial",
    "title": "Distribuciones de probabilidad",
    "section": "",
    "text": "\\[\nX \\sim \\text{Exponencial}(\\lambda)\n\\]\n\\[\np(x \\mid \\lambda) = \\lambda e^{-\\lambda x}\n\\]\n\\[\nP(X \\le x) = 1 - e^{-\\lambda x}\n\\]\n\n\\(X &gt; 0\\)\n\\(\\lambda &gt; 0\\)\n\\(\\mathbb{E}(X) = 1 / \\lambda\\)\n\\(\\mathbb{V}(X) = 1 / \\lambda ^ 2\\)\n\nEs un caso particular de \\(\\text{Gamma}(\\alpha, \\beta)\\) con \\(\\alpha = 1\\) y \\(\\beta = \\lambda\\)",
    "crumbs": [
      "Recursos",
      "Distribuciones"
    ]
  },
  {
    "objectID": "recursos/distribuciones/distribuciones.html#beta",
    "href": "recursos/distribuciones/distribuciones.html#beta",
    "title": "Distribuciones de probabilidad",
    "section": "",
    "text": "\\[\nX \\sim \\text{Beta}(a, b)\n\\]\n\\[\np(x \\mid a, b) = \\frac{x^{a-1} (1-x)^{b-1}}{B(a, b)}\n\\]\n\\[\nB(a, b) = \\int_0^1 x^{a-1} (1-x)^{b-1} dx = \\frac{\\Gamma(a)\\Gamma(b)}{\\Gamma(a+b)}\n\\]\n\\[\n\\Gamma(x) = \\int_0^\\infty u^{x-1} e^{-u} du\n\\]\n\n\\(X \\in (0, 1)\\)\n\\(\\displaystyle \\mathbb{E}(X) = \\frac{a}{a + b}\\)\n\\(\\displaystyle \\mathbb{V}(X) = \\frac{ab}{(a + b) ^ 2 (a + b + 1)}\\)",
    "crumbs": [
      "Recursos",
      "Distribuciones"
    ]
  },
  {
    "objectID": "recursos/distribuciones/distribuciones.html#binomial",
    "href": "recursos/distribuciones/distribuciones.html#binomial",
    "title": "Distribuciones de probabilidad",
    "section": "",
    "text": "\\[\nX \\sim \\text{Binomial}(\\theta, n)\n\\]\n\\[\np(x \\mid \\theta, n) = {n \\choose x} \\theta^x (1 - \\theta)^{(n - x)}\n\\]\n\\[\nP(X \\le x) = \\sum_{i = 0} ^ {x} {n \\choose i} \\theta^i (1 - \\theta)^{(n - i)}\n\\]\n\n\\(X \\in \\{0, 1, 2, \\cdots, n\\}\\)\n\\(\\theta \\in [0, 1]\\)\n\\(n \\in \\{0, 1, 2, \\cdots \\}\\)\n\\(\\mathbb{E}(X) = n \\theta\\)\n\\(\\mathbb{V}(X) = n \\theta (1 - \\theta)\\)",
    "crumbs": [
      "Recursos",
      "Distribuciones"
    ]
  },
  {
    "objectID": "recursos/distribuciones/distribuciones.html#poisson",
    "href": "recursos/distribuciones/distribuciones.html#poisson",
    "title": "Distribuciones de probabilidad",
    "section": "",
    "text": "\\[\nX \\sim \\text{Poisson}(\\lambda)\n\\]\n\\[\np(x \\mid \\lambda) = \\frac{\\lambda^x e^{-\\lambda}}{x!}\n\\]\n\\[\nP(X \\le x) = e^{-\\lambda} \\sum_{i = 0} ^ {x} \\frac{\\lambda^i}{i!}\n\\]\n\n\\(X \\in \\{0, 1, 2, \\cdots\\}\\)\n\\(\\lambda &gt; 0\\)\n\\(\\mathbb{E}(X) = \\lambda\\)\n\\(\\mathbb{V}(X) = \\lambda\\)",
    "crumbs": [
      "Recursos",
      "Distribuciones"
    ]
  },
  {
    "objectID": "recursos/distribuciones/distribuciones.html#binomial-negativa",
    "href": "recursos/distribuciones/distribuciones.html#binomial-negativa",
    "title": "Distribuciones de probabilidad",
    "section": "",
    "text": "\\[\nX \\sim \\text{BinomialNegativa}(r, p)\n\\]\n\\[\np(x \\mid k, p)= \\binom{x + r - 1}{x}(1 - p)^x p^r\n\\]\n\n\\(X \\in \\{0, 1, 2, \\cdots \\}\\)\n\\(r \\in \\{1, 2, 3, \\cdots \\}\\)\n\\(p \\in [0, 1]\\)\n\\(\\mathbb{E}(X) = r(1 - p) / p\\)\n\\(\\mathbb{V}(X) = r(1 - p) / p^2\\)",
    "crumbs": [
      "Recursos",
      "Distribuciones"
    ]
  },
  {
    "objectID": "presentaciones/presentacion_03.html#gamma-poisson",
    "href": "presentaciones/presentacion_03.html#gamma-poisson",
    "title": "Estad√≠stica Bayesiana",
    "section": "Gamma-Poisson",
    "text": "Gamma-Poisson\nSea una muestra \\(\\mathbf{y} = (y_1,y_2,\\dots,y_n)\\) obtenida de un modelo Poisson, es decir:\n\\[Y_i \\sim \\mathrm{Poisson}(\\lambda)\\]\nInteresa realizar una inferencia sobre el valor de \\(\\lambda\\)\n\n¬øC√≥mo asignamos una credibilidad a priori para \\(\\lambda\\)?"
  },
  {
    "objectID": "presentaciones/presentacion_03.html#section",
    "href": "presentaciones/presentacion_03.html#section",
    "title": "Estad√≠stica Bayesiana",
    "section": "",
    "text": "\\[\n\\lambda \\sim \\mathrm{Gamma}(s, r)\n\\]\n\\[\np(\\lambda \\mid s, r) = p(\\lambda) = \\frac{r^s}{\\Gamma(s)} \\lambda^{s-1}e^{-r\\lambda}\n\\]\n\n\n\n\n\n\nCuidado\n\n\n\\(\\mathrm{Gamma}(s, r)\\) en R es dgamma(x, shape = s, scale = 1/r)"
  },
  {
    "objectID": "presentaciones/presentacion_03.html#section-1",
    "href": "presentaciones/presentacion_03.html#section-1",
    "title": "Estad√≠stica Bayesiana",
    "section": "",
    "text": "El modelo propuesto es \\[\n\\begin{align*}\n    Y_i \\mid \\lambda & \\sim  Po(\\lambda)\\\\\n    \\lambda & \\sim  \\mathrm{Gamma}(s, r)\n\\end{align*}\n\\]\nEl likelihood es Poisson: \\[p(y_i\\mid \\lambda) = \\frac{\\lambda^{y_i}e^{-\\lambda}}{y_i!} \\rightarrow p(\\mathbf{y}\\mid \\lambda) = \\prod_i \\frac{\\lambda^{y_i}e^{-\\lambda}}{y_i!} = \\frac{\\lambda^{\\sum_i y_i}e^{-n\\lambda}}{\\prod_{i}y_i!}\\]\nEl prior es Gamma: \\[p(\\lambda) = \\frac{r^s}{\\Gamma(s)} \\lambda^{s-1}e^{-r\\lambda}\\]\nInteresa hallar \\(p(\\lambda\\mid \\mathbf{y})\\)"
  },
  {
    "objectID": "presentaciones/presentacion_03.html#section-2",
    "href": "presentaciones/presentacion_03.html#section-2",
    "title": "Estad√≠stica Bayesiana",
    "section": "",
    "text": "\\[p(\\lambda\\mid \\mathbf{y}) \\propto p(\\mathbf{y}\\mid\\lambda) p(\\lambda)\\]\n\n\\[\np(\\lambda \\mid \\mathbf{y}) \\propto \\frac{\\lambda^{\\sum_i y_i}e^{-n\\lambda}}{\\prod_{i}y_i!} \\frac{r^s}{\\Gamma(s)} \\lambda^{s-1}e^{-r\\lambda}\n\\]\n\\[\np(\\lambda \\mid \\mathbf{y}) \\propto \\frac{r^s}{\\Gamma(s)\\prod_i y_i!} \\lambda^{\\sum_iy_i+s-1} e^{-n\\lambda - r \\lambda}\n\\]\n\n\n\\[p(\\lambda \\mid \\mathbf{y}) = K C \\lambda^{\\sum_iy_i+s-1} e^{-n\\lambda - r \\lambda}\\]\n\n\n\\[p(\\lambda \\mid \\mathbf{y}) = K^* \\lambda^{\\sum_iy_i+s-1} e^{-(n + r)\\lambda}\\]"
  },
  {
    "objectID": "presentaciones/presentacion_03.html#section-3",
    "href": "presentaciones/presentacion_03.html#section-3",
    "title": "Estad√≠stica Bayesiana",
    "section": "",
    "text": "Para que \\(\\int_0^\\infty p(\\lambda\\mid \\mathbf{y})d \\lambda = 1\\), debe ser\n\n\\[K^* = \\frac{(n + r )^{\\sum_i y_i + s}}{\\Gamma(\\sum_i y_i + s)}\\]\n\n\nPor lo tanto, resulta que la distribuci√≥n a posteriori es Gamma de par√°metros \\(\\sum_i y_i + s\\) y \\(n+r\\)\n\n\n\\[\np(\\lambda\\mid \\mathbf{y}) = \\frac{(n + r )^{\\sum_i y_i + s}}{\\Gamma(\\sum_i y_i + s)} \\lambda^{\\sum_iy_i+s-1} e^{-(n + r)\\lambda}\n\\]\n\\[\n\\lambda\\mid \\mathbf{y} \\sim  \\mathrm{Gamma}(\\sum_i y_i + s, n+r)\n\\]"
  },
  {
    "objectID": "presentaciones/presentacion_03.html#normal-normal",
    "href": "presentaciones/presentacion_03.html#normal-normal",
    "title": "Estad√≠stica Bayesiana",
    "section": "Normal-normal",
    "text": "Normal-normal\nSea una muestra \\(\\mathbf{y} = (y_1,y_2,\\dots,y_n)\\) obtenida de un modelo normal con varianza conocida \\(\\sigma^2\\), es decir:\n\\[Y_i \\sim \\mathcal{N}(\\mu,\\sigma^2)\\]\nInteresa realizar una inferencia sobre el valor de \\(\\mu\\)\n\n¬øC√≥mo asignamos una credibilidad a priori para \\(\\mu\\)?"
  },
  {
    "objectID": "presentaciones/presentacion_03.html#section-4",
    "href": "presentaciones/presentacion_03.html#section-4",
    "title": "Estad√≠stica Bayesiana",
    "section": "",
    "text": "El modelo propuesto es: \\[\n\\small{\n  \\begin{align*}\n      y_i \\mid \\mu & \\sim  \\mathcal{N}(\\mu,\\sigma^2)\\\\\n      \\mu & \\sim \\mathcal{N}(\\theta,\\tau^2)\n  \\end{align*}\n}\n\\]\n\nEl likelihood es normal: \\[\n\\small{\n  \\begin{aligned}\n  p(y_i\\mid \\mu) &= \\frac{1}{\\sqrt{2\\pi\\sigma}} e^{-\\frac{(y_i-\\mu)^2}{2\\sigma^2}} \\rightarrow \\\\\n  p(\\mathbf{y}\\mid \\mu) &= \\left(\\frac{1}{2\\pi\\sigma}\\right)^{n/2}  e^{-\\frac{\\sum_i(y_i-\\mu)^2}{2\\sigma^2}} \\propto e^{-\\frac{(\\bar{y}-\\mu)^2}{2\\sigma^2/n}}\n  \\end{aligned}\n}\n\\]\n\n\nEl prior es normal: \\[\n\\small{\n  p(\\mu) = \\frac{1}{\\sqrt{2\\pi\\tau}} e^{-\\frac{(\\mu-\\theta)^2}{2\\tau^2}}\n}\n\\]\n\n\nInteresa hallar \\(p(\\mu \\mid \\mathbf{y})\\)"
  },
  {
    "objectID": "presentaciones/presentacion_03.html#section-5",
    "href": "presentaciones/presentacion_03.html#section-5",
    "title": "Estad√≠stica Bayesiana",
    "section": "",
    "text": "\\[p(\\mu\\mid \\mathbf{y}) \\propto p(\\mathbf{y}\\mid\\mu) p(\\mu)\\] \\[p(\\mu \\mid \\mathbf{y}) \\propto   e^{-\\frac{(\\bar{y}-\\mu)^2}{2\\sigma^2/n}} \\frac{1}{\\sqrt{2\\pi\\tau}} e^{-\\frac{(\\mu-\\theta)^2}{2\\tau^2}}\\]\n\\[\n\\begin{align*}\np(\\mu \\mid \\mathbf{y}) & \\propto  e^{-\\frac{(\\bar{y}-\\mu)^2}{2\\sigma^2/n}} e^{-\\frac{(\\mu-\\theta)^2}{2\\tau^2}} \\\\\n&  \\propto  e^{-\\left[\\frac{(\\bar{y}-\\mu)^2}{2\\sigma^2/n}+\\frac{(\\mu-\\theta)^2}{2\\tau^2}\\right]} \\\\\n& \\propto e^{-\\left[ \\frac{\\bar{y}^2 - 2\\bar{y} \\mu + \\mu^2}{2\\sigma^2/n} + \\frac{\\mu^2 - 2\\mu\\theta^2 + \\theta^2}{2\\tau^2} \\right]} \\\\\n& \\propto e^{\\left[ \\frac{ 2\\bar{y} \\mu - \\mu^2}{2\\sigma^2/n} + \\frac{-\\mu^2 + 2\\mu\\theta^2}{2\\tau^2} \\right]} \\\\\n& \\propto e^{\\left[ \\frac{(2\\bar{y} \\mu - \\mu^2)n\\tau^2 + (-\\mu^2 + 2\\mu\\theta^2)\\sigma^2}{2\\sigma^2\\tau^2} \\right]}\n\\end{align*}\n\\]"
  },
  {
    "objectID": "presentaciones/presentacion_03.html#section-6",
    "href": "presentaciones/presentacion_03.html#section-6",
    "title": "Estad√≠stica Bayesiana",
    "section": "",
    "text": "\\[\n    \\begin{align*}\nP(\\mu \\mid \\mathbf{y}) & \\propto e^{\\frac{2\\mu(\\theta\\sigma^2+ \\bar{y}n\\tau^2)-\\mu^2(n\\tau^2+\\sigma^2)}{2\\tau^2\\sigma^2}} \\\\\n& \\propto e^{\\frac{-\\mu^2 + 2\\mu \\left( \\frac{\\theta\\sigma^2 + \\bar{y}n\\tau^2}{n\\tau^2 + \\sigma^2} \\right)}{2\\tau^2\\sigma^2/(n\\tau^2 + \\sigma^2)}} e^{-\\left(\\frac{\\theta\\sigma^2 + \\bar{y}n\\tau^2}{n\\tau^2 + \\sigma^2}\\right)^2} \\\\\n& \\propto e^{-\\frac{\\left(\\mu -  \\frac{\\theta\\sigma^2 + \\bar{y}n\\tau^2}{n\\tau^2 + \\sigma^2} \\right)^2}{2\\tau^2\\sigma^2/(n\\tau^2+\\sigma^2)}}\n    \\end{align*}\n\\] \\[p(\\mu\\mid\\mathbf{y}) = K^* e^{-\\frac{\\left(\\mu -  \\frac{\\theta\\sigma^2 + \\bar{y}n\\tau^2}{n\\tau^2 + \\sigma^2} \\right)^2}{2\\tau^2\\sigma^2/(n\\tau^2+\\sigma^2)}}\\]"
  },
  {
    "objectID": "presentaciones/presentacion_03.html#section-7",
    "href": "presentaciones/presentacion_03.html#section-7",
    "title": "Estad√≠stica Bayesiana",
    "section": "",
    "text": "Por lo tanto, resulta que la distribuci√≥n a posteriori es normal de par√°metros \\(\\theta_n\\) y \\(\\tau_n^2\\)\n\n\\[\n\\begin{align*}\n\\mu\\mid \\mathbf{y} & \\sim  \\mathcal{N}\\left( \\frac{\\theta\\sigma^2 + \\bar{y}n\\tau^2}{n\\tau^2 + \\sigma^2},\\frac{\\tau^2\\sigma^2}{n\\tau^2+\\sigma^2} \\right) \\\\\n& \\sim \\mathcal{N}\\left( \\theta_n,\\tau_n^2 \\right)\n\\end{align*}\n\\]"
  },
  {
    "objectID": "presentaciones/presentacion_03.html#section-8",
    "href": "presentaciones/presentacion_03.html#section-8",
    "title": "Estad√≠stica Bayesiana",
    "section": "",
    "text": "Reflexionemos‚Ä¶ \\[\n\\begin{align*}\n    y_i\\mid\\mu & \\sim  \\mathcal{N}(\\mu,\\sigma^2) \\\\\n    \\mu & \\sim  \\mathcal{N}(\\theta,\\tau^2) \\\\\n    \\mu \\mid \\mathbf{y} & \\sim \\mathcal{N}(\\theta_n,\\tau_n^2)\n\\end{align*}\n\\]\n¬øPar√°metros desconocidos en la verosimilitud?\n\n¬øDimensi√≥n y caracter√≠stica del espacio de par√°metros?\n\n\n¬øConstantes de ajuste del prior?\n\n\n¬øForma del posterior?\n\n\n¬øQu√© son \\(\\theta_n\\) y \\(\\tau_n^2\\)?"
  },
  {
    "objectID": "presentaciones/presentacion_03.html#section-9",
    "href": "presentaciones/presentacion_03.html#section-9",
    "title": "Estad√≠stica Bayesiana",
    "section": "",
    "text": "¬øPar√°metros de la verosimilitud?"
  },
  {
    "objectID": "presentaciones/presentacion_03.html#section-10",
    "href": "presentaciones/presentacion_03.html#section-10",
    "title": "Estad√≠stica Bayesiana",
    "section": "",
    "text": "Otro modo de verlo"
  },
  {
    "objectID": "presentaciones/presentacion_03.html#section-11",
    "href": "presentaciones/presentacion_03.html#section-11",
    "title": "Estad√≠stica Bayesiana",
    "section": "",
    "text": "¬øPuedo representar los datos en el gr√°fico de la izquierda?\n\nNo, es el mundo de los par√°metros\n\n\n¬øQu√© representan los valores marcados con \\(\\mathbf{\\times}\\)?\n\nPosibles valores de \\(\\mu\\) que podr√≠an esperarse a priori.\n\n\n\n¬øMedia y varianza de la normal de la izquierda?\n\n\\(\\theta\\) y \\(\\tau^2\\)\n\n\n\n¬øMedia y varianza de las normales de la derecha?\n\n\\(\\mu\\) y \\(\\sigma^2\\)"
  },
  {
    "objectID": "presentaciones/presentacion_03.html#section-12",
    "href": "presentaciones/presentacion_03.html#section-12",
    "title": "Estad√≠stica Bayesiana",
    "section": "",
    "text": "¬øQu√© estamos viendo?"
  },
  {
    "objectID": "presentaciones/presentacion_03.html#section-13",
    "href": "presentaciones/presentacion_03.html#section-13",
    "title": "Estad√≠stica Bayesiana",
    "section": "",
    "text": "A posteriori (luego de observar los datos)‚Ä¶ ¬øqu√© ocurre con la plausibilidad de los valores de \\(\\mu\\)?"
  },
  {
    "objectID": "presentaciones/presentacion_03.html#section-14",
    "href": "presentaciones/presentacion_03.html#section-14",
    "title": "Estad√≠stica Bayesiana",
    "section": "",
    "text": "¬øMedia y varianza de la normal de la izquierda?\n\n\\(\\theta_n\\) y \\(\\tau_n^2\\)\n\n¬øMedia y varianza de las normales de la derecha?\n\n\\(\\mu\\) y \\(\\sigma^2\\)"
  },
  {
    "objectID": "presentaciones/presentacion_03.html#compromiso",
    "href": "presentaciones/presentacion_03.html#compromiso",
    "title": "Estad√≠stica Bayesiana",
    "section": "Compromiso",
    "text": "Compromiso\n\\[\n\\small{\n  \\mathbb{E}[p(\\mu\\mid \\mathbf{y})] = \\theta_n = \\frac{\\theta\\sigma^2 + \\bar{y}n\\tau^2}{n\\tau^2 + \\sigma^2}\n}\n\\]\n\\[\n\\small{\n  \\mathbb{E}[p(\\mu\\mid \\mathbf{y})] =  \\theta\\frac{\\sigma^2}{n\\tau^2 + \\sigma^2} + \\bar{y}\\frac{n\\tau^2}{n\\tau^2 + \\sigma^2}\n}\n\\]\n\nRepresenta un balance (promedio ponderado o combinaci√≥n convexa) entre la media muestral y la media esperada a priori."
  },
  {
    "objectID": "presentaciones/presentacion_03.html#compromiso-1",
    "href": "presentaciones/presentacion_03.html#compromiso-1",
    "title": "Estad√≠stica Bayesiana",
    "section": "Compromiso",
    "text": "Compromiso\n\\[\n\\small{\n  \\mathbb{V}[p(\\mu\\mid \\mathbf{y})] = \\tau_n^2 = \\frac{\\tau^2\\sigma^2}{n\\tau^2+\\sigma^2}\n}\n\\]\n\\[\n\\small{\n  \\mathbb{V}[p(\\mu\\mid \\mathbf{y})] = \\frac{1}{\\frac{n}{\\sigma^2}+\\frac{1}{\\tau^2}}\n}\n\\]\n\\[\n\\small{\n  \\frac{1}{\\mathbb{V}[p(\\mu\\mid \\mathbf{y})]} =  \\frac{n}{\\sigma^2} + \\frac{1}{\\tau^2}\n}\n\\]\n\nLa precisi√≥n a posteriori es la suma de las precisiones del prior y la muestra."
  },
  {
    "objectID": "presentaciones/presentacion_03.html#section-15",
    "href": "presentaciones/presentacion_03.html#section-15",
    "title": "Estad√≠stica Bayesiana",
    "section": "",
    "text": "Distribuci√≥n predictiva a posteriori\n\\[p(\\tilde{y}\\mid \\mathbf{y}) = \\int p(\\tilde{y}\\mid \\mu) p(\\mu\\mid \\mathbf{y})d\\mu\\] El integrando es el producto de dos normales: una normal bivariada. Por lo tanto toda la integral es una distribuci√≥n marginal de una normal: otra normal."
  },
  {
    "objectID": "presentaciones/presentacion_03.html#section-16",
    "href": "presentaciones/presentacion_03.html#section-16",
    "title": "Estad√≠stica Bayesiana",
    "section": "",
    "text": "Demostraci√≥n poco formal‚Ä¶\nA posteriori vale \\[\n\\begin{array}{ccc}\ny & = & (y-\\mu) + \\mu \\\\\ny-\\mu \\mid \\mu & \\sim & \\mathcal{N}(0,\\sigma^2) \\\\\n\\mu \\mid \\mathbf{y} & \\sim & \\mathcal{N}(\\theta_n,\\tau_n^2)\n\\end{array}\n\\]\nResulta\n\\[p(\\tilde{y}\\mid \\mathbf{y}) = \\mathcal{N}(\\mu_n,\\sigma^2 + \\tau_n^2)\\]"
  },
  {
    "objectID": "presentaciones/presentacion_03.html#section-17",
    "href": "presentaciones/presentacion_03.html#section-17",
    "title": "Estad√≠stica Bayesiana",
    "section": "",
    "text": "La varianza predictiva \\(\\sigma^2 + \\tau_n^2\\) es una medida de la incertidumbre a posteriori respecto a una observaci√≥n nueva \\(\\tilde{y}\\).\n\nLa incertidumbre en \\(\\tilde{y}\\) proviene de la variabilidad debida al azar (\\(\\sigma\\)) y de la variabilidad debida al desconocimiento de \\(\\mu\\) (\\(\\tau_n\\))\n\n\nEn otras palabras, si supi√©ramos que \\(\\mu = 2\\), toda la variabilidad provendr√≠a de \\(\\sigma\\), ¬°pero no sabemos cu√°nto vale \\(\\mu\\)! Puede ser \\(2\\) o \\(1.98\\) o \\(1.43\\)‚Ä¶ Por lo que hay una componente adicional de varianza."
  },
  {
    "objectID": "presentaciones/presentacion_03.html#section-18",
    "href": "presentaciones/presentacion_03.html#section-18",
    "title": "Estad√≠stica Bayesiana",
    "section": "",
    "text": "No se entendi√≥ nada. Simular para creer.\n\n¬øC√≥mo obtenemos una observaci√≥n nueva si sabemos que \\(\\mu = 2\\) (sabiendo que \\(\\sigma = 1.2\\))?\n\n\nDirectamente tomamos una muestra \\(\\tilde{y}\\) de \\(\\mathcal{N}\\left(\\mu=2,\\sigma^2= 1.2^2\\right)\\)\n\ny_new &lt;- rnorm(1, mean = 2, sd = 1.2)\n\n\n\nPero en estad√≠stica bayesiana \\(\\mu\\) tiene una distribuci√≥n de probabilidad (por ejemplo \\(\\mathcal{N}\\left(\\theta_n=2,\\tau_n^2=1.8^2\\right)\\)), ¬øc√≥mo hacemos la simulaci√≥n?\n\n\n\nTomamos una muestra \\(\\mu^{(s)}\\) de la distribuci√≥n de \\(\\mu\\)\nObtenemos \\(\\tilde{y}\\) a partir de \\(\\mathcal{N}(\\mu=\\mu^{(s)},\\sigma^2=1.2^2)\\)\n\n\n\n\nmu_s &lt;- rnorm(1, mean = 2, sd = 1.8)\ny_new &lt;- rnorm(1, mean = mu_s, sd = 1.2)\n\n\n\n¬øQu√© va a pasar en cada caso si construimos la distribuci√≥n de \\(\\tilde{y}\\)?"
  },
  {
    "objectID": "presentaciones/presentacion_03.html#section-19",
    "href": "presentaciones/presentacion_03.html#section-19",
    "title": "Estad√≠stica Bayesiana",
    "section": "",
    "text": "La distribuci√≥n predictiva contiene la variabilidad inherente al fen√≥meno en estudio (\\(\\sigma\\)) y la incertidumbre en el par√°metro \\(\\mu\\)."
  },
  {
    "objectID": "presentaciones/presentacion_03.html#normal-normal-gamma-inversa",
    "href": "presentaciones/presentacion_03.html#normal-normal-gamma-inversa",
    "title": "Estad√≠stica Bayesiana",
    "section": "Normal ‚Äì normal-gamma-inversa",
    "text": "Normal ‚Äì normal-gamma-inversa\nSea una muestra \\(\\mathbf{y} = (y_1,y_2,\\dots,y_n)\\) obtenida de un modelo normal con varianza desconocida \\(\\sigma^2\\), es decir:\n\\[Y_i \\sim \\mathcal{N}(\\mu,\\sigma^2)\\]\nE interesa realizar una inferencia sobre el valor de \\(\\mu\\) y el valor de \\(\\sigma\\)\n\n¬øC√≥mo asignamos una credibilidad a priori para \\(\\mu\\) y \\(\\sigma\\)? ¬°Con una distribuci√≥n en dos dimensiones!"
  },
  {
    "objectID": "presentaciones/presentacion_03.html#section-20",
    "href": "presentaciones/presentacion_03.html#section-20",
    "title": "Estad√≠stica Bayesiana",
    "section": "",
    "text": "El modelo es\n\\[\n\\begin{align*}\n    Y_i\\mid\\mu,\\sigma^2 & \\sim  \\mathcal{N}(\\mu,\\sigma^2) \\\\\n    \\mu,\\sigma^2 & \\sim  \\mathcal{N}GI(\\theta,\\tau,\\alpha,\\beta)\n\\end{align*}\n\\]\n\\(\\mu\\) y \\(\\sigma^2\\) tienen distribuci√≥n conjunta normal-gamma-inversa:\n\\[\np(\\mu,\\sigma^2 \\mid \\theta, \\tau, \\alpha, \\beta ) =\n  \\frac{\\sqrt{\\tau}}{\\sqrt{2\\pi\\sigma^2}}\n  \\frac{\\beta^\\alpha}{\\Gamma(\\alpha)}\n  \\left( \\frac{1}{\\sigma^2} \\right)^{\\alpha+1}\n  e^{-\\frac{2\\beta + \\tau(\\mu-\\theta)^2}{2\\sigma^2}}\n\\]\nSi anticipamos que la normal-gamma-inversa es conjugada de la normal (para los par√°metros \\(\\mu\\) y \\(\\sigma^2\\)), ¬øqu√© podemos decir de la distribuci√≥n a posteriori (conjunta) de \\(\\mu\\) y \\(\\sigma^2\\)"
  },
  {
    "objectID": "presentaciones/presentacion_03.html#section-21",
    "href": "presentaciones/presentacion_03.html#section-21",
    "title": "Estad√≠stica Bayesiana",
    "section": "",
    "text": "Efectivamente, se puede probar que:\n\\[\n\\begin{align*}\n\\mu,\\sigma^2 \\mid \\mathbf{y} & \\sim  \\mathcal{N}GI(\\theta_n,\\tau_n,\\alpha_n,\\beta_n)\n\\end{align*}\n\\] con\n\\[\n\\begin{cases}\n\\theta_n = \\frac{\\tau\\theta + n \\bar{y}}{\\tau+n}\\\\\n\\tau_n = \\tau + n\\\\\n\\alpha_n = \\alpha + \\frac{n}{2}\\\\\n\\beta_n = \\beta + \\frac{1}{2} \\sum_i (y_i - \\bar{y})^2 + \\frac{n\\tau}{\\tau + n} \\frac{(\\bar{y}-\\theta)^2}{2}\n\\end{cases}\n\\]\n¬øPar√°metros desconocidos en la verosimilitud? ¬øDimensi√≥n del espacio de par√°metros? ¬øConstantes de ajuste del prior? ¬øForma del posterior?"
  },
  {
    "objectID": "presentaciones/presentacion_03.html#section-22",
    "href": "presentaciones/presentacion_03.html#section-22",
    "title": "Estad√≠stica Bayesiana",
    "section": "",
    "text": "Reflexionemos‚Ä¶\n\\[\n\\begin{align*}\n    Y_i \\mid\\mu & \\sim  \\mathcal{N}(\\mu,\\sigma^2) \\\\\n    \\mu & \\sim  \\mathcal{N}(\\theta,\\tau^2) \\\\\n    \\mu \\mid \\mathbf{y} & \\sim \\mathcal{N}(\\theta_n,\\tau_n^2)\n\\end{align*}\n\\]\n¬øPar√°metros desconocidos en la verosimilitud? ¬øDimensi√≥n y caracter√≠stica del espacio de par√°metros? ¬øConstantes de ajuste del prior? ¬øForma del posterior? ¬øQu√© son \\(\\theta_n\\) y \\(\\tau_n^2\\)?"
  },
  {
    "objectID": "presentaciones/presentacion_03.html#section-24",
    "href": "presentaciones/presentacion_03.html#section-24",
    "title": "Estad√≠stica Bayesiana",
    "section": "",
    "text": "¬øPuedo representar los datos en el gr√°fico de la izquierda?\n\nNo, es el mundo de los par√°metros\n\n\n¬øQu√© representan los valores marcados con \\(\\mathbf{\\times}\\)?\n\nPosibles valores de \\(\\mu\\) y \\(\\sigma^2\\) que podr√≠an esperarse a priori.\n\n\n\n¬øQu√© le da forma a la distribuci√≥n de la izquierda?\n\n\\(\\theta\\), \\(\\tau\\), \\(\\alpha\\) y \\(\\beta\\)\n\n\n\n¬øMedia y varianza de las normales de la derecha?\n\n\\(\\mu\\) y \\(\\sigma^2\\)"
  },
  {
    "objectID": "presentaciones/presentacion_03.html#section-25",
    "href": "presentaciones/presentacion_03.html#section-25",
    "title": "Estad√≠stica Bayesiana",
    "section": "",
    "text": "¬øQu√© estamos viendo?"
  },
  {
    "objectID": "presentaciones/presentacion_03.html#section-26",
    "href": "presentaciones/presentacion_03.html#section-26",
    "title": "Estad√≠stica Bayesiana",
    "section": "",
    "text": "A posteriori (luego de observar los datos)‚Ä¶ ¬øqu√© ocurre con la plausibilidad de los valores de \\(\\mu\\) y \\(\\sigma^2\\)?"
  },
  {
    "objectID": "presentaciones/presentacion_03.html#section-27",
    "href": "presentaciones/presentacion_03.html#section-27",
    "title": "Estad√≠stica Bayesiana",
    "section": "",
    "text": "¬øPar√°metros de la distribuci√≥n de la izquierda?\n\n\\(\\theta_n\\), \\(\\tau_n\\), \\(\\alpha_n\\) y \\(\\beta_n\\)\n\n\n¬øMedia y varianza de las normales de la derecha?\n\n\\(\\mu\\) y \\(\\sigma^2\\)\n\n\n\n\n\nEstad√≠stica Bayesiana ‚Äì 2025"
  },
  {
    "objectID": "presentaciones/presentacion_02.html#el-problema-de-las-urnas",
    "href": "presentaciones/presentacion_02.html#el-problema-de-las-urnas",
    "title": "Estad√≠stica Bayesiana",
    "section": "El problema de las urnas",
    "text": "El problema de las urnas\n\nSe cuenta con 11 urnas etiquetadas seg√∫n \\(u = 0,1,\\dots,10\\), que contienen diez bolas cada una. La urna \\(u\\) contiene \\(u\\) bolas azules y \\(10-u\\) bolas blancas. Fede elige una urna \\(u\\) al azar y extrae con reposici√≥n \\(N\\) bolas, obteniendo \\(n_A\\) azules y \\(N-n_A\\) blancas. Nico, el amigo de Fede, observa atentamente. Si despu√©s de \\(N=10\\) extracciones resulta \\(n_A = 3\\), ¬øcu√°l es la probabilidad de que la urna que Fede est√° usando sea la \\(u\\)?"
  },
  {
    "objectID": "presentaciones/presentacion_02.html#section",
    "href": "presentaciones/presentacion_02.html#section",
    "title": "Estad√≠stica Bayesiana",
    "section": "",
    "text": "La teor√≠a de las probabilidades permite predecir una distribuci√≥n sobre posibles valores de un resultado dado cierto conocimiento (o estado) del universo: probabilidad hacia adelante\n\nPor el contrario, muchas veces estamos interesados en realizar inferencias sobre el estado del universo a partir de observaciones: probabilidad inversa.\n\n\n\\[p(\\mathcal{H}\\mid E) = \\frac{p(E\\mid\\mathcal{H}) p(\\mathcal{H})}{p(E)}\\]\n\\[p(\\mathcal{H}\\mid E) \\propto p(E\\mid\\mathcal{H}) p(\\mathcal{H})\\]"
  },
  {
    "objectID": "presentaciones/presentacion_02.html#section-1",
    "href": "presentaciones/presentacion_02.html#section-1",
    "title": "Estad√≠stica Bayesiana",
    "section": "",
    "text": "Conociendo \\(N\\), si conoci√©ramos \\(u\\) podr√≠amos calcular las probabilidades de los diferentes \\(n_A\\): probabilidad hacia adelante.\n\nAqu√≠ observamos un \\(n_A\\) y queremos calcular las probabilidades de los posibles valores de \\(u\\): probabilidad inversa.\n\n\n\\[p(u\\mid n_A, N) = \\frac{p(n_A\\mid u, N)p(u)}{p(n_A\\mid N)}\\]\n\n\\(N\\) es una cantidad fija\n\\(n_A\\) es otra cantidad fija: lo que observamos al realizar el experimento\n\\(u\\) es la cantidad desconocida"
  },
  {
    "objectID": "presentaciones/presentacion_02.html#section-2",
    "href": "presentaciones/presentacion_02.html#section-2",
    "title": "Estad√≠stica Bayesiana",
    "section": "",
    "text": "Probabilidad conjunta de las cantidades observables (datos) y cantidades no observables (par√°metros):\n\n\\[\np(u,n_A\\mid N) = p(n_A\\mid u, N) p(u)\n\\]\n\n\nPodemos escribir la probabilidad de \\(u\\) condicionada a \\(n_A\\):\n\\[\n\\begin{array}{ccl}\np(u\\mid n_A,N) & = & \\frac{p(u,n_A\\mid N)}{p(n_A\\mid N)} \\\\\n& = & \\frac{p(n_A\\mid u, N) p(u)}{p(n_A\\mid N)}\n\\end{array}\n\\]\n\n\nEs la probabilidad de cada valor de \\(u\\) luego de haber observado \\(n_A = 3\\) bolas azules"
  },
  {
    "objectID": "presentaciones/presentacion_02.html#section-3",
    "href": "presentaciones/presentacion_02.html#section-3",
    "title": "Estad√≠stica Bayesiana",
    "section": "",
    "text": "La probabilidad marginal de \\(u\\) es\n\n\\[p(u) = \\frac{1}{11}\\]\nEs la probabilidad inicial de haber tomado la urna \\(u\\)"
  },
  {
    "objectID": "presentaciones/presentacion_02.html#section-4",
    "href": "presentaciones/presentacion_02.html#section-4",
    "title": "Estad√≠stica Bayesiana",
    "section": "",
    "text": "La probabilidad de \\(n_A\\) dado \\(u\\) (y \\(N\\)) es:\n\n\\[p(n_A\\mid u,N) = {N \\choose n_A} \\left( \\frac{u}{10} \\right)^{n_A} \\left( 1 - \\frac{u}{10} \\right)^{N-n_A}\\]\n\n\nComo \\(n_A=3\\) es fijo (¬°son los datos observados!), \\(p(n_A\\mid u,N)\\) es una funci√≥n de \\(u\\). Indica qu√© tan compatibles son los datos observados con los distintos valores de \\(u\\)"
  },
  {
    "objectID": "presentaciones/presentacion_02.html#section-5",
    "href": "presentaciones/presentacion_02.html#section-5",
    "title": "Estad√≠stica Bayesiana",
    "section": "",
    "text": "El denominador, \\(p(n_A\\mid N) = p(n_A)\\), es\n\n\\[\n\\begin{array}{ccl}\np(n_A\\mid N) & = & \\sum_u p(u,n_A\\mid N) \\\\\n& = & \\sum_u p(n_A\\mid u, N) p(u) \\\\\n& = & \\frac{1}{11} \\sum_u p(n_A\\mid u, N)\n\\end{array}\n\\]"
  },
  {
    "objectID": "presentaciones/presentacion_02.html#section-6",
    "href": "presentaciones/presentacion_02.html#section-6",
    "title": "Estad√≠stica Bayesiana",
    "section": "",
    "text": "Finalmente, la probabilidad de inter√©s \\(p(u\\mid n_A,N)\\) es\n\n\\[\np(u\\mid n_A,N) = \\frac{\\color{#00008B}{p(n_A\\mid u,N)p(u)}}{p(n_A\\mid N)}\n\\]\n\n\n\\[\np(u\\mid n_A,N) = \\color{#00008B}{{N \\choose n_A} \\left( \\frac{u}{10} \\right)^{n_A} \\left( 1 - \\frac{u}{10} \\right)^{N-n_A} \\frac{1}{11}} \\frac{1}{p(n_A\\mid N)}\n\\]\n\n\n\n\\(N\\) es una cantidad fija\n\\(n_A\\) es 3, otra cantidad fija: lo que observamos al realizar el experimento\n\\(u\\) es la cantidad desconocida\n\n\n\n\\(p(u\\mid n_A,N)\\) es una funci√≥n de \\(u\\): es la credibilidad de los valores de \\(u\\) luego de observar los datos (es decir, condicionada a \\(n_A=3\\))."
  },
  {
    "objectID": "presentaciones/presentacion_02.html#section-7",
    "href": "presentaciones/presentacion_02.html#section-7",
    "title": "Estad√≠stica Bayesiana",
    "section": "",
    "text": "Gr√°ficamente‚Ä¶"
  },
  {
    "objectID": "presentaciones/presentacion_02.html#section-8",
    "href": "presentaciones/presentacion_02.html#section-8",
    "title": "Estad√≠stica Bayesiana",
    "section": "",
    "text": "Gr√°ficamente‚Ä¶"
  },
  {
    "objectID": "presentaciones/presentacion_02.html#section-9",
    "href": "presentaciones/presentacion_02.html#section-9",
    "title": "Estad√≠stica Bayesiana",
    "section": "",
    "text": "Pasamos de una credibilidad a priori antes de observar los datos, a una a posteriori luego de observar \\(n_A = 3\\)"
  },
  {
    "objectID": "presentaciones/presentacion_02.html#intolerancia-al-gluten",
    "href": "presentaciones/presentacion_02.html#intolerancia-al-gluten",
    "title": "Estad√≠stica Bayesiana",
    "section": "Intolerancia al gluten",
    "text": "Intolerancia al gluten\n\n¬øPueden las personas al√©rgicas al gluten distinguir harina com√∫n de harina sin gluten en un ensayo ciego? En un experimento, de 35 sujetos, 12 identificaron correctamente la harina com√∫n y 23 se equivocaron o no supieron decir de qu√© harina se trataba.\nIncluso si no hubiera al√©rgicos al gluten en el experimento, esperar√≠amos encontrar algunas identificaciones correctas‚Ä¶ Bas√°ndonos en el n√∫mero de identificaciones correctas, ¬øcu√°ntos de los sujetos son al√©rgicos al gluten y cu√°ntos estaban adivinando?\n\n\nSupongamos que una persona al√©rgica al gluten tiene una probabilidad de \\(0.90\\) de detectar la harina com√∫n mientras que una persona sin alergia detecta harina com√∫n con una probabilidad de \\(0.40\\) (y con una probabilidad de \\(0.6\\) se equivoca o no sabe decir)."
  },
  {
    "objectID": "presentaciones/presentacion_02.html#section-10",
    "href": "presentaciones/presentacion_02.html#section-10",
    "title": "Estad√≠stica Bayesiana",
    "section": "",
    "text": "Llamemos:\n\n\n\\(N\\) a la cantidad total de personas en el ensayo\n\\(N_a\\) al n√∫mero de personas al√©rgicas al gluten\n\\(\\pi_a\\) a la probabilidad de que un al√©rgico identifique correctamente\n\\(\\pi_f\\) a la probabilidad de que un no al√©rgico identifique correctamente\n\\(n_i\\) al n√∫mero de identificaciones correctas\n\n\n\n¬øCu√°les son las cantidades conocidas? ¬øCu√°les son las cantidades desconocidas? ¬øC√≥mo es el modelo de probabilidad hacia adelante? ¬øC√≥mo es el problema inverso?"
  },
  {
    "objectID": "presentaciones/presentacion_02.html#section-11",
    "href": "presentaciones/presentacion_02.html#section-11",
    "title": "Estad√≠stica Bayesiana",
    "section": "",
    "text": "Conociendo \\(N\\), \\(\\pi_a\\) y \\(\\pi_f\\), si conoci√©ramos \\(N_a\\) podr√≠amos calcular las probabilidades de los diferentes \\(n_i\\): probabilidad hacia adelante\n\nAqu√≠ observamos \\(n_i\\) y queremos realizar inferencias sobre \\(N_a\\): probabilidad inversa"
  },
  {
    "objectID": "presentaciones/presentacion_02.html#section-12",
    "href": "presentaciones/presentacion_02.html#section-12",
    "title": "Estad√≠stica Bayesiana",
    "section": "",
    "text": "Digamos que a priori cualquier n√∫mero de \\(N_a\\) es igualmente probable o esperable:\n\n\\[p(N_a) = \\frac{1}{36}\\]"
  },
  {
    "objectID": "presentaciones/presentacion_02.html#section-13",
    "href": "presentaciones/presentacion_02.html#section-13",
    "title": "Estad√≠stica Bayesiana",
    "section": "",
    "text": "¬øC√≥mo construimos la verosimilitud de los diferentes valores de \\(N_a\\) \\(p(n_i\\mid N_a)\\)?\n\nPensemos de forma generativa (con el modelo de probabilidad hacia adelante). Imaginemos que conocemos \\(N_a\\) (adem√°s de \\(N\\), \\(\\pi_a\\) y \\(\\pi_f\\)), ¬øpodr√≠amos escribir un programa que simule diferentes valores de \\(n_i\\)?"
  },
  {
    "objectID": "presentaciones/presentacion_02.html#section-14",
    "href": "presentaciones/presentacion_02.html#section-14",
    "title": "Estad√≠stica Bayesiana",
    "section": "",
    "text": "El n√∫mero de identificaciones correctas \\(n_i\\) es la suma de las identificaciones correctas entre los \\(N_a\\) al√©rgicos (\\(n_{ia}\\)) y los \\(N-N_a\\) no al√©rgicos (\\(n_{if}\\)). ¬øCu√°ntas identificaciones habr√° en cada grupo?\n\n\\[n_{ia} \\sim Bi(N_a,\\pi_a)\\] \\[n_{if} \\sim Bi(N-N_a,\\pi_f)\\] \\[n_i = n_{ia} + n_{if}\\]\n\n\n\nN &lt;- 35\npi_a &lt;- 0.9\npi_f &lt;- 0.4\nN_a &lt;- 10 # lo suponemos conocido para simular\n\nn_ia &lt;- rbinom(1, N_a, pi_a)\nn_if &lt;- rbinom(1, N-N_a, pi_f)\n\nn_i &lt;- n_ia + n_if\n\n\n\nSabr√≠amos calcular las probabilidades de los diferentes valores de \\(n_{ia}\\) y \\(n_{if}\\), ¬øno?."
  },
  {
    "objectID": "presentaciones/presentacion_02.html#section-15",
    "href": "presentaciones/presentacion_02.html#section-15",
    "title": "Estad√≠stica Bayesiana",
    "section": "",
    "text": "Recordemos que no conocemos \\(N_a\\). En nuestro caso, la verosimilitud de cada valor de \\(N_a\\) es la probabilidad de observar \\(n_i=12\\) para ese valor de \\(N_a\\).\n\n\\[\n\\begin{array}{lll}\np(n_i=12\\mid N_a) & = & p(n_{ia}=0\\mid N_a)p(n_{if}=12\\mid N_a) \\\\\n& & \\quad + p(n_{ia}=1\\mid N_a)p(n_{if}=11\\mid N_a) + \\dots\n\\end{array}\n\\]\n\n\nQueda como ejercicio calcular a mano \\(p(n_i\\mid N_a)\\) o, mejor a√∫n, escribir un programita que calcule \\(p(n_i\\mid N_a)\\)"
  },
  {
    "objectID": "presentaciones/presentacion_02.html#section-16",
    "href": "presentaciones/presentacion_02.html#section-16",
    "title": "Estad√≠stica Bayesiana",
    "section": "",
    "text": "Finalmente,\n\\[p(N_a\\mid n_i) = \\frac{p(n_i\\mid N_a) p(N_a)}{p(n_i)}\\]"
  },
  {
    "objectID": "presentaciones/presentacion_02.html#vocabulario-limitado",
    "href": "presentaciones/presentacion_02.html#vocabulario-limitado",
    "title": "Estad√≠stica Bayesiana",
    "section": "Vocabulario limitado",
    "text": "Vocabulario limitado\n\nSupongamos que existe un idioma con seis palabras: \\[ \\text{\\{perro, parra, farra, carro, corro, tarro\\}} \\]\n\n\nTodas las palabras son igualmente probables, excepto por ‚Äòperro‚Äô, que es \\(\\alpha=3\\) veces m√°s probable que las otras.\nCuando se tipean, un caracter se introduce err√≥neamente con probabilidad \\(\\pi=0.1\\).\nTodas las letras tienen la misma probabilidad de producir un error de tipeo.\nSi una letra se tipe√≥ mal, la probabilidad de cometer un error en otro caracter no cambia.\nLos errores son independientes a lo largo de una palabra."
  },
  {
    "objectID": "presentaciones/presentacion_02.html#section-18",
    "href": "presentaciones/presentacion_02.html#section-18",
    "title": "Estad√≠stica Bayesiana",
    "section": "",
    "text": "¬øCu√°l es la probabilidad de escribir correctamente ‚Äòtarro‚Äô?\n¬øCu√°l es la probabilidad de tipear ‚Äòcerro‚Äô o ‚Äòcurro‚Äô al querer escribir ‚Äòcarro‚Äô?\nDesarrollar un corrector gramatical para esta lengua: para las palabras tipeadas ‚Äòfarra‚Äô, ‚Äòbirra‚Äô y ‚Äòlocos‚Äô, ¬øcu√°l es la palabra que se quiso escribir?"
  },
  {
    "objectID": "presentaciones/presentacion_02.html#section-19",
    "href": "presentaciones/presentacion_02.html#section-19",
    "title": "Estad√≠stica Bayesiana",
    "section": "",
    "text": "La probabilidad de escribir correctamente ‚Äòtarro‚Äô es \\((1-\\pi)^5\\)\nLa probabilidad de escribir correctamente ‚Äòcerro‚Äô o ‚Äòcurro‚Äô al querer escribir ‚Äòcarro‚Äô es \\(\\pi (1-\\pi)^4\\)\nAll√° vamos‚Ä¶"
  },
  {
    "objectID": "presentaciones/presentacion_02.html#section-20",
    "href": "presentaciones/presentacion_02.html#section-20",
    "title": "Estad√≠stica Bayesiana",
    "section": "",
    "text": "Estas son las probabilidades a priori de cada una de las palabras del vocabulario"
  },
  {
    "objectID": "presentaciones/presentacion_02.html#section-21",
    "href": "presentaciones/presentacion_02.html#section-21",
    "title": "Estad√≠stica Bayesiana",
    "section": "",
    "text": "Alguien escribe ‚Äòfarra‚Äô, ¬øqu√© quiso escribir?\n\n¬øQu√© ser√≠a en este caso la verosimilitud?\n\n\nLa verosimilitud de ‚Äòperro‚Äô es qu√© tan probable es escribir ‚Äòfarra‚Äô cuando se quer√≠a escribir ‚Äòperro‚Äô: \\(p(\\mathrm{farra}\\mid\\mathrm{perro})=\\pi^3(1-\\pi)^2\\)"
  },
  {
    "objectID": "presentaciones/presentacion_02.html#section-23",
    "href": "presentaciones/presentacion_02.html#section-23",
    "title": "Estad√≠stica Bayesiana",
    "section": "",
    "text": "Para obtener la probabilidad a posteriori de cada palabra, necesitamos combinar la informaci√≥n a priori con los datos (¬øcu√°les son los datos?). Aplicamos la Regla de Bayes:\n\\[p(\\mathrm{palabra}\\mid \\mathrm{farra}) = \\frac{p(\\mathrm{farra}\\mid \\mathrm{palabra})p(\\mathrm{palabra})}{p(\\mathrm{farra})}\\]"
  },
  {
    "objectID": "presentaciones/presentacion_02.html#section-25",
    "href": "presentaciones/presentacion_02.html#section-25",
    "title": "Estad√≠stica Bayesiana",
    "section": "",
    "text": "La inferencia bayesiana es la realocaci√≥n de la credibilidad del conjunto de cantidades desconocidas (par√°metros) de un modelo, una vez observado un conjunto de datos."
  },
  {
    "objectID": "presentaciones/presentacion_02.html#peque√±o-mundo",
    "href": "presentaciones/presentacion_02.html#peque√±o-mundo",
    "title": "Estad√≠stica Bayesiana",
    "section": "Peque√±o mundo",
    "text": "Peque√±o mundo\n\nSe desea estimar la proporci√≥n de agua que cubre el planeta Tierra. Para ello se arroja hacia arriba un ‚Äúglobo terr√°queo antiestr√©s‚Äù y se registra la posici√≥n del dedo √≠ndice al volver a tomarlo.\nSe arroja el globo 11 veces hacia arriba y se obtiene la siguiente secuencia: \\[TAAATTAATAA\\]"
  },
  {
    "objectID": "presentaciones/presentacion_02.html#section-26",
    "href": "presentaciones/presentacion_02.html#section-26",
    "title": "Estad√≠stica Bayesiana",
    "section": "",
    "text": "Llamemos:\n\n\n\\(\\pi\\) a la proporci√≥n de agua en el planeta Tierra\n\\(N\\) al n√∫mero de tiradas\n\\(y\\) al n√∫mero de veces que sali√≥ agua\n\n\n\n\\(\\pi\\) es una cantidad continua entre 0 y 1. Esta vez no la discretizaremos."
  },
  {
    "objectID": "presentaciones/presentacion_02.html#prior",
    "href": "presentaciones/presentacion_02.html#prior",
    "title": "Estad√≠stica Bayesiana",
    "section": "Prior",
    "text": "Prior\n\n¬øC√≥mo asignamos una credibilidad a priori para los valores de \\(\\pi_a\\)?\n\n\nCon una distribuci√≥n de probabilidad.\n\n\n\\[\n\\pi \\sim \\mathrm{Beta}(a,b)\n\\]\n\n\n\\[\np(\\pi\\mid a,b) = p(\\pi) = \\frac{\\pi^{a-1} (1-\\pi)^{b-1}}{B(a,b)}\n\\]\n\n\n\\[\nB(a,b) = \\int_0^1 \\pi^{a-1} (1-\\pi)^{b-1} d\\pi = \\frac{\\Gamma(a)\\Gamma(b)}{\\Gamma(a+b)}\n\\]\n\n\n\\[\n\\Gamma(x) = \\int_0^\\infty u^{x-1} e^{-u} du\n\\]"
  },
  {
    "objectID": "presentaciones/presentacion_02.html#section-27",
    "href": "presentaciones/presentacion_02.html#section-27",
    "title": "Estad√≠stica Bayesiana",
    "section": "",
    "text": "La distribuci√≥n beta"
  },
  {
    "objectID": "presentaciones/presentacion_02.html#section-28",
    "href": "presentaciones/presentacion_02.html#section-28",
    "title": "Estad√≠stica Bayesiana",
    "section": "",
    "text": "La distribuci√≥n beta"
  },
  {
    "objectID": "presentaciones/presentacion_02.html#section-29",
    "href": "presentaciones/presentacion_02.html#section-29",
    "title": "Estad√≠stica Bayesiana",
    "section": "",
    "text": "Una posible elecci√≥n de valores para la distribuci√≥n a priori es \\(\\text{Beta}(2,2)\\)"
  },
  {
    "objectID": "presentaciones/presentacion_02.html#likelihood",
    "href": "presentaciones/presentacion_02.html#likelihood",
    "title": "Estad√≠stica Bayesiana",
    "section": "Likelihood",
    "text": "Likelihood\n¬øCu√°l es la probabilidad de observar los datos que observamos para diferentes valores del par√°metro?\n\n\\[\nY \\mid \\pi, N \\sim Bi(N,\\pi)\n\\]\n\\[p(y\\mid \\pi, N) = {N \\choose y }\\pi^y (1-\\pi)^{N-y} = p(y\\mid \\pi)\\]"
  },
  {
    "objectID": "presentaciones/presentacion_02.html#posterior",
    "href": "presentaciones/presentacion_02.html#posterior",
    "title": "Estad√≠stica Bayesiana",
    "section": "Posterior",
    "text": "Posterior\n\\[p(\\pi\\mid y) = \\frac{p(y\\mid \\pi)p(\\pi)}{p(y)}\\]\n\n\\[p(\\pi\\mid y) = \\frac{{N \\choose y }\\pi^y (1-\\pi)^{N-y}  \\frac{\\pi^{a-1} (1-\\pi)^{b-1}}{B(a,b)}}{\\int p(y\\mid\\pi) p(\\pi) d \\pi}\\]\n\n\nLa integral en el denominador suele ser un problema. Con dos par√°metros es una integral doble, con tres par√°metros, una triple, etc. Esta integral puede ser intratable (intractable) (no tener soluci√≥n exacta, anal√≠tica, cerrada). No hay vaca vestida de uniforme que nos salve."
  },
  {
    "objectID": "presentaciones/presentacion_02.html#section-30",
    "href": "presentaciones/presentacion_02.html#section-30",
    "title": "Estad√≠stica Bayesiana",
    "section": "",
    "text": "Recordando que: \\[\n\\mathrm{posterior} \\propto \\mathrm{prior}\\times\\mathrm{likelihood}\n\\]\n\nResulta \\[p(\\pi\\mid y) \\propto p(y\\mid\\pi) p(\\pi)\\]\n\n\n\\[p(\\pi\\mid y) \\propto {N \\choose y }\\pi^y (1-\\pi)^{N-y} \\frac{1}{B(a,b)} \\pi^{a-1}(1-\\pi)^{b-1}\\]\n\n\n\\[p(\\pi\\mid y) \\propto {N \\choose y } \\frac{1}{B(a,b)} \\pi^{(y+a)-1} (1-\\pi)^{(N-y+b)-1}\\]\n\n\n\\[p(\\pi\\mid y) = K C  \\pi^{(y+a)-1} (1-\\pi)^{(N-y+b)-1}\\]\n\n\n\\[p(\\pi\\mid y) = K^* \\pi^{(y+a)-1} (1-\\pi)^{(N-y+b)-1}\\]"
  },
  {
    "objectID": "presentaciones/presentacion_02.html#section-31",
    "href": "presentaciones/presentacion_02.html#section-31",
    "title": "Estad√≠stica Bayesiana",
    "section": "",
    "text": "Para que \\(\\int_0^1 p(\\pi\\mid y) d \\pi = 1\\), debe ser\n\n\\[K^* = \\frac{1}{B(y+a,N-y+b)} = \\frac{\\Gamma\\left[(y+a)+(N-y+b)\\right]}{\\Gamma(y+a)\\Gamma(N-y+b)}\\]\n\n\nPor lo tanto, resulta que la distribuci√≥n a posteriori es Beta de par√°metros \\(y+a\\) y \\(N-y+b\\)\n\\[\np(\\pi\\mid y) = \\frac{\\pi^{(y+a)-1}(1-\\pi)^{(N-y+b)-1}}{B(y+a,N-y+b)}\n\\]\n\\[\n\\pi\\mid y \\sim  \\text{Beta}(y+a,N-y+b)\n\\]"
  },
  {
    "objectID": "presentaciones/presentacion_02.html#qu√©-hicimos",
    "href": "presentaciones/presentacion_02.html#qu√©-hicimos",
    "title": "Estad√≠stica Bayesiana",
    "section": "¬øQu√© hicimos?",
    "text": "¬øQu√© hicimos?\n\nNos las arreglamos para encontrar la soluci√≥n exacta al problema de inferir el par√°metro de una distribuci√≥n binomial a partir del n√∫mero de √©xitos observados.\n\n\nEl prior y el posterior tienen la misma forma distribucional. Esto ocurre por la elecci√≥n del prior y el likelihood."
  },
  {
    "objectID": "presentaciones/presentacion_02.html#section-32",
    "href": "presentaciones/presentacion_02.html#section-32",
    "title": "Estad√≠stica Bayesiana",
    "section": "",
    "text": "Una distribuci√≥n \\(\\mathcal{F}\\) se dice conjugada de una verosimilitud \\(\\mathcal{L}\\) si cuando la distribuci√≥n a priori es \\(\\mathcal{F}\\), la distribuci√≥n a posteriori tambi√©n es \\(\\mathcal{F}\\)"
  },
  {
    "objectID": "presentaciones/presentacion_02.html#peque√±o-mundo-1",
    "href": "presentaciones/presentacion_02.html#peque√±o-mundo-1",
    "title": "Estad√≠stica Bayesiana",
    "section": "Peque√±o mundo",
    "text": "Peque√±o mundo\n\nSe desea estimar la proporci√≥n de agua que cubre el planeta Tierra. Para ello se arroja hacia arriba un ‚Äúglobo terr√°queo antiestr√©s‚Äù y se registra la posici√≥n del dedo √≠ndice al volver a tomarlo.\nSe arroja el globo 11 veces hacia arriba y se obtiene la siguiente secuencia: \\[TAAATTAATAA\\]\n\n\n\\[\n\\begin{align*}\n    Y \\mid\\pi & \\sim  \\text{Binomial}(N,\\pi)\\\\\n    \\pi & \\sim  \\text{Beta}(a,b)\n\\end{align*}\n\\]"
  },
  {
    "objectID": "presentaciones/presentacion_02.html#section-33",
    "href": "presentaciones/presentacion_02.html#section-33",
    "title": "Estad√≠stica Bayesiana",
    "section": "",
    "text": "con \\(N=11\\), \\(a=2\\) y \\(b=2\\).\nAl observar \\(y=7\\) resulta\n\\[\\pi\\mid y \\sim \\text{Beta}(a+y,b+N-y)\\] \\[p(\\pi\\mid y) = \\text{Beta}(2+7,2+4)\\]"
  },
  {
    "objectID": "presentaciones/presentacion_02.html#m√°s-ejemplos",
    "href": "presentaciones/presentacion_02.html#m√°s-ejemplos",
    "title": "Estad√≠stica Bayesiana",
    "section": "M√°s ejemplos",
    "text": "M√°s ejemplos\nQueremos estimar la probabilidad \\(\\pi\\) de que salga cara al arrojar una moneda.\n\nCredibilidad a priori: \\(\\text{Beta}(2,2)\\)\n\n\n¬øC√≥mo cambia nuestra creencia si‚Ä¶\n\n‚Ä¶realizamos 6 tiradas y observamos 4 caras?\n‚Ä¶realizamos 60 tiradas y observamos 40 caras?\n‚Ä¶realizamos 2 tiradas y observamos 2 caras?\n‚Ä¶realizamos 40 tiradas y observamos 40 caras?\n‚Ä¶realizamos 4 tiradas y obtenemos 3 caras y luego realizamos 2 tiradas m√°s y observamos 1 caras?"
  },
  {
    "objectID": "presentaciones/presentacion_02.html#section-35",
    "href": "presentaciones/presentacion_02.html#section-35",
    "title": "Estad√≠stica Bayesiana",
    "section": "",
    "text": "\\(\\pi \\mid y \\sim \\text{Beta}(2+4,2+2)\\)\n\\(\\pi \\mid y \\sim \\text{Beta}(2+40,2+20)\\)\n\\(\\pi \\mid y \\sim \\text{Beta}(2+2,2+0)\\)\n\\(\\pi \\mid y \\sim \\text{Beta}(2+40,2+0)\\)\n\\(\\pi \\mid y \\sim \\text{Beta}((2+3)+1,(2+1)+1)\\)"
  },
  {
    "objectID": "presentaciones/presentacion_02.html#section-36",
    "href": "presentaciones/presentacion_02.html#section-36",
    "title": "Estad√≠stica Bayesiana",
    "section": "",
    "text": "4 caras en 6 tiradas"
  },
  {
    "objectID": "presentaciones/presentacion_02.html#section-37",
    "href": "presentaciones/presentacion_02.html#section-37",
    "title": "Estad√≠stica Bayesiana",
    "section": "",
    "text": "40 caras en 60 tiradas"
  },
  {
    "objectID": "presentaciones/presentacion_02.html#section-38",
    "href": "presentaciones/presentacion_02.html#section-38",
    "title": "Estad√≠stica Bayesiana",
    "section": "",
    "text": "2 caras en 2 tiradas"
  },
  {
    "objectID": "presentaciones/presentacion_02.html#section-39",
    "href": "presentaciones/presentacion_02.html#section-39",
    "title": "Estad√≠stica Bayesiana",
    "section": "",
    "text": "40 caras en 40 tiradas"
  },
  {
    "objectID": "presentaciones/presentacion_02.html#section-40",
    "href": "presentaciones/presentacion_02.html#section-40",
    "title": "Estad√≠stica Bayesiana",
    "section": "",
    "text": "3 caras en 4 tiradas, luego 1 cara en 2 tiradas"
  },
  {
    "objectID": "presentaciones/presentacion_02.html#caracter√≠sticas-generales",
    "href": "presentaciones/presentacion_02.html#caracter√≠sticas-generales",
    "title": "Estad√≠stica Bayesiana",
    "section": "Caracter√≠sticas generales",
    "text": "Caracter√≠sticas generales\nLa inferencia bayesiana presenta ciertas caracter√≠sticas que se repiten independientemente de las distribuciones elegidas."
  },
  {
    "objectID": "presentaciones/presentacion_02.html#compromiso",
    "href": "presentaciones/presentacion_02.html#compromiso",
    "title": "Estad√≠stica Bayesiana",
    "section": "Compromiso",
    "text": "Compromiso\nVamos a formalizar lo que observamos en el ejemplo para el modelo Beta‚ÄìBinomial. Para esto ser√° √∫til el siguiente resultado:\n\\[\\text{Si } X \\sim \\text{Beta}(a,b)\\]\n\\[\\mathbb{E}({X}) = \\frac{a}{a+b}\\]"
  },
  {
    "objectID": "presentaciones/presentacion_02.html#section-41",
    "href": "presentaciones/presentacion_02.html#section-41",
    "title": "Estad√≠stica Bayesiana",
    "section": "",
    "text": "La distribuci√≥n a priori es \\(\\text{Beta}(a,b)\\) y la distribuci√≥n a posteriori es \\(\\text{Beta}(y+a,N-y+b)\\). La media del es:\n\\[\n\\begin{align*}\n\\mathbb{E}[{p(\\pi\\mid y)}] & = \\frac{y+a}{a+b+N} \\\\\n& = \\frac{y}{a+b+N} + \\frac{a}{a+b+N} \\\\\n& = \\frac{N}{a+b+N}\\frac{y}{N} + \\frac{a+b}{a+b+N} \\frac{a}{a+b} \\\\\n& = \\frac{N}{a+b+N}\\frac{y}{N} + \\frac{a+b}{a+b+N} \\mathbb{E}{[p(\\pi)]}\n\\end{align*}\n\\]"
  },
  {
    "objectID": "presentaciones/presentacion_02.html#section-42",
    "href": "presentaciones/presentacion_02.html#section-42",
    "title": "Estad√≠stica Bayesiana",
    "section": "",
    "text": "La distribuci√≥n a posteriori representa un balance (promedio ponderado o combinaci√≥n convexa) entre la proporci√≥n observada y la proporci√≥n esperada a priori. Hay un shrinkage hacia la media del prior."
  },
  {
    "objectID": "presentaciones/presentacion_02.html#secuencialidad",
    "href": "presentaciones/presentacion_02.html#secuencialidad",
    "title": "Estad√≠stica Bayesiana",
    "section": "Secuencialidad",
    "text": "Secuencialidad\nSi primero observamos \\(y_1\\) en \\(N_1\\) y luego observamos \\(y_2\\) en \\(N_2\\)‚Ä¶ Con el primer conjunto de datos pasamos del prior al posterior y luego esa distribuci√≥n se convierte en el nuevo prior:\n\n\\[\\text{Beta}(a,b) \\rightarrow \\text{Beta}(y_1 + a, N_1 - y_1 + b)\\]\n\n\n\\[\\text{Beta}(y_1 + a, N_1 - y_1 + b) \\rightarrow \\text{Beta}(y_2 + y_1 + a,N_2 - y_2 + N_1 - y_1 + b)\\]\n\n\n\\[\\text{Beta}(a,b) \\rightarrow \\text{Beta}((y_1+y_2) + a, (N_1+N_2) - (y_1+y_2) + b)\\]\n\n\nEs id√©ntico a observar \\(y_1+y_2\\) en \\(N_1+N_2\\)"
  },
  {
    "objectID": "presentaciones/presentacion_02.html#section-43",
    "href": "presentaciones/presentacion_02.html#section-43",
    "title": "Estad√≠stica Bayesiana",
    "section": "",
    "text": "La Regla de Bayes permite combinar dos fuentes de informaci√≥n: la informaci√≥n a priori (lo que sabemos hasta el momento), y la nueva informaci√≥n (representada por la verosimilitud). La distribuci√≥n a posteriori representa un compromiso entre la verosimilitud de los datos y la credibilidad a priori."
  },
  {
    "objectID": "presentaciones/presentacion_02.html#predicciones",
    "href": "presentaciones/presentacion_02.html#predicciones",
    "title": "Estad√≠stica Bayesiana",
    "section": "Predicciones",
    "text": "Predicciones\nDistribuci√≥n predictiva a posteriori (tambi√©n distribuci√≥n posterior predictiva) (en ingl√©s posterior predictive distribution): queremos predecir un valor futuro de la variable de inter√©s, \\(\\tilde{y}\\). M√°s a√∫n, interesa la distribuci√≥n de \\(\\tilde{y}\\) a posteriori, es decir, luego de observar los datos \\(y\\): \\(\\tilde{y}\\mid y\\)\n\\[\np(\\tilde{y}\\mid y) = \\int p(\\tilde{y}\\mid\\pi) p(\\pi\\mid y) d\\pi\n\\]"
  },
  {
    "objectID": "presentaciones/presentacion_02.html#section-53",
    "href": "presentaciones/presentacion_02.html#section-53",
    "title": "Estad√≠stica Bayesiana",
    "section": "",
    "text": "\\(\\tilde{y}\\) tiene una distribuci√≥n de probabilidad\nSi \\(\\pi\\) fuera fijo, la distribuci√≥n de \\(\\tilde{y}\\) viene dada por \\(p(\\tilde{y}\\mid\\pi)\\) (la verosimilitud, aunque ahora es funci√≥n de \\(\\tilde{y}\\))\nPero ahora hay incertidumbre en \\(\\pi\\) (tiene una distribuci√≥n a posteriori), por lo tanto se hace una ponderaci√≥n para los distintos valores de \\(\\pi\\) (\\(\\pi\\) var√≠a en la integral anterior)\nCombinamos lo que no sabemos porque es aleatorio per se, con aquello que desconocemos (aunque podemos reducir nuestra incertidumbre recolectando m√°s informaci√≥n)"
  },
  {
    "objectID": "presentaciones/presentacion_02.html#section-54",
    "href": "presentaciones/presentacion_02.html#section-54",
    "title": "Estad√≠stica Bayesiana",
    "section": "",
    "text": "Para el caso binomial que venimos estudiando, consideramos una realizaci√≥n m√°s (tirar el globo terr√°queo y agarrarlo). ¬øCu√°l es la probabilidad de obtener \\(A\\) (agua)?\n\\[\n\\begin{align*}\np(\\tilde{y} = 1\\mid y) & = \\int_0^1 \\pi \\vphantom{\\tilde{y}}p(\\pi\\mid x) d \\pi = \\mathbb{E}[{p(\\pi\\mid x)}] \\\\\n& = \\frac{y+a}{y+a+N-y+b} = \\frac{y+a}{N+a+b} \\\\\n\\end{align*}\n\\]"
  },
  {
    "objectID": "presentaciones/presentacion_02.html#section-55",
    "href": "presentaciones/presentacion_02.html#section-55",
    "title": "Estad√≠stica Bayesiana",
    "section": "",
    "text": "muestras_pi &lt;- rbeta(2000,a+y,b+N-y) # muestras del posterior\nx_new &lt;- rbinom(2000,1,muestras_pi) # predicciones para cada valor de pi"
  },
  {
    "objectID": "presentaciones/presentacion_02.html#section-56",
    "href": "presentaciones/presentacion_02.html#section-56",
    "title": "Estad√≠stica Bayesiana",
    "section": "",
    "text": "Consideremos un caso particular:\n\nEn una bolsa hay bolitas negras y blancas, queremos saber cu√°l es la probabilidad de sacar una bolita negra. A priori no sabemos nada. Sacamos (con reposici√≥n) tres veces una bolita. Las tres veces sale negra. ¬øCu√°l es la probabilidad de que la pr√≥xima bolita sea negra?\n\n\\[p(\\tilde{y}=1\\mid y) = \\frac{y+1}{N+2}\\]"
  },
  {
    "objectID": "presentaciones/presentacion_02.html#section-57",
    "href": "presentaciones/presentacion_02.html#section-57",
    "title": "Estad√≠stica Bayesiana",
    "section": "",
    "text": "Los par√°metros tienen una distribuci√≥n de probabilidad. Incorporar la incertidumbre en el valor de \\(\\pi\\) nos permite no entusiasmarnos tanto con los datos, hacer predicciones m√°s conservadoras con a pocos datos, regularizar.\n\n\n\n\nEstad√≠stica Bayesiana ‚Äì 2025"
  },
  {
    "objectID": "presentaciones/presentacion_04.html#section",
    "href": "presentaciones/presentacion_04.html#section",
    "title": "Estad√≠stica Bayesiana",
    "section": "",
    "text": "Un meteor√≥logo estima, con un 95% de confianza, que la probabilidad de que el hurac√°n no llegue a la ciudad est√° entre 99% y 100%. Muy feliz con su precisi√≥n y su modelo, aconseja que la evacuaci√≥n de la ciudad no es necesaria. Desafortunadamente, el hurac√°n llega a la ciudad produciendo una grave inundaci√≥n.\n\n\n‚ÄúI would rather be vaguely right than very wrong.‚Äù"
  },
  {
    "objectID": "presentaciones/presentacion_04.html#section-1",
    "href": "presentaciones/presentacion_04.html#section-1",
    "title": "Estad√≠stica Bayesiana",
    "section": "",
    "text": "Con el clima, las personas tienden a notar un error m√°s que otro. Cuando llueve sin estar anunciado, se tiende a insultar al servicio meteorol√≥gico mientras que la ausencia de lluvia a pesar del pron√≥stico se toma con buena cara.\n\n\n\nEl Weather Channel exagera ligeramente la probabilidad de lluvia cuando es poco probable que ocurra: dicen que es de 20% cuando en realidad es de 5% o 10%"
  },
  {
    "objectID": "presentaciones/presentacion_04.html#section-2",
    "href": "presentaciones/presentacion_04.html#section-2",
    "title": "Estad√≠stica Bayesiana",
    "section": "",
    "text": "En la estad√≠stica bayesiana, la distribuci√≥n a posteriori es la base de todas inferencia: combina el conocimiento a priori con la informaci√≥n provista por los datos. Contiene todo lo que se sabe y no se sabe sobre un par√°metro desconocido.\n\nLa respuesta a los problemas es toda la distribuci√≥n a posteriori de los par√°metros (y de otras cantidades de inter√©s)."
  },
  {
    "objectID": "presentaciones/presentacion_04.html#section-3",
    "href": "presentaciones/presentacion_04.html#section-3",
    "title": "Estad√≠stica Bayesiana",
    "section": "",
    "text": "No obstante, puede ser de utilidad (o incluso necesario) tomar decisiones concretas o resumir la distribuci√≥n a posteriori.\n\n\\(a\\) es la acci√≥n que tomamos (intervenir o no intervenir quir√∫rgicamente a una persona) o la respuesta que damos (ganancia de una campa√±a de marketing).\n\n\nPuede ser una estimaci√≥n puntual \\(\\hat{\\theta}\\): dada una inferencia sobre la ganancia de una campa√±a de marketing, es necesario informar un valor puntual (quiz√°s con un intervalo)"
  },
  {
    "objectID": "presentaciones/presentacion_04.html#section-4",
    "href": "presentaciones/presentacion_04.html#section-4",
    "title": "Estad√≠stica Bayesiana",
    "section": "",
    "text": "Tratamos a los par√°metros sobre los que realizamos inferencias como variables aleatorias. Una muestra de la distribuci√≥n a posteriori es una posible realizaci√≥n del verdadero valor del par√°metro.\n\nAl dar una respuesta (o resumir la informaci√≥n a posteriori), podemos incurrir en un error (grande o chico) seg√∫n se den los eventos posibles.\n\n\n¬øQu√© es un error? ¬øC√≥mo definimos si el error es grande o chico? ¬øC√≥mo definimos si el error es relevante o no?"
  },
  {
    "objectID": "presentaciones/presentacion_04.html#funciones-de-p√©rdida",
    "href": "presentaciones/presentacion_04.html#funciones-de-p√©rdida",
    "title": "Estad√≠stica Bayesiana",
    "section": "Funciones de p√©rdida",
    "text": "Funciones de p√©rdida\n\\[L(\\theta,\\hat{\\theta}) = f(\\theta,\\hat{\\theta})\\] es una funci√≥n de p√©rdida, qu√© tanto pierdo por usar \\(\\hat{\\theta}\\) para estimar \\(\\theta\\)."
  },
  {
    "objectID": "presentaciones/presentacion_04.html#section-5",
    "href": "presentaciones/presentacion_04.html#section-5",
    "title": "Estad√≠stica Bayesiana",
    "section": "",
    "text": "Por ejemplo:\n\\[L_2 = (\\theta - \\hat{\\theta})^2\\]\n\\[L_1 = |\\theta - \\hat{\\theta}|\\]\n\\[L_{0/1} =\n\\begin{cases}\n0 \\text{ si } \\hat{\\theta} = \\theta  \\\\\n1 \\text{ si } \\hat{\\theta} \\neq \\theta\n\\end{cases}\\]\n\\[L( \\theta, \\hat{\\theta} ) = \\begin{cases} ( \\theta -  \\hat{\\theta} )^2 & \\hat{\\theta} \\lt \\theta \\\\\\\\ c( \\theta -  \\hat{\\theta} )^2 & \\hat{\\theta} \\ge \\theta, \\;\\; 0\\lt c \\lt 1 \\end{cases}\\]"
  },
  {
    "objectID": "presentaciones/presentacion_04.html#section-6",
    "href": "presentaciones/presentacion_04.html#section-6",
    "title": "Estad√≠stica Bayesiana",
    "section": "",
    "text": "Buscamos elegir \\(\\hat{\\theta}\\) de manera tal que minimice \\(L\\). El problema es que no conocemos \\(\\theta\\) y por lo tanto no podemos calcular \\(L(\\theta,\\hat{\\theta})\\).\n\n¬øSabemos algo sobre \\(\\theta\\) que nos pueda ayudar? Conocemos su distribuci√≥n a posteriori\n\n\nPodemos promediar \\(L\\) para los valores posibles de \\(\\theta\\) (ponderando seg√∫n la distribuci√≥n a posteriori)"
  },
  {
    "objectID": "presentaciones/presentacion_04.html#section-7",
    "href": "presentaciones/presentacion_04.html#section-7",
    "title": "Estad√≠stica Bayesiana",
    "section": "",
    "text": "El riesgo a posteriori (posterior risk o posterior expected loss) es la p√©rdida esperada ponderada por los valores de \\(\\theta\\) (y su distribuci√≥n a posteriori).\n\n\\[R(\\hat{\\theta}) = \\mathbb{E}_{\\theta\\mid y}[L(\\theta,\\hat\\theta)] = \\int L(\\theta,\\hat\\theta) p(\\theta\\mid y) d\\theta\\]\n\n\nEs una funci√≥n de los posibles valores que puede tomar \\(\\hat\\theta\\).\n\n\nPodemos obtener el \\(\\hat\\theta\\) que minimice \\(R(\\hat\\theta)\\). Es decir, buscamos un valor (un estimador) que minimice la p√©rdida esperada al usarlo para resumir \\(p(\\theta\\mid y)\\): \\[\\hat{\\theta} = \\underset{\\hat\\theta}{\\mathrm{arg\\,min}}\\left[ R(\\hat\\theta) \\right]\\]"
  },
  {
    "objectID": "presentaciones/presentacion_04.html#section-8",
    "href": "presentaciones/presentacion_04.html#section-8",
    "title": "Estad√≠stica Bayesiana",
    "section": "",
    "text": "Simulemos‚Ä¶\n\nSupongamos que \\(\\theta\\mid y \\sim \\mathrm{Beta}(2,9)\\)\n\n\n\n\\(R\\) es una funci√≥n de \\(\\hat\\theta\\) (los distintos valores que podemos usar para resumir \\(p(\\theta\\mid y)\\))\nPara distintos valores de \\(\\hat\\theta\\) voy a tomar muestras de \\(p(\\theta\\mid y)\\) y calcular la p√©rdida \\(L\\)\nPara cada valor de \\(\\hat\\theta\\) voy a calcular la p√©rdida promedio (ya va a estar ponderada por la probabilidad a posteriori de \\(\\theta\\))"
  },
  {
    "objectID": "presentaciones/presentacion_04.html#section-9",
    "href": "presentaciones/presentacion_04.html#section-9",
    "title": "Estad√≠stica Bayesiana",
    "section": "",
    "text": "L_2 &lt;- function(theta,theta_hat) (theta-theta_hat)^2\n\nloss &lt;- data.frame(theta_hat = double(),\n                   theta = double(),\n                   L = double())\n\nfor(theta_hat in seq(0,1,0.008)){\n  theta &lt;- rbeta(2000, shape1 = 2, shape2 = 9)\n  L &lt;- L_2(theta,theta_hat)\n  loss &lt;- bind_rows(loss,data.frame(theta_hat,theta,L))\n}\n\nexpected.loss &lt;- loss |&gt;\n  group_by(theta_hat) |&gt;\n  summarise(loss.mean = mean(L))"
  },
  {
    "objectID": "presentaciones/presentacion_04.html#section-11",
    "href": "presentaciones/presentacion_04.html#section-11",
    "title": "Estad√≠stica Bayesiana",
    "section": "",
    "text": "Si deseamos resumir la distribuci√≥n a posteriori con un √∫nico valor (¬°perdiendo informaci√≥n!), puede usarse:\n\n\nLa media: minimiza la p√©rdida cuadr√°tica esperada a posteriori\nLa mediana: minimiza la p√©rdida absoluta esperada a posteriori\nLa moda (tambi√©n llamado MAP por maximum a posteriori o estimador generalizado de m√°xima verosimilitud): minimiza la p√©rdida \\(0/1\\) esperada a posteriori"
  },
  {
    "objectID": "presentaciones/presentacion_04.html#section-12",
    "href": "presentaciones/presentacion_04.html#section-12",
    "title": "Estad√≠stica Bayesiana",
    "section": "",
    "text": "Una prueba m√°s formal para el caso de la media‚Ä¶\n\nSea la p√©rdida cuadr√°tica \\(L(\\theta,\\hat\\theta)=(\\theta-\\hat\\theta)^2\\), el riesgo (posterior expected loss) es:\n\n\n\\[\n\\mathbb{E}_{\\theta\\mid y}[L(\\theta,\\hat\\theta)] = \\mathbb{E}_{\\theta\\mid y}[\\theta^2] - 2 \\hat{\\theta}\\mathbb{E}_{\\theta\\mid y}[\\theta] + {\\hat{\\theta}}^2\n\\]\n\n\nderivando respecto a \\(\\hat{\\theta}\\) e igualando a cero se obtiene que \\(\\underset{\\hat\\theta}{\\mathrm{arg\\,min}}\\left[ R(\\hat\\theta) \\right] = \\mathbb{E}_{\\theta\\mid y}[\\theta] = \\mathbb{E}[p(\\theta\\mid y)]\\)"
  },
  {
    "objectID": "presentaciones/presentacion_04.html#section-13",
    "href": "presentaciones/presentacion_04.html#section-13",
    "title": "Estad√≠stica Bayesiana",
    "section": "",
    "text": "Una prueba m√°s formal para el caso de la mediana‚Ä¶\n\nSea la p√©rdida absoluta \\(L(\\theta,\\hat\\theta)=|\\theta-\\hat\\theta|\\), el riesgo (posterior expected loss) es:\n\n\n\\[\n\\mathbb{E}_{\\theta\\mid y}[L(\\theta,\\hat\\theta)] = \\int_{-\\infty}^\\infty |\\theta-\\hat\\theta| p(\\theta\\mid y)d\\theta\n\\]\n\n\n\\[\n\\int_{-\\infty}^\\hat{\\theta} (\\hat\\theta-\\theta) p(\\theta\\mid y)d\\theta + \\int_{\\hat{\\theta}}^\\hat{\\infty} (\\theta-\\hat\\theta) p(\\theta\\mid y)d\\theta\n\\]\n\n\nPara derivar, se utiliza la regla integral de Leibniz:\n\n\n\\[\\frac{d}{d\\hat\\theta}\\int_{-\\infty}^{\\hat\\theta} g(\\hat\\theta,\\theta)d\\theta = g(\\hat\\theta,\\hat\\theta) + \\int_{-\\infty}^\\hat\\theta \\frac{\\partial}{\\partial\\hat\\theta}g(\\hat\\theta,\\theta)d\\theta\\]\n\n\nSe puede probar que \\(\\int_{-\\infty}^\\hat\\theta p(\\theta\\mid y)d\\theta = \\frac{1}{2}\\), por lo que el \\(\\hat\\theta\\) que minimiza la expresi√≥n es la mediana."
  },
  {
    "objectID": "presentaciones/presentacion_04.html#intervalos-de-credibilidad",
    "href": "presentaciones/presentacion_04.html#intervalos-de-credibilidad",
    "title": "Estad√≠stica Bayesiana",
    "section": "Intervalos de Credibilidad",
    "text": "Intervalos de Credibilidad\nTambi√©n llamados: intervalos de probabilidad, intervalo de confianza bayesiano, regi√≥n de credibilidad. Es una regi√≥n del dominio del par√°metro que tiene alta probabilidad de contenerlo. Se utiliza para resumir el posterior.\n\nUn intervalo de credibilidad es una regi√≥n \\(C\\) tal que la probabilidad de que contenga al par√°metro sea al menos \\(1 - \\alpha\\):\n\n\n\\[p(\\theta \\in C \\mid y) = \\int_C p(\\theta\\mid y) d\\theta = 1-\\alpha\\] en el caso discreto es (\\(\\geq 1-\\alpha\\))\n\n\nDecimos: la probabilidad de que \\(\\theta\\) est√© contenido en \\(C\\), dados los datos (y el modelo) es de \\(1-\\alpha\\)"
  },
  {
    "objectID": "presentaciones/presentacion_04.html#section-15",
    "href": "presentaciones/presentacion_04.html#section-15",
    "title": "Estad√≠stica Bayesiana",
    "section": "",
    "text": "En el an√°lisis de datos bayesiano, es habitual resumir los hallazgos reportando:\n\n\nUn gr√°fico de la distribuci√≥n a posteriori\nAlg√∫n medida de centralidad de la distribuci√≥n a posteriori\nPercentiles relevantes de la distribuci√≥n a posteriori\nProbabilidades a posteriori de inter√©s \\(p(\\theta&gt;c\\mid y)\\) para alg√∫n \\(c\\) interesante, por ejemplo \\(c=0\\)"
  },
  {
    "objectID": "presentaciones/presentacion_04.html#simulaciones",
    "href": "presentaciones/presentacion_04.html#simulaciones",
    "title": "Estad√≠stica Bayesiana",
    "section": "Simulaciones",
    "text": "Simulaciones\nPara interpretar los resultados de la inferencia bayesiana podemos simplemente realizar simulaciones a partir del posterior y estimar probabilidades contando.\n\n\nlos par√°metros\nfunciones de los par√°metros\nla variable respuesta (predicciones)"
  },
  {
    "objectID": "presentaciones/presentacion_04.html#section-16",
    "href": "presentaciones/presentacion_04.html#section-16",
    "title": "Estad√≠stica Bayesiana",
    "section": "",
    "text": "Un nuevo ejemplo del modelo beta‚ÄìBinomial: partimos de \\(\\mathrm{Beta}(2,2)\\), observamos \\(4\\) caras en \\(6\\) tiradas y nuestra creencia a posteriori pasa a ser \\(Beta(6,4)\\)."
  },
  {
    "objectID": "presentaciones/presentacion_04.html#section-17",
    "href": "presentaciones/presentacion_04.html#section-17",
    "title": "Estad√≠stica Bayesiana",
    "section": "",
    "text": "Par√°metros\n¬øCu√°l es la probabilidad a posteriori de que \\(\\pi\\) sea mayor a \\(0.50\\)? ¬øy de que sea mayor a ?\n\nmuestras_pi &lt;- rbeta(2000,6,4)\nmean(muestras_pi &gt; 0.5)\nmean(muestras_pi &gt; 0.8)"
  },
  {
    "objectID": "presentaciones/presentacion_04.html#section-18",
    "href": "presentaciones/presentacion_04.html#section-18",
    "title": "Estad√≠stica Bayesiana",
    "section": "",
    "text": "Funciones de los par√°metros\n¬øCu√°l es la distribuci√≥n a posteriori de la chance de obtener cara \\(\\frac{\\pi}{1-\\pi}\\)?\n\nmuestras_pi &lt;- rbeta(2000,6,4)\nmuestras_odds &lt;- muestras_pi/(1-muestras_pi)"
  },
  {
    "objectID": "presentaciones/presentacion_04.html#section-19",
    "href": "presentaciones/presentacion_04.html#section-19",
    "title": "Estad√≠stica Bayesiana",
    "section": "",
    "text": "Predicciones\nSi se arroja la moneda 11 veces m√°s ¬øcu√°l es la distribuci√≥n de probabilidad de la cantidad de caras? (es la distribuci√≥n predictiva a posteriori)\n\nmuestras_pi &lt;- rbeta(2000,6,4)\ny_new &lt;- rbinom(2000,11,muestras_pi)"
  },
  {
    "objectID": "presentaciones/presentacion_04.html#section-20",
    "href": "presentaciones/presentacion_04.html#section-20",
    "title": "Estad√≠stica Bayesiana",
    "section": "",
    "text": "Toda cantidad que dependa de los par√°metros tiene una distribuci√≥n a posteriori: una incertidumbre asociada.\n\n\n\n\nEstad√≠stica Bayesiana ‚Äì 2025"
  },
  {
    "objectID": "presentaciones/presentacion_07.html#introducci√≥n",
    "href": "presentaciones/presentacion_07.html#introducci√≥n",
    "title": "Estad√≠stica Bayesiana",
    "section": "Introducci√≥n",
    "text": "Introducci√≥n\n\nEn un problema de regresi√≥n, no siempre la respuesta (condicionada) es normal\nA veces, la respuesta ni siquiera es cuantitativa"
  },
  {
    "objectID": "presentaciones/presentacion_07.html#section",
    "href": "presentaciones/presentacion_07.html#section",
    "title": "Estad√≠stica Bayesiana",
    "section": "",
    "text": "Supongamos que nos interesa modelizar las siguientes variables:\n\nSi una persona vota o no por un determinado candidato\nSi un estudiante aprueba o no un examen\nSi ma√±ana llover√° o no\n\nEs decir, nos interesa modelizar una variable \\(Y\\), una variable respuesta categ√≥rica binaria:\n\\[Y = \\begin{cases}\n1 \\text{ si ma√±ana llueve} \\\\\n0 \\text{ en caso contrario}\n\\end{cases}\\]\nen funci√≥n de ciertas variables explicativas potenciales‚Ä¶\nEn otros contextos se habla de un problema de clasificaci√≥n"
  },
  {
    "objectID": "presentaciones/presentacion_07.html#section-1",
    "href": "presentaciones/presentacion_07.html#section-1",
    "title": "Estad√≠stica Bayesiana",
    "section": "",
    "text": "Seg√∫n los valores que puede tomar \\(Y\\), ¬øqu√© modelo de probabilidad podemos asumir?\n\\[Y_i \\mid \\pi_i \\sim \\mathrm{Bern}(\\pi_i)\\]\n\\[\\mathbb{E}(Y_i) = \\pi_i\\]"
  },
  {
    "objectID": "presentaciones/presentacion_07.html#section-2",
    "href": "presentaciones/presentacion_07.html#section-2",
    "title": "Estad√≠stica Bayesiana",
    "section": "",
    "text": "En la regresi√≥n normal que conoc√≠amos, ten√≠amos\n\\[Y_i \\mid \\mu_i \\sim \\mathcal{N}(\\mu_i,\\sigma^2)\\]\n\\[\\mathbb{E}(Y_i) = \\mu_i\\]\nPor analog√≠a, ¬øpodemos hacer \\(\\pi_i = \\beta_0 + \\beta_1 x_{1_i} + \\beta_2 x_{2_i} + \\dots\\)?\n¬øQu√© problemas identificamos?"
  },
  {
    "objectID": "presentaciones/presentacion_07.html#section-3",
    "href": "presentaciones/presentacion_07.html#section-3",
    "title": "Estad√≠stica Bayesiana",
    "section": "",
    "text": "Tendremos que hacer\n\\[g(\\pi_i) = \\beta_0 + \\beta_1 x_{1_i} + \\beta_2 x_{2_i} + \\dots\\]\n\\(g(\\cdot)\\) se conoce como funci√≥n de enlace (link function). ¬øCu√°l es una \\(g\\) apropiada en este caso?"
  },
  {
    "objectID": "presentaciones/presentacion_07.html#section-4",
    "href": "presentaciones/presentacion_07.html#section-4",
    "title": "Estad√≠stica Bayesiana",
    "section": "",
    "text": "Si \\(\\pi_i\\) es la probabilidad del evento de inter√©s, \\(\\frac{\\pi_i}{1-\\pi_i}\\) es la chance (odds) del evento de inter√©s.\nMientras que \\(\\pi_i \\in [0,1]\\), \\(\\frac{\\pi_i}{1-\\pi_i} \\in [0,+\\infty)\\)"
  },
  {
    "objectID": "presentaciones/presentacion_07.html#section-5",
    "href": "presentaciones/presentacion_07.html#section-5",
    "title": "Estad√≠stica Bayesiana",
    "section": "",
    "text": "Establecemos un modelo lineal para el log-odds del evento de inter√©s\n\\[\\log(\\mathrm{odds}_i)=\\log\\left( \\frac{\\pi_i}{1-\\pi_i} \\right) = \\beta_0 + \\beta_1 x_{1_i} + \\beta_2 x_{2_i} + \\dots\\]\nLa funci√≥n \\(g(x) = \\log\\left(\\frac{x}{1-x}\\right)\\) se conoce como funci√≥n logit. Es una funci√≥n no lineal."
  },
  {
    "objectID": "presentaciones/presentacion_07.html#section-6",
    "href": "presentaciones/presentacion_07.html#section-6",
    "title": "Estad√≠stica Bayesiana",
    "section": "",
    "text": "Analicemos lo que vimos hasta ahora:\n\n¬øCu√°l es el dominio de la funci√≥n logit?\n¬øPara qu√© necesitamos la funci√≥n \\(g(\\cdot)\\)?\n¬øCu√°l es la relaci√≥n entre el predictor lineal y la variable respuesta?\n¬øCu√°l es la distribuci√≥n de la variable respuesta?"
  },
  {
    "objectID": "presentaciones/presentacion_07.html#section-7",
    "href": "presentaciones/presentacion_07.html#section-7",
    "title": "Estad√≠stica Bayesiana",
    "section": "",
    "text": "Consideremos el caso con una sola variable explicativa\n\\[\\log\\left(\\frac{\\pi_i}{1-\\pi_i}\\right) = \\beta_0 + \\beta_1 x_{1_i}\\]\nSe cumple:\n\\[\n\\frac{\\pi_i}{1-\\pi_i} = e^{\\beta_0 + \\beta_1 x_{1_i}} \\qquad\n\\pi_i = \\frac{e^{\\beta_0 + \\beta_1 x_{1_i}}}{1 + e^{\\beta_0 + \\beta_1 x_{1_i}}}\n\\]\n¬°La esperanza de la variable respuesta se relaciona de manera no lineal con las variables explicativas!"
  },
  {
    "objectID": "presentaciones/presentacion_07.html#ejemplo",
    "href": "presentaciones/presentacion_07.html#ejemplo",
    "title": "Estad√≠stica Bayesiana",
    "section": "Ejemplo",
    "text": "Ejemplo\nConsideremos la siguiente relaci√≥n\n\\[\\log\\left(\\frac{\\pi_i}{1-\\pi_i}\\right) = -4 + 0.1\\ x_{1_i}\\]\ne imaginemos que \\(\\pi_i\\) es la probabilidad de que llueva el d√≠a \\(i\\) y \\(x_{1_i}\\) la humedad a las 9 de la ma√±ana del d√≠a anterior al d√≠a \\(i\\)."
  },
  {
    "objectID": "presentaciones/presentacion_07.html#section-8",
    "href": "presentaciones/presentacion_07.html#section-8",
    "title": "Estad√≠stica Bayesiana",
    "section": "",
    "text": "x1 &lt;- seq(0, 100, length.out = 100)\nbeta0 &lt;- -4\nbeta1 &lt;- 0.1\n\ndata &lt;- tibble(x1 = x1,\n               log_odds = beta0 + beta1*x1,\n               odds = exp(log_odds),\n               pi = odds/(1+odds))"
  },
  {
    "objectID": "presentaciones/presentacion_07.html#interpretaci√≥n-de-los-coeficientes",
    "href": "presentaciones/presentacion_07.html#interpretaci√≥n-de-los-coeficientes",
    "title": "Estad√≠stica Bayesiana",
    "section": "Interpretaci√≥n de los coeficientes",
    "text": "Interpretaci√≥n de los coeficientes\n\\(\\beta_0\\)\n\\(\\beta_0\\) es la log-chance (log-odds) del evento de inter√©s cuando todas las variables explicativas valen 0. \\(e^{\\beta_0}\\) es la chance. En t√©rminos del problema: \\(e^{\\beta_0}\\) es la chance de que llueva ma√±ana si la humedad de hoy a las 9 de la ma√±ana es 0."
  },
  {
    "objectID": "presentaciones/presentacion_07.html#section-10",
    "href": "presentaciones/presentacion_07.html#section-10",
    "title": "Estad√≠stica Bayesiana",
    "section": "",
    "text": "\\(\\beta_1\\)\n\\(\\beta_1\\) no es el incremento en la probabilidad del evento de inter√©s cuando \\(x_1\\) aumenta en una unidad‚Ä¶\n\n\\(\\mathrm{odds}_x\\) es la chance del evento de inter√©s cuando \\(x_1=x\\)\n\\(\\mathrm{odds}_{x+\\Delta x}\\) es la chance del evento de inter√©s cuando \\(x_1 = x + \\Delta x\\)\n\n\\[\\log (\\mathrm{odds}_x) =\\log\\left( \\frac{\\pi_x}{1-\\pi_x} \\right) = \\beta_0 + \\beta_1 x\\]\n\\[\\log (\\mathrm{odds}_{x+\\Delta x}) =\\log\\left( \\frac{\\pi_{x+\\Delta x}}{1-\\pi_{x+\\Delta x}} \\right) = \\beta_0 + \\beta_1 (x+\\Delta x)\\]"
  },
  {
    "objectID": "presentaciones/presentacion_07.html#section-11",
    "href": "presentaciones/presentacion_07.html#section-11",
    "title": "Estad√≠stica Bayesiana",
    "section": "",
    "text": "Entonces\n\\[\\log (\\mathrm{odds}_{x+\\Delta x}) - \\log (\\mathrm{odds}_{x}) = \\beta_1 \\Delta x\\]\n\\[ e^{\\beta_1 \\Delta x} = \\frac{\\mathrm{odds}_{x+\\Delta x}}{\\mathrm{odds}_{x}}\\]\nLa chance del evento de inter√©s aumenta \\(e^{\\beta_1 \\Delta x}\\) veces cuando \\(x_1\\) aumenta en \\(\\Delta x\\) (y el resto de las variables se mantienen constantes). En t√©rminos del problema: La chance de que llueva ma√±ana aumenta \\(e^{\\beta_1}\\) veces si la humedad aumenta en 1."
  },
  {
    "objectID": "presentaciones/presentacion_07.html#section-12",
    "href": "presentaciones/presentacion_07.html#section-12",
    "title": "Estad√≠stica Bayesiana",
    "section": "",
    "text": "M√°s en t√©rminos del problema:\n\\[e^{\\beta_1} = \\frac{\\mathrm{odds}_{x+1}}{\\mathrm{odds}_{x}} \\Rightarrow \\mathrm{odds}_{x+1} = e^{\\beta_1} \\mathrm{odds}_{x}\\]\n\\[e^{\\beta_1} = 1.11\\]\n\nLa chance de que llueva ma√±ana aumenta \\(1.11\\) veces cuando la humedad a las 9 de la ma√±ana de hoy aumenta en una unidad\nLa chance de que llueva ma√±ana aumenta en un \\(11\\%\\) cuando la humedad a las 9 de la ma√±ana de hoy aumenta en una unidad"
  },
  {
    "objectID": "presentaciones/presentacion_07.html#section-13",
    "href": "presentaciones/presentacion_07.html#section-13",
    "title": "Estad√≠stica Bayesiana",
    "section": "",
    "text": "Analicemos juntos el siguiente caso:\n\\[\\log\\left(\\frac{\\pi_i}{1-\\pi_i}\\right) = 1.1 - 0.2\\ \\mathrm{despierto}_{i}\\]\ne imaginemos que \\(\\pi_i\\) es la probabilidad de que un estudiante \\(i\\) apruebe el parcial de An√°lisis de Datos de Duraci√≥n \\(i\\) y \\(\\mathrm{despierto}_i\\) la cantidad de horas que el estudiante \\(i\\) estuvo despierto la noche anterior al parcial."
  },
  {
    "objectID": "presentaciones/presentacion_07.html#section-14",
    "href": "presentaciones/presentacion_07.html#section-14",
    "title": "Estad√≠stica Bayesiana",
    "section": "",
    "text": "¬øY Bayes?\nLa especificaci√≥n del modelo se completa con la elecci√≥n de distribuciones a priori para \\(\\beta_0,\\ \\beta_1,\\ \\dots\\)‚Ä¶\nCada cantidad que dependa de los \\(\\beta_0,\\ \\beta_1,\\ \\dots\\) tendr√° una distribuci√≥n de probabilidad.\nLas predicciones tambi√©n son probabil√≠sticas."
  },
  {
    "objectID": "presentaciones/presentacion_07.html#section-15",
    "href": "presentaciones/presentacion_07.html#section-15",
    "title": "Estad√≠stica Bayesiana",
    "section": "",
    "text": "¬øC√≥mo se estiman \\(\\beta_0,\\ \\beta_1,\\ \\dots\\)? Como siempre. Aplicando la Regla de Bayes. Solo que la verosimilitud ahora es Bernoulli.\n\n\n\n\nEstad√≠stica Bayesiana ‚Äì 2025"
  },
  {
    "objectID": "info/faq.html",
    "href": "info/faq.html",
    "title": "Preguntas frecuentes",
    "section": "",
    "text": "Preguntas frecuentes\n\nPertenezo al plan 2003 de la carrera, ¬øpuedo participar de las clases?\nSi, pod√©s asistir a las clases como oyente y tambi√©n realizar los trabajos pr√°cticos. Sin embargo, los profesores no se comprometen a realizar devoluciones o correcciones sobre los trabajos de estudiantes que asistan en calidad de oyente."
  },
  {
    "objectID": "info/enlaces_utiles.html",
    "href": "info/enlaces_utiles.html",
    "title": "Enlaces √∫tiles",
    "section": "",
    "text": "Awesome Bayesian Statistics. Es un listado de recursos en l√≠nea (y gratuitos!) relacionados al mundo de la Estad√≠stica Bayesiana."
  },
  {
    "objectID": "info/calendario.html",
    "href": "info/calendario.html",
    "title": "Calendario",
    "section": "",
    "text": "Warning in attr(x, \"align\"): 'xfun::attr()' is deprecated.\nUse 'xfun::attr2()' instead.\nSee help(\"Deprecated\")\n\n\nWarning in attr(x, \"format\"): 'xfun::attr()' is deprecated.\nUse 'xfun::attr2()' instead.\nSee help(\"Deprecated\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nSemana\nFecha\nUnidad\nTemas\nLectura sugerida\nOtras actividades\n\n\n\n\n1\n17 de marzo\n1, 2\n‚Ä¢ Presentaci√≥n de la materia‚Ä¢ Probabilidad‚Ä¢ Regla de Bayes‚Ä¢ Inferencia bayesiana‚Ä¢ Distribuci√≥n a priori, funci√≥n de verosimilitud y distribuci√≥n a posteriori‚Ä¢ Modelos conjugados\n‚Ä¢ McElreath (2020): Cap√≠tulos 1 y 2‚Ä¢ Kruschke (2014): Cap√≠tulos 1, 4, 5 y 6‚Ä¢ Johnson, Ott, y Dogucu (2022): Cap√≠tulos 1, 3, 4, 5 y 8\n\n\n\n2\n24 de marzo\n1, 2\n‚Ä¢ Modelos conjugados‚Ä¢ Modelos de varios par√°metros\n‚Ä¢ Johnson, Ott, y Dogucu (2022): Cap√≠tulos 1, 3, 4, 5 y 8‚Ä¢ Kruschke (2014): Cap√≠tulos 1, 5 y 6‚Ä¢ McElreath (2020): Cap√≠tulo 2‚Ä¢ Gelman et¬†al. (2013): Cap√≠tulo 3\n\n\n\n3\n31 de marzo\n2\n‚Ä¢ Nociones de Teoria de Decisi√≥n Bayesiana\n\nPresentaci√≥n TP1 (4-abr)\n\n\n4\n7 de abril\n2, 3\n‚Ä¢ Limitaciones del enfoque anal√≠tico‚Ä¢ Simulaciones‚Ä¢ Aproximaci√≥n mediante grilla‚Ä¢ Introducci√≥n al c√≥mputo bayesiano‚Ä¢ Markov-chain Montecarlo‚Ä¢ Metropolis-Hastings\n‚Ä¢ Johnson, Ott, y Dogucu (2022): Cap√≠tulos 6 y 7‚Ä¢ McElreath (2020): Cap√≠tulo 3‚Ä¢ Kruschke (2014): Cap√≠tulo 7\n\n\n\n5\n14 de abril\n3\n‚Ä¢ Metropolis-Hastings‚Ä¢ Diagn√≥sticos‚Ä¢ Hamiltonian Montecarlo\n‚Ä¢ Johnson, Ott, y Dogucu (2022): Cap√≠tulo 7‚Ä¢ Kruschke (2014): Cap√≠tulo 7\nEntrega TP1 (18-abr)\n\n\n6\n21 de abril\n3\n‚Ä¢ Programaci√≥n probabil√≠stica‚Ä¢ Stan‚Ä¢ Diagn√≥sticos‚Ä¢ Visualizaciones‚Ä¢ Hamiltonian Montecarlo‚Ä¢ Modelos lineales‚Ä¢ Paquetes brms y rstanarm\n‚Ä¢ Kruschke (2014): Cap√≠tulo 14‚Ä¢ Lambert (2018): Cap√≠tulo 15‚Ä¢ Johnson, Ott, y Dogucu (2022): Cap√≠tulo 9‚Ä¢ McElreath (2020): Cap√≠tulo 4‚Ä¢ Gelman y Hill (2006): Cap√≠tulos 3 y 4‚Ä¢ Gelman, Hill, y Vehtari (2021): Cap√≠tulos 6, 7 y 8\n\n\n\n7\n28 de abril\n3\n‚Ä¢ Modelos lineales‚Ä¢ Paquetes brms y rstanarm‚Ä¢ Trabajo TP2\n‚Ä¢ Johnson, Ott, y Dogucu (2022): Cap√≠tulo 9‚Ä¢ McElreath (2020): Cap√≠tulo 4‚Ä¢ Gelman y Hill (2006): Cap√≠tulos 3 y 4‚Ä¢ Gelman, Hill, y Vehtari (2021): Cap√≠tulos 6, 7 y 8\nPresentaci√≥n TP2 (30-abr)\n\n\n8\n5 de mayo\n4\n‚Ä¢ Validaci√≥n interna de modelos‚Ä¢ Pruebas predictivas a posteriori\n‚Ä¢ Gelman, Hill, y Vehtari (2021): Cap√≠tulo 11‚Ä¢ Johnson, Ott, y Dogucu (2022): Cap√≠tulo 10‚Ä¢ McElreath (2020): Cap√≠tulo 7\n\n\n\n9\n12 de mayo\n4\n‚Ä¢ Regularizaci√≥n‚Ä¢ Selecci√≥n de modelos‚Ä¢ Criterios de informaci√≥n‚Ä¢ Validaci√≥n cruzada‚Ä¢ Sobreajuste y subajuste‚Ä¢ Repaso\n‚Ä¢ Gelman, Hill, y Vehtari (2021): Cap√≠tulo 11‚Ä¢ Johnson, Ott, y Dogucu (2022): Cap√≠tulo 10‚Ä¢ McElreath (2020): Cap√≠tulo 7\nEntrega TP2 (14-may)\n\n\n10\n19 de mayo\n5\n‚Ä¢ Parcial‚Ä¢ Trabajo TP3\n\nParcial (21-may)  Presentaci√≥n TP3 (23-may)\n\n\n11\n26 de mayo\n5\n‚Ä¢ Regresi√≥n log√≠stica‚Ä¢ Regresi√≥n Poisson\n‚Ä¢ Gelman, Hill, y Vehtari (2021): Cap√≠tulos 13 y 15‚Ä¢ Gelman y Hill (2006): Cap√≠tulos 5 y 6‚Ä¢ Johnson, Ott, y Dogucu (2022): Cap√≠tulos 12 y 13\n\n\n\n12\n2 de junio\n5\n‚Ä¢ Enfoque multinivel‚Ä¢ Modelos jer√°rquicos‚Ä¢ Shrinkage de par√°metros\n‚Ä¢ Kruschke (2014): Cap√≠tulo 9‚Ä¢ Johnson, Ott, y Dogucu (2022): Cap√≠tulos 15 y 16‚Ä¢ McElreath (2020): Cap√≠tulo 13‚Ä¢ Gelman y Hill (2006): Cap√≠tulo 11\nRecuperatorio (2-jun)  Entrega TP3 (4-jun)  Presentaci√≥n TP Final (4-jun)\n\n\n13\n9 de junio\n5\n‚Ä¢ Modelos lineales jer√°rquicos‚Ä¢ Variaci√≥n en el intercepto‚Ä¢ Variaci√≥n en la pendiente‚Ä¢ Problemas de estimaci√≥n\n‚Ä¢ Johnson, Ott, y Dogucu (2022): Cap√≠tulo 17‚Ä¢ Gelman y Hill (2006): Cap√≠tulos 12 y 13\n\n\n\n14\n16 de junio\n\n‚Ä¢ Trabajo TP Final\n\n\n\n\n15\n23 de junio\n\n\n\nEntrega TP Final + Defensa oral\n\n\n\n\n\n\n\n\n\n\nReferencias\n\nGelman, Andrew, John B. Carlin, Hal S. Stern, David B. Dunson, Aki Vehtari, y Donald B. Rubin. 2013. Bayesian Data Analysis. 3rd edition. Chapman; Hall/CRC.\n\n\nGelman, Andrew, y Jennifer Hill. 2006. Data Analysis Using Regression and Multilevel-Hierarchical Models. 1st edition. Cambridge University Press.\n\n\nGelman, Andrew, Jennifer Hill, y Aki Vehtari. 2021. Regression and Other Stories. 1st edition. Cambridge University Press. https://users.aalto.fi/~ave/ROS.pdf.\n\n\nJohnson, Alicia A., Miles Q. Ott, y Mine Dogucu. 2022. Bayes Rules! An Introduction to Bayesian Modeling. 1st edition. Chapman; Hall/CRC. https://www.bayesrulesbook.com/.\n\n\nKruschke, John. 2014. Doing Bayesian Data Analysis: A Tutorial with R, JAGS, and Stan. 2nd edition. Academic Press.\n\n\nLambert, Ben. 2018. A Student‚Äôs Guide to Bayesian Statistics. 1st edition. SAGE Publications Ltd.\n\n\nMcElreath, Richard. 2020. Statistical Rethinking: A Bayesian Course with Examples in R and Stan. 2nd edition. Chapman; Hall/CRC.",
    "crumbs": [
      "Informaci√≥n",
      "Calendario"
    ]
  }
]