---
title: "TP2: Metropolis-Hastings [WIP]"
practica: "Trabajo Práctico 2"
---

```{r}
#| echo: false
#| include: false
library(dplyr)
library(ggplot2)
library(patchwork)

colores <- c(
    "#644296",
    "#F08533",
    "#3B78B0",
    "#D1352C"
)

semilla <- strsplit("metropolis hastings", "")[[1]] |>
    match(c(letters), nomatch = 0) |>
    sum()

set.seed(semilla)
```

# Metropolis-Hastings en 1D

El algoritmo de Metropolis-Hastings (MH) permite generar muestras (pseudo-)aleatorias a 
partir de una distribución de probabilidad $P$ que no necesariamente pertence a una 
familia de distribuciones conocida. El único requisito es que se pueda evaluar la función
de densidad (o de masa de probabilidad) $p^*(\theta)$ en cualquier valor de $\theta$,
incluso cuando $p^*(\theta)$ sea impropia (es decir, incluso aunque sea
desconocida la constante de normalización que hace que la integral en el soporte de la 
función sea igual a uno).

1.  Escriba una función que implemente el algoritmo de Metropolis-Hastings para tomar 
    muestras de una distribución de probabilidad unidimensional a partir de su función de 
    densidad. Separe en funciones cada uno de los pasos del procedimiento. 
    Otorgue flexibilidad al algoritmo permitiendo elegir entre un punto de inicio arbitrario
    o al azar y utilizar distribuciones de propuesta de transición arbitrarias 
    (por defecto, se utiliza una distribución normal estándar).

1.  Utilice la función implementada en el punto anterior para obtener muestras de una 
    distribución normal de parámetros $\mu=5$ y $\sigma=3$. 
    ¿Obtiene una representación fiel de la distribución objetivo?
    Compare cuantiles empíricos con cuantiles teóricos.

## Metropolis-Hastings en espacios paramétricos acotados

* La distribución normal es conveniente para proponer saltos.
* La distribución normal presenta desventajas cuando el espacio paramétrico es acotado.
  * Se realizarán propuestas que caen fuera del espacio parámetrico.
* Soluciones posibles:
  * Utilizar distribuciones cuyo espacio parámetrico sea acotado e igual al de la distribución objetivo.
    * Dificultad: No es sencillo encontrar para todas ellas parametrizaciones basadas en la media.
  * Utilizar transfomación de variables para muestrear en un espacio paramétrico no acotado, lo que permite utilizar siempre la distribución normal.
    * Dificultad: Se necesita la función de densidad objetivo en el nuevo espacio paramétrico, es decir, en el espacio de la variable transformada. Esto implica
    el cálculo del determinante del jacobiano de la transormación.
    * Ventaja: El procedimiento es sencillo para las transformaciones de variables
    más utilizadas. Además, este proceso puede ser automatizado mediante sistemas de diferenciación automática (simbólica) existentes.
  
### Transformación de variables aleatorias

Sean $X$ e $Y = g(X)$ variables aleatorias continuas, donde $g$ es una función uno a uno
y $X$ tiene función de densidad $f_X(x)$. Luego, la función de densidad de $Y$ es:
$$
f_Y(y) = f_X(g^{-1}(y)) \left\lvert \frac{d}{dy}g^{-1}(y) \right\rvert
$$

Por ejemplo, en el caso de la función $g(x) = \log(x)$, se tiene:
$$
\begin{aligned}
g : \mathbb{R}^+ \to \mathbb{R} & = \log(x) \\
g^{-1} : \mathbb{R} \to \mathbb{R}^+ &= \exp(x)
\end{aligned}
$$

y luego:
$$
f_Y(y) = f_X(\exp(y)) \exp(y)
$$

### Transformacion de v.a. Gamma

Sea $X \sim \text{Gamma}(\alpha=2, \beta=2)$ e $Y = \log(X)$. 
Se tiene que:
$$
f_Y = f_X(\exp(y)) \exp(y)
$$

donde $f_X$ es la función de densidad de una variable aleatoria 
$\text{Gamma}(\alpha=2, \beta=2)$.

La distribución de $Y = log(X)$ puede visualizarse mediante:

* La evaluación de la función de densidad $f_Y$ en una grilla de valores de $y$.
* La transformación de muestras aleatorias: se obtienen muestras de $X$ a las que luego
se les calcula el logaritmo.

```{r}
#| echo: false
#| fig-align: center
#| fig-width: 12
#| fig-height: 5
#| fig-cap: leyenda...

x_grid <- seq(0, 5.5, length.out = 200)
x_pdf <- dgamma(x_grid, shape = 2, rate = 2)

y_grid <- log(x_grid)
y_pdf <- dgamma(exp(y_grid), shape = 2, rate = 2) * exp(y_grid)

p1 <- data.frame(x = x_grid, densidad = x_pdf) |>
  ggplot() +
  geom_line(
    aes(x = x, y = densidad),
    color = "grey30",
    size = 1.5,
  ) +
  labs(x = "X", y = "Densidad") +
  lims(x = c(0, 5.5), y = c(0, 0.8)) +
  theme_bw()

p2 <- data.frame(x = y_grid, densidad = y_pdf) |>
  ggplot() +
  geom_line(
    aes(x = x, y = densidad),
    color = "grey30",
    size = 1.5,
  ) +
  labs(x = "Y = log(X)", y = "Densidad") +
  lims(y = c(0, 0.8)) +
  theme_bw()

p1 | p2
```

```{r}
#| echo: false
#| fig-align: center
#| fig-width: 12
#| fig-height: 5
#| fig-cap: otra leyenda...

x_rvs <- rgamma(5000, shape = 2, rate = 2)
y_rvs <- log(x_rvs)

p1_hist <- ggplot() +
  geom_histogram(
    aes(x = x, y = after_stat(density)),
    bins = 50,
    fill = colores[2],
    color = NA,
    alpha = 0.7,
    data = data.frame(x = x_rvs)
  ) + 
  geom_line(
    aes(x = x, y = densidad),
    color = "grey30",
    size = 1.5,
    data = data.frame(x = x_grid, densidad = x_pdf)
  ) +
  labs(x = "X", y = "Densidad") +
  lims(x = c(0, 5.5), y = c(0, 0.8)) +
  theme_bw()

p2_hist <- ggplot() +
  geom_histogram(
    aes(x = x, y = after_stat(density)),
    bins = 50,
    fill = colores[2],
    color = NA,
    alpha = 0.7,
    data = data.frame(x = y_rvs)
  ) + 
  geom_line(
    aes(x = x, y = densidad),
    color = "grey30",
    size = 1.5,
    data = data.frame(x = y_grid, densidad = y_pdf)
  ) +
  labs(x = "Y = log(X)", y = "Densidad") +
  lims(y = c(0, 0.8)) +
  theme_bw()

p1_hist | p2_hist
```

### Obtención de muestras de v.a. Gamma con Metropolis-Hastings 

1. Implementar la función de densidad de $Y = \log(X)$
1. Obtener muestras de $Y$.
1. Transformar muestras $x_i = \exp(y_i)$.

# Metropolis-Hastings en espacios paramétricos generales

Como veremos en esta sección del trabajo práctico, la verdadera utilidad del algoritmo de
Metropolis-Hastings se aprecia cuando se obtienen muestras de distribuciones en más de 
una dimensión, incluso cuando no se conoce la constante de normalización. 
Paradójicamente, los ejemplos trabajados a continuación también serán los que nos 
permitirán advertir sus limitaciones y motivarán la búsqueda de mejores alternativas.

* Extensión a espacios paramétricos multidimensionales.
* Utilización de transformaciones 
  * Ergo, siempre se muestrea en $\mathbb{R}^p$ y por lo tanto se pueden realizar
  propuestas con una normal multivariada.

## Metropolis-Hastings en escala logarítmica

* Posiblidad de subdesbordamiento (_underflow_)
* Utilización de escala logarítmica para estabilizar y robustecer los cálculos.
* Mostrar implementación multivariada en escala logaritmica, usando normal multivariada como propuesta.

## Precalentamiento

Supongamos el siguiente modelo:

$$
\begin{aligned}
Y_i & \sim \text{Normal}(\mu, \sigma^2) \\
\mu & \sim \text{Normal}(0, 1^2) \\
\sigma & \sim \text{Gamma}(\alpha=2, \beta=2) \\
\end{aligned}
$$

con $i = 1, \dots, N$. Se desea obtener el _posterior_ de $\boldsymbol{\theta} = \{\mu, \sigma\}$.

## Caso de aplicación

### Modelo normal

$$
\begin{aligned}
Y_{O, i} &\sim \text{Normal}(\mu_O, \sigma_O^2) \\
Y_{C, i} &\sim \text{Normal}(\mu_C, \sigma_C^2) \\
\mu_O, \mu_C &\sim \text{Normal}(0, 5^2) \\
\sigma_O, \sigma_C & \sim \text{Gamma}(\alpha=6, \beta=2)
\end{aligned}
$$


### Modelo robusto

$$
\begin{aligned}
Y_{O, i} &\sim \text{Student-T}(\mu_O, \sigma_O^2, \nu=3) \\
Y_{C, i} &\sim \text{Student-T}(\mu_C, \sigma_C^2, \nu=3) \\
\mu_O, \mu_C &\sim \text{Normal}(0, 5^2) \\
\sigma_O, \sigma_C & \sim \text{Gamma}(\alpha=6, \beta=2)
\end{aligned}
$$




<!--
# Apéndice (para mover a teoría)

::: {.callout-tip}
## Algoritmo de Metropolis Hastings

Se desea generar una muestra de valores $\{y^{(1)}, y^{(2)}, \cdots, y^{(n)} \}$ a partir
de una distribución de probabilidad $P$ con función de densidad $p$.

1. Seleccionar un punto inicial $y^{(1)}$.

1. Para cada $t\in \{1, \cdots, n\}$, repetir:

    i.  **Proponer un nuevo valor**
    
        Obtener un valor aleatorio $y'$ de una variable $Y'$ cuya distribución está
        dada por la distribución de propuesta $Q$ y el valor de la última muestra 
        obtenida:
    
        $$
        Y' \sim Q(y^{(t)})
        $$
    
    i.  **Calcular la probabilidad de aceptación**

        Calcular el cociente entre la función de densidad en el punto propuesto y en el
        punto actual. La probabilidad de aceptación es igual a este cociente si es menor 
        a 1, caso contrario es igual a 1.

        $$
        \alpha = \min \left\{ 1, \frac{p(y')}{p(y)} \right\}
        $$

    i.  **Seleccionar el nuevo valor**

        Generar un valor aleatorio $u$ de una distribución $\mathcal{U}(0, 1)$ y determinar
        $y^{(t + 1)}$ de la siguiente manera:

        $$
        y^{(t + 1)} = \left\{
        \begin{array}{ll}
        y' & \text{si} \quad u \le \alpha \\
        y^{(t)} & \text{si} \quad u > \alpha
        \end{array}\right.
        $$

**Notas**

El cálculo del cociente en la determinación de la probabilidad de aceptación es
en realidad:

$$
\frac{p(y')q(y^{(t)} \mid y')}{p(y)q(y' \mid y^{(t)})}
$$

donde $q$ es la función de densidad de la distribución de propuesta.

Esta se simplifica a la expresión utilizada en el algoritmo cuando $q$ es una función 
simétrica alrededor de su media.
:::


::: {.callout-tip}
## _Effective sample size_ (ESS)

El número efectivo de muestras $N_{eff}$ es el número de muestras independientes que tienen el mismo _poder de estimación_ que $S$ muestras correlacionadas.

Este valor puede aproximarse por:

$$N_{eff} = \frac{S}{1 + 2 \sum_{k=1}^\infty ACF(k)}$$

Notar que la suma infinita del denominador empieza en $k=1$ (y no en $k=0$, donde $ACF(0)=1$). Además, en la práctica, una regla para truncar la $ACF$ es hacerlo a partir del primer $k$ valor donde $ACF(k)<0.05$ [@Kruschke2014, p. 184].
:::

::: {.callout-tip}
## _Montecarlo standard error_ (MCSE)

Por el Teorema Central del Límite sabemos que, si $\bar{X}_N$ es el promedio de $N$ observaciones 
independientes e idénticamente distribuidas, entonces $\sqrt{N}(\bar{X}_N-\mu)$ converge en distribución 
a $\mathcal{N}(0,\sigma^2)$ cuando $N$ tiende a infinito, donde $\mu$ es la media de la distribución
de las $X_i$ y $\sigma$ es su desvío estándar. Si $\sigma$ se estima por $\hat{\sigma}$, el término 
$\frac{\sigma}{\sqrt{N}}$ se conoce como error estándar.

Cuando se realiza integración por Montecarlo y se estima $\mathbb{E}({X})$ con $N_{eff}$ muestras dependientes
que se comportan como $N$ muestras independientes, el error estándar se aproxima por:
$$
MCSE = \frac{\hat{\sigma}}{\sqrt{N_{eff}}}
$$
:::

-->
